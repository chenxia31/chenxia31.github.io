

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="chenlongxu">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文主要介绍EM算法，作为统计学习中可以通过迭代的方式估计含有隐变量模型的方法。同时给出两个典型的例子作为注释并给出相关代码的分析作为注解。 0x01 摘要（个人感悟）对于人工智能三大流派的分析，统计学习、神经网络和行为学习的一些新的感悟。首先统计学习依靠概率统计的知识建立起的模型和神经网络的范式并不完全一样，可能在模型、推理、优化等方面有名称的雷同但两者是完全不同的概念。同时以约束编程或者强化学">
<meta property="og:type" content="article">
<meta property="og:title" content="CS229 机器学习Vol13 ｜ EM 算法与 HMM 和 CRF">
<meta property="og:url" content="https://blog.tjdata.site/posts/8b4f9c0c.html">
<meta property="og:site_name" content="Chenlong&#39;s blog">
<meta property="og:description" content="本文主要介绍EM算法，作为统计学习中可以通过迭代的方式估计含有隐变量模型的方法。同时给出两个典型的例子作为注释并给出相关代码的分析作为注解。 0x01 摘要（个人感悟）对于人工智能三大流派的分析，统计学习、神经网络和行为学习的一些新的感悟。首先统计学习依靠概率统计的知识建立起的模型和神经网络的范式并不完全一样，可能在模型、推理、优化等方面有名称的雷同但两者是完全不同的概念。同时以约束编程或者强化学">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3af821a567b7471d91e5d90a8d3b9a3d_1440w.png">
<meta property="og:image" content="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-72b267e75679db40d6f187296881442e_1440w.jpg">
<meta property="article:published_time" content="2024-04-29T11:08:53.000Z">
<meta property="article:modified_time" content="2024-04-29T11:45:10.000Z">
<meta property="article:author" content="chenlongxu">
<meta property="article:tag" content="CS229">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="EM 算法">
<meta property="article:tag" content="HMM">
<meta property="article:tag" content="CRF">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3af821a567b7471d91e5d90a8d3b9a3d_1440w.png">
  
  
  
  <title>CS229 机器学习Vol13 ｜ EM 算法与 HMM 和 CRF - Chenlong&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.tjdata.site","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":20,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"Python"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"leancloud":{"app_id":"a3q5ohNSDjkVMwk3Blp6NunC-gzGzoHsz","app_key":"dFO07CA3WzWD6PlBNQwhQHuy","server_url":"https://a3q5ohns.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Chenlong&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CS229 机器学习Vol13 ｜ EM 算法与 HMM 和 CRF"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-04-29 19:08" pubdate>
          2024年4月29日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          30 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CS229 机器学习Vol13 ｜ EM 算法与 HMM 和 CRF</h1>
            
            
              <div class="markdown-body">
                
                <p>本文主要介绍EM算法，作为统计学习中可以通过迭代的方式估计含有隐变量模型的方法。同时给出两个典型的例子作为注释并给出相关代码的分析作为注解。</p>
<h3 id="0x01-摘要"><a href="#0x01-摘要" class="headerlink" title="0x01 摘要"></a>0x01 摘要</h3><p>（个人感悟）对于人工智能三大流派的分析，统计学习、神经网络和行为学习的一些新的感悟。首先统计学习依靠概率统计的知识建立起的模型和神经网络的范式并不完全一样，可能在模型、推理、优化等方面有名称的雷同但两者是完全不同的概念。同时以约束编程或者强化学习的代表的行为学习是处于这两者之上层的控制决策手段。</p>
<p>针对统计学习而言，其基本的任务分为参数估计和假设检验。这里的参数估计更多的是使用统计的方法来进行，最常见的MLE为极大似然估计（通过最大化likelihood function）来得到对应的估计。这次介绍的EM算法可以看作是一种对含隐变量的新估计方法。</p>
<h3 id="0x02-EM算法"><a href="#0x02-EM算法" class="headerlink" title="0x02 EM算法"></a>0x02 EM算法</h3><p>首先回顾一下，对于不含隐变量的方法我们会如何建模，{𝑥𝑖,𝑦𝑖}，最常见的是可以建立起判别式模型（Discriminate）比如逻辑回归、支持向量机、决策树等进行学习得到𝑃(𝑌|𝑋)。但是有的时候这样太直接了，我们可以从另外生成式的角度从每个y进行建模得到𝑎𝑟𝑔𝑚𝑎𝑥𝑦𝑃(𝑋|𝑌)𝑃(𝑌),来实现类似的效果。</p>
<p>但是加入对于没有标签的数据，我们通常会使用非监督学习(unsupervised learning)的方式来得到答案，比如Kmeans的方式。而EM算法是从另外一个角度来对其进行建模。其中心思想分为两步：</p>
<ol>
<li>随机初始化参数</li>
<li>E步 即然不知道隐藏状态，那么计算所有隐藏状态下的概率。也就是计算出不同隐藏状态下，对应的概率 𝑃(𝑋,𝑍;𝜃) </li>
<li>M步 优化参数，最大化似然值来得到答案</li>
</ol>
<p>假设坐标轴中分为两类点 {𝑥𝑖,𝑧𝑖},其中相同类型点的分布满足高斯分布，也就是𝑥|𝑧&#x3D;𝑗:𝑁(𝑢𝑗,Σ𝑗) </p>
<p>，同时假设点的隐藏变量为二项分布，则𝑧:𝐵(𝜙),由此我们可以写出：</p>
<p>𝑝(𝑥𝑖,𝑧𝑖)&#x3D;𝑝(𝑥𝑖|𝑧𝑖;𝑢,Σ)𝑝(𝑧𝑖,𝜙) </p>
<p>这样可以很轻松的写出含有隐变量的似然函数</p>
<p>𝑙(𝑢,Σ,𝜙)&#x3D;𝑙𝑜𝑔∏𝑖𝑝(𝑥𝑖,𝑧𝑖)&#x3D;Σ𝑖𝑙𝑜𝑔Σ𝑗𝑝(𝑥𝑖|𝑧𝑖&#x3D;𝑗;𝑢𝑗,Σ𝑗)𝑝(𝑧𝑖&#x3D;𝑗,𝜙) </p>
<p>如果我们知道隐变量取值,就可以使用GDA轻松求出上述的变量梯度，并开始下降。</p>
<p>但是我们不知道，因此我们可以先计算m个样本隐变量k中情况下的概率：</p>
<p>𝑄𝑖(𝑍𝑖)&#x3D;𝑝(𝑧𝑖|𝑥𝑖,𝜙,𝑢,Σ)∈𝑅(𝑚,𝑘) </p>
<p>之后我们通过琴生不等式的证明来优化上述含有隐变量的似然函数下界</p>
<p>𝑙(𝑢,Σ,𝜙)≥Σ𝑖Σ𝑗𝑄𝑖(𝑧&#x3D;𝑗)𝑙𝑜𝑔(𝑃(𝑥𝑖,𝑧&#x3D;𝑗;𝜃)𝑄𝑖(𝑧&#x3D;𝑗)) </p>
<p>因为这里的隐变量的分布是已知的，所以必然是可以被优化的。</p>
<p>𝑢^,Σ^,𝜙^&#x3D;𝑎𝑟𝑔𝑚𝑎𝑥(Σ𝑙𝑜𝑔(𝑃(𝑥𝑖,𝑧&#x3D;𝑗;𝜃)𝑄𝑖(𝑧&#x3D;𝑗)) </p>
<p>例如GMM的标准的求导之后的已经有人解析过：</p>
<p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3af821a567b7471d91e5d90a8d3b9a3d_1440w.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>GMM中的M步</p>
<p>由此不断的迭代求解得到答案，或者是解析解得到答案</p>
<p>到这一步其实发现EM算法并不是一个单纯的模型，而更多是一种解决手段，它是类似MLE一样单纯作为估计的方式，如何将其应用在不同的建模场景中可以得到不同的模型，比如应用在序列标注中的HMM，CRF。个人的直觉上觉得它很多的代表一种类似蒙特卡洛的思想，这里的猜测或者掷骰子是一种没有先验知识而去探索exploration的感觉，在GAN、知识蒸馏、强化学习、序列预测中感觉都会用到相关的概念。</p>
<h3 id="0x03-隐马尔可夫模型-HMM"><a href="#0x03-隐马尔可夫模型-HMM" class="headerlink" title="0x03 隐马尔可夫模型 HMM"></a>0x03 隐马尔可夫模型 HMM</h3><p>隐马尔可夫模型是用于标记问题的统计学习模型，比如说给照片流进行标记，同时在这类随机过程的研究中主要问题有：1. 如何计算概率  2. 如何根据观测值确定参数 3. 如何根据给定的观测值确定背后的状态</p>
<h3 id="3-1-HMM定义与模拟"><a href="#3-1-HMM定义与模拟" class="headerlink" title="3.1 HMM定义与模拟"></a>3.1 HMM定义与模拟</h3><p>隐马尔可夫模型是关于时序 的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各状态生成不同观测随机序列的过程，其中常见的参数和集合的描述如下：</p>
<p>状态序列 State&#x2F;Track : state sequence(which is latent)</p>
<p>观察序列 Observation : observation sequence</p>
<p>状态空间 Q: state spcae 1,..,N</p>
<p>观测空间 V: observation spae 1,…,M</p>
<p>I: state sequence</p>
<p>O: observation sequence</p>
<p>转移矩阵 A: transition matrix P(I|I) R(N,N)</p>
<p>状态观测矩阵B: observation matrix P(O|I) R(N,M)</p>
<p>初始概率分布 Pi: init state distribution Pi(I) R(1,N])</p>
<p>在这之中包含两个重要的基础假设（随机过程 is all you need）：</p>
<ol>
<li>齐次马尔可夫假设，假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也和时刻无关</li>
<li>观测独立性假设，假设任意时刻表的观测只依赖于该时刻的马尔可夫链的状态，与其他的观测和状态无关</li>
</ol>
<p>下面介绍一个例子，我们将 “Health”和“Fever“作为状态，以不同的表现形式”normal“，‘cold’，‘dizzy”作为观测，同时定义转移概率和转移矩阵等参数，注意这里给出的map形式后面用了函数调整，让解释性变差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># state space</span><br>Q = &#123;<span class="hljs-string">&#x27;Healthy&#x27;</span>,<span class="hljs-string">&#x27;Fever&#x27;</span>&#125;<br><br><span class="hljs-comment"># observation space</span><br>V = &#123;<span class="hljs-string">&#x27;normal&#x27;</span>,<span class="hljs-string">&#x27;cold&#x27;</span>,<span class="hljs-string">&#x27;dizzy&#x27;</span>&#125;<br><br><span class="hljs-comment"># init state distribution</span><br>Pi = &#123;<br>    <span class="hljs-string">&#x27;Healthy&#x27;</span>:<span class="hljs-number">0.6</span>,<br>    <span class="hljs-string">&#x27;Fever&#x27;</span>:<span class="hljs-number">0.4</span><br>&#125;<br><br><span class="hljs-comment"># state transition</span><br>A = &#123;<br>    <span class="hljs-string">&#x27;Healthy&#x27;</span>: &#123;<span class="hljs-string">&#x27;Healthy&#x27;</span>: <span class="hljs-number">0.7</span>, <span class="hljs-string">&#x27;Fever&#x27;</span>: <span class="hljs-number">0.3</span>&#125;,<br>    <span class="hljs-string">&#x27;Fever&#x27;</span>: &#123;<span class="hljs-string">&#x27;Healthy&#x27;</span>: <span class="hljs-number">0.4</span>, <span class="hljs-string">&#x27;Fever&#x27;</span>: <span class="hljs-number">0.6</span>&#125;,<br>&#125;<br><br><span class="hljs-comment"># observation transition</span><br>B = &#123;<br>    <span class="hljs-string">&#x27;Healthy&#x27;</span>: &#123;<span class="hljs-string">&#x27;normal&#x27;</span>: <span class="hljs-number">0.5</span>, <span class="hljs-string">&#x27;cold&#x27;</span>: <span class="hljs-number">0.4</span>, <span class="hljs-string">&#x27;dizzy&#x27;</span>: <span class="hljs-number">0.1</span>&#125;,<br>    <span class="hljs-string">&#x27;Fever&#x27;</span>: &#123;<span class="hljs-string">&#x27;normal&#x27;</span>: <span class="hljs-number">0.1</span>, <span class="hljs-string">&#x27;cold&#x27;</span>: <span class="hljs-number">0.3</span>, <span class="hljs-string">&#x27;dizzy&#x27;</span>: <span class="hljs-number">0.6</span>&#125;,<br>&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_index_map</span>(<span class="hljs-params">labels</span>):<br>    index2label = &#123;&#125;<br>    label2index = &#123;&#125;<br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels:<br>        index2label[i] = label<br>        label2index[label] = i<br>        i += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> index2label,label2index<br><br>Qlabel,Qindex = generate_index_map(Q)<br>Vlabel,Vindex = generate_index_map(V)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Qlabel,Qindex,Vlabel,Vindex:&#x27;</span>)<br><span class="hljs-built_in">print</span>(Qlabel,Qindex)<br><span class="hljs-built_in">print</span>(Vlabel,Vindex)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gen_Pi</span>(<span class="hljs-params">pi_dict,Qlabel</span>):<br>    v = np.zeros(<span class="hljs-built_in">len</span>(Qlabel),dtype=<span class="hljs-built_in">float</span>)<br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> pi_dict:<br>        v[Qlabel[key]] = pi_dict[key]<br>    <span class="hljs-keyword">return</span> v <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gen_matrix</span>(<span class="hljs-params">trans_dict,Qlabel,Vlabel</span>):<br>    res = np.zeros((<span class="hljs-built_in">len</span>(Qlabel),<span class="hljs-built_in">len</span>(Vlabel)),dtype=<span class="hljs-built_in">float</span>)<br>    <span class="hljs-keyword">for</span> si <span class="hljs-keyword">in</span> trans_dict:<br>        <span class="hljs-keyword">for</span> sj <span class="hljs-keyword">in</span> trans_dict[si]:<br>            res[Qlabel[si]][Vlabel[sj]] =  trans_dict[si][sj]<br>    <span class="hljs-keyword">return</span> res <br><br>A = gen_matrix(A,Qindex,Qindex)<br>B = gen_matrix(B,Qindex,Vindex)<br>pi = gen_Pi(Pi,Qindex)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state transition matrix&#x27;</span>)<br><span class="hljs-built_in">print</span>(A)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state-observation matrix&#x27;</span>)<br><span class="hljs-built_in">print</span>(B)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state prob distribution&#x27;</span>)<br><span class="hljs-built_in">print</span>(pi)<br></code></pre></td></tr></table></figure>

<p>之后我们来模拟生成</p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs inform7">def simulate(T,A,B,pi):<br>    def draw_from(state_prob_distribution):<br>        return np.where(np.random.multinomial(1,state_prob_distribution)==1)<span class="hljs-comment">[0]</span><span class="hljs-comment">[0]</span><br>    observations = np.zeros(T,dtype=int)<br>    states = np.zeros(T,dtype=int)<br>    states<span class="hljs-comment">[0]</span> = draw_from(pi)<br>    observations<span class="hljs-comment">[0]</span> = draw_from(B<span class="hljs-comment">[states<span class="hljs-comment">[0]</span>,:]</span>)<br>    for t in range(1, T):<br>        states<span class="hljs-comment">[t]</span> = draw_from(A<span class="hljs-comment">[states<span class="hljs-comment">[t-1]</span>,:]</span>)<br>        observations<span class="hljs-comment">[t]</span> = draw_from(B<span class="hljs-comment">[states<span class="hljs-comment">[t]</span>,:]</span>)<br>    return observations, states<br><br># simulate the data <br>O,I = simulate(10,A,B,pi)<br>print(<span class="hljs-comment">[Qlabel<span class="hljs-comment">[i]</span> for i in I]</span>)<br>print(<span class="hljs-comment">[Vlabel<span class="hljs-comment">[j]</span> for j in O]</span>)<br></code></pre></td></tr></table></figure>

<h3 id="3-2-HMM概率计算"><a href="#3-2-HMM概率计算" class="headerlink" title="3.2 HMM概率计算"></a>3.2 HMM概率计算</h3><p>如何计算一条轨迹的概率 其实是一件非常复杂的事情，这个看上去其实可以使用一个R(m^t)来描述，这里参考李航老师的书中给出前向概率的计算和后向概率,之后引申地给出和的计算方式，注意后者都是未来后面问题2用的</p>
<p>其实前向传播计算和后向传播计算的逻辑很简单，</p>
<p>在给定的观察轨迹情况下：指的是在时刻t的时候状态s&#x3D;q的概率，所有 R(K,T)</p>
<p>指的是在给定的观察轨迹情况下，时刻t到T给给定轨迹下，t时刻s&#x3D;q的概率，大小同样为R(K,T)</p>
<p>由此可以得到</p>
<p> 表示给定模型和观测值的情况下，时刻表t处于状态q的概率为：</p>
<p>表示给定模型和观察情况下在，在时刻表t处于状态i，在时刻表t+1处于状态j的概率</p>
<p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-72b267e75679db40d6f187296881442e_1440w.jpg" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>概率计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In the study of HMM, there are three main problems</span><br><span class="hljs-comment"># Track Probability: give the O=&#123;o1,o2,...,oT&#125;,P(O)</span><br><span class="hljs-comment"># Learning problem: give the O, estimate the parameters</span><br><span class="hljs-comment"># Forecasting problem: (decoding problem also),given the observation track, </span><br><span class="hljs-comment">#                       show the most likely state track</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">obs_seq,A,B,pi</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;cal the probability from the observation sequence with forward&#x27;&#x27;&#x27;</span> <br>    state_len = A.shape[<span class="hljs-number">0</span>]<br>    track_len = <span class="hljs-built_in">len</span>(obs_seq)<br>    F = np.zeros((state_len,track_len))<br><br>    <span class="hljs-comment"># init the state with the pi</span><br>    F[:,<span class="hljs-number">0</span>] = pi*B[:,obs_seq[<span class="hljs-number">0</span>]]<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,track_len):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(state_len):<br>            F[n,t] = np.dot(F[:,t-<span class="hljs-number">1</span>],(A[:,n]))*B[n,obs_seq[t]]<br>    <span class="hljs-keyword">return</span> F<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">obs_seq,A,B,pi</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;cal the probability from the observation sequence with backward&#x27;&#x27;&#x27;</span> <br>    N = A.shape[<span class="hljs-number">0</span>]<br>    T = <span class="hljs-built_in">len</span>(obs_seq)<br>    <span class="hljs-comment"># X保存后向概率矩阵</span><br>    X = np.zeros((N,T))<br>    X[:,-<span class="hljs-number">1</span>:] = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(T-<span class="hljs-number">1</span>)):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>            X[n,t] = np.<span class="hljs-built_in">sum</span>(X[:,t+<span class="hljs-number">1</span>] * A[n,:] * B[:, obs_seq[t+<span class="hljs-number">1</span>]])<br><br>    <span class="hljs-keyword">return</span> X<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;track probability forward&#x27;</span>)<br><span class="hljs-built_in">print</span>(forward(O,A,B,pi))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;track probability backward&#x27;</span>)<br><span class="hljs-built_in">print</span>(backward(O,A,B,pi))<br></code></pre></td></tr></table></figure>

<h3 id="3-3-HMM参数估计"><a href="#3-3-HMM参数估计" class="headerlink" title="3.3 HMM参数估计"></a>3.3 HMM参数估计</h3><p>这里使用学习方法，将EM用在HMM中得到Baum Welch算法</p>
<p>首先确定E步中的Q函数</p>
<p>&#x3D;  </p>
<p>之后在M步中更新参数，这里有人求解过了</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># Based on the EM algrithm, the HMM learning process can be solved by Baum-Weich algorithm<br>def baum_weich(obs_seq,A,B,pi,criterion=<span class="hljs-number">0.05</span>):<br>    &#x27;&#x27;&#x27;<br>    Unsupervised learning algorithm<br>    &#x27;&#x27;&#x27;<br>    n_states = A.shape[<span class="hljs-number">0</span>]<br>    n_samples = len(obs_seq)<br><br>    done = False<br><br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>        # Cal the <span class="hljs-built_in">track</span> probability<br>        # alpha(i) = p(obs_seq | q0=s_i,hmm)<br>        alpha = forward(obs_seq,A,B,pi)<br>        # <span class="hljs-built_in">beta</span>(i)=p(obs_seq | q0=s_i,hmm)<br>        <span class="hljs-built_in">beta</span> = backward(obs_seq,A,B,pi)<br><br>        # xi: <span class="hljs-keyword">in</span> the timestamp t, the t state <span class="hljs-built_in">is</span> i <span class="hljs-keyword">and</span> the t+<span class="hljs-number">1</span> state <span class="hljs-built_in">is</span> j<br>        xi = <span class="hljs-built_in">np</span>.zeros((n_states,n_states,n_samples-<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples-<span class="hljs-number">1</span>):<br>            <span class="hljs-built_in">denom</span> = <span class="hljs-built_in">np</span>.dot(<span class="hljs-built_in">np</span>.dot(alpha[:,t].T, A) * B[:,obs_seq[t+<span class="hljs-number">1</span>]].T, <span class="hljs-built_in">beta</span>[:,t+<span class="hljs-number">1</span>])<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_states):<br>                <span class="hljs-built_in">numer</span> = alpha[i,t] * A[i,:] * B[:,obs_seq[t+<span class="hljs-number">1</span>]].T * <span class="hljs-built_in">beta</span>[:,t+<span class="hljs-number">1</span>].T<br>                xi[i,:,t] = <span class="hljs-built_in">numer</span> / <span class="hljs-built_in">denom</span><br><br>        # <span class="hljs-built_in">gamma</span>, <span class="hljs-keyword">in</span> the timestamp t ,the t state <span class="hljs-built_in">is</span> i<br>        <span class="hljs-built_in">gamma</span> = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(xi,axis=<span class="hljs-number">1</span>)<br>        # prod <span class="hljs-keyword">for</span> what<br>        prod = (alpha[:,n_samples-<span class="hljs-number">1</span>]*<span class="hljs-built_in">beta</span>[:,n_samples-<span class="hljs-number">1</span>]).reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>        <span class="hljs-built_in">gamma</span> = <span class="hljs-built_in">np</span>.hstack((<span class="hljs-built_in">gamma</span>,prod/<span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(prod)))<br><br>        # M<br>        newpi = <span class="hljs-built_in">gamma</span>[:,<span class="hljs-number">0</span>]<br>        newA = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(xi,<span class="hljs-number">2</span>) / <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">gamma</span>[:,:-<span class="hljs-number">1</span>],axis=<span class="hljs-number">1</span>).reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>        newB = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">copy</span>(B)<br>        num_levels = B.shape[<span class="hljs-number">1</span>]<br>        sumgamma = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">gamma</span>,axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">for</span> lev <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_levels):<br>            mask = obs_seq == lev<br>            newB[:,lev] = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">gamma</span>[:,mask],axis=<span class="hljs-number">1</span>) / sumgamma<br>        # 检查是否满足阈值<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(<span class="hljs-built_in">abs</span>(pi - newpi)) &lt; criterion <span class="hljs-keyword">and</span> \<br>                        <span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(<span class="hljs-built_in">abs</span>(A - newA)) &lt; criterion <span class="hljs-keyword">and</span> \<br>                        <span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(<span class="hljs-built_in">abs</span>(B - newB)) &lt; criterion:<br>            done = <span class="hljs-number">1</span><br>        A[:], B[:], pi[:] = newA, newB, newpi<br>    <span class="hljs-built_in">return</span> newA, newB, newpi<br><br>A = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>],[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>]])<br>B = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>],[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>]])<br>pi = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([<span class="hljs-number">0.6</span>, <span class="hljs-number">0.4</span>])<br><br>observations_data, states_data = simulate(<span class="hljs-number">100</span>,A,B,pi)<br>newA, newB, newpi = baum_weich(observations_data, A, B, pi)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;newA: &quot;</span>, newA)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;newB: &quot;</span>, newB)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;newpi: &quot;</span>, newpi)<br></code></pre></td></tr></table></figure>

<h3 id="3-4-预测-维特比算法Viterbi-algorithm"><a href="#3-4-预测-维特比算法Viterbi-algorithm" class="headerlink" title="3.4 预测 维特比算法Viterbi algorithm"></a>3.4 预测 维特比算法Viterbi algorithm</h3><p>本质上是一个用动态规划求解最优路径的问题，很简单～只是和时刻表有点相似,这里不在详细的介绍。参考连接</p>
<p><a target="_blank" rel="noopener" href="https://applenob.github.io/machine_learning/HMM/">https://applenob.github.io/machine_learning/HMM/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">viterbi</span>(<span class="hljs-params">obs_seq, A, B, pi</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    V : numpy.ndarray</span><br><span class="hljs-string">        V [s][t] = Maximum probability of an observation sequence ending</span><br><span class="hljs-string">                   at time &#x27;t&#x27; with final state &#x27;s&#x27;</span><br><span class="hljs-string">    prev : numpy.ndarray</span><br><span class="hljs-string">        Contains a pointer to the previous state at t-1 that maximizes</span><br><span class="hljs-string">        V[state][t]</span><br><span class="hljs-string"></span><br><span class="hljs-string">    V对应δ，prev对应ψ</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    N = A.shape[<span class="hljs-number">0</span>]<br>    T = <span class="hljs-built_in">len</span>(obs_seq)<br>    prev = np.zeros((T - <span class="hljs-number">1</span>, N), dtype=<span class="hljs-built_in">int</span>)<br><br>    <span class="hljs-comment"># DP matrix containing max likelihood of state at a given time</span><br>    V = np.zeros((N, T))<br>    V[:,<span class="hljs-number">0</span>] = pi * B[:,obs_seq[<span class="hljs-number">0</span>]]<br><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, T):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>            seq_probs = V[:,t-<span class="hljs-number">1</span>] * A[:,n] * B[n, obs_seq[t]]<br>            prev[t-<span class="hljs-number">1</span>,n] = np.argmax(seq_probs)<br>            V[n,t] = np.<span class="hljs-built_in">max</span>(seq_probs)<br><br>    <span class="hljs-keyword">return</span> V, prev<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_viterbi_path</span>(<span class="hljs-params">prev, last_state</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Returns a state path ending in last_state in reverse order.</span><br><span class="hljs-string">    最优路径回溯</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    T = <span class="hljs-built_in">len</span>(prev)<br>    <span class="hljs-keyword">yield</span>(last_state)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">yield</span>(prev[i, last_state])<br>        last_state = prev[i, last_state]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">observation_prob</span>(<span class="hljs-params">obs_seq</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; P( entire observation sequence | A, B, pi ) &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(forward(obs_seq)[:,-<span class="hljs-number">1</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">state_path</span>(<span class="hljs-params">obs_seq, A, B, pi</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    V[last_state, -1] : float</span><br><span class="hljs-string">        Probability of the optimal state path</span><br><span class="hljs-string">    path : list(int)</span><br><span class="hljs-string">        Optimal state path for the observation sequence</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    V, prev = viterbi(obs_seq, A, B, pi)<br>    <span class="hljs-comment"># Build state path with greatest probability</span><br>    last_state = np.argmax(V[:,-<span class="hljs-number">1</span>])<br>    path = <span class="hljs-built_in">list</span>(build_viterbi_path(prev, last_state))<br><br>    <span class="hljs-keyword">return</span> V[last_state,-<span class="hljs-number">1</span>], <span class="hljs-built_in">reversed</span>(path)<br></code></pre></td></tr></table></figure>

<h3 id="0x04-条件随机场CRF"><a href="#0x04-条件随机场CRF" class="headerlink" title="0x04 条件随机场CRF"></a>0x04 条件随机场CRF</h3><p>在之前的分析中HMM中的马尔可夫假设非常强，但是很多时候标准存在序列相关的情况，因此常见的是需要参考前面多个时刻表的隐藏状态作为后续的分析，因此这种马尔可夫随机场的方式给出一种新的求解方。这里看理论过于深奥了，参考Torch官方文档中给出的BiLSTM-CRF的例子，可能会比较好理解一点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.autograd <span class="hljs-keyword">as</span> autograd<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn <br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br>torch.manual_seed(<span class="hljs-number">2023</span>)<br><br><span class="hljs-comment"># Define the helper functions to make the code more readable</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">argmax</span>(<span class="hljs-params">vector</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    return the argmax index </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    _, idx = torch.<span class="hljs-built_in">max</span>(vector,<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> idx.item()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_sequence</span>(<span class="hljs-params">seq,to_ix</span>):<br>    idxs = [to_ix[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq]<br>    <span class="hljs-keyword">return</span> torch.tensor(idxs,dtype=torch.long)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_sum_exp</span>(<span class="hljs-params">vector</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    return the log sum exp in a numerically stable way fot the forward algorithm </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    max_score = vector[<span class="hljs-number">0</span>,argmax(vector)]<br>    max_score_broadcast = max_score.view(<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>).expand(<span class="hljs-number">1</span>,vector.size()[<span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">return</span> max_score+torch.log(torch.<span class="hljs-built_in">sum</span>(torch.exp(vector-max_score_broadcast)))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BiLSTM_CRF</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, tag_to_ix, embedding_dim, hidden_dim</span>):<br>        <span class="hljs-built_in">super</span>(BiLSTM_CRF, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.embedding_dim = embedding_dim<br>        <span class="hljs-variable language_">self</span>.hidden_dim = hidden_dim<br>        <span class="hljs-variable language_">self</span>.vocab_size = vocab_size<br>        <span class="hljs-variable language_">self</span>.tag_to_ix = tag_to_ix<br>        <span class="hljs-variable language_">self</span>.tagset_size = <span class="hljs-built_in">len</span>(tag_to_ix)<br><br>        <span class="hljs-variable language_">self</span>.word_embeds = nn.Embedding(vocab_size, embedding_dim)<br>        <span class="hljs-variable language_">self</span>.lstm = nn.LSTM(embedding_dim, hidden_dim // <span class="hljs-number">2</span>,<br>                            num_layers=<span class="hljs-number">1</span>, bidirectional=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># Maps the output of the LSTM into tag space.</span><br>        <span class="hljs-variable language_">self</span>.hidden2tag = nn.Linear(hidden_dim, <span class="hljs-variable language_">self</span>.tagset_size)<br><br>        <span class="hljs-comment"># Matrix of transition parameters.  Entry i,j is the score of</span><br>        <span class="hljs-comment"># transitioning *to* i *from* j.</span><br>        <span class="hljs-variable language_">self</span>.transitions = nn.Parameter(<br>            torch.randn(<span class="hljs-variable language_">self</span>.tagset_size, <span class="hljs-variable language_">self</span>.tagset_size))<br><br>        <span class="hljs-comment"># These two statements enforce the constraint that we never transfer</span><br>        <span class="hljs-comment"># to the start tag and we never transfer from the stop tag</span><br>        <span class="hljs-variable language_">self</span>.transitions.data[tag_to_ix[START_TAG], :] = -<span class="hljs-number">10000</span><br>        <span class="hljs-variable language_">self</span>.transitions.data[:, tag_to_ix[STOP_TAG]] = -<span class="hljs-number">10000</span><br><br>        <span class="hljs-variable language_">self</span>.hidden = <span class="hljs-variable language_">self</span>.init_hidden()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_hidden</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> (torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.hidden_dim // <span class="hljs-number">2</span>),<br>                torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.hidden_dim // <span class="hljs-number">2</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_alg</span>(<span class="hljs-params">self, feats</span>):<br>        <span class="hljs-comment"># Do the forward algorithm to compute the partition function</span><br>        init_alphas = torch.full((<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.tagset_size), -<span class="hljs-number">10000.</span>)<br>        <span class="hljs-comment"># START_TAG has all of the score.</span><br>        init_alphas[<span class="hljs-number">0</span>][<span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]] = <span class="hljs-number">0.</span><br><br>        <span class="hljs-comment"># Wrap in a variable so that we will get automatic backprop</span><br>        forward_var = init_alphas<br><br>        <span class="hljs-comment"># Iterate through the sentence</span><br>        <span class="hljs-keyword">for</span> feat <span class="hljs-keyword">in</span> feats:<br>            alphas_t = []  <span class="hljs-comment"># The forward tensors at this timestep</span><br>            <span class="hljs-keyword">for</span> next_tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.tagset_size):<br>                <span class="hljs-comment"># broadcast the emission score: it is the same regardless of</span><br>                <span class="hljs-comment"># the previous tag</span><br>                emit_score = feat[next_tag].view(<br>                    <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).expand(<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.tagset_size)<br>                <span class="hljs-comment"># the ith entry of trans_score is the score of transitioning to</span><br>                <span class="hljs-comment"># next_tag from i</span><br>                trans_score = <span class="hljs-variable language_">self</span>.transitions[next_tag].view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>                <span class="hljs-comment"># The ith entry of next_tag_var is the value for the</span><br>                <span class="hljs-comment"># edge (i -&gt; next_tag) before we do log-sum-exp</span><br>                next_tag_var = forward_var + trans_score + emit_score<br>                <span class="hljs-comment"># The forward variable for this tag is log-sum-exp of all the</span><br>                <span class="hljs-comment"># scores.</span><br>                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="hljs-number">1</span>))<br>            forward_var = torch.cat(alphas_t).view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>        terminal_var = forward_var + <span class="hljs-variable language_">self</span>.transitions[<span class="hljs-variable language_">self</span>.tag_to_ix[STOP_TAG]]<br>        alpha = log_sum_exp(terminal_var)<br>        <span class="hljs-keyword">return</span> alpha<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_lstm_features</span>(<span class="hljs-params">self, sentence</span>):<br>        <span class="hljs-variable language_">self</span>.hidden = <span class="hljs-variable language_">self</span>.init_hidden()<br>        embeds = <span class="hljs-variable language_">self</span>.word_embeds(sentence).view(<span class="hljs-built_in">len</span>(sentence), <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>        lstm_out, <span class="hljs-variable language_">self</span>.hidden = <span class="hljs-variable language_">self</span>.lstm(embeds, <span class="hljs-variable language_">self</span>.hidden)<br>        lstm_out = lstm_out.view(<span class="hljs-built_in">len</span>(sentence), <span class="hljs-variable language_">self</span>.hidden_dim)<br>        lstm_feats = <span class="hljs-variable language_">self</span>.hidden2tag(lstm_out)<br>        <span class="hljs-keyword">return</span> lstm_feats<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_score_sentence</span>(<span class="hljs-params">self, feats, tags</span>):<br>        <span class="hljs-comment"># Gives the score of a provided tag sequence</span><br>        score = torch.zeros(<span class="hljs-number">1</span>)<br>        tags = torch.cat([torch.tensor([<span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]], dtype=torch.long), tags])<br>        <span class="hljs-keyword">for</span> i, feat <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(feats):<br>            score = score + \<br>                <span class="hljs-variable language_">self</span>.transitions[tags[i + <span class="hljs-number">1</span>], tags[i]] + feat[tags[i + <span class="hljs-number">1</span>]]<br>        score = score + <span class="hljs-variable language_">self</span>.transitions[<span class="hljs-variable language_">self</span>.tag_to_ix[STOP_TAG], tags[-<span class="hljs-number">1</span>]]<br>        <span class="hljs-keyword">return</span> score<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_viterbi_decode</span>(<span class="hljs-params">self, feats</span>):<br>        backpointers = []<br><br>        <span class="hljs-comment"># Initialize the viterbi variables in log space</span><br>        init_vvars = torch.full((<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.tagset_size), -<span class="hljs-number">10000.</span>)<br>        init_vvars[<span class="hljs-number">0</span>][<span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># forward_var at step i holds the viterbi variables for step i-1</span><br>        forward_var = init_vvars<br>        <span class="hljs-keyword">for</span> feat <span class="hljs-keyword">in</span> feats:<br>            bptrs_t = []  <span class="hljs-comment"># holds the backpointers for this step</span><br>            viterbivars_t = []  <span class="hljs-comment"># holds the viterbi variables for this step</span><br><br>            <span class="hljs-keyword">for</span> next_tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.tagset_size):<br>                <span class="hljs-comment"># next_tag_var[i] holds the viterbi variable for tag i at the</span><br>                <span class="hljs-comment"># previous step, plus the score of transitioning</span><br>                <span class="hljs-comment"># from tag i to next_tag.</span><br>                <span class="hljs-comment"># We don&#x27;t include the emission scores here because the max</span><br>                <span class="hljs-comment"># does not depend on them (we add them in below)</span><br>                next_tag_var = forward_var + <span class="hljs-variable language_">self</span>.transitions[next_tag]<br>                best_tag_id = argmax(next_tag_var)<br>                bptrs_t.append(best_tag_id)<br>                viterbivars_t.append(next_tag_var[<span class="hljs-number">0</span>][best_tag_id].view(<span class="hljs-number">1</span>))<br>            <span class="hljs-comment"># Now add in the emission scores, and assign forward_var to the set</span><br>            <span class="hljs-comment"># of viterbi variables we just computed</span><br>            forward_var = (torch.cat(viterbivars_t) + feat).view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>            backpointers.append(bptrs_t)<br><br>        <span class="hljs-comment"># Transition to STOP_TAG</span><br>        terminal_var = forward_var + <span class="hljs-variable language_">self</span>.transitions[<span class="hljs-variable language_">self</span>.tag_to_ix[STOP_TAG]]<br>        best_tag_id = argmax(terminal_var)<br>        path_score = terminal_var[<span class="hljs-number">0</span>][best_tag_id]<br><br>        <span class="hljs-comment"># Follow the back pointers to decode the best path.</span><br>        best_path = [best_tag_id]<br>        <span class="hljs-keyword">for</span> bptrs_t <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(backpointers):<br>            best_tag_id = bptrs_t[best_tag_id]<br>            best_path.append(best_tag_id)<br>        <span class="hljs-comment"># Pop off the start tag (we dont want to return that to the caller)</span><br>        start = best_path.pop()<br>        <span class="hljs-keyword">assert</span> start == <span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]  <span class="hljs-comment"># Sanity check</span><br>        best_path.reverse()<br>        <span class="hljs-keyword">return</span> path_score, best_path<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">neg_log_likelihood</span>(<span class="hljs-params">self, sentence, tags</span>):<br>        <span class="hljs-comment"># Input the sentence with the dirsed tags</span><br>        feats = <span class="hljs-variable language_">self</span>._get_lstm_features(sentence)<br>        <span class="hljs-comment"># Cancaluate the feature from the lsmt layers</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;FEAT&#x27;</span>,feats)<br>        forward_score = <span class="hljs-variable language_">self</span>._forward_alg(feats)<br>        <span class="hljs-comment"># Calculante the forward scoress, based on the feats</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;FORWARD_SCORE&#x27;</span>,forward_score)<br>        gold_score = <span class="hljs-variable language_">self</span>._score_sentence(feats, tags)<br>        <span class="hljs-comment"># calculate the score from the sentence</span><br>        <span class="hljs-keyword">return</span> forward_score - gold_score<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sentence</span>):  <span class="hljs-comment"># dont confuse this with _forward_alg above.</span><br>        <span class="hljs-comment"># Get the emission scores from the BiLSTM</span><br>        lstm_feats = <span class="hljs-variable language_">self</span>._get_lstm_features(sentence)<br><br>        <span class="hljs-comment"># Find the best path, given the features.</span><br>        score, tag_seq = <span class="hljs-variable language_">self</span>._viterbi_decode(lstm_feats)<br>        <span class="hljs-keyword">return</span> score, tag_seq<br><br><span class="hljs-comment"># Training process</span><br>START_TAG = <span class="hljs-string">&#x27;&lt;START&gt;&#x27;</span><br>STOP_TAG = <span class="hljs-string">&#x27;&lt;STOP&gt;&#x27;</span><br>EMBEDDING_DIM = <span class="hljs-number">5</span><br>HIDDEN_DEM = <span class="hljs-number">4</span><br><br><span class="hljs-comment"># Makding up the training data</span><br>training_data = [<br>    ( <span class="hljs-string">&quot;the wall street journal reported today that apple corporation made money&quot;</span>.split(),<br>    <span class="hljs-string">&quot;B I I I O O O B I O O&quot;</span>.split()),<br>    ( <span class="hljs-string">&quot;georgia tech is a university in georgia&quot;</span>.split(),<br>    <span class="hljs-string">&quot;B I O O O O B&quot;</span>.split() )<br>]<br>word_to_ix = &#123;&#125;<br><span class="hljs-keyword">for</span> sentence,tags <span class="hljs-keyword">in</span> training_data:<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence:<br>        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> word_to_ix:<br>            word_to_ix[word] = <span class="hljs-built_in">len</span>(word_to_ix)<br><br>tag_to_ix = &#123;<span class="hljs-string">&quot;B&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;I&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;O&quot;</span>: <span class="hljs-number">2</span>, START_TAG: <span class="hljs-number">3</span>, STOP_TAG: <span class="hljs-number">4</span>&#125;<br>model = BiLSTM_CRF(<span class="hljs-built_in">len</span>(word_to_ix),tag_to_ix,EMBEDDING_DIM,HIDDEN_DEM)<br>optimizer = optim.SGD(model.parameters(),lr=<span class="hljs-number">0.01</span>,weight_decay=<span class="hljs-number">1e-4</span>)<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    precheck_sent = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],word_to_ix)<br>    precheck_tags = torch.tensor([tag_to_ix[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> training_data[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]],dtype=torch.long)<br><br>    <span class="hljs-built_in">print</span>(model.neg_log_likelihood(precheck_sent,precheck_tags))<br><br><span class="hljs-comment"># Make sure prepare_sequence from earlier in the LSTM section is loaded</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">300</span>):  <span class="hljs-comment"># again, normally you would NOT do 300 epochs, it is toy data</span><br>    <span class="hljs-keyword">for</span> sentence, tags <span class="hljs-keyword">in</span> training_data:<br>        <span class="hljs-comment"># Step 1. Remember that Pytorch accumulates gradients.</span><br>        <span class="hljs-comment"># We need to clear them out before each instance</span><br>        model.zero_grad()<br><br>        <span class="hljs-comment"># Step 2. Get our inputs ready for the network, that is,</span><br>        <span class="hljs-comment"># turn them into Tensors of word indices.</span><br>        sentence_in = prepare_sequence(sentence, word_to_ix)<br>        targets = torch.tensor([tag_to_ix[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tags], dtype=torch.long)<br><br>        <span class="hljs-comment"># Step 3. Run our forward pass.</span><br>        loss = model.neg_log_likelihood(sentence_in, targets)<br><br>        <span class="hljs-comment"># Step 4. Compute the loss, gradients, and update the parameters by</span><br>        <span class="hljs-comment"># calling optimizer.step()</span><br>        loss.backward()<br>        optimizer.step()<br><br><span class="hljs-comment"># Check predictions after training</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    precheck_sent = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], word_to_ix)<br>    <span class="hljs-built_in">print</span>(model(precheck_sent))<br></code></pre></td></tr></table></figure>

<h3 id="0x05-总结"><a href="#0x05-总结" class="headerlink" title="0x05 总结"></a>0x05 总结</h3><p>还是没有看懂EM算法，还在理论推导和代码实现之前做权衡</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/" class="category-chain-item">算法基础</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/CS229/" class="print-no-link">#CS229</a>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a>
      
        <a href="/tags/EM-%E7%AE%97%E6%B3%95/" class="print-no-link">#EM 算法</a>
      
        <a href="/tags/HMM/" class="print-no-link">#HMM</a>
      
        <a href="/tags/CRF/" class="print-no-link">#CRF</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CS229 机器学习Vol13 ｜ EM 算法与 HMM 和 CRF</div>
      <div>https://blog.tjdata.site/posts/8b4f9c0c.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>chenlongxu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年4月29日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/c61c1ac7.html" title="工科生研 1 使用MacBook Pro14 深度感受">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">工科生研 1 使用MacBook Pro14 深度感受</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/f2248357.html" title="Pytorch 官方文档阅读和实现">
                        <span class="hidden-mobile">Pytorch 官方文档阅读和实现</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"a3q5ohNSDjkVMwk3Blp6NunC-gzGzoHsz","appKey":"dFO07CA3WzWD6PlBNQwhQHuy","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
