<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chenxia&#39;s blog</title>
  
  
  <link href="https://blog.tjdata.site/atom.xml" rel="self"/>
  
  <link href="https://blog.tjdata.site/"/>
  <updated>2023-08-29T12:06:07.373Z</updated>
  <id>https://blog.tjdata.site/</id>
  
  <author>
    <name>chenxia</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python学习_matplotlib指南</title>
    <link href="https://blog.tjdata.site/posts/5fd26a11.html"/>
    <id>https://blog.tjdata.site/posts/5fd26a11.html</id>
    <published>2023-08-29T12:03:02.000Z</published>
    <updated>2023-08-29T12:06:07.373Z</updated>
    
    <content type="html"><![CDATA[<p>阅读Python的matplotlib的</p><span id="more"></span><h1 id="python-绘图教程"><a class="markdownIt-Anchor" href="#python-绘图教程"></a> Python 绘图教程</h1><h2 id="0x00-basic-setting"><a class="markdownIt-Anchor" href="#0x00-basic-setting"></a> 0x00 Basic setting</h2><blockquote><p>What is matplotlib?<br />Matplotlib is a comprehensive library for creating static, animated, interactive visualization in Python. We can as follows:</p></blockquote><ol><li>创建高质量的图片</li><li>创建可以交互的、放大、拖动、更新的图片</li><li>定制化风格和尺寸</li><li>导出为不同的格式</li><li>在Jupyter notebook或其他应用中使用</li><li>丰富的第三方库</li></ol><blockquote></blockquote><p>在使用中图片可以按照A4比例来进行设置，同时设置对应的字体和颜色</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">mm = 1/25.4<br><span class="hljs-comment"># plt.figure(figsize=(10*mm,20*mm)</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>]= <span class="hljs-string">&quot;Times New Roman&quot;</span><br>mpl.rcParams[<span class="hljs-string">&#x27;xtick.labelsize&#x27;</span>] = 10<br>mpl.rcParams[<span class="hljs-string">&#x27;ytick.labelsize&#x27;</span>] = 10<br>plt.rcParams[<span class="hljs-string">&#x27;mathtext.fontset&#x27;</span>]=<span class="hljs-string">&#x27;cm&#x27;</span><br>plt.style.use(<span class="hljs-string">&#x27;tableau-colorblind10&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="0x01-intro-to-matplotlib-pyplot"><a class="markdownIt-Anchor" href="#0x01-intro-to-matplotlib-pyplot"></a> 0x01 Intro to Matplotlib pyplot</h2><h3 id="11-两种绘图方式"><a class="markdownIt-Anchor" href="#11-两种绘图方式"></a> 1.1 两种绘图方式</h3><ol><li>面向对象（Objetc- oriented style）的绘图方式将图片的fig和axis区分开来，使用plt.subplots( )来创建figure对象</li><li>函数编程，直接使用封装好的plt.figure() plt.plot()来创建figure和绘图对象</li></ol><h3 id="12-图片对象包括哪些属性"><a class="markdownIt-Anchor" href="#12-图片对象包括哪些属性"></a> 1.2 图片对象包括哪些属性</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled.png" alt="Untitled" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%201.png" alt="Untitled" /></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs bash">fig,ax = plt.subplots(figsize=(150*mm,100*mm))<br><br>ax.set_title(<span class="hljs-string">&#x27;The title stays&#x27;</span>,fontsize=10)<br>ax.set_xlabel(<span class="hljs-string">&#x27;xlabel stays&#x27;</span>,fontsize=10)<br>ax.set_ylabel(<span class="hljs-string">&#x27;ylabel stays&#x27;</span>,fontsize=10)<br>ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(<span class="hljs-string">&#x27;%.1f&#x27;</span>))<br>ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(<span class="hljs-string">&#x27;%.1f&#x27;</span>))<br>ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))<br>ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))<br>ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.5))<br>ax.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.5))<br>ax.set_xscale(<span class="hljs-string">&#x27;linear&#x27;</span>)<br>ax.set_yscale(<span class="hljs-string">&#x27;linear&#x27;</span>)<br>ax.set_xticks(np.linspace(0,10,11))<br>ax.set_yticks(np.linspace(0,10,11))<br><br>ax.plot(np.linspace(0,10,100),np.linspace(0,10,100),label=<span class="hljs-string">&#x27;1:1&#x27;</span>,color=<span class="hljs-string">&#x27;k&#x27;</span>,linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br>ax.scatter(np.linspace(0,10,20),np.sin(np.linspace(0,10,20)+1),label=<span class="hljs-string">&#x27;1:2&#x27;</span>,color=<span class="hljs-string">&#x27;g&#x27;</span>,marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>ax.legend(loc=<span class="hljs-string">&#x27;upper left&#x27;</span>,fontsize=10,frameon=False)<br>ax.grid(<span class="hljs-string">&#x27;major&#x27;</span>,color=<span class="hljs-string">&#x27;k&#x27;</span>,linestyle=<span class="hljs-string">&#x27;--&#x27;</span>,linewidth=0.1)<br>ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;None&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;None&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,1)) <span class="hljs-comment">#将x轴移到y=0处</span><br>ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,1)) <span class="hljs-comment">#将x轴移到x=0处</span><br>ax.text(6,9,<span class="hljs-string">&quot;function:y=x&quot;</span>,size=15,color=<span class="hljs-string">&#x27;r&#x27;</span>,style=<span class="hljs-string">&#x27;italic&#x27;</span>,weight=<span class="hljs-string">&#x27;light&#x27;</span>,bbox=dict(facecolor=<span class="hljs-string">&#x27;w&#x27;</span>,alpha=0.5))<br><span class="hljs-keyword">for</span> coord <span class="hljs-keyword">in</span> np.linspace(3,7,10):<br>    ax.text(coord,coord+1,<span class="hljs-string">&#x27;%.0f&#x27;</span>%coord,ha=<span class="hljs-string">&#x27;center&#x27;</span>,va=<span class="hljs-string">&#x27;bottom&#x27;</span>)<br><br>ax.annotate(<span class="hljs-string">&#x27;annotate&#x27;</span>,xy=(9,9),xytext=(7,6),arrowprops=dict(arrowstyle=<span class="hljs-string">&#x27;-&gt;&#x27;</span>,connectionstyle=<span class="hljs-string">&#x27;arc3,rad=0.2&#x27;</span>))<br><br>ax2 = ax.twinx()<br>ax2.set_ylabel(<span class="hljs-string">&#x27;ylabel stays&#x27;</span>,fontsize=10)<br>ax2.set_yscale(<span class="hljs-string">&#x27;linear&#x27;</span>)<br>ax2.set_yticks(np.linspace(0,10,11))<br>ax2.set_ylim(0,10)<br>X,Y = np.meshgrid(np.linspace(0,10,11),np.linspace(0,10,11))<br>Z = np.sin(X)+np.cos(Y)<br>pc = ax2.pcolormesh(X,Y,Z,cmap=<span class="hljs-string">&#x27;RdBu_r&#x27;</span>,vmin=-1, vmax=1,alpha=0.3)<br>fig.colorbar(pc,ax=ax2,shrink=0.8,pad=0.1)<br>ax2.set_ylabel(<span class="hljs-string">&#x27;colorbar&#x27;</span>,fontsize=10)<br></code></pre></td></tr></table></figure><h3 id="13-pyplot-设置"><a class="markdownIt-Anchor" href="#13-pyplot-设置"></a> 1.3 Pyplot 设置</h3><p>上面的图形选项很多，从学习的角度可以全而广的了解，从实际使用的角度可能只会用到其中的几个。</p><p>color：pink。lightblue；</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">ax.set_prop_cycle(color=[&#x27;#1f77b4&#x27;, &#x27;#aec7e8&#x27;, &#x27;#ff7f0e&#x27;, &#x27;#ffbb78&#x27;, &#x27;#2ca02c&#x27;, &#x27;#98df8a&#x27;,&#x27;#d<span class="hljs-number">6272</span>8&#x27;, &#x27;#ff<span class="hljs-number">9896</span>&#x27;, &#x27;#<span class="hljs-number">9467</span>bd&#x27;, &#x27;#c5b0d5&#x27;, &#x27;#8c564b&#x27;, &#x27;#c49c94&#x27;,&#x27;#e377c2&#x27;, &#x27;#f7b6d2&#x27;, &#x27;#7f7f7f&#x27;, &#x27;#c7c7c7&#x27;, &#x27;#bcbd22&#x27;, &#x27;#dbdb8d&#x27;,&#x27;#17becf&#x27;, &#x27;#9edae5&#x27;])<br></code></pre></td></tr></table></figure><p>colormap：RdBu</p><p>linestyle：<a href="https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html#">https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html#</a></p><p>marker：<a href="https://matplotlib.org/stable/gallery/lines_bars_and_markers/marker_reference.html">https://matplotlib.org/stable/gallery/lines_bars_and_markers/marker_reference.html</a></p><p>等等，除此之外还需要设置针对不同的图增加不同的表达方式。这个就需要从图的种类开始学习。</p><h3 id="14-image展示设置"><a class="markdownIt-Anchor" href="#14-image展示设置"></a> 1.4 Image展示设置</h3><p>可以通过PIL的Image.open实现图像转换为numpy.array，之后使用imshow方式来可视化图片。同时可以图像进行通道遮盖、色彩映射、添加colorbar等展示，并计算色彩范围，重塑图片大小、插值等操作</p><h2 id="0x02-figure-from-the-matplotlib"><a class="markdownIt-Anchor" href="#0x02-figure-from-the-matplotlib"></a> 0x02 Figure from the Matplotlib</h2><p>选择官方<a href="https://matplotlib.org/stable/gallery/index.html">https://matplotlib.org/stable/gallery/index.html</a> 中一些比较有意思的作为参考</p><h3 id="21-直线lines-条形图bar-标记markers"><a class="markdownIt-Anchor" href="#21-直线lines-条形图bar-标记markers"></a> 2.1 直线（Lines）、条形图（Bar）、标记（Markers）</h3><ul><li>ax.bar_label 给bar添加标签</li><li>ax.bar(…,xerr=error,…) 添加误差线</li><li>通过添加upperlimits来设置误差线的箭头</li><li>fill_between 填充，可以设置where参数</li><li>dash style的设置。set_dash间隔</li><li>steps 和 stair设计</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%202.png" alt="Untitled" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%203.png" alt="Untitled" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%204.png" alt="Untitled" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%205.png" alt="Untitled" /></p><h3 id="22-图片images轮廓contours和平面filed"><a class="markdownIt-Anchor" href="#22-图片images轮廓contours和平面filed"></a> 2.2 图片（Images），轮廓（Contours）和平面（Filed）</h3><ul><li>添加水印 fig.figimage</li><li>绘制轮廓</li><li>matshow显示二维数组</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%206.png" alt="Untitled" /></p><h3 id="23-子图subplot坐标轴axes和图figures"><a class="markdownIt-Anchor" href="#23-子图subplot坐标轴axes和图figures"></a> 2.3 子图（Subplot），坐标轴（Axes）和图（Figures）</h3><ul><li>在axes中增加add_axes()</li><li>创建多张axes之间的链接</li><li>放大效果，axins插入一个框，同时设置set_xlim和set_ylim</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%207.png" alt="Untitled" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%208.png" alt="Untitled" /></p><h3 id="24-统计statistics"><a class="markdownIt-Anchor" href="#24-统计statistics"></a> 2.4 统计（Statistics）</h3><ul><li>箱图boxplot，可以选择vert垂直标记，path_artist是否填充颜色，labels</li><li>箱型图hist，可以返回cumulative的数据</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%209.png" alt="Untitled" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%2010.png" alt="Untitled" /></p><h3 id="25-饼图pie和极坐标图polar-charts"><a class="markdownIt-Anchor" href="#25-饼图pie和极坐标图polar-charts"></a> 2.5 饼图（Pie）和极坐标图（Polar charts）</h3><ul><li>有pie绘制图，用bar绘制堆叠图，之后使用connectionPatch来进行两者的链接</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%2011.png" alt="Untitled" /></p><h3 id="26-文本text-标签label和标记annotation"><a class="markdownIt-Anchor" href="#26-文本text-标签label和标记annotation"></a> 2.6 文本（Text）、标签（Label）和标记（Annotation）</h3><ul><li>不玩花的</li></ul><h3 id="27-pyplot-module"><a class="markdownIt-Anchor" href="#27-pyplot-module"></a> 2.7 pyplot module</h3><ul><li>辅助线 axhlines，axvhline</li></ul><h3 id="28-颜色color"><a class="markdownIt-Anchor" href="#28-颜色color"></a> 2.8 颜色（Color）</h3><ul><li>colormap，<a href="https://matplotlib.org/stable/gallery/color/colormap_reference.html#sphx-glr-gallery-color-colormap-reference-py">https://matplotlib.org/stable/gallery/color/colormap_reference.html#sphx-glr-gallery-color-colormap-reference-py</a></li><li>colors，<a href="https://matplotlib.org/stable/gallery/color/named_colors.html#sphx-glr-gallery-color-named-colors-py">https://matplotlib.org/stable/gallery/color/named_colors.html#sphx-glr-gallery-color-named-colors-py</a></li></ul><h3 id="29-形状和shapes和集合collections"><a class="markdownIt-Anchor" href="#29-形状和shapes和集合collections"></a> 2.9 形状和（Shapes）和集合（Collections）</h3><ul><li>不玩花的</li><li>通常在matplotlib patch中的形状或者路径<a href="https://blog.csdn.net/qq_27825451/article/details/82967904">https://blog.csdn.net/qq_27825451/article/details/82967904</a></li></ul><h3 id="210-风格指南style-sheet"><a class="markdownIt-Anchor" href="#210-风格指南style-sheet"></a> 2.10 风格指南（Style sheet）</h3><ul><li>集合 <a href="https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py">https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py</a></li><li>选择多了很难呀</li></ul><h3 id="211-坐标轴网格"><a class="markdownIt-Anchor" href="#211-坐标轴网格"></a> 2.11 坐标轴网格</h3><ul><li>略</li></ul><h3 id="212-坐标轴设计"><a class="markdownIt-Anchor" href="#212-坐标轴设计"></a> 2.12 坐标轴设计</h3><ul><li>略</li></ul><h3 id="213-特殊例子-show-case"><a class="markdownIt-Anchor" href="#213-特殊例子-show-case"></a> 2.13 特殊例子 Show case</h3><ul><li>xkcd非常好玩</li><li>好看的颜色</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%2012.png" alt="Untitled" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%2013.png" alt="Untitled" /></p><h3 id="214-动画-animation"><a class="markdownIt-Anchor" href="#214-动画-animation"></a> 2.14 动画 Animation</h3><ul><li>调用animation.FuncAnimation就可以实现动图</li><li>不玩花的</li></ul><h3 id="215-事件句柄-event-handling"><a class="markdownIt-Anchor" href="#215-事件句柄-event-handling"></a> 2.15 事件句柄 （Event handling）</h3><ul><li>动态交互，不玩花的</li></ul><h3 id="216-混杂的miscellaneous"><a class="markdownIt-Anchor" href="#216-混杂的miscellaneous"></a> 2.16 混杂的（Miscellaneous）</h3><ul><li>给图添加table，可以使用plt.table对象</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%2014.png" alt="Untitled" /></p><h3 id="217-3d绘图3d-plotting"><a class="markdownIt-Anchor" href="#217-3d绘图3d-plotting"></a> 2.17 3D绘图（3D plotting）</h3><h3 id="218-缩放scales"><a class="markdownIt-Anchor" href="#218-缩放scales"></a> 2.18 缩放（Scales）</h3><h3 id="219-特殊绘图specialty-plots"><a class="markdownIt-Anchor" href="#219-特殊绘图specialty-plots"></a> 2.19 特殊绘图（Specialty plots）</h3><h3 id="220-坐标框splines"><a class="markdownIt-Anchor" href="#220-坐标框splines"></a> 2.20 坐标框（Splines）</h3><h3 id="221-标签ticks"><a class="markdownIt-Anchor" href="#221-标签ticks"></a> 2.21 标签（Ticks）</h3><ul><li>坐标</li></ul><p><a href="https://matplotlib.org/stable/gallery/ticks/tick-formatters.html#sphx-glr-gallery-ticks-tick-formatters-py">https://matplotlib.org/stable/gallery/ticks/tick-formatters.html#sphx-glr-gallery-ticks-tick-formatters-py</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%2015.png" alt="Untitled" /></p><h3 id="222-单位units"><a class="markdownIt-Anchor" href="#222-单位units"></a> 2.22 单位（Units）</h3><ul><li>我超！可以从basic_unit中导入cm</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled%2016.png" alt="Untitled" /></p><h3 id="223-嵌入embedding-into-gui"><a class="markdownIt-Anchor" href="#223-嵌入embedding-into-gui"></a> 2.23 嵌入（Embedding into GUI）</h3><ul><li>略</li></ul><h3 id="224-组件widgets"><a class="markdownIt-Anchor" href="#224-组件widgets"></a> 2.24 组件（Widgets）</h3><ul><li>指针 cursor添加一些操作，tracking position</li><li>按钮 button</li><li>确认 check</li><li>选择 selector</li><li>选择器 menu</li><li>鼠标 mouse</li><li>区间选择 rangeslider</li></ul><p>似乎没这个必要哈哈哈</p><h3 id="225-用户例子userdemo"><a class="markdownIt-Anchor" href="#225-用户例子userdemo"></a> 2.25 用户例子（Userdemo）</h3><ul><li>别人使用的例子</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;阅读Python的matplotlib的&lt;/p&gt;</summary>
    
    
    
    <category term="baseline" scheme="https://blog.tjdata.site/categories/baseline/"/>
    
    
    <category term="wiki" scheme="https://blog.tjdata.site/tags/wiki/"/>
    
  </entry>
  
  <entry>
    <title>行为流派02_从gymnasium开始自己的环境</title>
    <link href="https://blog.tjdata.site/posts/a4f3c685.html"/>
    <id>https://blog.tjdata.site/posts/a4f3c685.html</id>
    <published>2023-08-25T08:23:26.000Z</published>
    <updated>2023-08-25T11:21:49.962Z</updated>
    
    <content type="html"><![CDATA[<p>openAI的gym中提供了很多封装好的环境，在此基础上我们可以使用其来跑通深度强化学习的代码，但是更多的时候我们希望调用算法来解决一个实际问题，因此尝试为定制化的问题转换成为MDP六元组《变量、状态、动作、奖励、状态转移、终止条件》后编程为可以交互的环境即可。本文介绍学习gymnasium和stable- baseline3的学习思路并手动实现一个MyCar的环境。</p><span id="more"></span><h2 id="0x01-巨人的肩膀-调库"><a class="markdownIt-Anchor" href="#0x01-巨人的肩膀-调库"></a> 0x01 巨人的肩膀 ：调库</h2><p>根据MDP过程，环境和智能体两个抽象类主要需要包括几个API操作：</p><ol><li>环境：参数设置（init），初始化环境（reset），状态更新（step），关闭（closed），显示（render）</li><li>智能体：深度学习参数（net），学习行为（learn），生成行为（predict）</li></ol><p>所以抽象来看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> stable_baselines3 <span class="hljs-keyword">as</span> sb3<br>env = gym.make(<span class="hljs-string">&#x27;[env_name]&#x27;</span>,render_mode=<span class="hljs-string">&#x27;human&#x27;</span>)<br><span class="hljs-comment"># 定义好参数，已经学习</span><br>agent = sb3.load(<span class="hljs-string">&#x27;[saved_model_dir]&#x27;</span>) <br><br>observation,info = env.reset(seed=<span class="hljs-number">42</span>)<br>terminated = false<br><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> terminated:<br>action = agent.predict(observation)<br>observation,reward,terminated,truncated,info = env.step(action)<br><span class="hljs-keyword">if</span> truncated:<br>env.reset()<br>env.closed<br></code></pre></td></tr></table></figure><h3 id="11-环境库-gymnasiumenv"><a class="markdownIt-Anchor" href="#11-环境库-gymnasiumenv"></a> 1.1 环境库 gymnasium.env</h3><p>目前主流的强化学习环境主要是基于<a href="https://www.gymlibrary.dev/index.html">openai-gym</a>，主要介绍为</p><blockquote><p>Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym’s API has become the field standard for doing this.</p><p>Gym是一个开源的Python库，通过提供标准API在学习算法和环境之间进行通信，以及一组符合该API的标准环境，来开发和比较强化学习算法。自发布以来，Gym的API已成为这样做的现场标准。</p></blockquote><p>但是代码安装有点太屎山了，现有另外的一个fork版本<a href="https://gymnasium.farama.org">gymnasium</a>更加的简单和包容</p><blockquote><p><strong>Gymnasium is a maintained fork of OpenAI’s Gym library.</strong> The Gymnasium interface is simple, pythonic, and capable of representing general RL problems, and has a <a href="https://gymnasium.farama.org/content/gym_compatibility/">compatibility wrapper</a> for old Gym environments</p><p>Gymasium是OpenAI gym library的一个维护分支。Gymnasium界面简单，pythonic，能够表示一般的RL问题，并具有旧gym环境的兼容性warp器</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs code">pip install gymnasium<br></code></pre></td></tr></table></figure><h3 id="12-强化学习算法库-stable_baselines3"><a class="markdownIt-Anchor" href="#12-强化学习算法库-stable_baselines3"></a> 1.2 强化学习算法库 stable_baselines3</h3><p><a href="https://stable-baselines3.readthedocs.io/en/master/">Stable_baseline3</a>是基于OpenAI baselines改进的实现，类似gymnasium和gym的关系，主要实现的修改为：</p><ol><li>统一算法结构</li><li>实现PEP8兼容</li><li>文档化函数和类</li><li>更多的测试和代码覆盖</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> stable_baseline3<br></code></pre></td></tr></table></figure><p>另外在stable_baseline3的基础上包括预训练好的智能体平台<a href="https://stable-baselines.readthedocs.io/en/master/guide/rl_zoo.html">RL Baseline zoo</a>，同时也提供训练、评估智能体行为、微调超参数和录屏的功能，具体的使用可以参考官方文档。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg<br>pip install stable-baselines box2d box2d-kengz pyyaml pybullet optuna pytablewriter<br></code></pre></td></tr></table></figure><p>PS: 如果不习惯用conda管理环境，或者有迁移环境的需求可以参考使用docker创建镜像</p><p>另外还有一些其他优秀的RL库，比如<a href="https://github.com/datawhalechina/joyrl">蘑菇书-joyrl</a>、<a href="https://github.com/tensorforce/tensorforce">Tensorforce</a></p><h2 id="0x02-优秀环境欣赏"><a class="markdownIt-Anchor" href="#0x02-优秀环境欣赏"></a> 0x02 优秀环境欣赏</h2><p>在gymnasium的<a href="https://gymnasium.farama.org/environments/classic_control/">官网环境</a>中给出一些典型的环境，可以分类为：</p><ol><li><strong>经典控制（Classic control）</strong>，比如杂技演员（Acrobat）、单臂摆（Cart pole）、小车上山（Mountain car）、钟摆（Pendulum）</li><li><strong>二维环境（Box2D）</strong>，双足行走（Bipedal walker）、赛车（Car racing）、登月（Lunar lander）</li><li><strong>文本游戏（Toy Text）</strong>，二十一点（Blackjack）、悬崖寻路（Cliff walking）、冰湖（Frozen lake）、出租车（Taxi）</li><li><strong>多关节接触动力学（Multi Joint Dynamics with Contact，MoJoCo）</strong></li><li><strong>雅达利（Atari）</strong>，是的就是被任天堂打败的“雅达利大崩溃”的雅达利</li><li><strong><a href="https://gymnasium.farama.org/environments/third_party_environments/">第三方环境</a></strong>，<a href="https://github.com/robertoschiavone/flappy-bird-env">flappy-bird-env</a>，<a href="https://github.com/eleurent/highway-env">highway-env</a>，<a href="https://github.com/LucasAlegre/sumo-rl">sumo-rl</a>，等等</li></ol><h2 id="0x03-gymnasiumenv-详细介绍"><a class="markdownIt-Anchor" href="#0x03-gymnasiumenv-详细介绍"></a> 0x03 gymnasium.env 详细介绍</h2><p>关于基类的介绍，在<a href="https://gymnasium.farama.org/api/env/">gymnasium.env</a>中很清楚，但是一堆英文可能看着有点累，这里主要介绍作为一个抽象类它的外部接口和基本常见属性：</p><p>方法：</p><ol><li>Step() ，根据agent的action更新state，同时返回五元组(更新状态obs，奖励信号reward，是否结束terminated，是否中断truncated，信息info)，注意这里对gym.env中的done更加细致</li><li>reset()，重置环境到初始状态</li><li>Render() 图形引擎，用于可视化过程，不要也可以</li><li>close（） 关闭环境</li></ol><p>属性</p><ol><li>actions_space 定义动作环境</li><li>observation_space 定义状态环境</li><li>Reward_range 奖励范围</li><li>spec</li><li>metadata</li><li>np.random</li></ol><p>Stable_baseline3也提供一些<a href="https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html">教程给出自定义类的属性</a>并且提供了一个<a href="https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb">colab-GoLeftEnv</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> gymnasium <span class="hljs-keyword">import</span> spaces<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomEnv</span>(gym.Env):<br>    <span class="hljs-string">&quot;&quot;&quot;Custom Environment that follows gym interface.&quot;&quot;&quot;</span><br><br>    metadata = &#123;<span class="hljs-string">&quot;render_modes&quot;</span>: [<span class="hljs-string">&quot;human&quot;</span>], <span class="hljs-string">&quot;render_fps&quot;</span>: <span class="hljs-number">30</span>&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, arg1, arg2, ...</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># Define action and observation space</span><br>        <span class="hljs-comment"># They must be gym.spaces objects</span><br>        <span class="hljs-comment"># Example when using discrete actions:</span><br>        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)<br>        <span class="hljs-comment"># Example for using image as input (channel-first; channel-last also works):</span><br>        self.observation_space = spaces.Box(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">255</span>,<br>                                            shape=(N_CHANNELS, HEIGHT, WIDTH), <br>                                            dtype=np.uint8)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, action</span>):<br>        ...<br>        <span class="hljs-keyword">return</span> observation, reward, terminated, truncated, info<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self, seed=<span class="hljs-literal">None</span>, options=<span class="hljs-literal">None</span></span>):<br>        ...<br>        <span class="hljs-keyword">return</span> observation, info<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render</span>(<span class="hljs-params">self</span>):<br>        ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close</span>(<span class="hljs-params">self</span>):<br>        ...<br></code></pre></td></tr></table></figure><h2 id="0x04-从零开始的mycar"><a class="markdownIt-Anchor" href="#0x04-从零开始的mycar"></a> 0x04 从零开始的MyCar</h2><p>假设我们现在希望训练一个智能体，可以在出现下列的网格中出现时都会向原点前进，在定义的环境时可以使用gymnaisum.env定义自己的环境类MyCar，之后使用stable_baselines3中的check_env对环境的输入和输出做检查：</p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230825190216001.png" alt="MyCar env" style="zoom:50%;" /><p>由此分析环境中的属性：</p><p>状态空间：二维的空间和问题的size有关</p><p>动作空间：离散的五种动作，暂停和上下左右</p><p>是否结束：到达原点</p><p>是否中止：跑到环境之外</p><p>奖励：当前状态距离原点的距离</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium<br><span class="hljs-keyword">from</span> gymnasium <span class="hljs-keyword">import</span> spaces<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-comment"># Path: modelTimetable/DRL/myEnv.ipynb</span><br><span class="hljs-comment"># Implementing the environment</span><br><span class="hljs-comment"># Reproduction of the cartpole environment</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># Discription: </span><br><span class="hljs-comment"># Create a car in a two-dimensional plane with a width of 20, and the coordinates of </span><br><span class="hljs-comment"># the center point are the destination of the car to reach.</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># State:</span><br><span class="hljs-comment"># The state of the car is represented by the coordinates of the center point of the car.(x,y)</span><br><span class="hljs-comment"># Action:</span><br><span class="hljs-comment"># The action of the car is represented by the speed of the car.(vx,vy)</span><br><span class="hljs-comment"># Reward:</span><br><span class="hljs-comment"># The reward is the distance between the car and the destination.</span><br><span class="hljs-comment"># Termination:</span><br><span class="hljs-comment"># The car reaches the destination.(0,0)</span><br><span class="hljs-comment"># truncation:</span><br><span class="hljs-comment"># The car is out of the screen.</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">gymnasium is the main class that we will use to create our environment.</span><br><span class="hljs-string"></span><br><span class="hljs-string">The gymnasium class has the following methods:</span><br><span class="hljs-string">__init__(): This method is used to initialize the environment. It takes the following parameters:</span><br><span class="hljs-string"></span><br><span class="hljs-string">step(): This method is used to take an action and return the next state, reward, and whether the episode is over. </span><br><span class="hljs-string">Physical engine</span><br><span class="hljs-string">- input: action</span><br><span class="hljs-string">- output: observation, reward,terminated,truncated,info</span><br><span class="hljs-string"></span><br><span class="hljs-string">reset(): This method is used to reset the environment to its initial state.</span><br><span class="hljs-string">- input: None</span><br><span class="hljs-string">- output: observation</span><br><span class="hljs-string"></span><br><span class="hljs-string">render(): This method is used to render the environment:</span><br><span class="hljs-string">Image engine</span><br><span class="hljs-string">- input: mode(default=&#x27;human&#x27;,&#x27;human&#x27;,&#x27;rgb_array&#x27;,&#x27;ansi&#x27;,&#x27;rgb_array_list)</span><br><span class="hljs-string">- output: None</span><br><span class="hljs-string">eg:gymnasium.make(&#x27;CartPole-v0&#x27;,render_mode=&#x27;human&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">close(): This method is used to close the environment.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyCar</span>(gymnasium.Env):<br>    metadata = &#123;<br>        <span class="hljs-string">&#x27;render.modes&#x27;</span>: [<span class="hljs-string">&#x27;human&#x27;</span>, <span class="hljs-string">&#x27;rgb_array&#x27;</span>],<br>        <span class="hljs-string">&#x27;video.frames_per_second&#x27;</span>: <span class="hljs-number">2</span><br>        &#125;<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.target_x = <span class="hljs-number">0</span><br>        self.target_y = <span class="hljs-number">0</span><br>        self.size = <span class="hljs-number">10</span><br>        self.action_space = spaces.Discrete(<span class="hljs-number">5</span>) <span class="hljs-comment"># 0:stop, 1:up, 2:down, 3:left, 4:right</span><br>        self.observation_space = spaces.Box(np.array([-self.size,-self.size]), np.array([self.size,self.size]))<br>        self.state = <span class="hljs-literal">None</span><br>        self.info = &#123;&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, action</span>):<br>        <span class="hljs-keyword">assert</span> self.action_space.contains(action), <span class="hljs-string">&quot;%r (%s) invalid&quot;</span>%(action, <span class="hljs-built_in">type</span>(action))<br>        <span class="hljs-comment"># update the state by the action</span><br>        x,y = self.state<br>        <span class="hljs-keyword">if</span> action == <span class="hljs-number">0</span>:<br>            x += <span class="hljs-number">0</span><br>            y += <span class="hljs-number">0</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">1</span>:<br>            x += <span class="hljs-number">0</span><br>            y += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">2</span>:<br>            x += <span class="hljs-number">0</span><br>            y += -<span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">3</span>:<br>            x += -<span class="hljs-number">1</span><br>            y += <span class="hljs-number">0</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">4</span>:<br>            x += <span class="hljs-number">1</span><br>            y += <span class="hljs-number">0</span><br>        <span class="hljs-comment"># the next state</span><br>        self.state = np.array([x,y])<br>        self.state = self.state.astype(np.float32)<br>        reward = self._get_reward()<br>        terminated = self._get_terminated()<br>        terminated = <span class="hljs-built_in">bool</span>(terminated)<br>        truncated = self._get_truncated()<br>        truncated = <span class="hljs-built_in">bool</span>(truncated)<br>        info = &#123;&#125;<br>        <span class="hljs-keyword">return</span> self.state, reward, terminated,truncated, info<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self,seed=<span class="hljs-literal">None</span></span>):<br>        self.state = np.ceil(np.random.rand(<span class="hljs-number">2</span>)*<span class="hljs-number">2</span>*self.size)-self.size<br>        self.state = self.state.astype(np.float32)<br>        self.counts = <span class="hljs-number">0</span><br>        self.info = &#123;&#125;<br>        <span class="hljs-keyword">return</span> self.state,self.info<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render</span>(<span class="hljs-params">self, mode=<span class="hljs-string">&#x27;human&#x27;</span></span>):<br>        <span class="hljs-built_in">print</span>(self.state)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().close()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_reward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> -np.sqrt(self.state[<span class="hljs-number">0</span>]**<span class="hljs-number">2</span>+self.state[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_terminated</span>(<span class="hljs-params">self</span>):<br>        x,y = self.state<br>        <span class="hljs-keyword">return</span> x==self.target_x <span class="hljs-keyword">and</span> y==self.target_y<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_truncated</span>(<span class="hljs-params">self</span>):<br>        x,y = self.state<br>        <span class="hljs-keyword">return</span> x&lt;-self.size <span class="hljs-keyword">or</span> x&gt;self.size <span class="hljs-keyword">or</span> y&lt;-self.size <span class="hljs-keyword">or</span> y&gt;self.size<br><br><span class="hljs-keyword">from</span> stable_baselines3.common.env_checker <span class="hljs-keyword">import</span> check_env<br>env = MyCar()<br>check_env(env, warn=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>测试它的输出输出</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pf">env = MyCar()<br>env.reset()<br><span class="hljs-keyword">state</span>,reward,terminated,truncated,info = env.step(env.action_space.sample())<br><span class="hljs-keyword">log</span> = <span class="hljs-number">0</span><br>while not terminated:<br>    env.render()<br>    <span class="hljs-keyword">state</span>,reward,terminated,truncated,info = env.step(env.action_space.sample())<br>    if truncated:<br>        env.reset()<br>    <span class="hljs-keyword">log</span> += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230825190658480.png" alt="output" /></p><h2 id="0x05-开始训练"><a class="markdownIt-Anchor" href="#0x05-开始训练"></a> 0x05 开始训练</h2><p>这里只是调用stable_baselines的最简单的DQN库，没有调整参数和网络结构</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> stable_baselines3 <span class="hljs-keyword">import</span> DQN<br><span class="hljs-keyword">from</span> stable_baselines3.common <span class="hljs-keyword">import</span> logger<br># Train the agent <span class="hljs-keyword">by</span> the stable_baselines3<br><span class="hljs-keyword">import</span> os<br>models_dir = <span class="hljs-string">&#x27;./models/DQN&#x27;</span><br>logdir = <span class="hljs-string">&#x27;./logs&#x27;</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.<span class="hljs-keyword">exists</span>(models_dir):<br>    os.makedirs(models_dir)<br><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.<span class="hljs-keyword">exists</span>(logdir):<br>    os.makedirs(logdir)<br><br>env = MyCar()<br>agent = DQN(<span class="hljs-string">&#x27;MlpPolicy&#x27;</span>, env, <span class="hljs-keyword">verbose</span>=<span class="hljs-number">1</span>,tensorboard_log=logdir)<br>agent.learn(total_timesteps=<span class="hljs-number">100000</span>, log_interval=<span class="hljs-number">100</span>,tb_log_name=<span class="hljs-string">&#x27;DQN&#x27;</span>)<br>agent.save(&quot;DQN_MyCar&quot;)<br></code></pre></td></tr></table></figure><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230825190803558.png" alt="DQN训练结果" style="zoom:25%;" /><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230825190828588.png" alt="训练最终结果" style="zoom:25%;" /><p>之后可以通过保存的环境来测试结果</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">env = MyCar()<br>obs = env.reset()<br>agent = DQN.load(<span class="hljs-string">&#x27;deepq_cartpole.zip&#x27;</span>,<span class="hljs-attribute">env</span>=env)<br>terminated = <span class="hljs-literal">False</span><br><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> terminated:<br>    action,_state = agent.predict(obs)<br>    obs,rew,terminated,truncated,<span class="hljs-built_in">info</span> = env.<span class="hljs-keyword">step</span>(action)<br>    <span class="hljs-built_in">print</span>(env.state)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230825190955305.png" alt="解决方案" /></p><p>并使用</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf">tensorboard --logdir <span class="hljs-operator">=</span> logs<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230825191114705.png" alt="参数" /></p><h2 id="0x06-总结"><a class="markdownIt-Anchor" href="#0x06-总结"></a> 0x06 总结</h2><p>最感动的是stable_baselines3提供的custom_gym_env.ipynb中最后给出的be creative！</p><p>建立环境又何尝不是一种创造。</p><p>参考链接很多，感谢互联网～</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;openAI的gym中提供了很多封装好的环境，在此基础上我们可以使用其来跑通深度强化学习的代码，但是更多的时候我们希望调用算法来解决一个实际问题，因此尝试为定制化的问题转换成为MDP六元组《变量、状态、动作、奖励、状态转移、终止条件》后编程为可以交互的环境即可。本文介绍学习gymnasium和stable- baseline3的学习思路并手动实现一个MyCar的环境。&lt;/p&gt;</summary>
    
    
    
    <category term="RL" scheme="https://blog.tjdata.site/categories/RL/"/>
    
    
    <category term="CS188" scheme="https://blog.tjdata.site/tags/CS188/"/>
    
  </entry>
  
  <entry>
    <title>行为流派01_智能体与搜索</title>
    <link href="https://blog.tjdata.site/posts/9c6b1465.html"/>
    <id>https://blog.tjdata.site/posts/9c6b1465.html</id>
    <published>2023-08-22T12:12:57.000Z</published>
    <updated>2023-08-22T12:26:21.371Z</updated>
    
    <content type="html"><![CDATA[<p>根据CS188《Intro to AI》来对搜索策略进行一定的总结</p><span id="more"></span><h2 id="0x01-智能体"><a class="markdownIt-Anchor" href="#0x01-智能体"></a> 0x01 智能体</h2><p>在人工智能中，核心问题是创建一个理性的<strong>智能体(Rational agent)</strong>，是一个实体通过一些系列的<strong>动作(Actions)<strong>来实现目标或者是</strong>喜好(Goal or perferences),<strong>它存在于</strong>环境中（Environment）</strong>，智能体通过<strong>传感器（sensors）<strong>获取信息并驱动自己的</strong>动作（Actuators）</strong>。</p><p>Reflex agent：仅仅根据现有来选择动作</p><p>Planning agent：根据一系列的动作来确定现有状态下的动作</p><p>定义一个任务通常会用到PEAS（评价指标，环境，动作，传感器）</p><p>Performance Measure：指的是智能体需要提高的衡量指标</p><p>Environment：总结影响智能体的因素</p><p>Actions and Sensors：说明智能体如何对环境造成改变，同时如何获取信息</p><p>特定的我们确定几类不同特征的任务：</p><ol><li>部分观察的任务(Partially observable environments) 或者 完全观察(Fully observation)</li><li>随机过程(Stochastic) 或者 确定过程（Deterministic）</li><li>多智能体环境（multi-agent）</li><li>静态环境（static）或者动态环境（Dynamic）</li></ol><h2 id="0x02-搜索问题定义"><a class="markdownIt-Anchor" href="#0x02-搜索问题定义"></a> 0x02 搜索问题定义</h2><p>搜索问题包括以下几个部分：</p><ol><li>State 在所给案例中的所有状态</li><li>Action 在每个状态可取动作的集合</li><li>Transition 在某一状态下采用动作得到的下一个状态</li><li>Cost 从一个状态到另外一个状态的成本</li><li>Start state 初始状态</li><li>Goal test 判断是否到达最终的状态</li></ol><p>例如吃豆人Pathing和吃掉所有豆子Eat-all-dots问题之间的区别：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Untitled.png" alt="搜索问题举例" /></p><p>常见的表现形式为状态空间图和搜索树</p><p>state space graphs and search trees</p><p>状态空间图：用不同状态之间的转移关系来表示</p><p>搜索树：以状态为节点，动作作为边，但是每个状态和节点不仅编码状态本身，还编码路径</p><ul><li>getNextState</li><li>get Action</li><li>get Action Cost</li></ul><p>我们只存储立即处理的state，并计算最新的状态</p><h2 id="0x03-基本搜索normal-search"><a class="markdownIt-Anchor" href="#0x03-基本搜索normal-search"></a> 0x03 基本搜索（normal search）</h2><p>一个标准的方式来寻找到轨迹（Plan），是从初始状态开始(getStartState)来到达最终状态（Goal state），同时维护可能的过程（Frontier），同时拓展我们的当前过程通过删除节点state为state- action- state。（称作strategy），当我们删除缩手的frontiers，我们得到最终的路径（path）</p><p>这些搜索中具有一些基本属性：</p><ol><li>搜索的完整性 completeness</li><li>搜索的最优性 optimality</li><li>分枝因素</li><li>最大深度</li><li>最浅解的深度</li></ol><p>常见的无信息搜索包括：DFS、BFS、UCS</p><p>常见的信息搜索包括；Greedy search and A star search</p><p>后者相比较前者主要增加代价，代价的含义包括两个方面</p><ol><li><p>Estimation of distance to goal state，as the cost function may be the lower bound for the problem, the heuristics are typically solutions to relaxed problem(where some of the constraint  of the original problem have been removed)</p><p>比如利用启发式的方法可以让PACMAN更加倾向于到达最终的目的地</p></li><li><p>Estimation of the empirical cost function</p></li></ol><h2 id="0x04-约束问题constraint-satisfied-problem和回溯backtracking-search和local-search"><a class="markdownIt-Anchor" href="#0x04-约束问题constraint-satisfied-problem和回溯backtracking-search和local-search"></a> 0x04 约束问题（Constraint satisfied problem）和回溯（Backtracking search）和local search</h2><p>之前的文章中介绍什么是规划问题（Planing problem），可以使用搜索策略（Search strategy）进行求解。对于另外一种问题可以描述为约束满足问题（Constraint satisfaction problems，CSP）是一类识别（Identification problem）</p><blockquote><p><strong>识别问题（Identification problem）：我们必须简单的识别最终的结果是否为目标状态</strong></p><p>这个术语通常指在机器学习和人工智能中,系统需要从一组潜在的候选项中识别出正确的项目的任务。这类问题通常涉及模式识别,即从输入的数据中提取有意义的信息模式。</p><p>一些常见的识别问题示例包括:</p><ul><li>图像识别:从图片中识别出特定对象或场景</li><li>语音识别:从语音输入中识别出说话内容</li><li>面部识别:从图片或视频中识别出特定人脸</li><li>手写识别:识别手写输入的文字内容</li><li>生物识别:通过指纹、声纹等生物特征识别个体身份</li><li>文档分类:将文本文档分类到预定义的类别中</li></ul><p>识别问题通常需要构建合适的机器学习模型,然后使用大量带标签的训练数据对模型进行训练,使其能够对新输入进行可靠的识别与分类。同时还需要处理各种困难情况,如光线、角度、遮挡等对图像识别的影响,以提高模型的鲁棒性。</p></blockquote><p>三元组：</p><ol><li>variables，一系列的变量集合</li><li>domain，不同变量可能存在的取值范围</li><li>constraints，不同变量之间的约束关系</li></ol><p>例如：八皇后问题</p><p>约束编程问题的求解方式是NP-hard问题，随着问题规模的扩大时间呈现指数型增加。我们可以将<strong>约束满足问题转换成为搜索问题：</strong></p><p>state 为部分赋值（对CSP的可变赋值，其中一些变量被分配了值，而另一些变量没有）。</p><p>successor function 输出所有状态，并分配一个新变量，目标测试验证所有变量都被分配，并在其测试的状态中满足所有约束。</p><p>约束问题比之前的搜索问题具有更复杂的框架，因此可以尝试将上述公示和合适的启发式方法结合起来</p><p>之前的文章中介绍什么是规划问题（Planing problem），可以使用搜索策略（Search strategy）进行求解。对于另外一种问题可以描述为约束满足问题（Constraint satisfaction problems，CSP）是一类识别（Identification problem）</p><blockquote><p><strong>识别问题（Identification problem）：我们必须简单的识别最终的结果是否为目标状态</strong></p><p>这个术语通常指在机器学习和人工智能中,系统需要从一组潜在的候选项中识别出正确的项目的任务。这类问题通常涉及模式识别,即从输入的数据中提取有意义的信息模式。</p><p>一些常见的识别问题示例包括:</p><ul><li>图像识别:从图片中识别出特定对象或场景</li><li>语音识别:从语音输入中识别出说话内容</li><li>面部识别:从图片或视频中识别出特定人脸</li><li>手写识别:识别手写输入的文字内容</li><li>生物识别:通过指纹、声纹等生物特征识别个体身份</li><li>文档分类:将文本文档分类到预定义的类别中</li></ul><p>识别问题通常需要构建合适的机器学习模型,然后使用大量带标签的训练数据对模型进行训练,使其能够对新输入进行可靠的识别与分类。同时还需要处理各种困难情况,如光线、角度、遮挡等对图像识别的影响,以提高模型的鲁棒性。</p></blockquote><p>三元组：</p><ol><li>variables，一系列的变量集合</li><li>domain，不同变量可能存在的取值范围</li><li>constraints，不同变量之间的约束关系</li></ol><p>例如：八皇后问题</p><p>约束编程问题的求解方式是NP-hard问题，随着问题规模的扩大时间呈现指数型增加。我们可以将<strong>约束满足问题转换成为搜索问题：</strong></p><p>state 为部分赋值（对CSP的可变赋值，其中一些变量被分配了值，而另一些变量没有）。</p><p>successor function 输出所有状态，并分配一个新变量，目标测试验证所有变量都被分配，并在其测试的状态中满足所有约束。</p><p>约束问题比之前的搜索问题具有更复杂的框架，因此可以尝试将上述公示和合适的启发式方法结合起来</p><p>map coloring problem : 给定一堆颜色，要求相邻的地区之间的颜色不能相等</p><p>通常我们可以对约束进行分类：</p><ol><li>unary constraints 专有约束，在约束图中表示 点</li><li>binary constraint：两个变量，在约束图中表示 边</li><li>higher-order constraint：非常规</li></ol><p>另一方面，回溯搜索仅在变量值不违反任何约束的情况下为变量分配值，从而显著减少回溯。虽然回溯搜索是比深度优先搜索的野蛮强化的巨大改进，但通过过滤、变量/值排序和结构拓宽的进一步改进，我们仍然可以获得更多的速度收益。</p><ul><li>变量的取值范围已经是一个较大的进步</li><li>Tricks1: 过滤（Filter）：</li><li>Tricks2:变量排序（Variable or value ordering）<ul><li>minimum remaining values，选择分配有效剩余值最少的变量</li><li>least constraining value，从剩余未份配值的domain中选择prunes最少的值</li></ul></li><li>Tricks3: 结构拓宽（Structural exploration）</li></ul><p>另外一种方法：本地搜索通过迭代的改进对一些值的随机分配开始，然后迭代的选择一个随机冲突变量，并对变量重新分配给违反约束最小的变量，直到不再存在约束违规，称为一种具有最小冲突启发式的策略</p><ol><li>爬山搜索 Hill-Climbing Search</li><li>模拟退火搜索 Simulate annealing search</li><li>遗传搜索 generic algorithm</li></ol><h2 id="0x05-游戏game-和对抗搜索adversarial-search-problem"><a class="markdownIt-Anchor" href="#0x05-游戏game-和对抗搜索adversarial-search-problem"></a> 0x05 游戏（Game） 和对抗搜索（Adversarial search problem）</h2><p>在之前的搜索问题中，我们尝试解决他们高效并且最优的，利用不同的搜索方式。以及更加具有启发式的强大的搜索算法。但是在面临一些问题中搜索算法中会遇到对方的阻碍，这种情况下单纯的利用搜索策略。这一类新的问题通常可以被称为adversarial search problem，或者叫做游戏games。</p><p>不同于传统搜索问题返回直接的答案，对抗搜索由于problem一直在发生变化，这样复杂的场景中往往要求我们返回一个策略（Strategy or policy），一个简单的对抗搜索问题包括一下策略：</p><ol><li>Initial state</li><li>players ：每一轮属于谁的player</li><li>actions，player可以实现的动作</li><li>transition model 状态转移的结果</li><li>terminal test</li><li>terminal values</li></ol><p>可以使用</p><ol><li>Min-max搜索</li><li>Alpha-beta搜索</li><li>evaluation function</li></ol><h2 id="0x06-动态决策过程mdp和强化学习reinforcement-learning"><a class="markdownIt-Anchor" href="#0x06-动态决策过程mdp和强化学习reinforcement-learning"></a> 0x06 动态决策过程（MDP）和强化学习（Reinforcement learning）</h2><p>～ to be continued</p><p>参考链接：</p><p><a href="https://oi-wiki.org/search/">OI wiki：搜索部分简介</a></p><p><a href="https://inst.eecs.berkeley.edu/~cs188/fa22/">《CS188: Intro to atrificial intelligent》</a></p><p>伯克利人工智能入门课按经典教材章节顺序授课,内容涵盖各主要领域;课程笔记通俗易懂,作业可在Gradescope平台完成实时测评,项目复现经典游戏,运用所学知识实现算法,使吃豆人在迷宫自由行动。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;根据CS188《Intro to AI》来对搜索策略进行一定的总结&lt;/p&gt;</summary>
    
    
    
    <category term="RL" scheme="https://blog.tjdata.site/categories/RL/"/>
    
    
    <category term="CS188" scheme="https://blog.tjdata.site/tags/CS188/"/>
    
  </entry>
  
  <entry>
    <title>datawhale_NLP任务二</title>
    <link href="https://blog.tjdata.site/posts/12d9621e.html"/>
    <id>https://blog.tjdata.site/posts/12d9621e.html</id>
    <published>2023-08-21T06:33:47.000Z</published>
    <updated>2023-08-21T08:01:12.709Z</updated>
    
    <content type="html"><![CDATA[<p>比赛的基本信息在之前的任务一中已经有所介绍，属于文本领域（Natural language processing，NLP）的分类任务，之前的方法是<a href="https://blog.tjdata.site/posts/29594cb7.html">采用的是“抽取特征+分类器”的方式来实现的</a>。这样的方法范式通常需要手动构造许多特征，并且需要较适应的分类器来训练，其中主要的困难点在于（1）手工构造的特征是否有效 （2）如何保证机器学习模型训练中的偏差-方差权衡。但是随着Attention范式的爆火，自动抽取文本特征成为困难，所以在Baseline中我们可以尝试使用BERT（Bidirectional encoder representations from transformers）来自动构造特征，但是和计算机视觉（Computer vision，CV）类似端到端（End-to-end）的思维总是很诱人，所以尝试利用“input+prompt”对训练集进行修改，同时对训练好的大模型进行微调（Fine-tuning）成为一种新的范式。</p><p>本文将借助本次项目中提供的Topline代码（深度学习）进行解读。</p><span id="more"></span><h2 id="目录设计"><a class="markdownIt-Anchor" href="#目录设计"></a> 目录设计</h2><p>Kaggle大神开源了一本 <a href="https://docdrop.org/download_annotation_doc/AAAMLP-569to.pdf">《approaching any machine learning problem》</a>其中并不是介绍机器学习的各种数学原理，而更多的是从开发的角度、代码文件设计来介绍常见配置。感觉很像是Project profile，在讨论如何将Dirty work更好的组织起来。本次项目代码的路径设计中包括</p><figure class="highlight ldif"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs ldif"><span class="hljs-literal">-</span> checkpoints 模型训练的权重<br><span class="hljs-literal">-</span> dataset 存放数据集<br><span class="hljs-literal">-</span> evaluate_predictions 评估文件<br><span class="hljs-literal">-</span> models_input_files 存放模型输入文件<br><span class="hljs-literal">-</span> model_predictions 存放推理概率文件<br><span class="hljs-literal">-</span> premodels 预训练模型权重，config.json文件<br><span class="hljs-comment"># 以及各种src(.py)</span><br><span class="hljs-literal">-</span> data_process.py<br><span class="hljs-literal">-</span> models_training.py<br><span class="hljs-literal">-</span> evaluate_models.py<br><span class="hljs-literal">-</span> ensemble_and_submit.py<br></code></pre></td></tr></table></figure><p>虽然之前的学习的过程中我们将深度学习总结为八股：data process、dataset and dataloader、models、loss function、optimizer、hyper-parameters、model train、model eval。但是从调库的角度，可以将其中的第二步～第七步看作一部分，通常可以从hugging face获取。</p><h2 id="数据预处理"><a class="markdownIt-Anchor" href="#数据预处理"></a> 数据预处理</h2><p>这一部分是将数据集划分为 ：训练集、验证集、测试集。不同数据集之间的区别是</p><ul><li><p>训练集 train dataset：直接对于模型的参数进行调整的数据，直接影响梯度下降的结果</p></li><li><p>验证集 validation dataset：可以通过在训练集和训练得到的模型在验证集中结果对比，来调整模型的超参数，直接影响超参数，间接影响模型结果</p></li><li><p>测试集 test dataset：模型完全没有见过的数据，属于公平评价模型泛化能力的结果</p></li></ul><p>难点在于将原始文本（Raw documentation）通过分词器（transformer.AutoTokenizer)转换成为 （输入id，注意力掩码，标记类型）（inputs_ids_list,attention_mask_list,token_tyope_ids_list）重点在于如何将文本表示成为向量形式。<a href="https://blog.tjdata.site/posts/2e4e1168.html">参考之前阅读李沐的</a>RNN教程中从零构建文本处理的代码，这一段的核心代码参考hugging face为：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> transformers import AutoTokenizer  # 导入AutoTokenizer类，用于文本分词<br>model_name = <span class="hljs-string">&#x27;roberta-base&#x27;</span><br><span class="hljs-comment"># 从模型实例中得到分词器</span><br>tokenizer = AutoTokenizer.from_pretrained(model_name,<br><span class="hljs-attribute">max_length</span>=MAX_LENGTH,<br><span class="hljs-attribute">cache_dir</span>=<span class="hljs-string">&#x27;./premodels/&#123;model_name&#125;_saved&#x27;</span>)<br><span class="hljs-comment"># 根据分词器构建标签</span><br>input_ids_list, attention_mask_list, token_type_ids_list = [], [], []<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(range(len(train[<span class="hljs-string">&#x27;content&#x27;</span>]))):  <br>    sample = train[<span class="hljs-string">&#x27;content&#x27;</span>][i]<br>     # 分词处理，使用最长优先方式截断<br>    tokenized = tokenizer(sample, <span class="hljs-attribute">truncation</span>=<span class="hljs-string">&#x27;longest_first&#x27;</span>) <br>    # 获取输入id，注意力掩码，标注类型并转换为torch对象<br>    input_ids, attention_mask = tokenized[<span class="hljs-string">&#x27;input_ids&#x27;</span>], tokenized[<span class="hljs-string">&#x27;attention_mask&#x27;</span>]<br>    input_ids, attention_mask = torch.tensor(input_ids), torch.tensor(attention_mask)  <br>    # 可能有的没有id，所以需要加一个try- except<br>    try:<br>        token_type_ids = tokenized[<span class="hljs-string">&#x27;token_type_ids&#x27;</span>]  # 获取标记类型ID<br>        token_type_ids = torch.tensor(token_type_ids)  # 转换为PyTorch张量<br>    except:<br>        token_type_ids = torch.zeros_like(input_ids)<br>    input_ids_list.append(input_ids)  # 将输入ID添加到列表中<br>    attention_mask_list.append(attention_mask)  # 将注意力掩码添加到列表中<br>    token_type_ids_list.append(token_type_ids)  # 将标记类型ID添加到列表中<br>    # 得到训练数据标签<br>    y_train.append(train[<span class="hljs-string">&#x27;label&#x27;</span>][i])  # 将训练数据的标签添加到列表中<br><br><span class="hljs-comment"># 同时为了保证向量维度大小一致，使用torch pad_sequence</span><br>  input_ids_tensor = pad_sequence(input_ids_list, <span class="hljs-attribute">batch_first</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">padding_value</span>=0) <br>  attention_mask_tensor = pad_sequence(attention_mask_list, <span class="hljs-attribute">batch_first</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">padding_value</span>=0) <br>  token_type_ids_tensor = pad_sequence(token_type_ids_list, <span class="hljs-attribute">batch_first</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">padding_value</span>=0) <br>  <br><span class="hljs-comment"># 建立数据集</span><br>x_train = torch.stack([input_ids_tensor, attention_mask_tensor, token_type_ids_tensor], <span class="hljs-attribute">dim</span>=1)  <br>x_train = x_train.numpy()  <br><br><span class="hljs-comment"># 最后保存数据，这里转换成为numpy，不知道有没有其他格式</span><br>np.save(f<span class="hljs-string">&#x27;./models_input_files/x_train&#123;model_index&#125;.npy&#x27;</span>, x_train)  # 保存训练数据<br></code></pre></td></tr></table></figure><h2 id="模型训练"><a class="markdownIt-Anchor" href="#模型训练"></a> 模型训练</h2><p>这里的操作主要是加载数据、确定超参数、定义模型（从预训练）、模型训练。其中的核心在于加载数据、如何定义模型和如何从pretrain权重训练模型</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-symbol">opt:</span><br>seed = <span class="hljs-number">42</span><br>...<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>,model_name</span>):<br>    <span class="hljs-variable language_">super</span>().__init__()<br>    <span class="hljs-comment"># 下载权重</span><br>    <span class="hljs-variable language_">self</span>.model = AutoModel.from_pretrained(model_name,cahe_dir=<span class="hljs-string">&#x27;../premodels/&#x27;</span>+model_name+<span class="hljs-string">&#x27;_saved&#x27;</span>)<br>    <span class="hljs-comment"># 获取现有模型的纬度</span><br>    config = AutoConfig.from_pretrained(model_name,cahe_dir=<span class="hljs-string">&#x27;../premodels/&#x27;</span>+model_name+<span class="hljs-string">&#x27;_saved&#x27;</span>)<br>    last_dim = config.hidden_size <span class="hljs-comment"># 最后一层的维度</span><br>    <br>    <span class="hljs-comment"># 基于最后的纬度开始一些参数的额设计</span><br>    <span class="hljs-keyword">if</span> opt.<span class="hljs-symbol">use_BCE:</span>out_size = <span class="hljs-number">1</span> <span class="hljs-comment"># 损失函数如果使用BCE,则输出大小为1</span><br>    <span class="hljs-keyword">else</span>          <span class="hljs-symbol">:out_size</span> = <span class="hljs-number">2</span> <span class="hljs-comment"># 否则则使用CE,输出大小为2</span><br>    feature_size = <span class="hljs-number">128</span> <span class="hljs-comment"># 设置特征的维度大小</span><br>    <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(last_dim, feature_size) <span class="hljs-comment"># 全连接层1</span><br>    <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(last_dim, feature_size) <span class="hljs-comment"># 全连接层2</span><br>    <span class="hljs-variable language_">self</span>.classifier = nn.Linear(feature_size, out_size) <span class="hljs-comment"># 分类器</span><br>    <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(<span class="hljs-number">0.3</span>) <span class="hljs-comment"># Dropout层</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>,x</span>):<br>    input_ids, attention_mask, token_type_ids = x[<span class="hljs-symbol">:</span>,<span class="hljs-number">0</span>],x[<span class="hljs-symbol">:</span>,<span class="hljs-number">1</span>],x[<span class="hljs-symbol">:</span>,<span class="hljs-number">2</span>] <span class="hljs-comment"># 获取输入</span><br>        x = <span class="hljs-variable language_">self</span>.model(input_ids, attention_mask) <span class="hljs-comment"># 通过模型</span><br>        all_token     = x[<span class="hljs-number">0</span>] <span class="hljs-comment"># 全部序列分词的表征向量</span><br>        pooled_output = x[<span class="hljs-number">1</span>] <span class="hljs-comment"># [CLS]的表征向量+一个全连接层+Tanh激活函数</span><br><br><span class="hljs-comment"># 对于大模型的输出进行特征融合</span><br>        feature1 = all_token.mean(dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># 对全部序列分词的表征向量取均值</span><br>        feature1 = <span class="hljs-variable language_">self</span>.fc1(feature1)    <span class="hljs-comment"># 再输入进全连接层，得到feature1</span><br>        feature2 = pooled_output      <span class="hljs-comment"># [CLS]的表征向量+一个全连接层+Tanh激活函数</span><br>        feature2 = <span class="hljs-variable language_">self</span>.fc2(feature2) <span class="hljs-comment"># 再输入进全连接层，得到feature2</span><br>        feature  = <span class="hljs-number">0.5</span>*feature1 + <span class="hljs-number">0.5</span>*feature2 <span class="hljs-comment"># 加权融合特征</span><br>        feature  = <span class="hljs-variable language_">self</span>.dropout(feature) <span class="hljs-comment"># Dropout</span><br>        x  = <span class="hljs-variable language_">self</span>.classifier(feature) <span class="hljs-comment"># 分类</span><br>        <span class="hljs-keyword">return</span> x<br>        <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>():<br>返回dataset<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_pretrain</span>():<br><span class="hljs-comment"># 定义超参数</span><br>model_save_dir = <br>device = <br>等<br><br><span class="hljs-comment"># 模型初始化</span><br>model = <span class="hljs-variable constant_">MODEL</span>(model_name).to(device)<br><span class="hljs-comment"># 优化器初始化</span><br>opt = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay)<br><span class="hljs-comment"># 损失函数</span><br>loss_func = nn.CrossEntropy()<br><span class="hljs-comment"># 训练过程</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epoches):<br>model.train()<br>train_loss = <span class="hljs-number">0</span><br>train_acc = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> <span class="hljs-symbol">train_loader:</span><br>y_hat = model(X)<br>loss = loss_func(y_hat,y)<br>train_loss += loss.item()<br>opt.zero_grad()<br>loss.backward()<br>opt.step()<br><span class="hljs-comment"># 计算acc</span><br>每间隔一定轮次开始现实验证过程<br>f1<br>recall<br><span class="hljs-comment"># 保存最好的模型</span><br>torch.save(model.<span class="hljs-keyword">module</span>.state_dict(),<span class="hljs-string">&#x27;&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="另外一种方式lora微调"><a class="markdownIt-Anchor" href="#另外一种方式lora微调"></a> 另外一种方式：Lora微调</h2><p>这里更多的是对数据中增加一种prompt,核心代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 构建包括instruction、input、output的数据集</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_df)):<br>    <span class="hljs-comment"># 获取当前行的数据</span><br>    paper_item = train_df.loc[i]<br>    <span class="hljs-comment"># 创建一个字典，包含指令、输入和输出信息</span><br>    tmp = &#123;<br>    <span class="hljs-string">&quot;instruction&quot;</span>: <span class="hljs-string">&quot;Please judge whether it is a medical field paper according to the given paper title and abstract, output 1 or 0, the following is the paper title and abstract --&gt;&quot;</span>,<br>    <span class="hljs-string">&quot;input&quot;</span>: <span class="hljs-string">f&quot;title:<span class="hljs-subst">&#123;paper_item[<span class="hljs-number">1</span>]&#125;</span>,abstract:<span class="hljs-subst">&#123;paper_item[<span class="hljs-number">3</span>]&#125;</span>&quot;</span>,<br>    <span class="hljs-string">&quot;output&quot;</span>: <span class="hljs-built_in">str</span>(paper_item[<span class="hljs-number">5</span>])<br>  &#125;<br>    <span class="hljs-comment"># 将字典添加到结果列表中</span><br>    res.append(tmp)<br>    <br>导入所需的库和模块<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftModel<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel, GenerationConfig, AutoModelForCausalLM<br><br><span class="hljs-comment"># 定义预训练模型的路径</span><br>model_path = <span class="hljs-string">&quot;../chatglm2-6b&quot;</span><br>model = AutoModel.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>).half().cuda()<br>tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 加载 label lora权重</span><br>model = PeftModel.from_pretrained(model, <span class="hljs-string">&#x27;./output/label_xfg&#x27;</span>).half()<br>model = model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-comment"># 使用加载的模型和分词器进行聊天，生成回复</span><br>response, history = model.chat(tokenizer, <span class="hljs-string">&quot;你好&quot;</span>, history=[])<br>response<br></code></pre></td></tr></table></figure><h2 id="one-more-thing"><a class="markdownIt-Anchor" href="#one-more-thing"></a> One more thing</h2><p>为什么是Attention？</p><p>这里<a href="https://blog.tjdata.site/posts/907ab853.html">借助自己笔记的理解</a>，所以attention是啥，如果利用QKV的角度来看比较复杂，如果从简单的回归问题角度来看它只是另外一种计算权重方式的函数。</p><p>但是！attention的作用不仅局限于回归问题，它从seq问题中而来，解决的就是CNN或者RNN对于距离的敏感性。</p><ul><li><p>就像CNN的感受野，对于单层感受野可能是固定好的（与kernel size、padding、step有关），但是多层累积中感受野是可以逐渐扩大的。但是CNN适用于图像的可能解释是一，图像的维度是固定size的（像素无论多大都是有限的）而</p></li><li><p>RNN的感受野，借鉴被人的说法是取决于词元模型和hidden layer，但是由于hidden layer也是具有顺序的特征，它的感受野也不会传递的太远。</p></li><li><p>attention利用attention socre对不同的key进行筛选，来得到一个足够远但是量不大的感受野。选择重要的数据。</p></li></ul><p>警惕大模型？</p><p>调用别人的模型可能很舒服，但要是完全端到端那么也太无力了，这里给出hugging face常见使用的代码片段</p><ol><li><p><a href="https://huggingface.co/course/chapter1/3?fw=pt">Pipeline</a>: 需要良好的网络连接</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-keyword">from</span> transformers import pipeline<br>classifier = pipeline(<span class="hljs-comment">&#x27;sentiment-analysis&#x27;)</span><br>classfifier = (<span class="hljs-comment">&#x27;I have waiting for a hugging face course my whole life&#x27;)</span><br></code></pre></td></tr></table></figure></li><li><p>模型特征提取：Tokenizer、Model、Pretrain</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer,AutoModel<br># tokenizer<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>input = tokenizer(raw_inputs, padding=<span class="hljs-keyword">True</span>, truncation=<span class="hljs-keyword">True</span>, return_tensors=&quot;pt&quot;)<br># model<br>model = AutoModel.from_pretrained(<span class="hljs-keyword">checkpoint</span>)<br># 增加自己的操作 post- process<br></code></pre></td></tr></table></figure><p>Model除了Automodel还包括<code>*Model</code> (retrieve the hidden states)<code>*ForCausalLM</code> <code>*ForMaskedLM</code><br /><code>ForMultipleChoice</code> <code>*ForQuestionAnswering</code> <code>*ForSequenceClassification</code> <code>*ForTokenClassification</code></p></li><li><p>大模型Finetune</p><ol><li><strong>大尺寸预训练</strong>：在这个阶段，模型在大规模的文本数据集上进行预训练。这是一个非监督学习的过程，模型需要预测在给定的文本序列中下一个词是什么。预训练的目标是让模型学会理解和生成人类语言的基本模式。</li><li><strong>指令微调</strong>：在预训练之后，模型会在一个更小但专门针对特定任务的数据集上进行微调。这个数据集通常由人工生成，包含了模型需要学会的任务的特定指令。例如，如果我们想要让模型学会如何进行数学计算，我们就需要提供一些包含数学问题和对应解答的数据。</li><li><strong>RLHF（Reinforcement Learning from Human Feedback）</strong>：这是一个强化学习过程，模型会根据人类提供的反馈进行学习和优化。首先，我们会收集一些模型的预测结果，并让人类评估这些结果的好坏。然后，我们会使用这些评估结果作为奖励，训练模型优化其预测性能。通过这种方式，模型可以学会生成更符合人类期望的结果。</li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;比赛的基本信息在之前的任务一中已经有所介绍，属于文本领域（Natural language processing，NLP）的分类任务，之前的方法是&lt;a href=&quot;https://blog.tjdata.site/posts/29594cb7.html&quot;&gt;采用的是“抽取特征+分类器”的方式来实现的&lt;/a&gt;。这样的方法范式通常需要手动构造许多特征，并且需要较适应的分类器来训练，其中主要的困难点在于（1）手工构造的特征是否有效 （2）如何保证机器学习模型训练中的偏差-方差权衡。但是随着Attention范式的爆火，自动抽取文本特征成为困难，所以在Baseline中我们可以尝试使用BERT（Bidirectional encoder representations from transformers）来自动构造特征，但是和计算机视觉（Computer vision，CV）类似端到端（End-to-end）的思维总是很诱人，所以尝试利用“input+prompt”对训练集进行修改，同时对训练好的大模型进行微调（Fine-tuning）成为一种新的范式。&lt;/p&gt;
&lt;p&gt;本文将借助本次项目中提供的Topline代码（深度学习）进行解读。&lt;/p&gt;</summary>
    
    
    
    <category term="memo" scheme="https://blog.tjdata.site/categories/memo/"/>
    
    
    <category term="datawhale" scheme="https://blog.tjdata.site/tags/datawhale/"/>
    
  </entry>
  
  <entry>
    <title>datawhale_NLP任务一</title>
    <link href="https://blog.tjdata.site/posts/29594cb7.html"/>
    <id>https://blog.tjdata.site/posts/29594cb7.html</id>
    <published>2023-08-18T02:45:43.000Z</published>
    <updated>2023-08-18T02:48:30.698Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><span id="more"></span><h2 id="0x01-问题描述"><a class="markdownIt-Anchor" href="#0x01-问题描述"></a> 0x01 问题描述</h2><p>通过机器学习的方式对论文摘要等信息的理解，来判断该论文是否属于医学领域的文献。<br />数据集中包括publicdata-train,publicdata-test,trainB,数据集中包含标题、作者、摘要和关键词<br />模型：可以采用task1基于文本提取的TF-IDF+LR分类的机器学习方式；也可以task2采用基于Bert大模型微调的方式<br />评价：最终的评价标准采用F1-score来进行分析</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># input：</span><br>Inflammatory Breast Cancer: What to Know About This Unique, Aggressive Breast Cancer.，<br><br>[Arjun Menta, Tamer M Fouad, Anthony Lucci, Huong Le-Petross, Michael C Stauder, Wendy A Woodward, Naoto T Ueno, Bora Lim]，<br><br>Inflammatory breast cancer (IBC) <span class="hljs-keyword">is</span> a rare form of breast cancer that accounts <span class="hljs-keyword">for</span> only <span class="hljs-number">2</span>% to <span class="hljs-number">4</span>% of <span class="hljs-built_in">all</span> breast cancer cases. Despite its low incidence, IBC contributes to <span class="hljs-number">7</span>% to <span class="hljs-number">10</span>% of breast cancer caused mortality. Despite ongoing international efforts to formulate better diagnosis, treatment, <span class="hljs-keyword">and</span> research, the survival of patients <span class="hljs-keyword">with</span> IBC has <span class="hljs-keyword">not</span> been significantly improved, <span class="hljs-keyword">and</span> there are no therapeutic agents that specifically target IBC to date. The authors present a comprehensive overview that aims to assess the present <span class="hljs-keyword">and</span> new management strategies of IBC.，<br><br>Breast changes; Clinical trials; Inflammatory breast cancer; Trimodality care.<br><br><span class="hljs-comment"># output:</span><br><span class="hljs-number">1</span> <span class="hljs-keyword">for</span> yes <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> no<br><br></code></pre></td></tr></table></figure><h2 id="0x02-baseline"><a class="markdownIt-Anchor" href="#0x02-baseline"></a> 0x02 Baseline</h2><p>可以使用CV或者TF来对原始文本进行分析</p><p>之后利用分类器比如LR或者lightGBM进行分类处理</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer,TfidfVectorizer<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(&quot;ignore&quot;)<br><br># <span class="hljs-keyword">Read</span> the dataset<br>df_train = pd.read_csv(<span class="hljs-string">&#x27;../data/train.csv&#x27;</span>)<br>df_valid = pd.read_csv(<span class="hljs-string">&#x27;../data/test.csv&#x27;</span>)<br>df_test = pd.read_csv(<span class="hljs-string">&#x27;../data/testB.csv&#x27;</span>)<br>df_train = df_train.fillna(<span class="hljs-string">&#x27;&#x27;</span>)<br>df_valid.fillna(<span class="hljs-string">&#x27;&#x27;</span>)<br>df_test = df_test.fillna(<span class="hljs-string">&#x27;&#x27;</span>)<br><br># Features<br>df_train[<span class="hljs-string">&#x27;text&#x27;</span>] = df_train.apply(lambda x:<span class="hljs-string">&#x27; &#x27;</span>.<span class="hljs-keyword">join</span>(x.<span class="hljs-keyword">drop</span>(<span class="hljs-string">&#x27;uuid&#x27;</span>).astype(str)),axis=<span class="hljs-number">1</span>)<br>df_valid[<span class="hljs-string">&#x27;text&#x27;</span>] = df_valid.apply(lambda x:<span class="hljs-string">&#x27; &#x27;</span>.<span class="hljs-keyword">join</span>(x.<span class="hljs-keyword">drop</span>(<span class="hljs-string">&#x27;uuid&#x27;</span>).astype(str)),axis=<span class="hljs-number">1</span>)<br>df_test[<span class="hljs-string">&#x27;text&#x27;</span>] = df_test.apply(lambda x:<span class="hljs-string">&#x27; &#x27;</span>.<span class="hljs-keyword">join</span>(x.<span class="hljs-keyword">drop</span>(<span class="hljs-string">&#x27;uuid&#x27;</span>).astype(str)),axis=<span class="hljs-number">1</span>)<br><br># df_train[<span class="hljs-string">&#x27;text&#x27;</span>]<br>vector = TfidfVectorizer().fit(df_train[<span class="hljs-string">&#x27;text&#x27;</span>].tolist())<br><br>vocab = vector.vocabulary_<br>train_vector = vector.<span class="hljs-keyword">transform</span>(df_train[<span class="hljs-string">&#x27;text&#x27;</span>])<br>valid_vector = vector.<span class="hljs-keyword">transform</span>(df_valid[<span class="hljs-string">&#x27;text&#x27;</span>])<br>test_vector = vector.<span class="hljs-keyword">transform</span>(df_test[<span class="hljs-string">&#x27;text&#x27;</span>])<br><br>df_trainv = pd.DataFrame(train_vector.toarray(),<span class="hljs-keyword">columns</span>=vocab)<br>df_validv = pd.DataFrame(valid_vector.toarray(),<span class="hljs-keyword">columns</span>=vocab)<br>df_testv = pd.DataFrame(test_vector.toarray(),<span class="hljs-keyword">columns</span>=vocab)<br><br>df_train.describe()<br><br># lightGBM model<br><span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb <br>data = lgb.Dataset(df_trainv,df_train[<span class="hljs-string">&#x27;label&#x27;</span>])<br># data_val = lgb.Dataset(df_validv,df_valid[<span class="hljs-string">&#x27;label&#x27;</span>])<br>params = &#123;<br>    <span class="hljs-string">&#x27;boosting_type&#x27;</span>: <span class="hljs-string">&#x27;gbdt&#x27;</span>,<br>    <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;binary&#x27;</span>, <br>    <span class="hljs-string">&#x27;metric&#x27;</span>: <span class="hljs-string">&#x27;binary_logloss&#x27;</span>,<br>    <span class="hljs-string">&#x27;num_leaves&#x27;</span>: <span class="hljs-number">31</span>,<br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">0.05</span>,<br>    <span class="hljs-string">&#x27;feature_fraction&#x27;</span>: <span class="hljs-number">0.9</span>,<br>    <span class="hljs-string">&#x27;bagging_fraction&#x27;</span>: <span class="hljs-number">0.8</span>,<br>    <span class="hljs-string">&#x27;bagging_freq&#x27;</span>: <span class="hljs-number">5</span>, <br>    <span class="hljs-string">&#x27;verbose&#x27;</span>: <span class="hljs-number">0</span><br>&#125;<br>model = lgb.train(params, data)<br><br>y_pred = model.predict(test_vector)<br>y_pred = np.<span class="hljs-keyword">where</span>(y_pred&gt;=<span class="hljs-number">0.5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)<br>df_test[<span class="hljs-string">&#x27;label&#x27;</span>] = y_pred<br><br>df_test[<span class="hljs-string">&#x27;Keywords&#x27;</span>] = df_test[<span class="hljs-string">&#x27;title&#x27;</span>].fillna(<span class="hljs-string">&#x27;&#x27;</span>)<br>df_test[[<span class="hljs-string">&#x27;uuid&#x27;</span>, <span class="hljs-string">&#x27;Keywords&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>]].to_csv(<span class="hljs-string">&#x27;../data/task1_lightGBM.csv&#x27;</span>, <span class="hljs-keyword">index</span>=<span class="hljs-keyword">None</span>)<br></code></pre></td></tr></table></figure><h2 id="0x03-结果"><a class="markdownIt-Anchor" href="#0x03-结果"></a> 0x03 结果</h2><p>最终的提交结果为</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230818104801276.png" alt="lightGBM结果" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要&lt;/p&gt;</summary>
    
    
    
    <category term="memo" scheme="https://blog.tjdata.site/categories/memo/"/>
    
    
    <category term="datawhale" scheme="https://blog.tjdata.site/tags/datawhale/"/>
    
  </entry>
  
  <entry>
    <title>工科生研1的MacbookPro14深度使用感受</title>
    <link href="https://blog.tjdata.site/posts/f57a89c.html"/>
    <id>https://blog.tjdata.site/posts/f57a89c.html</id>
    <published>2023-08-07T07:04:17.000Z</published>
    <updated>2023-08-07T10:58:15.084Z</updated>
    
    <content type="html"><![CDATA[<p>在研究生使用半年的Macbook Air（2020M1）之后由于存储、屏幕等原因，转向具有诸多优点的MacbookPro14（2021），在深度磨合半年之后介绍这台设备的优点。从工业设计、外部接口、硬件配置、屏幕素质介绍使用感受，并从文件处理、知识管理、系统工具、娱乐角度介绍软件资源，最后给出文件管理和待办管理的工作流。</p><p>本文主要介绍2021款MacBook Pro 14inch, 具体的<a href="https://www.apple.com/hk/en/macbook-pro-14-and-16/">参数规格</a></p><span id="more"></span><h2 id="0x01-使用感受"><a class="markdownIt-Anchor" href="#0x01-使用感受"></a> 0x01 使用感受</h2><p>一下从工业设计、外部接口、硬件配置、屏幕素质角度介绍使用感受。</p><p>从<strong>工业设计</strong>角度，外观上虽然失去了Macbook Air楔形设计的优美，但是坚实的底座和方正的设计会让人觉得它很踏实。但是真的很重，相比Air的重真的需要加一个档次的心理准备。另外这次的键盘相比air更重，按键也更大，特别是非常大的esc键，非常舒服。</p><p>从<strong>外部接口</strong>来看尤其是 3thunderbolt4+MagSafe3+headphone jack +HDMI+SDXC 真是非常实用，所以直接抛弃贝尔金七合一的拓展坞。HDMI在外出接投影仪的时候真的非常好用，SD卡在导出相机照片的时候非常方便。美中不足的有两点：一是没有USB-A接口，在日常使用移动硬盘和连接安卓手机不方便 二是HDMI和SD卡的速度有一定的限制。</p><blockquote><p>观点1：外设永远没有自带方便</p></blockquote><p>从<strong>硬件配置</strong>角度，后续我会详细结合软件操作来介绍。这里总的体验感受是M1pro相比M1从速度上真的会快很多，但是功耗也会快很多。但是菜狗很少会遇到性能瓶颈（doge），日常使用的续航大概只有6～12小时不等（之前的Air大概有12～18小时）。另外对于电脑来说，256G还是太小了，虽然我有一台4T的windows台式机，但还是感觉移动办公的电脑存储也不能太低（点名微信）。同时虽然移动硬盘很便宜，但是使用建议参考观点1。外放的感觉超级赞！远超iPhone 13pro，略超iPad pro12.9inch。</p><p>从<strong>屏幕素质</strong>角度，3024*1964的254ppi的屏幕日常使用非常的惊艳，同时基于miniLED观看HDR视频可以高达1600nit。但是总感觉屏幕太亮很刺眼，看时间长了屏幕有点头晕。但是理论上基于miniLED的LCD面板频闪很低，具体原因未知。将其显示从XDR调整为Display，同时开启True Tone和Night shift会稍微好一点。刘海没有Face ID很让人失望，但是刘海正好处于菜单栏和状态栏之间的交界处，这样的设计可以正好将显示上移，在屏下技术不完善的今天，日常使用的关注点还是很少的。</p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230807153128603.png" alt="显示选项" style="zoom:50%;" /><h2 id="0x02-软件资源库"><a class="markdownIt-Anchor" href="#0x02-软件资源库"></a> 0x02 软件资源库</h2><p>这里主要介绍具有GUI的应用程序，通常可以分为以下，推荐下载软件的网站：</p><p><a href="https://www.macyy.cn/">macyy</a>、<a href="https://xclient.info/">xclient</a>、<a href="https://appnee.com/category/mac-software/">AppNee</a>以及相关telegram群聊</p><h3 id="21-文件处理"><a class="markdownIt-Anchor" href="#21-文件处理"></a> 2.1 文件处理</h3><ul><li>Office 365三件套：Word、Excel、PowerPoint</li><li>macOS自带：Keynote、iMovie</li><li>Typora：markdown的最佳工具</li><li>PDF expert和预览：PDF处理工具</li><li>Tex Studio：不会用VScode来编辑Latex，还是用这个</li><li>nPlayer：相比IINA打开HDR视频会更有优势</li><li>VS code：最强的文本编辑器，sublime text有点麻烦的。</li><li>GifSki：压缩视频生成Gif</li><li>SVG view：查看SVG</li><li>R studio：处理R文件</li></ul><h3 id="22-知识管理"><a class="markdownIt-Anchor" href="#22-知识管理"></a> 2.2 知识管理</h3><ul><li>Notion：all in one的笔记软件</li><li>Zotero7: 文献管理软件</li><li>Marginnote3: 桌面级的读书笔记摘要工具</li><li>微信读书：macOS可以直接下载！</li><li>xmind：生成思维导图很好看，但是现在逐渐转向PPT</li><li>One drive：拼车的Office365非常香，iCloud太贵了！</li><li>Dash：API管理</li><li>Cubox：真的知识管理软件，稍后读。</li><li>Note+Reminder+Calendar：系统自带的GTD管理系统</li></ul><h3 id="23-系统功能"><a class="markdownIt-Anchor" href="#23-系统功能"></a> 2.3 系统功能</h3><ul><li>iStat Menus Status，状态栏系统状态检测，可以显示现有功率！</li><li>Alfred，相比spotlight而言功能更多，但是文件检索的效率略低。有的时候one drive文件搜索不到</li><li>Bob：翻译软件，配合自带的翻译使用非常流畅，同时可以下载插件</li><li>Clash X：网络增强</li><li>Terminus：终端增强</li><li>Oh-My-Zsh：终端增强</li><li>Barender4：状态栏增强</li><li>Rectangle：窗口管理</li><li>iShot pro：截图</li><li>Better zip：解压缩</li><li>cheetsheet：快捷键提醒</li><li>超级右键：右键增强，但是有的时候增强也并不是好事</li><li>Monitor Control：管理显示器亮度</li></ul><blockquote><p>观点2：局部便捷化会破坏系统一致性，而产生更大的混乱。</p></blockquote><h3 id="24-娱乐"><a class="markdownIt-Anchor" href="#24-娱乐"></a> 2.4 娱乐</h3><ul><li>微信、telegram：即时通讯</li><li>Chrome、Safari、Arc：浏览器，通往互联网的窗口</li><li>Steam：有一部分Mac可以玩的，比如人类一败涂地、胡闹厨房等等</li><li>Music：自带的yyds</li><li>百度网盘+阿里云盘</li></ul><h2 id="0x03-工作流分享"><a class="markdownIt-Anchor" href="#0x03-工作流分享"></a> 0x03 工作流分享</h2><p>在上述的软件基础上，我们给出自己的工作流，主要包括3.1文件管理、3.2待办管理（GTD）、3.3 同步管理等等</p><h3 id="31-文件管理"><a class="markdownIt-Anchor" href="#31-文件管理"></a> 3.1 文件管理</h3><p>经过长时间的摸索，给出<strong>结合领域知识+标签的Finder文件管理方式</strong>。</p><p>树状的文件资源管理器在Linux、macOS、Windows中几乎达成共识。但是macOS依旧保留“标签”这个重要属性，标签可以将文件之间的关系从<strong>树的节点</strong>转换成为平权的<strong>图的节点</strong>，这样带来的好处是自由行的增加，坏处是管理的困难。通常在最终管理只会记住<strong>度比较大的节点</strong>。我们会面临的困境是，树状分类的文件夹不足以代表内部文件的所有特征，而标签的文件系统管理困难。所以我们需要在**合理文件夹分类和恰当的标签管理中做权衡。**给出的解决方案是：</p><ol><li>领域知识鲜明的设置领域文件夹，否则按照格式管理</li><li>标签仅表示状态，和额外的一次性的元信息</li></ol><p>首先对于第一点，感觉Flomo标签软件传授的笔记理念（虽然我不用flomo），其倡导的卡片分类我发现也适用于文件管理。系统默认会将用户文件分为：文档、音乐、视频、照片这样的格式。如果我们只按照格式来存储这样的文件，我们会发现我们的Project1需要的文档、代码、图片等等文件散落在不同的文件夹，不利于整理。因此我们可以设计另外一个专放领域知识的文件夹，里面可以存储课程1、课程2、项目1、项目2,…的领域文件夹，对于小型文件不易区分的文件存储在不同格式的文件夹，pdf文件存储在文档、png存储在图片文件夹。充分利用不同文件的<strong>强关系</strong></p><p>对于第二点，受益于spotlight、everything这样的文件，我们可以用哈希的方式来搜索我们的文件。因此我设计<strong>标签</strong>并不是对文件属性的强分类，而是对文件的信息补充，包括状态信息、所属成员、以及独一无二的meta信息，标签可以加的足够多来保证后续可以搜索到。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230807164511126.png" alt="领域知识和标签的Finder文件管理" /></p><h3 id="32-待办管理gtd"><a class="markdownIt-Anchor" href="#32-待办管理gtd"></a> 3.2 待办管理（GTD）</h3><p>《Get thing done》参考这本书可以给对于日常任务管理提升一个级别。在研究生之前的任务可能就是简单的复习，写题等等。回想生活那么简单就是因为自己是个单线程生物。但是研究生中不同任务的并行，有的需要跑代码，有的时候需要写文档，还会有偶发的中断任务去处理，由于我们并不是机器，任务也是死板的输入输出。因此在混乱的任务中找到自己的工作流是GTD需要做的事情。</p><p>之前尝试过滴答清单，最好发现还是多端同步的苹果自带的日历Calendar、提醒事项Reminder、笔记Note好用。</p><p>首先我们可以对任务分为三类：Event、Todo、DDL。这里的命名和书里面的可能不一样～</p><ol><li>Event：通常放在Calendar中，<strong>指的是有确定时间发生的，不需要check是否完成的</strong>。比如上课（不去也没关系）、比如演唱会抢票（抢不到也没关系）、比如开组会（是否完成也不需要确认）等等；</li><li>Todo：通常放在Reminder中，可能叫Task比较好，指的是<strong>需要check完成的，但是一般没有时间</strong>。比如买袜子、看一本书等等。这一类任务通常需要根据<strong>优先级来排序</strong>；</li><li>DDL：放在Reminder中，<strong>指的是需要check完成的，一般有截止日期。<strong>比如6月1号之前提交论文，比如下一月十号之前完成一项汇报。这一类任务需要</strong>根据截止日期来排序</strong>。</li></ol><p>分类好之后我们需要包括输入，通常即时可以用Siri、Alfred收集自己的任务，或者以周或月为单位整理自己的任务（用备忘录），之后再整理送入自己的系统。</p><blockquote><p>观点3: 友好的系统设计可以保证它易用，但是维护一个系统是需要持之以恒的用下去，并从中得到正反馈。</p></blockquote><h3 id="33-知识管理"><a class="markdownIt-Anchor" href="#33-知识管理"></a> 3.3 知识管理</h3><p>基于Notion实现All in one并不现实，不同的软件具有应对不同场景的优势。Cubox用于稍后阅读、Notion适用于笔记梳理、Note适用于即刻笔记、Marginnote适用于文档阅读等等，自己路需要自己去摸索！</p><p><a href="https://blog.tjdata.site/posts/3be9bc5f.html">Mermaid使用教程</a></p><p><a href="https://blog.tjdata.site/posts/d49191c7.html">知识管理流程</a></p><p><a href="https://blog.tjdata.site/posts/cc1a6fba.html">网络代理使用</a></p><p><a href="https://blog.tjdata.site/posts/de0c5bf1.html">预览工具使用</a></p><p>更多的参考</p><p><a href="https://blog.tjdata.site/posts/9148fb62.html">工科生研0的Macbook Air M1 13inch（16+256）深度使用感受</a></p><h2 id="one-more-thing"><a class="markdownIt-Anchor" href="#one-more-thing"></a> One more thing</h2><blockquote><p>Mac终究只是连接到网络世界的工具，屏幕背后的人是最重要的！</p></blockquote><p>在使用Mac的过程中，我可能陷入追求机器性能极致迷茫，似乎M1pro的H.264编码很厉害，似乎视频剪辑很厉害。但是如果你不需要这些功能，你不需要用这些功能来证明你确实需要M1pro，而是要在实现自己本身目的的前提下最大化的利用它并判断是否合适！</p><p>希望能启发你的一些思考和讨论！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在研究生使用半年的Macbook Air（2020M1）之后由于存储、屏幕等原因，转向具有诸多优点的MacbookPro14（2021），在深度磨合半年之后介绍这台设备的优点。从工业设计、外部接口、硬件配置、屏幕素质介绍使用感受，并从文件处理、知识管理、系统工具、娱乐角度介绍软件资源，最后给出文件管理和待办管理的工作流。&lt;/p&gt;
&lt;p&gt;本文主要介绍2021款MacBook Pro 14inch, 具体的&lt;a href=&quot;https://www.apple.com/hk/en/macbook-pro-14-and-16/&quot;&gt;参数规格&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="memo" scheme="https://blog.tjdata.site/categories/memo/"/>
    
    
    <category term="Mac" scheme="https://blog.tjdata.site/tags/Mac/"/>
    
  </entry>
  
  <entry>
    <title>EM算法HMM和CRF</title>
    <link href="https://blog.tjdata.site/posts/a77ab19e.html"/>
    <id>https://blog.tjdata.site/posts/a77ab19e.html</id>
    <published>2023-07-27T08:56:45.000Z</published>
    <updated>2023-08-12T07:34:14.252Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><span id="more"></span><p>正文</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>有意思的两道二叉树的题目</title>
    <link href="https://blog.tjdata.site/posts/67653c3b.html"/>
    <id>https://blog.tjdata.site/posts/67653c3b.html</id>
    <published>2023-07-12T13:16:48.000Z</published>
    <updated>2023-07-27T11:09:23.910Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><span id="more"></span><p>正文</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>关于tensorboard介绍的翻译</title>
    <link href="https://blog.tjdata.site/posts/5ddcb6b8.html"/>
    <id>https://blog.tjdata.site/posts/5ddcb6b8.html</id>
    <published>2023-07-05T01:49:45.000Z</published>
    <updated>2023-07-05T02:18:18.507Z</updated>
    
    <content type="html"><![CDATA[<p>原文地址<a href="https://blog.tensorflow.org/2019/12/introducing-tensorboarddev-new-way-to.html?hl=zh-cn&amp;_gl=1*1vgnixf*_ga*OTk3NjMzNjUuMTY4ODUyMTc0MQ..*_ga_W0YLR4190T*MTY4ODUyMTc0MC4xLjEuMTY4ODUyMTc0MC4wLjAuMA..">《Introducing Tensor Board.dev:  a new way to share your ML experiment results》</a></p><span id="more"></span><p>Tensorboard作为tensorflow的可视化组件，通常为研究者和工程师提供可视化和理解机器学习的实验结果。它可以被用来</p><ul><li>Tracking experiment metrics(记录实验指标)</li><li>Visualizing models(可视化模型)</li><li>Profiling ML programs(分析机器学习运行过程)</li><li>Visualizing hyperparameter tuning experiments(可视化超参数调节实验)</li></ul><p>除了可以简单的可视化模型，Tensorboard也可以多端合作。你可能想分享超参变动对结果的影响、解释一个复杂的训练过程和从失败中获取帮助</p><p>我们之前加过人们尝试分享TensorBoard的截图来实现，但是截图并不是可交互的并且没有展示所有的细节。在Google，研究人员和工程师往往通过tensorboard 可视化结果给团队成员来讨论观点。我们的目标是为更广泛的社区提供这个能力</p><p>这也是为什么我们开发tensorboard.dev：一项托管服务（目前正在预览中），使您能够轻松地免费托管、跟踪和共享您的ML实验。只需上传您的TensorBoard日志，即可收到一个每个人都可以查看的链接，无需安装或设置。</p><p>If  a picture is worth a thousand words, we believe an interactive TensorBoard can be even more valuable.</p><h2 id="使用逻辑"><a class="markdownIt-Anchor" href="#使用逻辑"></a> 使用逻辑</h2><p>通过SummaryWriter()类来实现数据保存和展示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorborad <span class="hljs-keyword">import</span> SummaryWriter<br>write = SummaryWrite(<span class="hljs-string">&#x27;../logs&#x27;</span>) <span class="hljs-comment"># 指定log保存的位置</span><br><br><span class="hljs-comment"># 常用方法</span><br>add_scalar(tag, scalar_value, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>, new_style=<span class="hljs-literal">False</span>, double_precision=<span class="hljs-literal">False</span>)<br>add_scalars(main_tag, tag_scalar_dict, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>)<br>add_histogram(tag, values, global_step=<span class="hljs-literal">None</span>, bins=<span class="hljs-string">&#x27;tensorflow&#x27;</span>, walltime=<span class="hljs-literal">None</span>, max_bins=<span class="hljs-literal">None</span>)<br>add_image(tag, img_tensor, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>, dataformats=<span class="hljs-string">&#x27;CHW&#x27;</span>)<br>add_images(tag, img_tensor, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>, dataformats=<span class="hljs-string">&#x27;NCHW&#x27;</span>)<br>add_figure(tag, figure, global_step=<span class="hljs-literal">None</span>, close=<span class="hljs-literal">True</span>, walltime=<span class="hljs-literal">None</span>)<br>add_video(tag, vid_tensor, global_step=<span class="hljs-literal">None</span>, fps=<span class="hljs-number">4</span>, walltime=<span class="hljs-literal">None</span>)<br>add_audio(tag, snd_tensor, global_step=<span class="hljs-literal">None</span>, sample_rate=<span class="hljs-number">44100</span>, walltime=<span class="hljs-literal">None</span>)<br>add_text(tag, text_string, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>)<br>add_graph(model, input_to_model=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">False</span>, use_strict_trace=<span class="hljs-literal">True</span>)<br>add_embedding(mat, metadata=<span class="hljs-literal">None</span>, label_img=<span class="hljs-literal">None</span>, global_step=<span class="hljs-literal">None</span>, tag=<span class="hljs-string">&#x27;default&#x27;</span>, metadata_header=<span class="hljs-literal">None</span>)<br><br><br></code></pre></td></tr></table></figure><h2 id="实际应用"><a class="markdownIt-Anchor" href="#实际应用"></a> 实际应用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-built_in">input</span> = torch.zeros(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>)<br>net = nn.Sequential(nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>), nn.ReLU(), nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>))<br>writer = SummaryWriter(<span class="hljs-string">&#x27;./logs&#x27;</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>,np.random.random(),epoch)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/test&#x27;</span>,np.random.random(),epoch)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuarcy/train&#x27;</span>,np.random.random(),epoch)<br>    writer.add_graph(net,input_to_model=<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;原文地址&lt;a href=&quot;https://blog.tensorflow.org/2019/12/introducing-tensorboarddev-new-way-to.html?hl=zh-cn&amp;amp;_gl=1*1vgnixf*_ga*OTk3NjMzNjUuMTY4ODUyMTc0MQ..*_ga_W0YLR4190T*MTY4ODUyMTc0MC4xLjEuMTY4ODUyMTc0MC4wLjAuMA..&quot;&gt;《Introducing Tensor Board.dev:  a new way to share your ML experiment results》&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="memo" scheme="https://blog.tjdata.site/categories/memo/"/>
    
    
    <category term="torch" scheme="https://blog.tjdata.site/tags/torch/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle竞赛手写问题识别</title>
    <link href="https://blog.tjdata.site/posts/eeffea24.html"/>
    <id>https://blog.tjdata.site/posts/eeffea24.html</id>
    <published>2023-06-05T10:17:10.000Z</published>
    <updated>2023-06-05T10:44:00.698Z</updated>
    
    <content type="html"><![CDATA[<p>最近看了<a href="https://peps.python.org/pep-0008/">Python的PEP8</a>，其中对于代码规范的要求是注释主要选择英文，本次项目的进行中也在竭力的进行，但是发现效果并不是很好，如果为了别人的代码可读性的标准而导致自己的代码低效，这个显然是不可取的。never mind，本章主要<a href="https://www.kaggle.com/c/digit-recognizer">介绍CNN届的Hello world————手写数字识别。</a>自己从无到有的敲一下。</p><span id="more"></span><h2 id="项目介绍"><a class="markdownIt-Anchor" href="#项目介绍"></a> 项目介绍</h2><p>MNIST(modifed national institute of standard and technology) is the “Hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classficiation algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for resource for researchers and leaaners alike.</p><p>In this competition, your goal is to correctly identify digits from a dataset from a dataset of tens of thousands of handwritten images. We have curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms  to learn first-hand works well and how techique compare.</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230605182432773.png" alt="Kaggle页面简介" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <span class="hljs-comment"># 线性代数</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <span class="hljs-comment"># 数据预处理</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-comment"># 绘图</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br>device = torch.device(<span class="hljs-string">&#x27;mps&#x27;</span>) <span class="hljs-comment"># mps需要torch&gt;=1.13</span><br></code></pre></td></tr></table></figure><h2 id="数据集"><a class="markdownIt-Anchor" href="#数据集"></a> 数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># csv -&gt; pd.DataFrame</span><br>trainDF = pd.read_csv(<span class="hljs-string">&#x27;../DigitRecongnier/data/train.csv&#x27;</span>)<br><br><span class="hljs-comment"># pd.DataFrame -&gt; numpy.array</span><br>labels = trainDF[<span class="hljs-string">&#x27;label&#x27;</span>].values<br>train = trainDF.drop([<span class="hljs-string">&#x27;label&#x27;</span>],axis=<span class="hljs-number">1</span>)<br>features = train.values<br><br><span class="hljs-comment"># 划分数据集和测试集</span><br>features_train,features_test,targets_train,targets_test = train_test_split(features,labels,test_size=<span class="hljs-number">0.2</span>,random_state=<span class="hljs-number">42</span>)<br><span class="hljs-comment"># def split_data(features,labels,per):</span><br><span class="hljs-comment">#     &#x27;&#x27;&#x27;按照per划分数据集&#x27;&#x27;&#x27;</span><br><span class="hljs-comment">#     split_len=int(per*len(labels))</span><br><span class="hljs-comment">#     trainFeatures = features[:split_len]</span><br><span class="hljs-comment">#     trainLabels = labels[:split_len]</span><br><span class="hljs-comment">#     valFeatures = features[split_len:]</span><br><span class="hljs-comment">#     valLabels = labels[split_len:]</span><br><span class="hljs-comment">#     return trainFeatures,trainLabels,valFeatures,valLabels</span><br><span class="hljs-comment"># trainFeatures,trainLabels,valFeatures,valLabels = split_data(features,labels,0.8)</span><br><br><span class="hljs-comment"># numpy.array -&gt; tensor</span><br>featuresTrain = torch.from_numpy(features_train)/<span class="hljs-number">255</span><br>targetsTrain = torch.from_numpy(targets_train)<br>featuresTest = torch.from_numpy(features_test)/<span class="hljs-number">255</span><br>targetsTest = torch.from_numpy(targets_test)<br><br><span class="hljs-comment"># trainF = torch.tensor(trainFeatures)</span><br><span class="hljs-comment"># trainFS = trainF.reshape((-1,28,28))</span><br><span class="hljs-comment"># # 证明前后是一样的</span><br><span class="hljs-comment"># trainF[0] == trainFS[0].reshape(1,-1)</span><br><br><span class="hljs-comment"># # tensor -&gt; dataset(using tensordataset) </span><br><span class="hljs-comment"># # 缺点是不能使用transform</span><br><span class="hljs-comment"># train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)</span><br><span class="hljs-comment"># test = torch.utils.data.TensorDataset(featuresTest,targetsTest)</span><br><br><span class="hljs-comment"># tensor -&gt; data (using super the dataset)</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TensorDatasetSelf</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; tensor dataset 继承 dataset 只需要重载len和getitem就可以&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,features,target,transform=<span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.datatensor = features<br>        self.labeltensor = target<br>        self.transform = transform <br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.datatensor.size(<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        cur = self.datatensor[index]<br>        X = cur.reshape((<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>))<br>        <span class="hljs-keyword">return</span> X,self.labeltensor[index]<br><br><span class="hljs-comment"># 定义一个变换，将数据缩放到 0 到 1 的范围内</span><br><span class="hljs-comment"># 坑：Resize返回一个image</span><br><span class="hljs-comment"># transform = transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor()])</span><br><br>train  = TensorDatasetSelf(featuresTrain,targetsTrain)<br>test = TensorDatasetSelf(featuresTest,targetsTest)<br></code></pre></td></tr></table></figure><p><strong>Hint01</strong> : torch构建dataset的几种方式</p><ol><li>自定义Dataset类，可以通过继承torch.utils.data.Dataset来实现，需要重写__len__(),<strong><strong>getitem</strong></strong>()类</li><li>使用tensorDataset类，是一个包装器类，实现从tensor到dataset的直接的转变</li></ol><p><strong>Hint02</strong> Dataset中添加数据预处理</p><p>我们输入features、labels进入dataset类之后，在输出的时候我们可能希望对其进行一些预处理</p><p>比如图像增强常用的预处理、增强、旋转等等</p><p>from torchvision import transforms</p><p>transforms.compose()类似pipeline，来实现一系列的transform操作，常见的transform操作有：</p><ol><li>transforms.ToTensor()</li><li>自定义transforms</li></ol><h2 id="数据迭代器"><a class="markdownIt-Anchor" href="#数据迭代器"></a> 数据迭代器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 构建数据集的超参数设置</span><br>batchsize = <span class="hljs-number">128</span><br>epoches = <span class="hljs-number">20</span><br><br><span class="hljs-comment"># dataset -&gt; dataloader</span><br>train_iter = DataLoader(train,batch_size=batchsize,shuffle=<span class="hljs-literal">False</span>)<br>test_iter = DataLoader(test,batch_size=batchsize,shuffle=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># tensor 也可以 plt！</span><br><span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>    <span class="hljs-built_in">print</span>(X.shape)<br>    cur = X[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    plt.imshow(cur)<br>    plt.show()<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230605182701970.png" alt="手写数字" /></p><h2 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h2><p>经典的CNN方面的论文，无论他最后需要实现的有多复杂，在深度学习的pipeline中，只是占据着模型构建的角色。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用经典的CNN模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;input :28*28,output:10&#x27;&#x27;&#x27;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>    <br>       <span class="hljs-comment"># Convolution 1</span><br>        self.cnn1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br>        self.relu1 = nn.ReLU()<br>        <br>        <span class="hljs-comment"># Max pool 1</span><br>        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)<br>     <br>        <span class="hljs-comment"># Convolution 2</span><br>        self.cnn2 = nn.Conv2d(in_channels=<span class="hljs-number">16</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br>        self.relu2 = nn.ReLU()<br>        <br>        <span class="hljs-comment"># Max pool 2</span><br>        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)<br>        <br>        <span class="hljs-comment"># Fully connected 1</span><br>        self.fc1 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">16</span>, <span class="hljs-number">10</span>) <br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># Convolution 1</span><br>        out = self.cnn1(x)<br>        out = self.relu1(out)<br>        <br>        <span class="hljs-comment"># Max pool 1</span><br>        out = self.maxpool1(out)<br>        <br>        <span class="hljs-comment"># Convolution 2 </span><br>        out = self.cnn2(out)<br>        out = self.relu2(out)<br>        <br>        <span class="hljs-comment"># Max pool 2 </span><br>        out = self.maxpool2(out)<br>        <br>        <span class="hljs-comment"># flatten</span><br>        <span class="hljs-comment"># out = out.view(1, -1)</span><br>        out = out.view(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Linear function (readout)</span><br>        out = self.fc1(out)<br>        <br>        <span class="hljs-keyword">return</span> out<br><br>model  = LeNet()<br><br><span class="hljs-comment"># testTensor = torch.randint(0,255,(1,28,28),dtype=torch.float)</span><br><span class="hljs-comment"># model (testTensor)</span><br></code></pre></td></tr></table></figure><h2 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">loss = nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><h2 id="优化器"><a class="markdownIt-Anchor" href="#优化器"></a> 优化器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">0.03</span><br>opt = torch.optim.SGD(model.parameters(),lr=learning_rate)<br></code></pre></td></tr></table></figure><h2 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># trian with(data_iter,model,loss,optimizer)</span><br><br><span class="hljs-comment"># 将模型添加到device中</span><br>model =model.to(device)<br><br><span class="hljs-comment"># 训练多个epoch</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoches):<br>    <span class="hljs-comment"># 训练多个iter</span><br>    <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> train_iter:<br>        <span class="hljs-comment"># </span><br>        X,y = X.to(device),y.to(device)<br>        model.train()<br>        opt.zero_grad()<br>        l = loss(model(X),y)<br>        l.mean().backward()<br>        opt.step()<br><br><span class="hljs-comment"># to choice a better model，this should be train ata</span><br></code></pre></td></tr></table></figure><h2 id="测试"><a class="markdownIt-Anchor" href="#测试"></a> 测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">valDF = pd.read_csv(<span class="hljs-string">&#x27;../DigitRecongnier/data/test.csv&#x27;</span>)<br>valtensor = torch.tensor(valDF.values,dtype=torch.<span class="hljs-built_in">float</span>,device=device)<br>valtensor = valtensor.view((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>))<br>result = model(valtensor)<br>res = torch.argmax(result,dim=<span class="hljs-number">1</span>)<br>index =pd.Series(np.arange(<span class="hljs-number">1</span>,<span class="hljs-number">28001</span>))<br>res =res.to(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br>label = pd.Series(res)<br>df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;ImageId&#x27;</span>:index,<span class="hljs-string">&#x27;Label&#x27;</span>:label&#125;)<br>df.to_csv(<span class="hljs-string">&#x27;../DigitRecongnier/data/sample_submission.csv&#x27;</span>,index=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230605184334337.png" alt="提交结果" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近看了&lt;a href=&quot;https://peps.python.org/pep-0008/&quot;&gt;Python的PEP8&lt;/a&gt;，其中对于代码规范的要求是注释主要选择英文，本次项目的进行中也在竭力的进行，但是发现效果并不是很好，如果为了别人的代码可读性的标准而导致自己的代码低效，这个显然是不可取的。never mind，本章主要&lt;a href=&quot;https://www.kaggle.com/c/digit-recognizer&quot;&gt;介绍CNN届的Hello world————手写数字识别。&lt;/a&gt;自己从无到有的敲一下。&lt;/p&gt;</summary>
    
    
    
    <category term="ML&amp;DL" scheme="https://blog.tjdata.site/categories/ML-DL/"/>
    
    
    <category term="Kaggle" scheme="https://blog.tjdata.site/tags/Kaggle/"/>
    
  </entry>
  
  <entry>
    <title>CS229局部线性回归review和实现</title>
    <link href="https://blog.tjdata.site/posts/994c246.html"/>
    <id>https://blog.tjdata.site/posts/994c246.html</id>
    <published>2023-05-31T10:29:14.000Z</published>
    <updated>2023-05-31T10:40:35.938Z</updated>
    
    <content type="html"><![CDATA[<p>在学习Torch之际，正好结合沐神的《动手学深度学习》和CS229中的局部线性回归讲义实践一下！终于跑通了第一个属于自己的代码！！</p><span id="more"></span><h2 id="0x01-什么是线性回归"><a class="markdownIt-Anchor" href="#0x01-什么是线性回归"></a> 0x01 什么是线性回归？</h2><p>在之前的CS229中，线性回归（Linear regression）作为机器学习的Hello world，让我们了解到学习模型（supervised learning algorithm）会包括几个部分：</p><ol><li>数据集（dataset）</li><li>假设（hypothesis）</li><li>损失函数（loss function）</li><li>优化方法（optimization method）</li></ol><p>详细可见 <a href="https://blog.tjdata.site/posts/7a22647b.html">cs229-01线性模型</a></p><h2 id="0x02-什么是局部线性回归"><a class="markdownIt-Anchor" href="#0x02-什么是局部线性回归"></a> 0x02 什么是局部线性回归？</h2><p>也就是给梯度一个权重</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230531183236712.png" alt="LSR的描述" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230531183226036.png" alt="LSR图片示例" /></p><h2 id="0x03-从零开始的代码实现"><a class="markdownIt-Anchor" href="#0x03-从零开始的代码实现"></a> 0x03 从零开始的代码实现</h2><p>在了解算法的基础上，如何使用代码构建出来是另外一个方面。两者可以是互补的！<br />参照Tensorflow2.0 八股文的描述，我们将深度学习网络构建的过程也可以分为几个典型的步骤：</p><ol><li>构建dataset</li><li>根据dataset生成不同的dataloader，方便后续批量梯度下降(batch gradient descent)</li><li>建立Model（根据hypothesis）</li><li>选择损失函数 loss function</li><li>优化器选择</li><li>超参数的设置 lr、epoch、batchsize</li><li>学习过程，批量梯度下降</li><li>评价指标，对于regression的分析常见的包括MSE、MAPE等等</li></ol><h3 id="31-定义数据集"><a class="markdownIt-Anchor" href="#31-定义数据集"></a> 3.1 定义数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gen_data</span>(<span class="hljs-params">w,b,num_examples</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;生成y=Xw+b的噪声&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># dtype=float</span><br>    X = torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,(num_examples,<span class="hljs-built_in">len</span>(w)),dtype=<span class="hljs-built_in">float</span>)<br>    y = torch.matmul(X,w)<br>    y += torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,y.shape,dtype=<span class="hljs-built_in">float</span>)<br>    <span class="hljs-comment"># reshape的意义在于将vector变成为matrix，vector应该是N，而matrix是N*1</span><br>    <span class="hljs-keyword">return</span> X,y.reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>true_w=torch.tensor([<span class="hljs-number">2</span>,-<span class="hljs-number">3.4</span>],dtype=<span class="hljs-built_in">float</span>)<br>true_b=<span class="hljs-number">4.2</span><br>features,labels=gen_data(true_w,true_b,<span class="hljs-number">1000</span>)<br>plt.scatter(features[:,<span class="hljs-number">1</span>],labels,<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230531183343979.png" alt="数据集可视化" /></p><h3 id="32-定义数据迭代"><a class="markdownIt-Anchor" href="#32-定义数据迭代"></a> 3.2 定义数据迭代</h3><p>基于Python原生生成的迭代器虽然可以连续的获取不同的小批量，但是执行效率很低，因为此****要求将所有数据加载到内存中，并执行大量的随机内存访问****，所以通常深度学习框架会内置迭代器，用于处理存储在文件中的数据和数据流提供的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_iter</span>(<span class="hljs-params">batch_size,features,labels</span>):<br>    num_examples = <span class="hljs-built_in">len</span>(features)<br>    indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(num_examples))<br>    <span class="hljs-comment"># 样本随机读取，没有特定的顺序</span><br>    random.shuffle(indices)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,num_examples,batch_size):<br>        batch_indices = torch.tensor(<br>            indices[i:<span class="hljs-built_in">min</span>(i+batch_size,num_examples)]<br>        )<br>        <span class="hljs-keyword">yield</span> features[batch_indices],labels[batch_indices]<br><br><span class="hljs-comment"># batchsize=10</span><br><span class="hljs-comment"># for X,y in data_iter(batch_size=batchsize,features=features,labels=labels):</span><br><span class="hljs-comment">#     &#x27;&#x27;&#x27;仅作为测试使用&#x27;&#x27;&#x27;</span><br><span class="hljs-comment">#     print(X,&#x27;\n&#x27;,y)</span><br><span class="hljs-comment">#     break</span><br></code></pre></td></tr></table></figure><h2 id="33-定义模型"><a class="markdownIt-Anchor" href="#33-定义模型"></a> 3.3 定义模型</h2><p>同时也能够返回对应参数的parameters的size，这里和教科书不一样，我们使用class的方式来建立新的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">network</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        self.w=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,size=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),requires_grad=<span class="hljs-literal">True</span>,dtype=<span class="hljs-built_in">float</span>)<br>        self.b=torch.zeros(<span class="hljs-number">1</span>,requires_grad=<span class="hljs-literal">True</span>,dtype=<span class="hljs-built_in">float</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self,X</span>):<br>        <span class="hljs-keyword">return</span> torch.matmul(X,self.w)+self.b<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;返回对应参数的size&#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">return</span> [self.w,self.b]<br>model=network()<br></code></pre></td></tr></table></figure><h2 id="34-定义损失函数"><a class="markdownIt-Anchor" href="#34-定义损失函数"></a> 3.4 定义损失函数</h2><p>Local loss是局部损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_weight</span>(<span class="hljs-params">X,features</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;权重分析&#x27;&#x27;&#x27;</span><br>    diff = X.unsqueeze(-<span class="hljs-number">2</span>).unsqueeze(-<span class="hljs-number">2</span>)- features.unsqueeze(-<span class="hljs-number">2</span>)<br>    w = torch.exp(-diff**<span class="hljs-number">2</span>/<span class="hljs-number">0.000002</span>).squeeze(-<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(axis=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> w<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">square_loss</span>(<span class="hljs-params">y_hat,y</span>):<br>    <span class="hljs-keyword">return</span> (y_hat-y.reshape(y_hat.shape))**<span class="hljs-number">2</span>/<span class="hljs-number">2</span><br><span class="hljs-comment"># y_hat=model.predict(X)</span><br><span class="hljs-comment"># res=square_loss(y_hat,y)</span><br><span class="hljs-comment"># res.mean()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">local_loss</span>(<span class="hljs-params">y_hat,y,X</span>):<br>    new_w=get_weight(X,features)<br>    <span class="hljs-keyword">return</span> new_w.reshape(shape=(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))*(y_hat-y.reshape(y_hat.shape))**<span class="hljs-number">2</span>/<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><h2 id="35-定义优化算法"><a class="markdownIt-Anchor" href="#35-定义优化算法"></a> 3.5 定义优化算法</h2><p>在计算机求解优化算法的过程中，我们可以先想一下我们需要什么东西？</p><p>按照梯度下降的方式来进行神经网络的优化过程中，必然需要知道parameters的size，之后是对参数更新之后的长度learning rate，依旧每次下降的batch-size</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd</span>(<span class="hljs-params">params,lr,batchsize</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;小批量随机梯度下降&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># model.eval() 和 with torch.node_grad()区别是什么</span><br>    <span class="hljs-comment"># model.eval()主要用于model的validation过程中，通知dropout和batchnorm层在train</span><br>    <span class="hljs-comment"># val模式之间切换</span><br>    <span class="hljs-comment"># with torch.no_grad()主要是停止autograd模块的工作，起到加速和节省显存的作用</span><br>    <span class="hljs-keyword">with</span> torch.no_grad(): <span class="hljs-comment">#??这是做什么的</span><br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param -= lr*param.grad/batchsize<br>            param.grad.zero_()<br></code></pre></td></tr></table></figure><h2 id="36-定义超参数"><a class="markdownIt-Anchor" href="#36-定义超参数"></a> 3.6 定义超参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">lr = <span class="hljs-number">0.05</span><br>batchsize=<span class="hljs-number">10</span><br>num_epoches = <span class="hljs-number">4</span><br>net = network()<br>loss = square_loss<br></code></pre></td></tr></table></figure><h2 id="37-开始训练"><a class="markdownIt-Anchor" href="#37-开始训练"></a> 3.7 开始训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epoches):<br>    <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> data_iter(batch_size=batchsize,features=features,labels=labels):<br>        <span class="hljs-comment"># l = loss(net.predict(X),y)</span><br>        l = local_loss(net.predict(X),y,X)<br>        l.<span class="hljs-built_in">sum</span>().backward()<br>        sgd(net.parameters(),lr,batchsize)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        train_l=loss(net.predict(features),labels).mean()<br>        <span class="hljs-built_in">print</span>(train_l)<br></code></pre></td></tr></table></figure><h2 id="38-评估模型"><a class="markdownIt-Anchor" href="#38-评估模型"></a> 3.8 评估模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(net.w)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230531183709870.png" alt="模型结果" /></p><h1 id="0x04-几个注意的点"><a class="markdownIt-Anchor" href="#0x04-几个注意的点"></a> 0x04 几个注意的点</h1><ol><li>数据dtype最好设置float</li><li>注意vector和matrix甚至tensor之间的区别</li><li>在weight计算中的维度的变化来保证broadcasting</li><li>with torch.no_grad( )和model.eval( )的区别</li><li>令人惊喜的torch的broadcasting的功能！！</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;在学习Torch之际，正好结合沐神的《动手学深度学习》和CS229中的局部线性回归讲义实践一下！终于跑通了第一个属于自己的代码！！&lt;/p&gt;</summary>
    
    
    
    <category term="ML&amp;DL" scheme="https://blog.tjdata.site/categories/ML-DL/"/>
    
    
    <category term="LSR" scheme="https://blog.tjdata.site/tags/LSR/"/>
    
  </entry>
  
  <entry>
    <title>QuantumultX使用教程记录</title>
    <link href="https://blog.tjdata.site/posts/cc1a6fba.html"/>
    <id>https://blog.tjdata.site/posts/cc1a6fba.html</id>
    <published>2023-05-24T00:47:34.000Z</published>
    <updated>2023-05-24T02:01:51.464Z</updated>
    
    <content type="html"><![CDATA[<p>一直使用圈x作为我的代理软件，但是出现了一些问题，现在想总结一些圈x使用的方法。</p><span id="more"></span><h2 id="0x01-overview"><a class="markdownIt-Anchor" href="#0x01-overview"></a> 0x01 Overview</h2><p>圈X的功能有哪些？</p><ol><li>Proxy server，导入订阅连接，可以使用代理访问网站 [server local] [server remote]</li><li>Filter rules,设置对应的策略组和分流规则，对应[proxy] [filter_remote] [filter_local]</li><li>Rewriter rule，重写规则和MITM解密，主要用来去广告和重定向[rewrite] [mitm]</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230524092006244.png" alt="image-20230524092006244" /></p><h2 id="0x02-proxy-server使用"><a class="markdownIt-Anchor" href="#0x02-proxy-server使用"></a> 0x02 Proxy server使用</h2><p>添加节点和订阅链接的方式有三种，一般使用2就足够了～</p><ol><li>通过Quantumult X 界面添加节点。setting-&gt;external proxy-&gt;add</li><li>通过Quantumult X 界面添加订阅。setting-&gt;external proxy-&gt;resources</li><li>手动修改配置文件添加节点，在配置文件中的[server_local]部分，setting-&gt;proxy file–&gt; edit</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">trojan 节点写法</span><br>trojan=example.com:443, password=pwd, over-tls=true, tls-verification=false, fast-open=false, udp-relay=false, tag=节点名称<br><span class="hljs-meta prompt_"># </span><span class="language-bash">vmess 节点写法</span><br>vmess=example.com:443, method=chacha20-ietf-poly1305, password=pwd, obfs-host=example.com, obfs=wss, obfs-uri=/ws, tls-verification=true, fast-open=false, udp-relay=false, tag=节点名称<br></code></pre></td></tr></table></figure><p>在实际使用中可以分为all proxy、all direct、based on filter等</p><ol><li>All proxy：所有的网络请求都通过Proxy下选中的节点进行</li><li>All direct：所有的网络请求都不使用代理进行</li><li>Based on filter: 根据分流规则将网络请求进行分流，并通过策略组将分流规则传递来的网络请求进行转发</li></ol><h2 id="0x03-filter-rules-使用"><a class="markdownIt-Anchor" href="#0x03-filter-rules-使用"></a> 0x03 Filter rules 使用</h2><pre><code class=" mermaid">graphQ[request]P1[Based on filter]J[Filter=filter_local+filter_remote]P2[Direct]P3[Proxy1]P4[Proxy2]P5[Reject]Q--&gt;P1P1--&gt;JJ--&gt;P2J--&gt;P3J--&gt;P4J--&gt;P5</code></pre><p>当我们发送请求的时候，确定请求代理之后，根据Filter rules，来判断走哪些策略组。首先策略组分为static、available、robin和SSID，常见的Proxy设置在配置文件中的[proxy]中，对应的分流规则在[filter_local] 和 [filter_remote]</p><p>首先分流规则是根据匹配来选择对应的策略组的，这一点和clash等类似，比如通过域名来判断，添加的方式主要有：</p><ol><li>导入远程分流规则订阅，这里默认在proxyfile的[filter_remote]</li><li>自己本地撰写分流规则，这里默认在proxyfile的[filter_local]</li><li>利用抓包的方式来确定选择对应的策略组（因为我们通常不知道域名是什么qwq）activity–&gt;one row–&gt;filter–&gt;new filter–&gt;proxy</li></ol><p>这里当然可以去寻找一些现有的filter</p><h2 id="0x04-rewrite-rules"><a class="markdownIt-Anchor" href="#0x04-rewrite-rules"></a> 0x04 Rewrite rules</h2><p>上述其实实现了代理软件的基本功能，但是QuantumultX还提供了重写的过程，<strong>个人理解是将软件的请求重写调整</strong>，而不是简单的换一个服务器访问。</p><blockquote><p>一个简单的例子就是你访问www.baidu.com之后，它必然会返回一个网页，我们可以通过正则匹配的方式挑选出其中img的路径，并重定向到你自己的网址，这样就可以更换logo</p></blockquote><p>因此这里需要MITM解密和设置[rewrite]</p><p>还是看大佬的配置比较好</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一直使用圈x作为我的代理软件，但是出现了一些问题，现在想总结一些圈x使用的方法。&lt;/p&gt;</summary>
    
    
    
    <category term="memo" scheme="https://blog.tjdata.site/categories/memo/"/>
    
    
    <category term="proxyApp" scheme="https://blog.tjdata.site/tags/proxyApp/"/>
    
  </entry>
  
  <entry>
    <title>Torch文档阅读</title>
    <link href="https://blog.tjdata.site/posts/5ef17af5.html"/>
    <id>https://blog.tjdata.site/posts/5ef17af5.html</id>
    <published>2023-05-23T14:25:41.000Z</published>
    <updated>2023-05-23T14:31:49.485Z</updated>
    
    <content type="html"><![CDATA[<p>Torch官方文档中最关键的类torch.tensor，最重要的机制autograd，这里对常见的API进行总结</p><span id="more"></span><h2 id="pytorch-基本教程"><a class="markdownIt-Anchor" href="#pytorch-基本教程"></a> Pytorch 基本教程</h2><p>从组成元素上来看学习Torch主要需要熟悉以下几个概念，个人总结以下几个概念</p><ol><li>什么是Tensor？这个是深度学习框架计算的源泉，参考NumPy的array和Pandas的Dataframe。我们对于基本变量的操作主要分为如何创建？如何生成？如何复制？如何运算？如何索引？如何删除？等等操作</li><li>数据的预处理，如何通过读取数据来构建自己的dataset、dataloader，并对数据进行一定的transform、随机抽取等等</li><li>深度的八股：网路结构Net、优化器选择Optim、损失函数的选择Loss等等</li><li>训练函数和测试函数。如何在epoch训练、如何打印loss、如何计算summary等等</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch<br><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-comment"># 生成torch通常可以包括以下几点</span><br><span class="hljs-comment"># 1. 利用内置的函数 empty.ones.rand.randn.randint randperm linspace等等</span><br><span class="hljs-comment"># 2. 利用现有的数据，list、numpy.array、tensor等</span><br><span class="hljs-comment"># 3. 通过运算broadcasting,cat,join等</span><br><br><span class="hljs-attribute">x</span> =torch.empty(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">x_numpy</span>=np.zeros((<span class="hljs-number">5</span>,<span class="hljs-number">3</span>))<br><br><span class="hljs-comment"># Torch 生成随机数的种类</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>) # 均匀分布<br><span class="hljs-attribute">torch</span>.randint(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)) #整数分布<br><span class="hljs-attribute">torch</span>.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 标准正态分布<br><span class="hljs-attribute">torch</span>.randperm(<span class="hljs-number">10</span>) # 不重复随机数<br><span class="hljs-attribute">torch</span>.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">1</span>) # 线性间距<br><span class="hljs-attribute">torch</span>.arange(<span class="hljs-number">12</span>)<br><span class="hljs-attribute">torch</span>.poisson(torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>)*<span class="hljs-number">5</span>) # poisson分布<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><span class="hljs-attribute">plt</span>.plot(torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,steps=<span class="hljs-number">50</span>),torch.poisson(torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,steps=<span class="hljs-number">50</span>)))<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230523222823589.png" alt="image-20230523222823589" /></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">x</span>=torch.randn(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>,dtype=torch.float32)<br><span class="hljs-attr">x</span> = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br><span class="hljs-attr">x</span> = torch.tensor(np.arange(<span class="hljs-number">10</span>))<br><span class="hljs-attr">x</span> = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attr">y</span> = torch.rand_like(x)<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Torch之间的运算</span><br><span class="hljs-comment"># torch默认的操作是elements和broadcasting的</span><br><span class="hljs-attribute">X</span> = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">y</span>= torch.rand(<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)<br><br><span class="hljs-comment"># torch.tensor之间的可能会有三种方式：运算符号，运算函数，内在的函数</span><br><span class="hljs-attribute">torch</span>.add(X,y)<br><span class="hljs-attribute">X</span>.add_(y)<br><span class="hljs-attribute">X</span>+y<br><br><span class="hljs-comment"># 对于tensor的操作</span><br><span class="hljs-attribute">x</span>=torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)<br><span class="hljs-attribute">x</span>[:,:,-<span class="hljs-number">1</span>]<br><span class="hljs-attribute">y</span>=x.view(<span class="hljs-number">60</span>)<br><span class="hljs-attribute">z</span>=x.view(<span class="hljs-number">5</span>,<span class="hljs-number">12</span>)<br><span class="hljs-attribute">z</span><br><span class="hljs-attribute">x</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].item()<br><br><span class="hljs-comment"># 对于tensor的reshape操作通常可以分为:名字name、维度shape和类型type</span><br><span class="hljs-comment"># view和reshape是在改变形状</span><br><span class="hljs-comment"># squeeze unsqueeze 增加维度和删减维度</span><br><span class="hljs-comment"># transpose permute 是变换维度</span><br><span class="hljs-comment"># expand repeat 维度拓展</span><br><span class="hljs-attribute">images</span>=torch.randn(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>) # <span class="hljs-number">4</span>张通道数为<span class="hljs-number">1</span>的<span class="hljs-number">28</span>*<span class="hljs-number">28</span>的图片<br><span class="hljs-attribute">images_batch</span>=images.view(<span class="hljs-number">4</span>,<span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br><br><span class="hljs-comment"># 在0维度拓展维度unsqueeze，相当于将tensor放入包装盒种</span><br><span class="hljs-attribute">images_batch</span>.unsqueeze(<span class="hljs-number">0</span>).shape<br><br><span class="hljs-comment"># Tensor 的合并和分割主要包括</span><br><span class="hljs-comment"># concatenate 连接，作用是将2个tensor按照特定的维度连接起来，其他维度必须相同</span><br><span class="hljs-comment"># stack 对接起来</span><br><span class="hljs-comment"># split 根据长度拆分tensor</span><br><span class="hljs-comment"># chunk 均等分的split</span><br><span class="hljs-attribute">a</span>=torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">b</span>=torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.cat([a,b],dim=<span class="hljs-number">0</span>)<br><span class="hljs-attribute">a</span>.split([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h2 id="0x02-自动微分-autograd"><a class="markdownIt-Anchor" href="#0x02-自动微分-autograd"></a> 0x02 自动微分 Autograd</h2><p>上述虽然给tensor的创建、索引、运算和形状改变，但其实更重要的是Torch基于此实现的自动微分机制。可能需要一些前向推理和反向传播的知识，这里需要结合tensor和gradient来介绍一些基础的知识。初步的需要了解</p><ol><li>tensor中grad的激活、分割、计算等等</li><li>运算</li></ol><p>torch.Tensor是package的核心类，如果将其属性</p><p>.require_grad设置为true，则会开始跟踪tensor的所有操作，在完成计算之后，可以调用</p><p>.backward()来自动就按所有的梯度，累积到</p><p>.grad属性之中</p><p>.detach() 可以停止跟踪历史记录和内存，或者在with torch.no_grad()</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">x = torch.randn(2,2,<span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-attribute">y</span>=x+2<br><span class="hljs-attribute">z</span>=y*y*3<br><span class="hljs-attribute">out</span>=z.sum()<br>out.backward()<br>x.grad # 这里想当于 d(out)/d(x)<br><br><span class="hljs-attribute">x</span>=torch.randn(10,3,requires_grad=True)<br><span class="hljs-attribute">b</span>=torch.randn(3,1,requires_grad=True)<br><span class="hljs-attribute">y</span>=torch.matmul(x,b).sum()<br>y.backward()<br>b<br></code></pre></td></tr></table></figure><h2 id="0x03-神经网络的搭建"><a class="markdownIt-Anchor" href="#0x03-神经网络的搭建"></a> 0x03 神经网络的搭建</h2><ol><li>网络内部需要考虑 input 和 output，内部需要考虑layer和forward</li><li>迭代整个输入</li><li>通过神经网络处理输入</li><li>计算损失 loss</li><li>反向传播计算梯度</li><li>更新网络的参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn <br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 假设图片的输入是 1channel,5*5,6channel 输出</span><br>        self.conv1=nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">5</span>)<br>        self.conv2=nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># 这里的16*5*5可以推理应该需要的size</span><br>        self.fc1=nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>)<br>        self.fc2=nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>)<br>        self.fc3=nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=F.max_pool2d(F.relu(self.conv1(x)),(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># If the size is a square you can only specify a single number</span><br>        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="hljs-number">2</span>)<br>        x = x.view(-<span class="hljs-number">1</span>, self.num_flat_features(x))<br>        x = F.relu(self.fc1(x))<br>        x = F.relu(self.fc2(x))<br>        x = self.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">num_flat_features</span>(<span class="hljs-params">self, x</span>):<br>        size = x.size()[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># all dimensions except the batch dimension</span><br>        num_features = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> size:<br>            num_features *= s<br>        <span class="hljs-keyword">return</span> num_features<br>net=Net()<br><span class="hljs-comment"># Question: 这里必须要加上一个梯度</span><br><span class="hljs-built_in">input</span>=torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,requires_grad=<span class="hljs-literal">True</span>)<br><br>output=net(<span class="hljs-built_in">input</span>)<br><br><span class="hljs-comment"># dummy target</span><br>target=torch.rand(<span class="hljs-number">10</span>)<br>target=target.unsqueeze(<span class="hljs-number">0</span>)<br>criterion=nn.MSELoss()<br><br>loss=criterion(target,output)<br><br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br>optimizer=optim.SGD(net.parameters(),lr=<span class="hljs-number">0.01</span>)<br></code></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(net.conv1.bias.grad)</span></span><br>optimizer<span class="hljs-selector-class">.zero_grad</span>()<br>out=<span class="hljs-built_in">net</span>(input)<br>loss=<span class="hljs-built_in">criterion</span>(output,target)<br>loss<span class="hljs-selector-class">.backward</span>()<br>optimizer<span class="hljs-selector-class">.step</span>()<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(net.conv1.bias.grad)</span></span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#用net来训练CIFA10</span><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><br>transform = transforms.Compose(<br>    [transforms.ToTensor(),<br>     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<br><br>trainset=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>,train=<span class="hljs-literal">True</span>,download=<span class="hljs-literal">False</span>,transform=transform)<br><br>trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="hljs-number">4</span>,<br>                                          shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>classes = (<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>,<br>           <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">imshow</span>(<span class="hljs-params">img</span>):<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>     <span class="hljs-comment"># unnormalize</span><br>    npimg = img.numpy()<br>    plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br><br><br><span class="hljs-comment"># get some random training images</span><br>dataiter = <span class="hljs-built_in">iter</span>(trainloader)<br>images, labels = <span class="hljs-built_in">next</span>(dataiter)<br><br><span class="hljs-comment"># show images</span><br>imshow(torchvision.utils.make_grid(images))<br><span class="hljs-comment"># print labels</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)))<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        self.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        self.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.pool(F.relu(self.conv1(x)))<br>        x = self.pool(F.relu(self.conv2(x)))<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)<br>        x = F.relu(self.fc1(x))<br>        x = F.relu(self.fc2(x))<br>        x = self.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>net = Net()<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):  <span class="hljs-comment"># loop over the dataset multiple times</span><br><br>    running_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader, <span class="hljs-number">0</span>):<br>        <span class="hljs-comment"># get the inputs</span><br>        inputs, labels = data<br><br>        <span class="hljs-comment"># zero the parameter gradients</span><br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># forward + backward + optimize</span><br>        outputs = net(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>        <span class="hljs-comment"># print statistics</span><br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2000</span> == <span class="hljs-number">1999</span>:    <span class="hljs-comment"># print every 2000 mini-batches</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %<br>                  (epoch + <span class="hljs-number">1</span>, i + <span class="hljs-number">1</span>, running_loss / <span class="hljs-number">2000</span>))<br>            running_loss = <span class="hljs-number">0.0</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br><br></code></pre></td></tr></table></figure><h2 id="one-more-thing-cheetsheet"><a class="markdownIt-Anchor" href="#one-more-thing-cheetsheet"></a> One more thing: cheetsheet</h2><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># creating tensors</span><br><span class="hljs-attribute">x</span> =torch.empty(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.manual_seed(<span class="hljs-number">1998</span>)<br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.rand_like(x)<br><span class="hljs-attribute">torch</span>.tensro([<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br><span class="hljs-attribute">torch</span>.one((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),dtype=torch.int16)<br><br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)+<span class="hljs-number">1</span><br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)*<span class="hljs-number">3</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)**<span class="hljs-number">4</span><br><br><span class="hljs-comment"># broadcasting</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*(torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*<span class="hljs-number">2</span>)<br><span class="hljs-comment"># more math with tensors</span><br><span class="hljs-attribute">torch</span>.abs(a)<br><span class="hljs-attribute">torch</span>.ceil(a)<br><span class="hljs-attribute">torch</span>.floor(a)<br><span class="hljs-attribute">torch</span>.clamp(a,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>)<br><span class="hljs-attribute">torch</span>.sin(a)<br><span class="hljs-attribute">torch</span>.asin(a)<br><span class="hljs-attribute">torch</span>.bitwise_and(a,x)<br><br><span class="hljs-attribute">torch</span>.max()<br><span class="hljs-attribute">torch</span>.mean()<br><span class="hljs-attribute">torch</span>.std()<br><span class="hljs-attribute">torch</span>.prod()<br><br><span class="hljs-attribute">v1</span> = torch.tensor([<span class="hljs-number">1</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.])         # x unit vector<br><span class="hljs-attribute">v2</span> = torch.tensor([<span class="hljs-number">0</span>., <span class="hljs-number">1</span>., <span class="hljs-number">0</span>.])         # y unit vector<br><span class="hljs-attribute">torch</span>.cross(v2,v1)<br><span class="hljs-attribute">torch</span>.matmul(a,x)<br><span class="hljs-attribute">torch</span>.mv(a,v2)<br><span class="hljs-attribute">torch</span>.svd(x)<br><br><span class="hljs-comment"># creating tensors</span><br><span class="hljs-attribute">x</span> =torch.empty(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.manual_seed(<span class="hljs-number">1998</span>)<br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.rand_like(x)<br><span class="hljs-attribute">torch</span>.tensro([<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br><span class="hljs-attribute">torch</span>.one((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),dtype=torch.int16)<br><br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)+<span class="hljs-number">1</span><br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)*<span class="hljs-number">3</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)**<span class="hljs-number">4</span><br><br><span class="hljs-comment"># broadcasting</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*(torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*<span class="hljs-number">2</span>)<br><span class="hljs-comment"># more math with tensors</span><br><span class="hljs-attribute">torch</span>.abs(a)<br><span class="hljs-attribute">torch</span>.ceil(a)<br><span class="hljs-attribute">torch</span>.floor(a)<br><span class="hljs-attribute">torch</span>.clamp(a,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>)<br><span class="hljs-attribute">torch</span>.sin(a)<br><span class="hljs-attribute">torch</span>.asin(a)<br><span class="hljs-attribute">torch</span>.bitwise_and(a,x)<br><br><span class="hljs-attribute">torch</span>.max()<br><span class="hljs-attribute">torch</span>.mean()<br><span class="hljs-attribute">torch</span>.std()<br><span class="hljs-attribute">torch</span>.prod()<br><br><span class="hljs-attribute">v1</span> = torch.tensor([<span class="hljs-number">1</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.])         # x unit vector<br><span class="hljs-attribute">v2</span> = torch.tensor([<span class="hljs-number">0</span>., <span class="hljs-number">1</span>., <span class="hljs-number">0</span>.])         # y unit vector<br><span class="hljs-attribute">torch</span>.cross(v2,v1)<br><span class="hljs-attribute">torch</span>.matmul(a,x)<br><span class="hljs-attribute">torch</span>.mv(a,v2)<br><span class="hljs-attribute">torch</span>.svd(x)<br><br><span class="hljs-attribute">x</span>=torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,requires_grad=True)<br><span class="hljs-attribute">y</span>=x+<span class="hljs-number">2</span><br><span class="hljs-attribute">y</span>.sum().backward()<br><span class="hljs-attribute">plt</span>.plot(x.detach(),y.detach())<br><span class="hljs-attribute">with</span> torch.no_grad():<br>    <span class="hljs-attribute">y</span>=x+<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Torch官方文档中最关键的类torch.tensor，最重要的机制autograd，这里对常见的API进行总结&lt;/p&gt;</summary>
    
    
    
    <category term="ML&amp;DL" scheme="https://blog.tjdata.site/categories/ML-DL/"/>
    
    
    <category term="document" scheme="https://blog.tjdata.site/tags/document/"/>
    
  </entry>
  
  <entry>
    <title>知识管理的一些想法</title>
    <link href="https://blog.tjdata.site/posts/d49191c7.html"/>
    <id>https://blog.tjdata.site/posts/d49191c7.html</id>
    <published>2023-05-17T05:55:36.000Z</published>
    <updated>2023-05-17T06:52:43.516Z</updated>
    
    <content type="html"><![CDATA[<p>作为一枚INFJ，经常会对效率工具进行反思。今天看了一本和知识管理联系不大的《如何有效阅读一本书》，然后对自己现有的知识管理系统中备忘录+日历+提醒事项，配合ShortCut快速输入、Cubox聚集、Notion整理的一套系统进行整理和反思。给出三个核心的观点。</p><span id="more"></span><h2 id="0x01-现有工具使用流程"><a class="markdownIt-Anchor" href="#0x01-现有工具使用流程"></a> 0x01 现有工具使用流程</h2><p>对于现代互联网的工具的感受是，它无处不在，但是在光鲜亮丽的覆盖率之下反而更加暴露了各种信息交换不流畅的问题。你在Apple日历中设置的事项很难在安卓手机上看到，你在推特发的推文很难被同步到你的备忘录，你在豆瓣收藏的书籍也不会出现在亚马逊的购物中，你在PDF中做出的批注也难导出到你的Notion中…</p><p>为了解决这些信息交流的问题，我们往往需要确定自己的工作流Workflow。通过自身的控制来更好的使用工具。当然当一个高效的工具出现的时候你也会开心的，比如看到Warp代替iTerm，时代毕竟是在发展的。电子工具的用途主要有三个：1. GTD日程管理、2. 信息收集、3. 信息处理</p><pre><code class=" mermaid">graphclassDef someclass fill:#f96A[GTD : Get thing done]:::someclassC(Calendar:Event)R(Reminder:ToDo Schedule)N(Notes)C--&gt;AR--&gt;AN--&gt;AN--&gt;CN--&gt;RB[知识收集 Cubox]:::someclassW(Wechat Helper)API(Cubox API)ALF(Alfred)S(Short Cut)W--&gt;BAPI--&gt;BALF--&gt;BS--&gt;BNO[知识整理 Notion]:::someclassDB(Database)BD(Database2)DB--&gt;NOBD--&gt;NOOUTPUT(Blog):::someclassA--&gt;B--&gt;NO--&gt;OUTPUT</code></pre><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230517141343789.png" alt="总的知识管理" /></p><p>接下来介绍一些原则：</p><ol><li>一元化原则：如果在这个地方找不到，那么其他地方也找不到</li><li>避免无穷原则：收藏很简单，但是清空很难</li><li>多思考，多输出：一件事情只有我们给别人能够讲清楚的时候，我们自己才能明白</li></ol><h2 id="0x02-一元化原则-gtd系统的可靠性"><a class="markdownIt-Anchor" href="#0x02-一元化原则-gtd系统的可靠性"></a> 0x02 一元化原则 – GTD系统的可靠性</h2><p>GTD是在一本书中提出的概念，用于个人的任务管理的一种方法论。同样这里还会有其他很多的任务管理的方法。最核心的要素是Get thing done。那么如何知道自己做什么？什么时候去做呢？原书给出作者思考的过程，这里介绍我目前摸索的对于日常事务的分类方法和实践规则。</p><p>首先事项根据主观和客观可以分为Calendar和Reminder，这也是两者之间的区别：</p><ol><li>Calendar：日历中发生的往往是一件Events（这也是macOS中的英文），它应该是一种不随个人意志发生的事情。比如国庆节、比如朋友的生日、比如上课、比如上课的DDL。这些事情的最大特点是发生完就结束了，不会有主动拖延这个概念。所以这个通常需要确定一个时间点</li><li>Reminder：这个发生的是我们主观想去做的事情（Item），这些事项可以是“去买个便利贴”“完成某一件作业”“在DDL之前做完PPT”</li></ol><p>但是对于不同的提醒事项也可以进行分类，经过可以经验可以分为</p><ol><li>Schedule：按照时间排序。规划好时间的事情，比如“明天早上9点新建一个论文文件夹”。在之前建立系统的时候，假如schedule设置太多，往往会导致自己完不成而逐渐拖延，所以这里建议计划一个星期会比较好</li><li>To Do List：按照优先级排序。这里是想做的，但是没有规划好时间的事情，比如说“看《奥本海默》”，这类事情往往需要构建一个优先级，当自己的Schedule做完的时候，可以从TODO中做起优先级高的事情</li><li>Deadline：按照时间排序。这里往往是一件事情的结束。代表的是一个过程。比如“下周一交作业”，这个时候需要我们在“下周一”这个时间点完成作业，并提交，我们不可能等到下周一再去完成作业。所以这个建议设置Flag</li></ol><p>到此为止我们的GTD系统已经可以建立起来了，但是这个系统往往会受到冲击：</p><ol><li>不可靠性，收集在Calendar和Reminder中的事项并不完全。导致我们对系统的信任度不高，经常还会依靠自己的大脑来回忆事情。</li><li>懒惰性，假如Schedule的东西太多，而自己不去完成，会导致系统的任务越来越多而崩溃</li><li>缺乏激励性，没有办法给自己正反馈，导致系统维护困难。</li></ol><p>这里给出一个建议：在实践中我们要坚持一元化的原则，也就是在微信或者口头上获取一个事件，可以立马将其添加到自己的inbox，然后再对其分类整理。我们要充分信任自己的系统，因为GTD不仅是记录我们做了什么，还是一个帮助我们大脑记忆的系统。如果我们陷入回忆自己需要做什么的魔咒中，往往会消耗大量的精力和时间。</p><pre><code class=" mermaid">graph cal[Calendar] re[Reminder] E(Event sorted by time) S(Schedule sorted by time) T(To Do List sorted by priority) D(DeadLine sorted by time) cal --&gt; E re--&gt;S re--&gt;T re--&gt;D</code></pre><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230517143808590.png" alt="GTD系统" /></p><h2 id="0x03-避免无穷收藏cubox"><a class="markdownIt-Anchor" href="#0x03-避免无穷收藏cubox"></a> 0x03 避免无穷收藏：Cubox</h2><p>互联网将知识串联在一起的后果是我们可以无限的滑动，我们永远看不完Google搜索的结果、看不完微博的帖子、看不完商品的简介…在知识管理也是这样。互联网带来的有用的知识也越来越多，当我们看到一个有意思的微信文章、或者诱人的博客、或者是B站大学的课程，往往会丢到自己的收藏夹中，然后不同软件中的收藏夹并不会互相沟通，唯一的共同点是逐渐增加但是从不清空。</p><p>所以将收集箱填满只是起点，终点是让收集箱回归空的状态。同样为了遵循“一元化”原则，希望可以将所有的知识和自己的想法收集到一个地方。并不想发生在微信收藏看文章、在B站看视频、在微信读书读想读的书、在浏览器看自己的read list，这样的劣势是不能整理。仿佛看完之后就结束了。</p><p>借助Cubox可以通过随机回顾来看自己曾经做过什么批注，可以将看完的东西分类放到架子（文件夹）中日后浏览，也可以将没有营养的归档或者删除。Review才是终点！</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230517144317271.png" alt="Cubox 快捷指令" /></p><h2 id="0x04-输出和all-in-onenotion"><a class="markdownIt-Anchor" href="#0x04-输出和all-in-onenotion"></a> 0x04 输出和All in one：Notion</h2><p>首先Notion作为知识管理的终点站在于其开放，它具有和其他系统联合并开放API，这样的开放性导致其可以综合其他内容。由此得到的后果是其可以作为所有知识收集工具的终点，同样它也是非常好的编辑器。</p><p>Databse是其最具特色的系统，我们可以通过设置不同的属性来对页面进行分类整理，将具有关系的页面根据Filter或Group来集聚，来达到自己系统管理的目的。</p><p>但是同样的在最终，在明白了自己需要做什么、在自己收集很多知识之后，考虑如何将其讲明白，写出一份给别人看的博客才是获取知识的最终途径～</p><p>共勉</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;作为一枚INFJ，经常会对效率工具进行反思。今天看了一本和知识管理联系不大的《如何有效阅读一本书》，然后对自己现有的知识管理系统中备忘录+日历+提醒事项，配合ShortCut快速输入、Cubox聚集、Notion整理的一套系统进行整理和反思。给出三个核心的观点。&lt;/p&gt;</summary>
    
    
    
    <category term="Memo" scheme="https://blog.tjdata.site/categories/Memo/"/>
    
    
    <category term="知识管理" scheme="https://blog.tjdata.site/tags/%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>基于JS的Mermaid使用教程</title>
    <link href="https://blog.tjdata.site/posts/3be9bc5f.html"/>
    <id>https://blog.tjdata.site/posts/3be9bc5f.html</id>
    <published>2023-05-16T01:25:31.000Z</published>
    <updated>2023-05-16T03:28:41.541Z</updated>
    
    <content type="html"><![CDATA[<p>关于流程图的绘制中有很多工具，比如微软的visio 或者 PowerPoint、贝尔的Graphviz、Lucidchart、OmniGraffle、<a href="http://Draw.io">Draw.io</a>、processOn、Latex的tikZ；但是这种基于图形界面的操作往往会受限于人本身的不精确导致不好看。所以希望借助代码生成流程图来尽可能保持协调一致的美观。</p><span id="more"></span><h2 id="0x01-使用教程"><a class="markdownIt-Anchor" href="#0x01-使用教程"></a> 0x01 使用教程</h2><p>可以直接在Typora中使用，只需要输入即可</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">``` mermaid<br></code></pre></td></tr></table></figure><p>什么是<a href="https://mermaid.js.org/intro/">Mermaid</a>，按照官网的定义是<strong>基于JavaScript</strong>的图表工具，可以渲染受markdown启发的文本定义，用于动态创建和修改图表。其基本的语法结构包括：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs merm"># Diagrams definitions begin with a declaration of the diagram type<br># Define the blocks<br># Link the blocks<br><br></code></pre></td></tr></table></figure><pre><code class=" mermaid">erDiagram          CUSTOMER &#125;|..|&#123; DELIVERY-ADDRESS : has          CUSTOMER ||--o&#123; ORDER : places          CUSTOMER ||--o&#123; INVOICE : &quot;liable for&quot;          DELIVERY-ADDRESS ||--o&#123; ORDER : receives          INVOICE ||--|&#123; ORDER : covers          ORDER ||--|&#123; ORDER-ITEM : includes          PRODUCT-CATEGORY ||--|&#123; PRODUCT : contains          PRODUCT ||--o&#123; ORDER-ITEM : &quot;ordered in&quot;</code></pre><p>常见的表格可以分为：流程图Flowchart、顺序图Sequence Diagram、类图Class Diagram、状态图State Diagram、实体关系图Entity Relationship Diagram、甘特图Gantt、饼图Pie chart、需求图Requirement Diagram、GitGraph、C4C图、思维导图Mindmaps、时间线Timelines</p><h2 id="0x02-流程图-flowchart"><a class="markdownIt-Anchor" href="#0x02-流程图-flowchart"></a> 0x02 流程图 Flowchart</h2><p>流程图由节点 (Nodes)、形状 (Edges)、箭头 （Arrows）和线条（Lines）组成，应用示例</p><pre><code class=" mermaid">flowchart id1(This is the text in the node1)id2[This is the text in the node2]db1[(database)]id1 --&gt; id2db1 --&gt; id1</code></pre><pre><code class=" mermaid">%% TB : top to bottom,TD%% BT: bottom to top%% RL: right to left%% LR: left to rightflowchart LR  subgraph BT[Define of the nodes]      id1(圆角)      id2([更大的圆角])      id3[长方形]      id4[(数据库)]      id5((圆形))      id6&gt;what]      id7&#123;方形&#125;      id8&#123;&#123;角&#125;&#125;      id10[\梯形 Trapezoid alt/] end  subgraph LR[define of the linkes]    id11[node1]    id12[node2]    id11 --&gt; id12    id11 --- id12    id11 -- Text ---id12    id11 ---|Text|id12    id11--&gt;|Text|id12    id11 -.-&gt; id12    id11 -. text .-&gt; id12    A == text ==&gt; id12    a --&gt;b &amp; c--&gt;d end  subgraph LR B[start] C&#123;Is it?&#125; B --&gt;|yes|C C--&gt;D[rethink] D--&gt;B B--&gt;|No|E[End] E[perhaps?]   end</code></pre><h2 id="0x03-时序图-sequence-diagrams"><a class="markdownIt-Anchor" href="#0x03-时序图-sequence-diagrams"></a> 0x03 时序图 Sequence diagrams</h2><p>时序图表示用户之间相互交流的过程的图表，其中的语法主要包括：参与者（Participant）、演员（Actor）、别名（Alias）、分组（Group and box）、信息（Arrows and lines）、激活（Activate）、备注（Notes）、循环（Loop）</p><pre><code class=" mermaid">sequenceDiagram  participant A as Alice    participant B as Bob    participant J as JohnA-&gt;&gt;B:hello JohnB-&gt;&gt;J:Great!</code></pre><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs diff">[Actor][Arrow][Actor]:Message Text<br><span class="hljs-deletion">-&gt; 实线</span><br><span class="hljs-deletion">--&gt; 没有箭头的点画线</span><br><span class="hljs-deletion">-&gt;&gt; 箭头线</span><br><span class="hljs-deletion">--&gt;&gt; 箭头点画线</span><br><span class="hljs-deletion">-X corss</span><br><span class="hljs-deletion">--X 点线带cross</span><br><span class="hljs-deletion">-）</span><br><span class="hljs-deletion">--）</span><br></code></pre></td></tr></table></figure><pre><code class=" mermaid">sequenceDiagram    Alice-&gt;John: Hello John, how are you?    activate John    loop Every minute        John--&gt;Alice: Great!    end    deactivate John</code></pre><p>初次之外顺序图还需要表示一些基本的逻辑概念，比如激活（Activate）、循环（Loop）、替代（Alt）、平行（Par）、关键区域（Critical）、Break、高亮显示（Rect）</p><pre><code class=" mermaid">sequenceDiagram    participant Alice    participant John    rect rgb(191, 223, 255)    note right of Alice: Alice calls John.    Alice-&gt;&gt;+John: Hello John, how are you?    rect rgb(200, 150, 255)    Alice-&gt;&gt;+John: John, can you hear me?    John--&gt;&gt;-Alice: Hi Alice, I can hear you!    end    John--&gt;&gt;-Alice: I feel great!    end    Alice -&gt;&gt;+ John: Did you want to go to the game tonight?    John --&gt;&gt;- Alice: Yeah! See you there.</code></pre><pre><code class=" mermaid">sequenceDiagramparticipant C as Clientparticipant S as Serverautonumbernote left of C: 选择初始seq number=x&lt;br/&gt;发送客户端的TCP SYN段C-&gt;&gt;S:SYNbit=1,Seq=Xnote right of S:选择初始seq number=y&lt;br/&gt;发送服务端TCP SYN信息&lt;br/&gt;利用SYN+1作为回复S-&gt;&gt;C:SYNbit=1,Seq=Y&lt;br/&gt;ACKbit=1,ACKnum=x+1note left of C:收到SYNACK(x),确认服务器存在&lt;br/&gt;发送ACK回复SYNACK&lt;br/&gt;不包含client-to-server数据C-&gt;&gt;S:ACKbit=1,ACKnum=y+1note right of S:收到 ACK(y)&lt;br/&gt;确认用户存在</code></pre><p><a href="https://zhuanlan.zhihu.com/p/53374516">https://zhuanlan.zhihu.com/p/53374516</a></p><h2 id="0x04-gitgraph"><a class="markdownIt-Anchor" href="#0x04-gitgraph"></a> 0x04 GitGraph</h2><pre><code class=" mermaid">gitGraphcommit id: &quot;Set up folder&quot;branch releasebranch developmentcheckout developmentcommitbranch feature01commit id:&quot;feature1 add&quot;commit id:&quot;feature1 add2&quot;checkout developmentbranch feature02commit id:&quot;fixed feature2&quot;checkout developmentmerge feature02commit id:&quot;feature1 add3&quot;checkout developmentmerge feature01commitcheckout mainmerge development tag:&quot;Beta1.0&quot;checkout releasemerge main tag:&quot;V1.0&quot;</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;关于流程图的绘制中有很多工具，比如微软的visio 或者 PowerPoint、贝尔的Graphviz、Lucidchart、OmniGraffle、&lt;a href=&quot;http://Draw.io&quot;&gt;Draw.io&lt;/a&gt;、processOn、Latex的tikZ；但是这种基于图形界面的操作往往会受限于人本身的不精确导致不好看。所以希望借助代码生成流程图来尽可能保持协调一致的美观。&lt;/p&gt;</summary>
    
    
    
    <category term="Baseline" scheme="https://blog.tjdata.site/categories/Baseline/"/>
    
    
    <category term="chart" scheme="https://blog.tjdata.site/tags/chart/"/>
    
  </entry>
  
  <entry>
    <title>GIT管理和常见操作</title>
    <link href="https://blog.tjdata.site/posts/cd758988.html"/>
    <id>https://blog.tjdata.site/posts/cd758988.html</id>
    <published>2023-05-09T07:24:09.000Z</published>
    <updated>2023-05-13T13:48:46.048Z</updated>
    
    <content type="html"><![CDATA[<p>从<a href="https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell">Git - Branches in a Nutshell</a>中看Git的一些常见操作，主要是设计理念和本地的操作。更复杂的远程协作和项目管理后续给出。</p><span id="more"></span><p>Git PRO</p><h2 id="what-is-git"><a class="markdownIt-Anchor" href="#what-is-git"></a> What is git</h2><p>分布式版本控制系统（具有很多优点巴拉巴拉）</p><p>几个显著的技术上的特点：</p><ol><li>snapshot，not differences，no delta- based system<ol><li>Git thinks of tis data more like a series of a snapshots of a miniature filesystem</li><li>every time you commit, or save the state of your project, Git basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot. If the data is not changed , Git doesn’t store the file again.</li></ol></li><li>Nearly every operation is local<ol><li>you have the entire history of the project right there on your local disk, most operations seem almost instantaneous</li><li>commit to local copy util you get to a network connection to upload</li></ol></li><li>Git has integrity<ol><li>you will see these hash values all over the place in Git because it uses them so much. In fact, Git stores everything in its database not by file name but 40 cahrs</li></ol></li><li>Git generally only adds data<ol><li>When you do actions in Git, nearly all over the place in Git because it uses them so much.  It is hard to get the system to do anything that is not undoable or to make it erase in any way</li></ol></li><li>three state<ol><li>modified</li><li>staged</li><li>committed</li></ol></li></ol><p>基本的工作流程可以看作：</p><ol><li>修改working directory的文件</li><li>当时觉得做的够好之后，add changed to the staging area</li><li>将所有暂存区的文件提交，stores that snapshot permanently to your Git directory</li></ol><h3 id="getting-start-first-time-git-setup"><a class="markdownIt-Anchor" href="#getting-start-first-time-git-setup"></a> Getting start —- First time git setup</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># git config 存储的三个地方</span><br>[path]/etc/gitconfig：包含应用于系统上每个用户及其所有存储库的值，如果你讲选项 system传递给<br>它会从专门的文件中读取和写入<br><br>~/.gitconfig 或者 ～/.config/git/config 特定用户个人的库<br><br>config 当前存储目录的呃设置<br></code></pre></td></tr></table></figure><h2 id="basic-topic"><a class="markdownIt-Anchor" href="#basic-topic"></a> Basic topic</h2><h3 id="31-获取git仓库"><a class="markdownIt-Anchor" href="#31-获取git仓库"></a> 3.1 获取Git仓库</h3><p>一种方式是将当前不受版本控制的本地目录转换为Git存储库</p><p>第二种方式是从其他地方clone现有的Git仓库</p><h3 id="32-记录更改"><a class="markdownIt-Anchor" href="#32-记录更改"></a> 3.2 记录更改</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">git status<br>git status -s <span class="hljs-comment"># for short</span><br></code></pre></td></tr></table></figure><p>工作目录中每个文件都存在 tracked or  untracked</p><p>从生命周期来看，文件可以分为untracked、unmodified、modified、staged</p><aside> 💡 Github将默认转换为main、但是git默认依旧是master</aside><p>同时可以设置gitignore来不希望Git自动添加，甚至限制您未追踪</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 常见的gitignore规则</span><br><span class="hljs-number">1.</span> 忽略空白行或者 <span class="hljs-comment">#</span><br><span class="hljs-number">2.</span> 在整个工作目录中递归<br><span class="hljs-number">3.</span> 可以使用正斜杠 / 开始模式<br><span class="hljs-number">4.</span> 可以使用正斜杠 / 结束模式来指定目录<br><span class="hljs-number">5.</span> 可以通过感叹号 ！ 来否定模式<br></code></pre></td></tr></table></figure><p>GitHub中维护了一个非常好的示例<a href="https://github.com/github/gitignore">https://github.com/github/gitignore</a></p><p>仅仅查看更改记录不够，你希望检查更改了哪些东西</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">git diff <span class="hljs-comment"># 查看更改但是未暂存</span><br>git diff --staged <span class="hljs-comment"># 查看已经提交下一个阶段的内容，或者-- cached</span><br></code></pre></td></tr></table></figure><p>提交记录</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git commit<br></code></pre></td></tr></table></figure><p>从Git中删除文件必须将其从tracked文件中删除，也就是需要从暂存区中删除</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">git rm<br>git rm - cached FILENAME <span class="hljs-comment"># 将文件保存在目录中，依旧保留在硬盘。但是从暂存区中删除。</span><br>git mv <span class="hljs-comment"># 重命名文件</span><br></code></pre></td></tr></table></figure><h3 id="33-查看提交的历史记录"><a class="markdownIt-Anchor" href="#33-查看提交的历史记录"></a> 3.3 查看提交的历史记录</h3><p>上述操作可以完成基本的工作流。在这个基础上，创建很多次提交之后，我们可以使用现有提交历史记录的存储库，回顾一下发生了什么，我们可以使用git log</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">git log <span class="hljs-comment"># 按照反向时间顺序列出提交的更改。同时也具有大量选项</span><br>git log -p <span class="hljs-comment"># 显示每个提交中引入的差异</span><br>git log --stat <span class="hljs-comment"># 缩写的统计信息</span><br>git log --pretty=oneline <span class="hljs-comment"># 将日志输出更改为默认格式以为的格式</span><br>git log --pretty=<span class="hljs-built_in">format</span> --graph<br></code></pre></td></tr></table></figure><h3 id="34-撤销操作"><a class="markdownIt-Anchor" href="#34-撤销操作"></a> 3.4 撤销操作</h3><p>在任何staged，你都困难想撤销一些事情，通过回顾一些基本工具来保存撤销的更改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git commit -amend <span class="hljs-comment"># 如果你想重做该提交，请你忘记额外的更改，暂存他们</span><br></code></pre></td></tr></table></figure><p>加入在修改两个文件之后，不小心add * 到暂存区</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">$ git add *<br>$ git status<br>On branch master<br>Changes to be committed:<br>  (use <span class="hljs-string">&quot;git restore --staged &lt;file&gt;...&quot;</span> to unstage)<br>modified:   CONTRIBUTING.md<br>renamed:    README.md -&gt; README<br></code></pre></td></tr></table></figure><p>但是只想commit其中的一个，那么需要</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git reset HEAD [filename you <span class="hljs-keyword">not</span> to commit]<br></code></pre></td></tr></table></figure><p>注意reset是非常危险的命令，checkout也可以实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git checkout --&lt;file&gt; <span class="hljs-comment"># 对文件所做的任何本地更改都已经消失，替换成为最后一个分阶段或者提交的版本。</span><br></code></pre></td></tr></table></figure><p>在之后常用git restore而不是git reset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">git restore --staged &lt;file&gt; <span class="hljs-comment"># 取消暂存</span><br></code></pre></td></tr></table></figure><h3 id="35-远程操作"><a class="markdownIt-Anchor" href="#35-远程操作"></a> 3.5 远程操作</h3><p>origin是Git赋予您克隆的服务器的默认名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">git clone [your-git-url]<br>git remote <span class="hljs-comment"># show your remote git name</span><br>git remote -v <span class="hljs-comment"># 显示存储的URL</span><br>git remote add shortname url <span class="hljs-comment"># git remote add pb &lt;https://github.com/chenxia31/blog&gt;</span><br><span class="hljs-comment">#  common line中可以使用short name来代替url</span><br></code></pre></td></tr></table></figure><p>从远程仓库连接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">git fetch &lt;remote&gt; <span class="hljs-comment"># 从远程仓库获取数据</span><br>git push origin master <span class="hljs-comment"># 推送本地的master分支</span><br>git remote show origin <span class="hljs-comment"># 显示一些常见的信息</span><br>git remote rename [oldname] [newname]<br>git remote remove [url]<br></code></pre></td></tr></table></figure><h3 id="36-标签"><a class="markdownIt-Anchor" href="#36-标签"></a> 3.6 标签</h3><p>和branch跟踪每次最新的commit 相反，tag是跟踪某一个特定commit的位置，如果不需要改动而仅查看的时候可以直接使用tag。推送到远程服务器需要显式的表达出tagname</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">git tag<br>git tag -l <span class="hljs-string">&#x27;v1.8.5.*&#x27;</span><br><br><span class="hljs-comment"># 标签分为轻量级和注释</span><br>git tag -a v1<span class="hljs-number">.4</span> -m <span class="hljs-string">&#x27;my version 1.4&#x27;</span><br><br>git tag v1<span class="hljs-number">.4</span><br><br>git push origin [tagname] <span class="hljs-comment"># git push origin v1.5</span><br>git push origin --tags<br><br><span class="hljs-comment"># 删除标签</span><br>git tag -d v1<span class="hljs-number">.4</span> <span class="hljs-comment"># 但是这样不会从任何远程服务器删除标签</span><br>git push origin --delete &lt;tagname&gt;<br><br><span class="hljs-comment"># 查看tag</span><br>git checkout v2<span class="hljs-number">.0</span> <span class="hljs-comment">#这种会导致仓库处于“分离的HEAD状态“，如果进行更改、创建提交、标签将保持不变</span><br></code></pre></td></tr></table></figure><h3 id="37-别名-alias"><a class="markdownIt-Anchor" href="#37-别名-alias"></a> 3.7 别名 alias</h3><p>创建自己snippets</p><h3 id="381-非常重要分支模式"><a class="markdownIt-Anchor" href="#381-非常重要分支模式"></a> 3.8.1 （非常重要）分支模式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">git add [filename]<br>git commit -m <span class="hljs-string">&#x27;message&#x27;</span><br><br><span class="hljs-comment">#git的默认分支名称是master，只不过很多人懒得改</span><br>git branch testing<br>git log --oneline --decorate<br>git checkout testing <span class="hljs-comment"># 更推荐git switch</span><br>git commit -a -m <span class="hljs-string">&#x27;new commit&#x27;</span><br>git checkout master<br>git commit -a -m <span class="hljs-string">&#x27;new commit2&#x27;</span><br>git log --oneline -decorate --graph --asll<br></code></pre></td></tr></table></figure><p>后序查看一些基本分支和合并的具体操作。想象一个工作流程，在网站上做一些工作，为一个新的故事创建一个分支，之后在另外分支做一些工作。因此可以参考一下的过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">git checkout -b [NewBranchName] <span class="hljs-comment"># git branch and git checkout 缩写</span><br><span class="hljs-comment"># 注意在签出的分支冲突未提交更改，但是可以允许 stashing and cleaning中了解</span><br>git checkout master <span class="hljs-comment"># 专注你的主线任务</span><br>git checkout -b <span class="hljs-string">&#x27;newfeature2&#x27;</span><br><br><span class="hljs-comment"># ~~ newfeature2结束</span><br>git checkout master<br>git merge newfeature2<br><span class="hljs-comment"># 删除分支</span><br>git branch -d hotfix<br><span class="hljs-comment"># 处理</span><br>git checkout NewBranchName<br><span class="hljs-comment"># 再次合并</span><br>git checkout master<br>git merge master<br><br><span class="hljs-comment"># 两个合并出现冲突</span><br>git status <span class="hljs-comment">#查看</span><br><span class="hljs-comment"># 手动打开文件，处理冲突</span><br>git mergetool<br></code></pre></td></tr></table></figure><p>上述导致已经会创建、合并、删除一些分支，这里还有一些分支管理工具</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">git branch -V <span class="hljs-comment"># 查看每个分支的最后一次提交</span><br>git branch --merged <span class="hljs-keyword">or</span> --no-merged<span class="hljs-comment"># 过滤您已经或尚未合并到您当前使用的分支中的分支</span><br>git brach --move <span class="hljs-comment"># 本地重命名分支</span><br>git push --<span class="hljs-built_in">set</span>-upstream origin main<br></code></pre></td></tr></table></figure><h3 id="382-分支工作流程"><a class="markdownIt-Anchor" href="#382-分支工作流程"></a> 3.8.2 分支工作流程</h3><p>已经有branch和merge的基础知识，应该用此做些什么？这里将轻量级分支的常见的工作流程展示，判断是否需要将其纳入自己的开发周期</p><p><a href="https://git-scm.com/book/en/v2/Git-Branching-Branching-Workflows">Git - Branching Workflows</a></p><p>常见的分类包括</p><ul><li>master 分支中完全稳定的代码</li><li>develop或next 工作中用于测试稳定性，不一定总是稳定的。当它稳定之后可以合并到master</li><li>proposed 协议更新</li><li>topic 短生命周期的分支，用于处理一些简单的feature add</li></ul><h3 id="383-远程分支"><a class="markdownIt-Anchor" href="#383-远程分支"></a> 3.8.3 远程分支</h3><ul><li>[ ]  待定</li></ul><p><a href="https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches">Git - Remote Branches</a></p><h3 id="384-分支管理"><a class="markdownIt-Anchor" href="#384-分支管理"></a> 3.8.4 分支管理</h3><h3 id="39-git-服务器"><a class="markdownIt-Anchor" href="#39-git-服务器"></a> 3.9 Git 服务器</h3><h3 id="310-分布式工作流"><a class="markdownIt-Anchor" href="#310-分布式工作流"></a> 3.10 分布式工作流</h3>]]></content>
    
    
    <summary type="html">&lt;p&gt;从&lt;a href=&quot;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&quot;&gt;Git - Branches in a Nutshell&lt;/a&gt;中看Git的一些常见操作，主要是设计理念和本地的操作。更复杂的远程协作和项目管理后续给出。&lt;/p&gt;</summary>
    
    
    
    <category term="CS" scheme="https://blog.tjdata.site/categories/CS/"/>
    
    
    <category term="git" scheme="https://blog.tjdata.site/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>官方文档_Pandas阅读和感悟</title>
    <link href="https://blog.tjdata.site/posts/9f94980a.html"/>
    <id>https://blog.tjdata.site/posts/9f94980a.html</id>
    <published>2023-05-07T05:42:41.000Z</published>
    <updated>2023-05-13T13:52:06.562Z</updated>
    
    <content type="html"><![CDATA[<p>官方文档是开发者对自己代码的解释。对于成熟的框架，官方文档可以最准确、权威的资料。如何阅读英文的官方文档一直是作为Coder weaker和English weaker的心魔，这里以Pandas文档为例子，尝试给出自己对于阅读官方文档、官方文档的查询工具、代码注释和Pandas文档的总结。</p><span id="more"></span><h2 id="0x01-why-阅读官方文档"><a class="markdownIt-Anchor" href="#0x01-why-阅读官方文档"></a> 0x01 Why 阅读官方文档</h2><p>对于质量比较高的项目，其官方文档往往能反映开发者的最直接的思想，而互联网上经过许多人的编码和解码，导致最后的意思可能和本意相差巨大。虽然可能存在其他更好的教程，但是官方文档给出的思想一定是最准确、权威的。</p><blockquote><p>之前一直不知道GitHub在国内为什么访问这么慢。</p><p>在网上找到很多答案，比如修改DNS的、修改Hosts的、修改镜像源或者增加代理的，但其实本质的问题是GitHub.com的域名解析在国内往往需要多层中转进而因为污染等原因造成速度较慢，因此上述方法对应的是修改DNS、本机解析、设置镜像、走代理等多种方法，这些方法都没有错。但是只是给出这些方法会让人不能一览背后的思想而困惑。</p></blockquote><h2 id="0x02-what-官方文档"><a class="markdownIt-Anchor" href="#0x02-what-官方文档"></a> 0x02 What 官方文档</h2><p>以Pandas为例子，文档通常可以分为</p><ul><li>Getting start：新手教程，一般会教你如何搭建一个简单的应用示例。</li><li>User guide：使用教程，介绍技术的关键知识、思想和概念。</li><li>API reference：API文档，包括具体的API使用细节以及机制</li><li>Developer guide：开发文档</li></ul><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230507140000120.png" alt="image-20230507140000120" style="zoom: 25%;" /><p>推荐使用  <a href="https://kapeli.com/dash">Dash gives your Mac instant offline access to 200+ API documentation sets.</a>  来进行官方API文档的管理。作为一个API文档浏览器(API documentation brower)和代码片段管理（Code snippet manager），可以提供的帮助有</p><ol><li>迅速</li><li>直接</li><li>本地</li><li>Alfred协作</li></ol><p>当然常见的函数用法可以直接通过命令来获取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> module_name <span class="hljs-comment">#such as import math</span><br><span class="hljs-built_in">help</span>(module_name) <span class="hljs-comment">#模块帮助查询</span><br><span class="hljs-built_in">dir</span>(module_name) <span class="hljs-comment"># 查询模块下所有的函数</span><br><span class="hljs-built_in">help</span>(module_name.func_name) <span class="hljs-comment">#查询具体函数的用法</span><br><span class="hljs-built_in">print</span>(func_name.__doc__) <span class="hljs-comment"># 打印函数的用法</span><br></code></pre></td></tr></table></figure><h2 id="0x03-pandas-新手教程"><a class="markdownIt-Anchor" href="#0x03-pandas-新手教程"></a> 0x03 Pandas 新手教程</h2><p>首先介绍pandas是什么：</p><blockquote><p>Pandas is a <a href="https://www.python.org/">Python</a> package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive.</p><p>It aims to be the fundamental high-level building block for doing practical, <strong>real-world</strong> data analysis in Python.</p></blockquote><p>其中基本的数据类型可以分为Series和DataFrame两种，基于NumPy构建。新手教程给出常见的Pandas使用的Topic。包括</p><h3 id="31-创建数据-查看数据-索引数据"><a class="markdownIt-Anchor" href="#31-创建数据-查看数据-索引数据"></a> 3.1 创建数据、查看数据、索引数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Series 一维标记数组，可以容纳各种数据类型(int、float、string、chat、python object)</span><br><span class="hljs-comment"># Pandas 二维标记数据框架，可以看作是一张数据表格，或者是SQL表格</span><br><span class="hljs-comment"># Insight01：Pandas中的数据结构可以看作是ndarray方式的标量（通过index索引），也可以看作是dict方式的字典（可以通过key索引）</span><br><span class="hljs-comment"># Attention01:Pandas数据之间的运算通常是按照元素的，并且是broadcasting的。对于缺失值通常使用numpy.nan填补</span><br><br><span class="hljs-comment"># 创建数据，可以来自于 1. ndarray、2. dict、3. scalar、4. tuple of dict</span><br>d = &#123;<br>    <span class="hljs-string">&quot;one&quot;</span>: pd.Series([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], index=[<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>]),<br>    <span class="hljs-string">&quot;two&quot;</span>: pd.Series([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>], index=[<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>]),<br>&#125;<br><br>d = &#123;<span class="hljs-string">&quot;one&quot;</span>: [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>], <span class="hljs-string">&quot;two&quot;</span>: [<span class="hljs-number">4.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>]&#125;<br>pd.DataFrame(d, index=[<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>])<br><br>data2 = [&#123;<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2</span>&#125;, &#123;<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">10</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-number">20</span>&#125;]<br>pd.DataFrame(data2, index=[<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;second&quot;</span>])<br><br>data3= &#123;<br>        (<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): <span class="hljs-number">1</span>, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): <span class="hljs-number">2</span>&#125;,<br>        (<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): <span class="hljs-number">3</span>, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): <span class="hljs-number">4</span>&#125;,<br>        (<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): <span class="hljs-number">5</span>, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): <span class="hljs-number">6</span>&#125;,<br>        (<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): <span class="hljs-number">7</span>, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): <span class="hljs-number">8</span>&#125;,<br>        (<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>): <span class="hljs-number">9</span>, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): <span class="hljs-number">10</span>&#125;,<br>    &#125;<br>pd.DataFrame(data3)<br><br><span class="hljs-comment"># 查看数据</span><br>.head() <span class="hljs-comment">#查看dataframe的顶部</span><br>.tail() <span class="hljs-comment">#查看dataframe的底部</span><br>.columns()<br>.index()<br>.to_numpy() <span class="hljs-comment">#注意这个可能是一个昂贵的操作</span><br>.describe() <span class="hljs-comment"># 显示数据的快速摘要</span><br>.T <span class="hljs-comment"># 转置</span><br><br>df.sort_index(axis=<span class="hljs-number">1</span>, ascending=<span class="hljs-literal">False</span>) <span class="hljs-comment">#按照轴进行排序</span><br>df.sort_values(by=<span class="hljs-string">&quot;B&quot;</span>)<br><br><span class="hljs-comment"># 更改控制台显示</span><br>info() <span class="hljs-comment"># 用于快速显示dataframe信息</span><br>baseball.info()<br><br>to_string() <span class="hljs-comment"># 以表格的形式返回dataframe的字符串表示</span><br><span class="hljs-built_in">print</span>(baseball.iloc[-<span class="hljs-number">20</span>:, :<span class="hljs-number">12</span>].to_string())<br>display.width <span class="hljs-comment"># 更改单行的打印量</span><br>pd.set_option(<span class="hljs-string">&quot;display.width&quot;</span>, <span class="hljs-number">40</span>)  <span class="hljs-comment"># default is 80</span><br>pd.set_option(<span class="hljs-string">&quot;display.max_colwidth&quot;</span>, <span class="hljs-number">30</span>)<br>pd.DataFrame(np.random.randn(<span class="hljs-number">3</span>, <span class="hljs-number">12</span>))<br><br><span class="hljs-comment"># 索引数据</span><br><span class="hljs-comment"># 继承series的索引</span><br>df[<span class="hljs-string">&quot;one&quot;</span>]<br>df[<span class="hljs-string">&quot;three&quot;</span>] = df[<span class="hljs-string">&quot;one&quot;</span>] * df[<span class="hljs-string">&quot;two&quot;</span>]<br>df[<span class="hljs-string">&quot;flag&quot;</span>] = df[<span class="hljs-string">&quot;one&quot;</span>] &gt; <span class="hljs-number">2</span><br><span class="hljs-keyword">del</span> df[<span class="hljs-string">&quot;two&quot;</span>]<br>three = df.pop(<span class="hljs-string">&quot;three&quot;</span>)<br>df[<span class="hljs-string">&quot;foo&quot;</span>] = <span class="hljs-string">&quot;bar&quot;</span> <span class="hljs-comment">#自动broadcasting</span><br>df[<span class="hljs-string">&quot;one_trunc&quot;</span>] = df[<span class="hljs-string">&quot;one&quot;</span>][:<span class="hljs-number">2</span>] <span class="hljs-comment"># 自动符合index</span><br>df.insert(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;bar&quot;</span>, df[<span class="hljs-string">&quot;one&quot;</span>])<br><br><span class="hljs-comment"># assign() 始终返回数据的副本，但是原始的dataframe并不会修改</span><br>iris.assign(sepal_ratio=<span class="hljs-keyword">lambda</span> x: (x[<span class="hljs-string">&quot;SepalWidth&quot;</span>] / x[<span class="hljs-string">&quot;SepalLength&quot;</span>])).head()<br>df.loc <span class="hljs-comment">#根据标签选择</span><br>df.iloc <span class="hljs-comment">#根据下标选择</span><br>df[<span class="hljs-number">5</span>:<span class="hljs-number">10</span>] <span class="hljs-comment"># 切片行</span><br>df[<span class="hljs-built_in">bool</span>] <span class="hljs-comment"># 切片行</span><br><br></code></pre></td></tr></table></figure><h3 id="32-数据缺失处理-运算函数"><a class="markdownIt-Anchor" href="#32-数据缺失处理-运算函数"></a> 3.2 数据缺失处理、运算函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">np.nan <span class="hljs-comment">#表示丢失的数据类型</span><br><br>df.reindex() <span class="hljs-comment"># 允许更改、添加和删除指定轴上的索引，同时返回数据副本</span><br>df1 = df.reindex(index=dates[<span class="hljs-number">0</span>:<span class="hljs-number">4</span>], columns=<span class="hljs-built_in">list</span>(df.columns) + [<span class="hljs-string">&quot;E&quot;</span>])<br>df1.loc[dates[<span class="hljs-number">0</span>] : dates[<span class="hljs-number">1</span>], <span class="hljs-string">&quot;E&quot;</span>] = <span class="hljs-number">1</span><br><br>df.dropna() <span class="hljs-comment"># 删除任何缺少数据的行</span><br>df.fillna() <span class="hljs-comment"># 填充确实的数据</span><br>isna() <span class="hljs-comment">#获取值是否为nan</span><br><br>默认按照broadcasting进行计算<br>df.mean() <span class="hljs-comment"># 计算平均值</span><br>df.mean(<span class="hljs-number">1</span>) <span class="hljs-comment"># 另外一个方向</span><br>df.apply() <span class="hljs-comment"># 将用户自定义的函数应用于数据</span><br>df.apply(np.cumsum)<br>df.apply(<span class="hljs-keyword">lambda</span> x: x.<span class="hljs-built_in">max</span>() - x.<span class="hljs-built_in">min</span>())<br><br>values_count()<br>df.<span class="hljs-built_in">str</span>.ufunc() <span class="hljs-comment"># 在str属性下，可以调用需要其他的字符串</span><br></code></pre></td></tr></table></figure><h3 id="33-集合之间的处理合并-分组-重塑和多索引"><a class="markdownIt-Anchor" href="#33-集合之间的处理合并-分组-重塑和多索引"></a> 3.3 集合之间的处理：合并、分组、重塑和多索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.concat([df1,df2,df3]) <span class="hljs-comment"># 按照axis进行合并，join参数为outer或者inner，ignore_inde重建索引</span><br>pd.merge(df1,df2,key) <span class="hljs-comment"># 类似SQL中的连接，可以根据一个或者多个键将不同的dataframe连接起来</span><br>pd.join() <span class="hljs-comment"># 用于key的合并</span><br><br>df.groupby(<span class="hljs-string">&quot;A&quot;</span>)[[<span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>]].<span class="hljs-built_in">sum</span>() <span class="hljs-comment"># 类似SQL中的group by</span><br><br><span class="hljs-comment"># 重塑和多索引看不懂～</span><br></code></pre></td></tr></table></figure><h3 id="34时间序列处理函数专题"><a class="markdownIt-Anchor" href="#34时间序列处理函数专题"></a> 3.4时间序列处理函数专题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">resample() <span class="hljs-comment">#将第二列数据转换成5min，有点意思</span><br>series.tz_localize() <span class="hljs-comment"># 将时间序列转换成为本地时区</span><br>series.tz_convert() <span class="hljs-comment"># 将时区感知时间序列转换成为另一个时区</span><br>.to_period() <span class="hljs-comment"># 转换成为时间跨度</span><br>.to_timestamp() <span class="hljs-comment"># 转换成为时间戳</span><br></code></pre></td></tr></table></figure><h2 id="0x04-pandas-用户教程"><a class="markdownIt-Anchor" href="#0x04-pandas-用户教程"></a> 0x04 Pandas 用户教程</h2><p>在用户教程中，Pandas提供了一些更加细致的专题。这里只给出有趣的话题</p><h3 id="41-数据结构简介-intro-to-data-structure"><a class="markdownIt-Anchor" href="#41-数据结构简介-intro-to-data-structure"></a> 4.1 数据结构简介 intro to data structure</h3><p>和第三章的类似</p><h3 id="42-关键基础函数-essential-basic-functionality"><a class="markdownIt-Anchor" href="#42-关键基础函数-essential-basic-functionality"></a> 4.2 关键基础函数 essential basic functionality</h3><p>Attention02:对于dataframe的操作可以三个维度：元素的维度、行或者列的维度、整个DataFrame的维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看</span><br>.head()<br>.tail()<br><br>.shape<br>.index<br>.columns<br><br>.to_numpy()<br>.array()<br><span class="hljs-comment"># Attention03:更加推荐使用这两个，而不是使用values来索引。因为dataframe中常见的操作包括扩展类型，或者不同类型，使用values可能会出现性能上的问题</span><br><br><span class="hljs-comment"># 运算（elementLevel）</span><br>df1 &gt; df2<br>df1*df2<br>df1+df2<br><br><span class="hljs-comment"># 二维运算，这里需要重视 1.高维和低维之间存在broadcasting 2.计算中数据丢失的情况</span><br>.add() <span class="hljs-comment"># 相加，可以设置axis函数</span><br>.sub() <span class="hljs-comment"># 相减，可以设置axis</span><br>.mul() <span class="hljs-comment"># 相乘，可以设置axis</span><br>.div() <span class="hljs-comment"># 相除，可以设置axis</span><br><br>.radd()<br>.rsub()<br>.<span class="hljs-built_in">divmod</span>() <span class="hljs-comment"># 商，和余</span><br><br><span class="hljs-comment"># 缺失值操作</span><br>fill_value=<span class="hljs-number">0</span> <span class="hljs-comment">#可选参数</span><br>.fill_na() <span class="hljs-comment">#利用函数填补</span><br><br><span class="hljs-comment"># 二进制比较方法 eq ne lt qt le ge</span><br><span class="hljs-comment"># 布尔运算符号</span><br>.empty() <span class="hljs-comment"># 测试对象是否为空</span><br>.<span class="hljs-built_in">all</span>() <span class="hljs-comment"># 测试并</span><br>.<span class="hljs-built_in">any</span>() <span class="hljs-comment"># 测试或</span><br>.<span class="hljs-built_in">bool</span>() <span class="hljs-comment"># 测试单个元素</span><br><br><span class="hljs-comment">#Attention 04：不可以使用 if df:...，这样不符合布尔逻辑</span><br><br><span class="hljs-comment"># 描述性分析</span><br><span class="hljs-built_in">sum</span>()<br>mean()<br>quantile()<br>cumsum()<br>cumprod()<br><br>describe()<br>idxmin() <span class="hljs-comment"># 寻找最小值下标 argmin</span><br>idxmax() <span class="hljs-comment"># 寻找最大值下标 argmax</span><br>res=df.value_counts()<br>mode()<br><br><span class="hljs-comment"># discretization and quantiling</span><br>cut() <span class="hljs-comment">#基于值的bin</span><br>qcut() <span class="hljs-comment">#基于分位数的bin</span><br><br><span class="hljs-comment"># 函数应用：pipeline(),apply(),applymap(); 聚合函数agg() and transform()</span><br><br><span class="hljs-comment"># 尝试想象一下如何新增两个列</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extractCityName</span>(<span class="hljs-params">df</span>):<br>df[<span class="hljs-string">&#x27;cityName&#x27;</span>]=df[<span class="hljs-string">&#x27;cityAndCode&#x27;</span>].<span class="hljs-built_in">str</span>.aplit(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>.get(<span class="hljs-number">0</span>)<br><span class="hljs-keyword">return</span> df<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">addCountryName</span>(<span class="hljs-params">df,countryName=<span class="hljs-literal">None</span></span>):<br>col=<span class="hljs-string">&#x27;cityName&#x27;</span><br>df[<span class="hljs-string">&#x27;cityAndCounty&#x27;</span>]=df[col]+countryName<br>df_p=pd.Dataframe(&#123;<span class="hljs-string">&#x27;cityAndCode&#x27;</span>:[<span class="hljs-string">&#x27;chicago&#x27;</span>,<span class="hljs-string">&#x27;IL&#x27;</span>]&#125;)<br><span class="hljs-comment"># 现在可以有两种方式来新增心得countryName</span><br>addCounryName(extractCityName(df_p),country_name=<span class="hljs-string">&#x27;US&#x27;</span>)<span class="hljs-comment">#法1</span><br>df_p.pipeline(extractCityName).pipe(addCountryName,countryName=<span class="hljs-string">&#x27;US&#x27;</span>)<span class="hljs-comment">#法2:或者使用pipeline</span><br><br><span class="hljs-comment"># 重索引 reindex</span><br><br><span class="hljs-comment"># 迭代 iteration</span><br><span class="hljs-keyword">for</span> label,ser <span class="hljs-keyword">in</span> df.items():<br>  <span class="hljs-built_in">print</span>(label)<br>  <span class="hljs-built_in">print</span>(ser)<br><span class="hljs-keyword">for</span> row_index,row <span class="hljs-keyword">in</span> df.iterrows():<br>  <span class="hljs-built_in">print</span>(row)<br><br><span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> df.itertuples():<br>  <span class="hljs-built_in">print</span>(row)<br><br><span class="hljs-comment"># 排序访问</span><br>.sort_index()<br>.sort_values()<br></code></pre></td></tr></table></figure><h3 id="45-索引和选择数据-indexing-and-selecting-data"><a class="markdownIt-Anchor" href="#45-索引和选择数据-indexing-and-selecting-data"></a> 4.5 索引和选择数据 Indexing and selecting data</h3><p>和新手教程差不多</p><h3 id="46-多重下标和高级索引-multiindex-and-advanced-indexing"><a class="markdownIt-Anchor" href="#46-多重下标和高级索引-multiindex-and-advanced-indexing"></a> 4.6 多重下标和高级索引 MultiIndex and advanced indexing</h3><p>和新手教程差不多</p><h3 id="465-merge-join-concatenate-compare"><a class="markdownIt-Anchor" href="#465-merge-join-concatenate-compare"></a> 4.6.5 Merge、join、concatenate、compare</h3><p>和SQL教程差不多</p><h3 id="47-重塑和数据透视表"><a class="markdownIt-Anchor" href="#47-重塑和数据透视表"></a> 4.7 重塑和数据透视表</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/reshaping_pivot.png" alt="../_images/reshaping_pivot.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pivot() 来按照元素进行堆叠,可以选择stack或者unstack</span><br>         date variable     value<br><span class="hljs-number">0</span>  <span class="hljs-number">2000</span>-01-03        A  <span class="hljs-number">0.469112</span><br><span class="hljs-number">1</span>  <span class="hljs-number">2000</span>-01-04        A -<span class="hljs-number">0.282863</span><br><span class="hljs-number">2</span>  <span class="hljs-number">2000</span>-01-05        A -<span class="hljs-number">1.509059</span><br><span class="hljs-number">3</span>  <span class="hljs-number">2000</span>-01-03        B -<span class="hljs-number">1.135632</span><br><span class="hljs-number">4</span>  <span class="hljs-number">2000</span>-01-04        B  <span class="hljs-number">1.212112</span><br><span class="hljs-number">5</span>  <span class="hljs-number">2000</span>-01-05        B -<span class="hljs-number">0.173215</span><br><span class="hljs-number">6</span>  <span class="hljs-number">2000</span>-01-03        C  <span class="hljs-number">0.119209</span><br><span class="hljs-number">7</span>  <span class="hljs-number">2000</span>-01-04        C -<span class="hljs-number">1.044236</span><br><span class="hljs-number">8</span>  <span class="hljs-number">2000</span>-01-05        C -<span class="hljs-number">0.861849</span><br><span class="hljs-number">9</span>  <span class="hljs-number">2000</span>-01-03        D -<span class="hljs-number">2.104569</span><br><span class="hljs-number">10</span> <span class="hljs-number">2000</span>-01-04        D -<span class="hljs-number">0.494929</span><br><span class="hljs-number">11</span> <span class="hljs-number">2000</span>-01-05        D  <span class="hljs-number">1.071804</span><br><br>pivoted = df.pivot(index=<span class="hljs-string">&quot;date&quot;</span>, columns=<span class="hljs-string">&quot;variable&quot;</span>, values=<span class="hljs-string">&quot;value&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/reshaping_stack.png" alt="../_images/reshaping_stack.png" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/reshaping_unstack.png" alt="../_images/reshaping_unstack.png" /></p><p>剩余的太理论了～</p><h2 id="0x05-心得"><a class="markdownIt-Anchor" href="#0x05-心得"></a> 0x05 心得</h2><p>文档很多，但是其中给出开发者的default的选项，可以提高对其的理解。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;官方文档是开发者对自己代码的解释。对于成熟的框架，官方文档可以最准确、权威的资料。如何阅读英文的官方文档一直是作为Coder weaker和English weaker的心魔，这里以Pandas文档为例子，尝试给出自己对于阅读官方文档、官方文档的查询工具、代码注释和Pandas文档的总结。&lt;/p&gt;</summary>
    
    
    
    <category term="Baseline" scheme="https://blog.tjdata.site/categories/Baseline/"/>
    
    
    <category term="pandas" scheme="https://blog.tjdata.site/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>macOS使用技巧_预览Preview</title>
    <link href="https://blog.tjdata.site/posts/de0c5bf1.html"/>
    <id>https://blog.tjdata.site/posts/de0c5bf1.html</id>
    <published>2023-05-04T05:39:12.000Z</published>
    <updated>2023-05-13T13:49:54.370Z</updated>
    
    <content type="html"><![CDATA[<p>预览Preview</p><span id="more"></span><p>在M系列的mac上，预览的可以将扫描的PDF中的文字转换成为可编辑状态。转换时间大概200页的PDF需要5分钟～</p><p>export就会有下列选项</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20230504134518543.png" alt="image-20230504134518543" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;预览Preview&lt;/p&gt;</summary>
    
    
    
    <category term="Memo" scheme="https://blog.tjdata.site/categories/Memo/"/>
    
    
    <category term="使用技巧" scheme="https://blog.tjdata.site/tags/%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>CS229-10-problemSet01</title>
    <link href="https://blog.tjdata.site/posts/7b309918.html"/>
    <id>https://blog.tjdata.site/posts/7b309918.html</id>
    <published>2023-01-10T01:03:17.000Z</published>
    <updated>2023-05-13T13:47:47.454Z</updated>
    
    <content type="html"><![CDATA[<p>Problem set00 是关于线性代数和多元微积分学的基本知识，Problem set 01主要是监督学习。作业要求最好使用LaTex进行编写，同时需要将library保存到environment.yml文件中，并保证run.py脚本可以正常运行。</p><span id="more"></span><h2 id="0x01-线性分类逻辑回归和广义线性模型"><a class="markdownIt-Anchor" href="#0x01-线性分类逻辑回归和广义线性模型"></a> 0x01 线性分类（逻辑回归和广义线性模型）</h2><h3 id="11-问题回顾"><a class="markdownIt-Anchor" href="#11-问题回顾"></a> 1.1 问题回顾</h3><p>Linear classifiers ( logistic regression and GDA)</p><p>在本次作业中将回顾之前的概率线性分类器，</p><ol><li>判别线性分类 (discriminative linear classifier) ：逻辑回归</li><li>生成线性分类 （generative linear classifier）：高斯判别模型</li></ol><p>两者均可以将一个数据集分成两类，但是基于不同的假设，本次问题的目的是找到两者的相同点和差异。</p><h3 id="12-实际问题"><a class="markdownIt-Anchor" href="#12-实际问题"></a> 1.2 实际问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301100927616.png" alt="问题1-(a)" /></p><p>注意Hessian矩阵为PSD，则说明损失函数是凸（convex）的，也就是存在极值点</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo separator="true">,</mo><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>)</mo><mo>=</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo separator="true">,</mo><mi>x</mi><mo separator="true">,</mo><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>)</mo><mi mathvariant="normal">/</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>)</mo><mo>=</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">∣</mi><mi>y</mi><mo>)</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>)</mo><mi mathvariant="normal">/</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(y|x,theta)=p(y,x,theta)/p(x,theta)=p(x,theta|y)p(y)/p(x,theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">t</span><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">t</span><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mclose">)</span><span class="mord mathrm">/</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">t</span><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">t</span><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathrm">/</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit">t</span><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span></span></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301101647405.png" alt="1-b" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301101647666.png" alt="1-c" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301101650344.png" alt="1-c(2)" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301101650705.png" alt="1-d" /></p><h3 id="13-问题求解"><a class="markdownIt-Anchor" href="#13-问题求解"></a> 1.3 问题求解</h3><p>由于公式较多，只能给出思路</p><p>（1） 展开求导就能发现是大于0</p><p>（2）coding就完事了，numpy的MATLAB的矩阵计算方式</p><p>（3）GDA按照贝叶斯展开就行了</p><p>（4） 发现是相同的</p><p>（5）（6）（7）（8）是一些可视化的东西</p><h2 id="0x02-不完整-只有正标签incompletepositive-only-labels"><a class="markdownIt-Anchor" href="#0x02-不完整-只有正标签incompletepositive-only-labels"></a> 0x02 不完整、只有正标签：Incomplete，Positive- only labels</h2><h3 id="21-问题回顾"><a class="markdownIt-Anchor" href="#21-问题回顾"></a> 2.1 问题回顾</h3><p>假设在我们没有得到完整的标签的情况下的，只能确定性得到部分的正样本，而不能得到其他样本的标签。也就是所有负样本和剩余的正样本是没有标签的。</p><p>这道题的问题设置更像是一种引导，如何理解这种情况下如何构建模型</p><h3 id="22-问题"><a class="markdownIt-Anchor" href="#22-问题"></a> 2.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301101657770.png" alt="2-a" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301101657662.png" alt="2-b" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301101657533.png" alt="2-c" /></p><p>后续是那个coding problem</p><h3 id="23-问题解答"><a class="markdownIt-Anchor" href="#23-问题解答"></a> 2.3 问题解答</h3><p>（1） 简单随机样本抽样</p><h2 id="0x03-泊松分布的拟合"><a class="markdownIt-Anchor" href="#0x03-泊松分布的拟合"></a> 0x03 泊松分布的拟合</h2><p>重新回归GLM中的三个基本假设：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301102059026.png" alt="GLM-assumption" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301102059330.png" alt="GLM-assumption02" /></p><p>本重新认识到：</p><ol><li>第一个assumption给出了y的分布！这可以用来计算似然函数</li><li>第二个assumption给出如何确定hypothesis，也就是模型</li><li>第三个assumption似乎是人们设计好的，这样我们才能对一些线性模型做假设</li></ol><h2 id="0x04-广义线性模型的convex相关的研究"><a class="markdownIt-Anchor" href="#0x04-广义线性模型的convex相关的研究"></a> 0x04 广义线性模型的convex相关的研究</h2><p>太理论了看不下去～</p><h2 id="0x05-加权线性回归"><a class="markdownIt-Anchor" href="#0x05-加权线性回归"></a> 0x05 加权线性回归</h2><p>除了 利用gradient descent求解，也可以使用normal equation来进行参数的求解；非常的奇妙！</p><p>注意这部分和后面的attention有关系</p><p>后面的公式太多了不想打，</p><h2 id="0x06-感悟"><a class="markdownIt-Anchor" href="#0x06-感悟"></a> 0x06 感悟</h2><p>动手才能发现学习的问题所在，动手才能知道代码应该如何实践！</p><p>对于现实世界的抽象可以帮助我们进一步学习和研究，虽然矩阵的定义很简单，但是只有在这个抽象定义的基础上才能发展出线性代数这门学科，才能用此解决问题。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Problem set00 是关于线性代数和多元微积分学的基本知识，Problem set 01主要是监督学习。作业要求最好使用LaTex进行编写，同时需要将library保存到environment.yml文件中，并保证run.py脚本可以正常运行。&lt;/p&gt;</summary>
    
    
    
    <category term="ML&amp;DL" scheme="https://blog.tjdata.site/categories/ML-DL/"/>
    
    
    <category term="CS229" scheme="https://blog.tjdata.site/tags/CS229/"/>
    
  </entry>
  
  <entry>
    <title>CS229-10-problemSet00</title>
    <link href="https://blog.tjdata.site/posts/c37a98e.html"/>
    <id>https://blog.tjdata.site/posts/c37a98e.html</id>
    <published>2023-01-09T02:11:12.000Z</published>
    <updated>2023-05-13T13:47:43.837Z</updated>
    
    <content type="html"><![CDATA[<p>CS229的homework之前一直没有写，趁这个寒假结束掉它！如有错误欢迎指正！</p><span id="more"></span><h2 id="0x01-gradients-and-hessians求导和海森矩阵"><a class="markdownIt-Anchor" href="#0x01-gradients-and-hessians求导和海森矩阵"></a> 0x01 Gradients and Hessians：<a href="https://en.wikipedia.org/wiki/Gradient">求导</a>和<a href="https://zh.m.wikipedia.org/zh-hans/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%99%A3">海森矩阵</a></h2><h3 id="11-定义回顾"><a class="markdownIt-Anchor" href="#11-定义回顾"></a> 1.1 定义回顾</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301091015744.png" alt="多元函数一阶导" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301091016793.png" alt="多元函数二阶导" /></p><p><a href="https://zh.m.wikipedia.org/zh-hans/%E5%B0%8D%E7%A8%B1%E7%9F%A9%E9%99%A3">Symmetric</a>: 对称矩阵</p><h3 id="12-问题"><a class="markdownIt-Anchor" href="#12-问题"></a> 1.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301091018345.png" alt="问题" /></p><h3 id="13-解答"><a class="markdownIt-Anchor" href="#13-解答"></a> 1.3 解答</h3><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>1</mn><mo>)</mo><mi mathvariant="normal">∇</mi><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>A</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">(1)\nabla f(x)=Ax+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">1</span><span class="mclose">)</span><span class="mord mathrm">∇</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">A</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit">b</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>2</mn><mo>)</mo><mi mathvariant="normal">∇</mi><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msup><mi>g</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>(</mo><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mi mathvariant="normal">∇</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(2)\nabla f(x)=g&#x27;(h(x)) \nabla h(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">2</span><span class="mclose">)</span><span class="mord mathrm">∇</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord mathrm">∇</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>3</mn><mo>)</mo><msup><mi mathvariant="normal">∇</mi><mn>2</mn></msup><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mo>=</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">(3)\nabla^2f(x)=A^T=A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8913309999999999em;"></span><span class="strut bottom" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">3</span><span class="mclose">)</span><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit">A</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit">A</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>4</mn><mo>)</mo><msup><mi mathvariant="normal">∇</mi><mn>2</mn></msup><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msup><mi>g</mi><mrow><mi mathvariant="normal">′</mi><mi mathvariant="normal">′</mi></mrow></msup><mo>(</mo><msup><mi>a</mi><mi>T</mi></msup><mi>x</mi><mo>)</mo><msup><mi>a</mi><mi>T</mi></msup><mi>a</mi></mrow><annotation encoding="application/x-tex">(4) \nabla^2 f(x)=g&#x27;&#x27;(a^Tx)a^Ta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8913309999999999em;"></span><span class="strut bottom" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">4</span><span class="mclose">)</span><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit">a</span></span></span></span></span></p><h2 id="0x02-positive-definite-matrices-正定矩阵"><a class="markdownIt-Anchor" href="#0x02-positive-definite-matrices-正定矩阵"></a> 0x02 Positive definite matrices : 正定矩阵</h2><h3 id="21-定义回顾"><a class="markdownIt-Anchor" href="#21-定义回顾"></a> 2.1 定义回顾</h3><p>positive semi-definite(PSD): 半正定矩阵</p><p><a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5">positive definite: 正定矩阵，eg：单位阵</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301091027124.png" alt="正定矩阵定义" /></p><p><a href="https://zh.wikipedia.org/wiki/%E9%9B%B6%E7%A9%BA%E9%97%B4">Null-space:</a> 核，表示一个算子的零空间是方程<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mi>V</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">AV=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span>的所有解<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">v</span></span></span></span>的集合</p><p><a href="https://zh.wikipedia.org/wiki/%E7%A7%A9_(%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0)">Rank</a>: 矩阵A的列秩是A线性无关的纵列的极大数目；可以用于计算线性方程组解的树木、也可以用来确定线性系统是否为可控制的、可观察的</p><h3 id="22-问题"><a class="markdownIt-Anchor" href="#22-问题"></a> 2.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301091028776.png" alt="2.2 问题" /></p><h3 id="23-解答"><a class="markdownIt-Anchor" href="#23-解答"></a> 2.3 解答</h3>(a) A^T=(zz^T)=z^Tz=zz^T=A\\x^Tzz^Tx=(x^Tz)(x^Tz)^T \geq0(b) Null（A）=\{x \in Z;Ax=0\}<p>两边同时乘<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">x^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>z</mi><msup><mi>z</mi><mi>T</mi></msup><mi>x</mi><mo>=</mo><mo>(</mo><msup><mi>z</mi><mi>T</mi></msup><mi>x</mi><msup><mo>)</mo><mi>T</mi></msup><mo>(</mo><msup><mi>z</mi><mi>T</mi></msup><mi>x</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x^Tzz^Tx=(z^Tx)^T(z^Tx)=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8913309999999999em;"></span><span class="strut bottom" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit">x</span><span class="mrel">=</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit">x</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span></span></p><p>可以化简为</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mi>u</mi><mi>l</mi><mi>l</mi><mo>(</mo><mi>A</mi><mo>)</mo><mo>=</mo><mi>N</mi><mi>u</mi><mi>l</mi><mi>l</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mrow><mi>x</mi><mo>∈</mo><mi>z</mi><mo separator="true">,</mo><msup><mi>z</mi><mi>x</mi></msup><mo>=</mo><mn>0</mn></mrow></mrow><annotation encoding="application/x-tex">Null(A)=Null(z)={x \in z, z^x=0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit">A</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">∈</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">x</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span></span></span></p><p>因为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">Rank(z) \leq 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mord mathit">a</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mrel">≤</span><span class="mord mathrm">1</span></span></span></span>,但是z非零;结合<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mi>u</mi><mi>l</mi><mi>l</mi><mo>(</mo><mi>A</mi><mo>)</mo><mo>=</mo><mi>N</mi><mi>u</mi><mi>l</mi><mi>l</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">Null(A)=Null(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit">A</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo>(</mo><mi>A</mi><mo>)</mo><mo>=</mo><mi>R</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">R(A)=R(z)=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathit">A</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>c</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(c)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">c</span><span class="mclose">)</span></span></span></span></span></p><p>结合A的PSD特点可以证明，略</p><h2 id="0x03-eigenvectorseigenvaluesspectral-theorem特征向量-特征值-谱定理"><a class="markdownIt-Anchor" href="#0x03-eigenvectorseigenvaluesspectral-theorem特征向量-特征值-谱定理"></a> 0x03 Eigenvectors，eigenvalues，spectral theorem：特征向量、特征值、谱定理</h2><h3 id="31-定义回顾"><a class="markdownIt-Anchor" href="#31-定义回顾"></a> 3.1 定义回顾</h3><p>Eigenvectors &amp; Eigenvalues：特征值和特征向量</p><p>求解<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>A</mi></msub><mo>(</mo><mi>λ</mi><mo>)</mo><mo>=</mo><mi>d</mi><mi>e</mi><mi>t</mi><mo>(</mo><mi>λ</mi><mi>I</mi><mo>−</mo><mi>A</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p_A(\lambda)=det(\lambda I-A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">A</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">λ</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mopen">(</span><span class="mord mathit">λ</span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mbin">−</span><span class="mord mathit">A</span><span class="mclose">)</span></span></span></span>或者<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>λ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">Ax=\lambda x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathit">λ</span><span class="mord mathit">x</span></span></span></span>之间的关系</p><p>Diagonal matrix：对角矩阵，可以用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>t</mi><mo>(</mo><mi>d</mi><mn>1</mn><mo separator="true">,</mo><mi>d</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">det(d1,d2,...)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="mord mathit">d</span><span class="mord mathrm">2</span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mclose">)</span></span></span></span>表示</p><p>Orthogonal：正交矩阵</p><p><a href="https://zh.m.wikipedia.org/zh-hans/%E8%B0%B1%E5%AE%9A%E7%90%86">Spectral theorem：谱定理</a></p><h3 id="32-问题"><a class="markdownIt-Anchor" href="#32-问题"></a> 3.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301091102746.png" alt="image-20230109110250716" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202301091102792.png" alt="image-20230109110258767" /></p><h3 id="33-解读"><a class="markdownIt-Anchor" href="#33-解读"></a> 3.3 解读</h3><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span></span></p><p>两边同乘逆</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mi>T</mi><mo>=</mo><mi>T</mi><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">AT=T\Lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">A</span><span class="mord mathit" style="margin-right:0.13889em;">T</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.13889em;">T</span><span class="mord mathrm">Λ</span></span></span></span></span></p><p>又<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Λ</mi><mo>=</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo>(</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>λ</mi><mi>n</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\Lambda=diag(\lambda_1,...,\lambda_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">Λ</span><span class="mrel">=</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">λ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">λ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><msub><mi>λ</mi><mi>i</mi></msub><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">At^{(i)}=\lambda_it^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.938em;"></span><span class="strut bottom" style="height:1.0879999999999999em;vertical-align:-0.15em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">A</span><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit">λ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p><p>所以特征值对应的向量为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>λ</mi><mi>i</mi></msub><mo separator="true">,</mo><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">(\lambda_i,t^{(i)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8879999999999999em;"></span><span class="strut bottom" style="height:1.138em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit">λ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">t</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathit">i</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>b</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">b</span><span class="mclose">)</span></span></span></span></span></p><p>因为A是对称矩阵，U为正交矩阵，同时</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mi>U</mi><mo>=</mo><mi mathvariant="normal">Λ</mi><mi>U</mi></mrow><annotation encoding="application/x-tex">AU=\Lambda U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">A</span><span class="mord mathit" style="margin-right:0.10903em;">U</span><span class="mrel">=</span><span class="mord mathrm">Λ</span><span class="mord mathit" style="margin-right:0.10903em;">U</span></span></span></span></span></p><p>所以同上 $$Au<sup>{(i)}=\lambda_iu</sup>{(i)}$$</p>(c)$$ 根据谱定理可以得到$$\Lambda=U^TAU \geq 0<p>因此大于等于0</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;CS229的homework之前一直没有写，趁这个寒假结束掉它！如有错误欢迎指正！&lt;/p&gt;</summary>
    
    
    
    <category term="ML&amp;DL" scheme="https://blog.tjdata.site/categories/ML-DL/"/>
    
    
    <category term="CS229" scheme="https://blog.tjdata.site/tags/CS229/"/>
    
  </entry>
  
</feed>
