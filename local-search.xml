<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>小白的在线支付方式折腾</title>
    <link href="/posts/16132.html"/>
    <url>/posts/16132.html</url>
    
    <content type="html"><![CDATA[<p>因为希望开通 OPENAI-API，需要有美区发行的信用卡，由此折腾了虚拟信用卡 wildcard、国内 MasterCard、美区 PayPal 来完成 OPENAI、App Store、Amazon 海淘等过程，以下为小白经验贴，如有不对的地方欢迎指正</p><h2 id="一、支付方式简介"><a href="#一、支付方式简介" class="headerlink" title="一、支付方式简介"></a>一、支付方式简介</h2><p>支付方式通常可以分为：银行卡支付、移动支付、数字钱包、银行转账、加密货币、预付卡、电子支票等方式，这里注重解释前三者：</p><ol><li>银行卡支付（Bank card payment），进一步分为信用卡（Credit card）和借记卡（Debit card）</li><li>移动支付（Mobile payment），包括 Apple pay、Google Pay、支付宝（Alipay）、微信支付（WeChat Pay）</li><li>数字钱包（Digital wallet and Online Payment），包括 Paypal、Venmo 等等</li></ol><p>支付中的参与方包括：</p><ol><li>发卡机构（Issuer），通常为银行，负责提供卡片</li><li>支付网络（Payment Network），连接发卡机构和收单机构的中介，处理交易请求和验证</li><li>收单机构（Acquirer），为商户提供银行卡支付处理服务的金融机构</li><li>商户（Merchant），提供商品或者服务的商家</li></ol><p>支付网络可以分为</p><ol><li>**全球性支付网络 (Global Payment Networks)**：这些网络覆盖全球，适用于国际和跨境交易。常见的全球性支付网络包括：</li></ol><p>•<strong>Visa</strong>：覆盖全球的大型信用卡和借记卡支付网络。</p><p>•<strong>MasterCard</strong>：全球通用的支付网络，支持信用卡、借记卡等多种支付方式。</p><p>•<strong>American Express</strong>：提供信用卡支付，覆盖全球多个国家，主要采用三方模式。</p><p>•**UnionPay (银联)**：在中国广泛使用，近年来也在全球扩展，支持信用卡、借记卡和二维码支付。</p><ol><li>**地区性支付网络 (Regional Payment Networks)**：这些支付网络主要在特定区域内使用，通常只支持该地区的银行账户或支付工具。</li></ol><p>•<strong>JCB</strong>（日本）：在亚洲和一些欧洲、北美市场使用广泛。</p><p>•<strong>Interac</strong>（加拿大）：用于加拿大的借记卡支付。</p><p>•<strong>Elo</strong>（巴西）：在巴西使用的支付网络，支持借记和信用支付。</p><p>•<strong>RuPay</strong>（印度）：主要用于印度国内支付。</p><h2 id="二、支付需求"><a href="#二、支付需求" class="headerlink" title="二、支付需求"></a>二、支付需求</h2><h3 id="2-1-OPENAI-支付要求"><a href="#2-1-OPENAI-支付要求" class="headerlink" title="2.1 OPENAI 支付要求"></a>2.1 OPENAI 支付要求</h3><p>支付要求包括</p><blockquote><p>OpenAI 的充值支付方式要求包括以下几点：</p><ol><li><strong>国际信用卡或借记卡</strong>：支持 Visa、MasterCard 和 American Express 等主流国际卡。确保卡片具有国际支付功能，国内银行的银联卡通常不支持。</li><li><strong>PayPal</strong>：在部分地区，OpenAI 也支持通过 PayPal 支付。需要一个经过验证的 PayPal 账户，并关联到支持国际支付的信用卡或银行账户。</li><li><strong>Apple Pay 和 Google Pay</strong>：部分移动端支持 Apple Pay 或 Google Pay 付款，但前提是这些支付账户也必须关联到支持国际支付的信用卡或借记卡。</li><li><strong>虚拟卡</strong>：部分虚拟卡（如虚拟预付卡）可能会被接受，但成功率不高，因为 OpenAI 对支付账户的真实性验证较为严格。</li><li><strong>支付地区限制</strong>：目前 OpenAI 充值服务并非对所有国家和地区开放，部分地区可能无法使用其服务，需参考 OpenAI 官网的支持列表。</li></ol><p>建议在充值前确保支付方式符合以上条件，并检查账户的国际支付功能是否开启，以避免支付失败。</p></blockquote><h3 id="2-2-Amazon-支付要求"><a href="#2-2-Amazon-支付要求" class="headerlink" title="2.2 Amazon 支付要求"></a>2.2 Amazon 支付要求</h3><blockquote><ol><li><strong>信用卡和借记卡</strong>：亚马逊支持主要的国际信用卡和借记卡品牌，如 Visa、MasterCard、American Express、Discover 等。在中国，您可以使用带有银联标识的信用卡或借记卡进行支付。</li><li><strong>第三方支付平台</strong>：在亚马逊中国（<a href="http://amazon.cn/">Amazon.cn</a>），您可以使用支付宝和微信支付等本地支付方式。</li></ol></blockquote><h3 id="2-3-Apple-Store-美区支付要求"><a href="#2-3-Apple-Store-美区支付要求" class="headerlink" title="2.3 Apple Store 美区支付要求"></a>2.3 Apple Store 美区支付要求</h3><blockquote><p>•<strong>美国发行的信用卡或借记卡</strong>：Apple Store 接受由美国银行发行的 Visa、MasterCard、American Express 等信用卡或借记卡。</p><p>•<strong>美国 PayPal 账户</strong>：您可以将美国 PayPal 账户绑定到您的 Apple ID 作为支付方式。</p><p>•<strong>Apple 礼品卡</strong>：购买并兑换美国地区的 Apple 礼品卡，可将其余额用于支付。</p></blockquote><h3 id="2-4-Paypal-支付方式要求"><a href="#2-4-Paypal-支付方式要求" class="headerlink" title="2.4 Paypal 支付方式要求"></a>2.4 Paypal 支付方式要求</h3><blockquote><ol><li><strong>有效的支付方式</strong>：</li></ol><p>•<strong>美国银行账户</strong>：您可以将美国的银行账户关联到您的 PayPal 账户，用于充值和支付。</p><p>•<strong>信用卡或借记卡</strong>：PayPal 支持 Visa、MasterCard、American Express 等主要信用卡和借记卡。确保您的卡片已关联到 PayPal 账户。</p><ol><li><strong>美国账单地址</strong>：在设置支付方式时，通常需要提供一个有效的美国账单地址。您可以使用真实的美国地址，或通过地址生成器获取。</li><li><strong>美国电话号码</strong>：在某些情况下，可能需要提供一个美国电话号码。您可以使用虚拟号码服务，或通过其他方式获取。</li><li><strong>账户验证</strong>：为确保账户安全，PayPal 可能要求您验证关联的银行卡或银行账户。验证过程可能包括小额扣款或其他确认步骤。</li></ol></blockquote><h2 id="三、折腾记录"><a href="#三、折腾记录" class="headerlink" title="三、折腾记录"></a>三、折腾记录</h2><h3 id="3-1-信用卡"><a href="#3-1-信用卡" class="headerlink" title="3.1 信用卡"></a>3.1 信用卡</h3><p><strong>第一种是开通虚拟卡</strong>，这里选择 wildcard 平台</p><p>非常方便但是需要付出手续费，我刚用不到 5 分钟升级了 openai plus 和绑了 api 付费，对于小白来说很方便，因为邀请有奖励这里做一些分享</p><ul><li>官网地址：<a href="https://bewildcard.com/card">https://bewildcard.com/card</a></li><li>创建账户费用： 11.99&#x2F;两年，16.99&#x2F;三年</li><li>手续费：3.5%</li><li>限额：3000 美元&#x2F;天</li><li>使用邀请码可以优惠 <a href="https://bewildcard.com/i/MP40YHIM">https://bewildcard.com/i/MP40YHIM</a></li></ul><p>开通之后可以 openai api platform 绑定自己的 card 作为支付方式，或者利用wildcard 提供的服务来绑定，官方提供 ChatGPT 一键升级的功能</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20241117102454663.png" alt="image-20241117102454663"></p><p>第二种是实际信用卡，这里不再赘述。</p><p>【上述两种方式都可以支付购买服务了！比如 Amazon、软件购买】</p><p>【但是中国发卡机构发放的支持支付组织的卡大部分情况不能直接完成美国相关线上服务的购买，比如 Apple Store、Google Pay 等】</p><h3 id="3-2-美区Paypal"><a href="#3-2-美区Paypal" class="headerlink" title="3.2 美区Paypal"></a>3.2 美区Paypal</h3><p>这里需要有一个非虚拟号的美国手机号，但是大部分都没有，这里有一个操作是可以通过商户支付页面进行注册，可以是为 Wiki 捐赠页面</p><p><a href="https://donate.wikimedia.org/w/index.php?title=Special:LandingPage&country=HK&uselang=en&wmf_medium=spontaneous&wmf_source=fr-redir&wmf_campaign=spontaneous">https://donate.wikimedia.org/w/index.php?title=Special:LandingPage&amp;country=HK&amp;uselang=en&amp;wmf_medium=spontaneous&amp;wmf_source=fr-redir&amp;wmf_campaign=spontaneous</a></p><p>【注意注册 PayPal 过程中全程美区 IP + 无痕浏览模式】</p><p>【之后可以通过 PayPal 来绑定美区 Apple store】</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数字移民</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据与分布式系统概述</title>
    <link href="/posts/51817.html"/>
    <url>/posts/51817.html</url>
    
    <content type="html"><![CDATA[<p>这里系统自顶向下的对于大数据系统下的分布式平台做一次综述，包括一些简单的实际操作。</p><h1 id="一、背景：什么是分布式系统"><a href="#一、背景：什么是分布式系统" class="headerlink" title="一、背景：什么是分布式系统"></a>一、背景：什么是分布式系统</h1><p><em><a href="https://link.springer.com/article/10.1007/s00607-016-0508-7">A brief introduction to distributed systems</a></em></p><p>分布式系统是以单一完整系统（Single coherent system）为表现的自助计算单元的集合（Collections of autonomous computing elements），实现的技术基础为：</p><ol><li>Node 性能强大的<strong>微处理器</strong></li><li>高速发展的<strong>计算机网络</strong></li></ol><p>为了完成上述目标，分布式系统需要满足一下四个准则：</p><ol><li>资源共享，包括外围设备、存储设备、数据、文件、服务和网络</li><li>分配透明化</li><li>开放性</li><li>具备可拓展性</li></ol><p>分布式系统的主要特点包括：</p><ol><li>分布式（<strong>Distribution</strong>）：节点可以分布在不同的计算机上</li><li>同步 （<strong>Synchronization</strong>）：多个节点可以通过执行任务</li><li>异步（<strong>Asynchrony</strong>）：节点之间通过信息传递进行通信，不同节点可以再不同的时间执行不同的任务</li></ol><p>常见的分布式系统为：</p><ol><li>高性能的分布式计算，包括集群计算、网格计算、云计算</li><li>分布式信息系统（不懂）</li><li>普适系统（不懂）</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20241021103055087.png" alt="云计算服务的分类"></p><h1 id="二、分布式计算框架组成"><a href="#二、分布式计算框架组成" class="headerlink" title="二、分布式计算框架组成"></a>二、分布式计算框架组成</h1><p><a href="https://ieeexplore.ieee.org/document/10026506"><em>Survey of Distributed Computing Frameworks for Supporting Big Data Analysis</em></a></p><h3 id="分布式系统架构-（System-architectures）"><a href="#分布式系统架构-（System-architectures）" class="headerlink" title="分布式系统架构 （System architectures）"></a>分布式系统架构 （System architectures）</h3><p>分布式系统需要大量的硬件资源：硬盘、内存、CPU&#x2F;GPU、传输带宽、IO 速度，这样的系统框架可以分为</p><ol><li>HPC，高性能计算系统常见的是用于解决复杂科学、工程和学术问题的超级计算机大型集群，特征是通过并行计算加速任务执行，通常处理的是高度复杂的计算机任务，包括气象模拟、基因组分析、流体动力学模拟</li><li>Cluster computing，主要用于并行处理、分布式计算和大规模数据处理，集群的目的是通过协作提高系统的可拓展性和可靠性</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20241021110228456.png" alt="Map Reduce 分布式框架"></p><h3 id="分布式文件系统-（Distributed-file-system）"><a href="#分布式文件系统-（Distributed-file-system）" class="headerlink" title="分布式文件系统 （Distributed file system）"></a>分布式文件系统 （Distributed file system）</h3><p>将一个大数据文件分区（partition）成为数个小文件块（data block）存储再分布式集群（cluster）中的节点（node）里，常见的分布式存储系统包括：</p><ul><li>GFS（Google File System）</li><li>HDFS（Hadoop Distributed File System） from Apache Hadoop</li><li>TFS（Taobao File System）</li><li>FastDFS</li><li>CEPH：统一分布式存储系统，支持对象、块和文件存储</li></ul><h3 id="分布式数据库"><a href="#分布式数据库" class="headerlink" title="分布式数据库"></a>分布式数据库</h3><ul><li><p>HBase，在上述文件系统的基础上，如基于 HDFS 构造NoSQL 的数据库 Hbase ，通过 HiveSQL 将 SQL 语句转换成为 Map Reduce 语句来实现计算功能。其本身并不是真正意义上的数据库，只是将结构化的数据文件转换成为一种数据库表，因此本质上是一种内存的 Hash 表，所以其相对于传统的关系型 SQL 会需要加上 Partition 和 Bucket 的字段</p></li><li><p>ClickHouse 用于联机分析处理的开源列式数据库</p></li><li><p>ElasticSearch 基于 Lucene 库的全文搜索引擎</p></li><li><p>Redis Cluster基于内存的键值对数据库</p></li><li><p>Mongo DB</p></li><li><p>Cassandra</p></li><li><p>DynamoDB</p></li><li><p>TiDB</p></li></ul><p><strong>注：普通的关系数据库的区别</strong></p><p>普通的关系性数据库常见的使用的是服务器本地的文件系统，利用自身设计的计算模型来实现实时查询，具备介绍的拓展性</p><h3 id="分布式计算处理框架-（distributed-process）"><a href="#分布式计算处理框架-（distributed-process）" class="headerlink" title="分布式计算处理框架 （distributed process）"></a>分布式计算处理框架 （distributed process）</h3><p>分布式计算是通过将算法切分成可以并行的处理框架来实现高效的计算效果，其最基本的思想是通过“分治法（Divide-and-conquer）”来进行计算，通常可以分为是基于离线数据处理和在线数据处理两种方法，常见的处理框架为</p><ul><li><strong>Apache Hadoop</strong>，基于 Mapreduce 模型，主要用于处理大规模数据，Hive 是基于 HDFS 的数据仓库工具，用于数据提取、转化和加载、可以实现数据存储、查询和分析存储的大规模数据的机制，适合用海量数据做数据挖掘，但是实时性较差，但是计算能力和存储拓展方便</li><li><strong>Apache spark</strong>，基于内存的数据处理，是强大的分布式处理框架，处理速度更快，且更加适合迭代计算的任务，主要组件包括 Spark core、Spark streaming、MLlib、GraphX</li><li><strong>Apache Flink</strong>，流处理框架</li><li>Kafka Streams，允许直接在消息中处理数据，用于消息传递系统中进行流式计算</li><li>Map-Reduce ，最原始的版本，在单一节点上进行计算来得到本地结构 local result，汇合多个节点的计算结果，来得到 global result</li></ul><h3 id="其余的平台（platform）与包（package）"><a href="#其余的平台（platform）与包（package）" class="headerlink" title="其余的平台（platform）与包（package）"></a>其余的平台（platform）与包（package）</h3><ul><li>基本程序语言：JVM、Java、Scala（面向对象和函数式编程）、Python</li><li>采集和传输：Kafaka（分布式订阅消息系统）、Flume、Datax</li><li>数据湖：在数据存储和数据处理之间：Iceberg、Hudi</li><li>资源调度框架：YARN、KUbernets、Mesos</li></ul><h1 id="三、如何学习分布式计算系统"><a href="#三、如何学习分布式计算系统" class="headerlink" title="三、如何学习分布式计算系统"></a>三、如何学习分布式计算系统</h1><h2 id="2-1-编程基础"><a href="#2-1-编程基础" class="headerlink" title="2.1 编程基础"></a>2.1 编程基础</h2><h3 id="Scala基础"><a href="#Scala基础" class="headerlink" title="Scala基础"></a>Scala基础</h3><p><a href="https://www.runoob.com/scala/scala-tutorial.html">Scala 菜鸟教程</a></p><p>Scala 是一门类 Java 的编程语言，结合了面向对象编程和函数式编程</p><ul><li>每个值都是一个对象，对象的类型和行为由类定义，不同的类可以通过混入的方式混合在一起</li><li>函数式编程芋圆，原生支持嵌套函数定义和高阶函数</li></ul><h3 id="PySpark-基础"><a href="#PySpark-基础" class="headerlink" title="PySpark 基础"></a>PySpark 基础</h3><p><a href="https://help.aliyun.com/zh/emr/emr-on-ecs/user-guide/use-spark-shell-and-rdds?spm=a2c4g.11186623.0.0.12522e0b0BmqTk">阿里云文档</a></p><p>需要深刻理解如何使用 pyspark 来完成 spark 的算力的厂检查熬做</p><h2 id="2-2-计算框架-Spark"><a href="#2-2-计算框架-Spark" class="headerlink" title="2.2 计算框架 Spark"></a>2.2 计算框架 Spark</h2><p><em><a href="https://spark.apache.org/docs/3.5.2/">Spark documentation overview</a></em></p><p>关键的Topic 包括</p><ol><li><strong>快速理解 spark 的 API</strong></li><li><strong>理解如何使用 RDD 来进行编程、核心 API、加速和广播操作</strong></li><li><strong>理解 SPARK 的 SQL、Dataset、Dataframe，利用最新的操作 API</strong></li><li><em>流处理structured streaming、spark streaming（可选）</em></li><li><strong>机器学习库 MLlib</strong></li><li><strong>图网络学习 GraphX</strong></li><li><em>利用 R 语言来进行学习（可选）</em></li><li><strong>利用 Python 来使用 Spark</strong></li></ol><h2 id="2-3-额外知识"><a href="#2-3-额外知识" class="headerlink" title="2.3 额外知识"></a>2.3 额外知识</h2><h4 id="SQL-CRUD"><a href="#SQL-CRUD" class="headerlink" title="SQL - CRUD"></a>SQL - CRUD</h4><h4 id="HDFS-常见操作"><a href="#HDFS-常见操作" class="headerlink" title="HDFS - 常见操作"></a>HDFS - 常见操作</h4><h4 id="Flink-流数据处理-pipeline（可选）"><a href="#Flink-流数据处理-pipeline（可选）" class="headerlink" title="Flink 流数据处理 pipeline（可选）"></a>Flink 流数据处理 pipeline（可选）</h4><ol><li>首先利用 Flink 从实时流数据采集中获取数据流，例如用户的行为日志</li><li>利用 Flink 处理的 api 来完成数据的清晰、格式化和特征提取操作（基于 MapReduce）</li><li>调用深度学习模型来完成预测，Map 操作</li><li>将更新的数据输入到 Kafka，或者根据应用常见写入到数据库、消息队列或者文件系统中</li></ol><h1 id="四、其他问题"><a href="#四、其他问题" class="headerlink" title="四、其他问题"></a>四、其他问题</h1><h4 id="流数据（Stream-data）和批数据（Batch-data）处理的区别？"><a href="#流数据（Stream-data）和批数据（Batch-data）处理的区别？" class="headerlink" title="流数据（Stream data）和批数据（Batch data）处理的区别？"></a>流数据（Stream data）和批数据（Batch data）处理的区别？</h4><ul><li>流数据是连续生成，需要实时采集并实时处理，通常的数据来源包括传感器用户日期和、社交媒体更新、金融交易等操作。<ul><li>金融服务的交易分析，比如检查用户的行为</li><li>实时用户和推荐系统，比如实时分析用户的需求</li></ul></li><li>批数据模式下，数据是成块或者成批次组成的，通常被定时的收集或者存储<ul><li>金融网贷业务的预处理</li></ul></li></ul><h4 id="全量更新和增量更新"><a href="#全量更新和增量更新" class="headerlink" title="全量更新和增量更新"></a>全量更新和增量更新</h4><ul><li>全量更新 （Full update） 每次更新的时候处理整个数据集<ul><li>Pro 实现逻辑简单，不需要复杂的变更追踪的逻辑，每次更新都是完整的数据集，可以直接避免数据不一致的风险</li><li>Con 每次都需要处理全部数据，对于系统资源要求高。特别是对于大型数据集需要很长时间才能完成更新</li></ul></li><li>增量更新（incremental update）只处理上次更新依赖发生变化的数据<ul><li>Pro 增量更新的效率高、实时性更强</li><li>Con 但是面临数据处理逻辑的复杂性，容易造成数据不一致的风险，对历史数据管理要求较高</li></ul></li></ul><p>增量更新常见的操作是根据 key 和 update time 来保留最新的记录</p><p>对应的表格分类包括：</p><ol><li>增量表：记录更新周期内新增的数据，在原表中数据的基础上新增本周起产生的新数据</li><li>全量表：记录更新周期内的全量数据，无论数据是否有变化都需要记录</li><li>拉链表：记录数据的历史信息，记录数据从开始 i 一直到当前所有变化的信息</li></ol><h4 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a>持续更新…</h4><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.sqlboy.tech/">https://www.sqlboy.tech/</a></p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>外部阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何建立自己的代码心智模型</title>
    <link href="/posts/54653.html"/>
    <url>/posts/54653.html</url>
    
    <content type="html"><![CDATA[<p>以 LLM 为底层的自动代码补全工具帮助我们从代码敲击的背景下剥离出来，一定程度上减轻了劳动量。但是其能力的限制使得通常无法在高度定制化的场景下给出最优方案，此时对于熟悉 Copilot 的我们会发现生疏到无法解决。因此如何权衡自动补全工具带来的便利性和自身能力成长是必要的，合理的解决方法是建立心智模型来得其意，再指挥工具免其劳帮助工作效率的提升。本文从自身的角度建立自己的心智模型，</p><h2 id="一、从-Cheetsheet、Cookbook-到-LLM"><a href="#一、从-Cheetsheet、Cookbook-到-LLM" class="headerlink" title="一、从 Cheetsheet、Cookbook 到 LLM"></a>一、从 Cheetsheet、Cookbook 到 LLM</h2><p>学习代码工具中速查表、官方文档、cookbook 是非常重要的资源。</p><ol><li>速查表（Cheetsheet）是列出某个库的常用的命令、语法和代码片段，给初学者快速参考的工具，可以帮助快速的查找特定命令和语法，共享的互联网资源总结了很多可行的速查表。eg: <a href="https://github.com/rahulmakwana1/machine-learning-cheat-sheets/tree/master">ML-cheetsheet</a></li><li>但是从学习的角度官方文档（Document）往往会给出详细的例子，让人望而却步。</li><li>在两者之间的 cookbook 更倾向于结合代码片段给出如何解决实际案例，除了代码片段本身，也还会解释、上下文信息以及在什么样的场景下采取特定的解决方法。</li></ol><p>在后 LLM 时代，copilot 的自动补全功能可以说完全代替了 cookbook，当用户提出一个需求场景，如“帮我写一个归并排序”，先天的代码补全功能可以给用户自动生成所需要的代码。</p><ol><li>优点一在于提升效率，可以从烦躁的代码敲击和函数名称记录中剥离出来，让一些常见的操作可以自动的从代码补全来完成</li><li>优点二让边界更进一步，代码补全有的时候会“涌现”出新的编辑方式，这通常是因为别人已经有过相同的表达，可以将大模型看作是共同记忆的集合</li><li>缺点在于无法完成创新工作，如果这件事情从来没有出现过，LLM 通常是无法完成固定的操作的，这个时候仍然依赖于写作者自身对于工具的理解，但是长时间未写相关论文的过程中，会消失对于 API 的熟练性而导致无从下手。</li></ol><p>解决这样问题的前提是思考我们是如何学习一件事物的：我们会通过学习相关概念和实际案例来获取属于自己的理解和知识，个体的理解程度决定对于知识的掌握程度。个人将在代码领域中的理解称为**<a href="https://zh.wikipedia.org/wiki/%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B">心智模型</a>**。</p><blockquote><p>认知心理学中将心智模型定义为我们脑海中的世界是一种概念，通过概念和关系来表达真实的系统</p></blockquote><p>通过建立自己对于某种概念的全局认识，再通过自动补全工具是操作 API 可以帮助我们在每次使用的过程中都是对自身理解外在表现应用的认知，可以进一步加深自己的印象而不是让自己变得生疏</p><h2 id="二、心智模型-Copilot-复利效应"><a href="#二、心智模型-Copilot-复利效应" class="headerlink" title="二、心智模型 + Copilot &#x3D; 复利效应"></a>二、心智模型 + Copilot &#x3D; 复利效应</h2><p>Copilot 的含义是辅助工具，抛开 LLM 时代不谈很多场景下的工业工具都是对人类个体自身的辅助增强：</p><ul><li>纺织机，通过工具的方式来提升纺织的效率，但是起无法替代苏绣的作用</li><li>汽车，通过新的能源转换方式代替双腿移动，但是起无法在所有道路上运行</li><li>社交媒体，通过互联网来代替 face-to-face 的表达，让物理距离变得不重要，但是其无法代替线下交流</li><li>洗衣机，是在洗衣场景下双手的解放，但是其无法适用所有的衣物</li><li>相机，对于人类视觉和绘画的简化，但是其无法替代艺术创作</li><li>…</li></ul><p>AI-Copilot 也是如此，通过对简单事物的替代，配合自己的理解可以增加创作效率，但是坐在主导位的只能是自身，为了更好的使用辅助驾驶需要理解其行为的含义，建立其工作的心智模型，来更好的使用 ta</p><h2 id="三、【个人向】需要建立什么样的心智模型"><a href="#三、【个人向】需要建立什么样的心智模型" class="headerlink" title="三、【个人向】需要建立什么样的心智模型"></a>三、【个人向】需要建立什么样的心智模型</h2><h3 id="3-1-编程基础"><a href="#3-1-编程基础" class="headerlink" title="3.1 编程基础"></a>3.1 编程基础</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/Python_cheetsheet.drawio.png" alt="Python_cheetsheet.drawio"></p><h3 id="3-2-数据获取、数据预处理、数据转换"><a href="#3-2-数据获取、数据预处理、数据转换" class="headerlink" title="3.2 数据获取、数据预处理、数据转换"></a>3.2 数据获取、数据预处理、数据转换</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/structure_data_cheetsheet.drawio.png" alt="structure_data_cheetsheet.drawio"></p><h3 id="3-3-数学操作、模型构建、框架展示"><a href="#3-3-数学操作、模型构建、框架展示" class="headerlink" title="3.3 数学操作、模型构建、框架展示"></a>3.3 数学操作、模型构建、框架展示</h3><h2 id="附录：关于写作的一些思考"><a href="#附录：关于写作的一些思考" class="headerlink" title="附录：关于写作的一些思考"></a>附录：关于写作的一些思考</h2><p>文字、语言、肢体表达是个体与个体之间有限的交流方式，内在的逻辑性是上述三者均需要的内核，其中文字是最为正式和准确的信息交流方式。现代社会的高度分工需要敏锐、清晰的文字表达，除了逻辑性之外，其还需要有准确的词汇和语法技巧，在图文编辑器发达的背景下恰当的格式排版更加重要。但是在中文互联网背景和推荐系统肆掠的背景下，如何培养自己的文字表达成为一种奢侈。这里给出自己的建议总结：</p><ol><li><strong>凝练表达，目的为先</strong>。用尽可能少的语言让别人来抓住自己的核心思想，达成从“听不懂”到“听懂但是不知道为什么”的目的，建立起快速联系的纽带</li><li><strong>逻辑连贯，有理有据。</strong>在说服别人的时候可以通过精炼表达，利用层次递进的方式来让别人接受自己的想法。注意每个论据也可以看作是一个需要精炼表达的命题，尽可能的用数据和事实去表达自己。重复冗余的话术只会让自己变得不够信服</li><li><strong>词汇专业，语法通顺。</strong>所谓的互联网黑话或者是英语的专业词汇本质上是一种语言共识，可以快速的将希望表达的含义浓缩的传递给别人，专业学习也是如此，很多的专业壁垒来自于浓缩的含义。除了词汇之外，句子的语法通顺也很重要，这部分中文的表达（图）不如英文的表达（树）方式有趣，平时要多加锻炼</li><li><strong>格式清晰，排版准确。</strong>现代编辑器带来从个体化（书法、签字）向工业化（字体、格式、排版、符号）的转变，如何善用这些模块可以更好的提升自己的表达。<ol><li>符号：字体、大小、颜色、中文符号、英文符号</li><li>工具：斜体、加粗、划线</li><li>排版：段落、缩进、表格</li></ol></li><li><strong>理解意图，尊重读者</strong>。表达的核心目的在于建立起双方的联系，而不是为了坚持自己的想法来辩论得到输赢，因此充分的理解读者的理解诉求，针对不同群众可以接受的方式来表达自己。</li><li><strong>敏锐思考，在反馈中成长</strong>。表达交流并不只是将内心的东西全部的释放，和别人交流反馈的过程中也是提升自己内在思考的一种手段因此表达的时候也要持续保持对于自身观点的思考，讨论是表达的最终目的。</li></ol><p>推荐书籍</p><p>《金字塔原理》</p><p>《批判性思维》</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>代码基础</tag>
      
      <tag>生产效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读 Casual inference_总结中</title>
    <link href="/posts/29675.html"/>
    <url>/posts/29675.html</url>
    
    <content type="html"><![CDATA[<p>上一篇文章从概念的角度解释了因果推断中常见的问题，同时介绍因果图的分析方法来得到相关性信息和因果性信息流之间的方法。在这篇将更加深入的理解干预（Invention）、后门路径（Backdoor path）和后门调整（Backdoor adjustment）来准确的计算因果效应。以及假设较为困难的前门准则和 do 算子的方式来估计因果效应。这些方法的核心目的是：将因果估计转换成为统计估计，进而估计结果。</p><h2 id="第四章、干预与后门调整"><a href="#第四章、干预与后门调整" class="headerlink" title="第四章、干预与后门调整"></a>第四章、干预与后门调整</h2><h3 id="4-1-Do-运算符"><a href="#4-1-Do-运算符" class="headerlink" title="4.1 Do 运算符"></a>4.1 Do 运算符</h3><p>注意干预 invention 和统计学中的 conditioning 之间的区别，为了表示区别因果中利用 do 算子表示</p><ol><li>统计中的conditional，指的是我们只将注意力限定在接受治疗的 subgroup 中</li><li>因果中的 invention，指的是针对所有的群体添加干预</li></ol><p>同时我们希望得到的结果是干预分布（interventional distributions）P（Y｜do（T）），同时也要注意干预分布和观测到的分布之间的区别。其中面临反事实的问题，那么什么样的情况下可识别？</p><blockquote><p>如果可以将包含 do 算子的表达式（Casual estimation）还原成为没有 do 算子的表达式（statistical estimate），则说明是可识别的</p></blockquote><p>因果机制（Casual mechanism）指的是，对于一个节点来说，影响其的结果只包含其parent 和对应的边。在因果机制的基础上，我们作出<strong>干预是局部的假设</strong>，我们假设对于一些变量的干预改变 X 对应的因果机制，而不是改变其产生其他变量的因果机制，也可以额被称为模块化（modularity），或者是独立机制、自主性和不变性机制。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918092041754.png" alt="模块化的机制"></p><p>干预操作的本质是希望改变原有的概率分布：</p><ol><li>第一种是观察数据中的因果图</li><li>第二种是针对 T 施加干预的结果，对于其他节点的因果机制没有发生改变，而针对干预节点由于其因果机制发生了改变，因此可以对其parent 节点进行截断</li><li>第三种与上述的情况保持一致</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918092059134.png" alt="详细解释概率图"></p><p>添加干预的好处是可以让概率发生变化，那么为什么需要将概率升变化呢？核心目的是让策略效应的计算可以更加的简单，通过模块化的假设可以让我们在计算联合概率分布中将干预的节点进行剔除，具体来说就是截断公式（Truncated factorization ）</p><p>针对下列经典的情况，我们可以计算在施加干预和不施加干预前后的输出</p><ol><li>干预分布 P(y|do(t)</li><li>观测分布P(y|t)</li></ol><p>通过计算可以发现核心在于 P(X) 和 P(X|T)之间的分布差异造成了观察结果和干预分布之间的问题</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918093111288.png" alt="经典混淆因果关系"></p><h3 id="4-2-后门路径和后门调整"><a href="#4-2-后门路径和后门调整" class="headerlink" title="4.2 后门路径和后门调整"></a>4.2 后门路径和后门调整</h3><p>在因果图中，如果希望识别两个节点的因果关系我们希望可以截断两个节点直接非因果的关联，而非因果的关联主要是通过非阻塞路径（unblocked path）进行传递，If 我们可以阻断这些路径，Then 就可以在干预 T 的情况下估计出 T 对 Y 的因果效应，由此得到的后门标准为：</p><p>需要有一系列变量的集合造成了：</p><ol><li>W 阻塞了所有 T 和 X 之间的后门路径</li><li>W 不会包含 T 的任何后台节点</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918093837862.png" alt="后门标准"></p><p>在后门准则的基础上，我们找到了调整变量，进而施加干预得到准确的因果效应估计</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918093954279.png" alt="后门调整公式 backdoor adjustment formula"></p><blockquote><p> 一个实际的例子来计算：干预和非干预情况下 的因果效应估计</p></blockquote><ol><li>第一种在施加干预的情况下，可以通过截断公式来得到 Y 对于 T 的效应识别</li><li>第二种是在不施加干预的情况下，利用相关性公式得到 Y 对于 T 的识别，可以发现是存在有偏差的</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918094608683.png" alt="示意图"></p><p>在无法准确的施加干预的背景下，我们可以使用施加找到后门调整的变量来估算出因果效应</p><blockquote><p>key point 总结</p><ol><li>为了计算干预之后的概率分布公式，我们可以使用局部性假设来对概率分布进行截断，进而计算出包含 do 算子下的因果效应分析公式</li><li>因果关系识别的核心在于如何将包含 do 算子的公式转换成为没有 do 算子的表达式</li><li>造成偏差的核心问题在于 P（X|T）和 P（X）之间的不一致造成的偏差</li><li>如果我们可以找到后门调整的路径并进行截断，我们可以完成上述的操作</li></ol></blockquote><h2 id="五、随机控制实验-randomized-control-experiments"><a href="#五、随机控制实验-randomized-control-experiments" class="headerlink" title="五、随机控制实验 randomized control experiments"></a>五、随机控制实验 randomized control experiments</h2><p>在 AB 实验中，通过随机的控制策略的分发机制，可以保障最终得到的关联性为因果性。因为 T 和协变量直接完全独立是的最终的因果效应可以识别。</p><p>在随机对照实验需要保证 AB 两组是完全可比的，更加精确的定义这里的可比是各组的协变量之间的分布为相似的状态，确保协变量均衡是有效对比和分析后续的基础。核心是实现<img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918100637222.png" alt="image-20240918100637222" style="zoom: 25%;" />，通过保证 X 和 T之间相互独立的状态，可以实现两者满足可交换的假设，进而准确的计算因果效应。如何实现协变量均衡可以从：</p><ol><li>随机化，在实验设计的过程中将变量随机的分配到不同的组别在满足条件</li><li>匹配，通过匹配的方式来均衡协变量的分布</li><li>统计套装，通过回归分析的方法对最终的结果进行纠偏的操作来调整对结果的影响 regression adjustment</li></ol><h2 id="六、非参数识别"><a href="#六、非参数识别" class="headerlink" title="六、非参数识别"></a>六、非参数识别</h2><h3 id="6-1-前门准则"><a href="#6-1-前门准则" class="headerlink" title="6.1 前门准则"></a>6.1 前门准则</h3><p>在第四章中我们在后门准则的基础上介绍了后门调整的方法（通过寻找到恰当的后门准则变量来实现后门调整，进而完成准确的因果效应估计），但是是否有其他的方式来实现因果效应的评估。或者说在之前的经验中我们尝试使用观察到充足的后门调整变量来实现因果效应的识别，但是如果在 W 无法被重插的情况下如何实现效应的评估。这里注重介绍 前门准则和前门调整。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918101428649.png" alt="前门准则和调整"></p><p>前门调整适用于在后门准则的变量无法被充分识别和观察的情况下+中介变量可以被观察的情况下，可以通过三个步骤来计算出Y 中 T 因果效应</p><ol><li>M 中 T 的因果效应，完全是因果关系</li><li>Y 中 M 的因果效应，可以通过对 T的后门准则的截断来完成</li><li>联合上述两个步骤，来得到 Y 中 T 的因果效应<img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918101922155.png" alt="image-20240918101922155" style="zoom: 25%;" /></li></ol><p>其中前门准则包括：</p><ol><li>M 是充分的变量，也就是所有的因果路径均会通过 M 流向 Y</li><li>所有 T 到 M 的后门路径均被截断</li><li>所有从 M 到 Y 的路径均被 T 截断</li></ol><h3 id="6-2-do-算子"><a href="#6-2-do-算子" class="headerlink" title="6.2 do 算子"></a>6.2 do 算子</h3><p>在不满足前门准则和后门准则的基础上，如何来实现因果效应的估计</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918103236733.png" alt="do 计算实现因果效应估计"></p><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p><a href="https://zhang-each.github.io/My-CS-Notebook/Causality/Causality04-Intervention%26BackdoorAdjustment/">小角龙的学习记录- 04</a></p><h5 id="因果发现的方法"><a href="#因果发现的方法" class="headerlink" title="因果发现的方法"></a>因果发现的方法</h5><p>因果发现的目的在于确定一个因果推的马尔可夫等价的方法，主要分为两种方式</p><ol><li>基于约束的方法，需要判定集合是否满足一系列假设检验（例如节点之间是否需要满足条件独立性假设），为了避免对于所有可能的子集合进行搜索，往往从一个全连接图开始逐渐增加条件集合的大小</li><li>基于分数的方法，主要通过对模型类别进行假设限制，直接拟合一个结构因果模型架构来得到最终的打分函数的定义来进行求解。为了得到最优的因果图结果，需要包括两个部分：1. 最大化对数据的拟合程度、2. 对图结构的复杂程度进行惩罚。比如使用贪婪搜索的方法或者动态规划的方式。</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240918103633239.png" alt="因果发现的方法"></p><h4 id="因果估计和统计估计的方法"><a href="#因果估计和统计估计的方法" class="headerlink" title="因果估计和统计估计的方法"></a>因果估计和统计估计的方法</h4><p>在假设后门变量均被观察到的假设的基础上，我们可以使用一系列的方法来进行纠偏的操作</p><ol><li>s learner 和 t learner</li><li>tarnet 和 xlearner</li><li>PS 分数</li><li>double machine learning</li></ol>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>因果推断</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读 Casual inference 总结上</title>
    <link href="/posts/29337.html"/>
    <url>/posts/29337.html</url>
    
    <content type="html"><![CDATA[<p>这篇文章主要针对 casual inference 的前置概念进行介绍，核心希望回答的问题是为什么需要因果性，如何得到因果估计。介绍到常见的因果发现的方法。之后会在因果路径的基础上介绍更多的因果方法。</p><h2 id="一、为什么需要因果性而不是相关性"><a href="#一、为什么需要因果性而不是相关性" class="headerlink" title="一、为什么需要因果性而不是相关性"></a>一、为什么需要因果性而不是相关性</h2><h3 id="1-1-从辛普森悖论说起"><a href="#1-1-从辛普森悖论说起" class="headerlink" title="1.1 从辛普森悖论说起"></a>1.1 从辛普森悖论说起</h3><ul><li>辛普森悖论的现象<ul><li>目的希望查看药物T（A，B） 在患者 X（轻、重、总体）下的效果</li><li>悖论：<ul><li>从分层人群来看，药物 A 均优于药物 B</li><li>从总体人群来看，药物 A 均差于药物 B</li><li>问题？应当对于一个新的人给于什么样的治疗</li><li><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913110909447.png" alt="image-20240913110909447" style="zoom:50%;" /></li></ul></li><li>解释：<ul><li>可能的解释 1：某种因素影响 T 的分配，比如更严重的人才会用 B，这个时候 B 更好</li><li>可能的解释 2：如果存在中介变量，例如药物 B 需要人停留很长的时间而不被允许，这个时候 A 更好</li></ul></li><li>结论：<ul><li>需要因果才能解释数据中出现违反直觉的结论</li></ul></li></ul></li></ul><h3 id="1-2-因果推断的应用"><a href="#1-2-因果推断的应用" class="headerlink" title="1.2 因果推断的应用"></a>1.2 因果推断的应用</h3><ul><li>希望找到因果的描述，而不是相关性的描述（Associational），这样才能做出有效的策略<ul><li>药物治疗</li><li>强化学习</li><li>社会实验</li></ul></li><li>因果推断对关键决策至关重要<ul><li>温室气体控制</li></ul></li></ul><h3 id="1-3-相关性不代表因果性"><a href="#1-3-相关性不代表因果性" class="headerlink" title="1.3 相关性不代表因果性"></a>1.3 相关性不代表因果性</h3><blockquote><p>Correlation does not imply causation</p></blockquote><p>关联关系（Association 或者 Correlation in linear statistical dependence）与因果关系</p><ul><li>并不是所有的 association 都是 casual</li><li>也不是所有的 association 都不是 casual</li><li>这句话的目的在于说明 association 和 causation 之间是有区别的</li><li>例子：不脱鞋和头疼，但是潜在的混杂在于喝酒 -&gt; 不脱鞋，喝酒 -&gt; 头疼。 这种为 casual association，但是不脱鞋 -&gt; 头疼，为confounding association</li></ul><p><strong>核心问题：希望找到因果关系来产出有效的策略方案</strong></p><h3 id="1-4-核心主题"><a href="#1-4-核心主题" class="headerlink" title="1.4 核心主题"></a>1.4 核心主题</h3><ul><li><p>统计和因果</p><ul><li><p>统计（Statistical）：在有限的样本中估计随机性</p></li><li><p>因果性（casual）：但是 statistical 中的correlation 不代表 casual</p></li></ul></li><li><p>识别和估计</p><ul><li>identification：识别因果效应（casual effect）是因果推断的关键</li><li>estimation：常用到 ML 中的估计方法来实现效应的估计</li></ul></li><li><p>干预和实验</p><ul><li>interventional 干预实验中可以很容易得到因果效应估计</li><li>observation 数据中容易出现很多的观察因素</li></ul></li><li><p>假设</p><ul><li>基于一些假设我们才能正确的计算 potential outcome</li></ul></li></ul><h2 id="二、基本假设：如何得到-Potential-outcome"><a href="#二、基本假设：如何得到-Potential-outcome" class="headerlink" title="二、基本假设：如何得到 Potential outcome"></a>二、基本假设：如何得到 Potential outcome</h2><p>T 表示策略、Y 表示输出值、X 表示可行的协变量，利用大写变量表示随机性、利用小写变量表示具体值。</p><h3 id="2-1-潜在输出到ITE"><a href="#2-1-潜在输出到ITE" class="headerlink" title="2.1 潜在输出到ITE"></a>2.1 潜在输出到ITE</h3><blockquote><p>例子 1：假如你现在很开心（Y&#x3D;1），现在给了你一个玩具狗 （T&#x3D;1），这个时候你仍然很开心（Y&#x3D;1），但是玩具狗对你开心的效应是很弱的</p><p>例子 2：假设你现在不开心（Y&#x3D;0），现在给了你一个玩具狗（T&#x3D;1），这个时候你变开心了（Y&#x3D;1），说明玩具狗具有很强的开心效应</p></blockquote><p>Y（t）表示实施某种策略下潜在的输出，因此对于单个个体的策略包括</p><p>$$ t_i &#x3D; Y_i(1)-Y_i(0) $$</p><h3 id="2-2-基本问题"><a href="#2-2-基本问题" class="headerlink" title="2.2  基本问题"></a>2.2  基本问题</h3><p>但是对于同一个个体无法在同一个时间下同时观察到两种策略下的输出，也就是反事实counterfactuals</p><h3 id="2-3-如何解决这些问题"><a href="#2-3-如何解决这些问题" class="headerlink" title="2.3 如何解决这些问题"></a>2.3 如何解决这些问题</h3><p>如何无法计算 ITE，是否可以有某种方式来计算 ATE</p><p>$$t &#x3D; E[t_i] &#x3D; E[Y_i(1)-E_i(0)] &#x3D; E[Y(1)-Y(0)]$$ </p><p>但是在通常的条件下，这个并不是等于</p><p>$$E[Y|T&#x3D;1] - E[Y|T&#x3D;0]$$</p><p>由此来计算对应的 ATE 输出</p><h4 id="2-3-1-假设1：ignorability-and-exchange"><a href="#2-3-1-假设1：ignorability-and-exchange" class="headerlink" title="2.3.1 假设1：ignorability and exchange"></a>2.3.1 假设1：ignorability and exchange</h4><p>需要保证施加策略的用户是可以实现互换的</p><ul><li>可忽略性假设是人们如何选择策略以及被策略选择</li><li>互换性假设希望两者之间可以互换的</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913134745864.png" alt="条件独立性"></p><p>如何使用 a2.1 来实现因果关系的识别，也就是可以从相关关系的识别中来观察到因果关系效应，这样可以将 causal expression 转换成为一个 statistical expression。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913142533915.png" alt="可识别性定义"></p><h4 id="2-3-2-假设2：unconfoundeness-（CIA）或者-conditional-exchange-ability"><a href="#2-3-2-假设2：unconfoundeness-（CIA）或者-conditional-exchange-ability" class="headerlink" title="2.3.2 假设2：unconfoundeness （CIA）或者 conditional exchange ability"></a>2.3.2 假设2：unconfoundeness （CIA）或者 conditional exchange ability</h4><p>在观察数据中，满足假设 1 是完全不可能的，因为不可能找到两者完全相同的组，但是如果我们可以控制住相关的变量，那么在相同条件下可以计算对应的因果效应。</p><ul><li>如何定义“相关变量”在因果图中可以表示</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913142834998.png" alt="条件独立性"></p><p>这里的核心思想在于 T 和 Y 虽然可能是相关联的 association，但是在控制住相同的协变量之后，不同 T 组别之间是可比的</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913143054989.png" alt="控制住 confounders 来满足计算"></p><p>通过控制混淆变量的方式可以让我们在无法观察到反事实的情况下，计算出因果效应，这里的公式被定义为 adjustment formula，进而可以计算除因果效应。通过将假设 2.1 转换为假设 2.2 ，让这个额条件更加实际，但但是我们仍然不知道控制的变量是什么形式，但是由于协变量中不可能保证完整的混淆变量 confounders 被观察到，因此仍然不满足条件独立性假设</p><ul><li>在自然对照实验中这并不是问题<ul><li>（个人想法，但是理论上完全的自然对照实验是不存在的，因此通过一些纠偏的方法来AB 实验的数据中进行清洗可能会得到一个更好的结果）</li></ul></li><li>在观察数据中很难解决的事情<ul><li>唯一可行的是观察和拟合尽可能多的协变量 X，来尝试保证（无混淆假设 unconfoundedness）</li></ul></li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913143236910.png" alt="调整公式"></p><h4 id="2-3-3-假设3：positivity"><a href="#2-3-3-假设3：positivity" class="headerlink" title="2.3.3 假设3：positivity"></a>2.3.3 假设3：positivity</h4><p>虽然尽可能的保证协变量 X 中包含尽可能多的混淆变量 confounders 可以让 CIA 假设得到的满足，但是为了计算因果效应，我们需要保证正值假设，而随着变量的增多可能会导致某些个体无法观察到对应的数据进而不能计算因果效应。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913143823575.png" alt="Assumption23"></p><p>证明如下：计算 ATE 提升期望的过程中，使用贝叶斯法则：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913143902540.png" alt="正值假设"></p><blockquote><p>直觉的解释，为什么需要正值假设？</p><p>因为如果不满足正值假设，说明其中某些子群体 subgroup 常常被分配到固定的策略方式，这种完全观察不到另一种方式会导致无法计算合理的casual effect</p></blockquote><p>因此我们往往面临 假设 2.2-假设 2.3 之间的 tradeoff。协变量 X 中表示更多的混杂变量可以更容易满足假设 2.2，但是随着变量的增多，我们更容易陷入不满足假设 2.3 中，因为一部分群体只会接受到固定的策略。此时在外推情况下往往会导致输出为 0</p><h4 id="2-3-4-假设-4：No-interference-consistency-SUTVA"><a href="#2-3-4-假设-4：No-interference-consistency-SUTVA" class="headerlink" title="2.3.4 假设 4：No interference + consistency &#x3D; SUTVA"></a>2.3.4 假设 4：No interference + consistency &#x3D; SUTVA</h4><p>没有干扰性的假设表示每个人的输出不会受到其他人的影响</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913144519294.png" alt="image-20240913144519294"></p><p>一致性假设表示在给定一个策略下，必然得到固定的产出</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913144556971.png" alt="image-20240913144556971"></p><h4 id="2-3-5-总结"><a href="#2-3-5-总结" class="headerlink" title="2.3.5 总结"></a>2.3.5 总结</h4><p>在满足上述假设的过程中，我们可以通过期望公式将条件因果效应得到对应的输出</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913144707018.png" alt="满足假设条件下的因果效应估计方法"></p><ul><li>Casual estimand 因果效应真实值（通过 identification）</li><li>Statistical estimand 统计效应真实值 （通过 estimation）</li><li>Estimate</li></ul><h2 id="三、违反基本假设的困难：从Casual-Graph-解释"><a href="#三、违反基本假设的困难：从Casual-Graph-解释" class="headerlink" title="三、违反基本假设的困难：从Casual Graph 解释"></a>三、违反基本假设的困难：从Casual Graph 解释</h2><h4 id="3-1-基本图的术语"><a href="#3-1-基本图的术语" class="headerlink" title="3.1 基本图的术语"></a>3.1 基本图的术语</h4><ul><li>graph <ul><li>nodes</li><li>edges</li><li>Connection </li><li>path</li></ul></li></ul><h4 id="3-2-贝叶斯网络"><a href="#3-2-贝叶斯网络" class="headerlink" title="3.2 贝叶斯网络"></a>3.2 贝叶斯网络</h4><p>The chain rule of probability </p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913150150844.png" alt="概率计算链条"></p><p>结合贝叶斯网络和马尔科夫性质</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913150342251.png" alt="如何计算 casual estimand"></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913150604517.png" alt="image-20240913150604517"></p><p>PS： 定义 3.2 没太看懂</p><h4 id="3-3-因果图-Casual-Graph"><a href="#3-3-因果图-Casual-Graph" class="headerlink" title="3.3 因果图 Casual Graph"></a>3.3 因果图 Casual Graph</h4><blockquote><p>什么是因果性？</p><p>如果 X 变化，Y 一定会随着变化的。在因果图中每一个父节点都是子节点的直接原因</p></blockquote><h4 id="3-4-两个节点"><a href="#3-4-两个节点" class="headerlink" title="3.4 两个节点"></a>3.4 两个节点</h4><p>对于两个没有关联的节点之间是存在独立性的 $P(x_1,x_2) &#x3D; P(x_1)P(x_2)$</p><p>对于两个存在关联的节点之间是存在 association 的 </p><h4 id="3-5-因果链-chain-和-因果叉-fork"><a href="#3-5-因果链-chain-和-因果叉-fork" class="headerlink" title="3.5 因果链 chain 和 因果叉 fork"></a>3.5 因果链 chain 和 因果叉 fork</h4><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913151026087.png" alt="因果图中常见的关系包括"></p><p>在 Chain 下如何计算 X1 对于 X3 的因果效应？</p><p>在 Fork 下如何计算 X1 对于 X3 的因果效应？</p><p>如果在保证变量 X2 的条件下，可以证明 X1 和 X3 之间存在独立性，因此可以计算因果性。由此可以满足条件独立性假设。</p><h4 id="3-6-因果碰撞-Collider-与其后代"><a href="#3-6-因果碰撞-Collider-与其后代" class="headerlink" title="3.6 因果碰撞 Collider 与其后代"></a>3.6 因果碰撞 Collider 与其后代</h4><p>看上去 Immorality 中 X1 和 X3 之间没有association，但是其都是指向了相同的变量 X2，因此在采用 X2 作为变量控制，可能会让两个不相关的变量存在 association</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913151519188.png" alt="Collider 因果发现"></p><blockquote><p>为什么通过控制碰撞变量可以得到因果效应的估计呢？</p><p>比如在相亲的过程中， X1 善良，X3 外表；你会发现大多数好看的男生都是混蛋，而大多数善良的男生都不好看，因此必需要在外表和善良之间做出选择，也就是两者存在负相关。<br>但是变量 X2 表示可获得性，也就是没有对象的人才会相亲。由于 X2 导致两者存在 association，在条件于 X2 的情况下，会导致两者存在 association</p></blockquote><p>因此通过控制 Collider 变量，会导致一个阻断链路成为一个非阻断链路。</p><ul><li>X1 和 X3 在整个人群中没有关系</li><li>但是条件与 X2下，会导致两者出现association</li><li>berkson‘s paradox，这也是导致 selection bias 的存在</li></ul><h4 id="3-7-Do-算子-｜-d-separation"><a href="#3-7-Do-算子-｜-d-separation" class="headerlink" title="3.7 Do 算子 ｜ d-separation"></a>3.7 Do 算子 ｜ d-separation</h4><p>那么上述的阻断路径指的是什么？</p><ul><li>对于 chain 和 fork 变量，需要控制住</li><li>对于 Collider及其子变量，都不需要控制</li></ul><p>未被阻断的路径就是路径中存在未被阻断的节点，其中可能会存在 association，但是在 blocked path 中不存在 association 而可以识别出casual，有次定义的 D -分离的定义<img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913153535071.png" alt="符号定义" style="zoom:25%;" /></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913153502329.png" alt="d 分离定义"></p><h4 id="3-8-因果图和相关图"><a href="#3-8-因果图和相关图" class="headerlink" title="3.8 因果图和相关图"></a>3.8 因果图和相关图</h4><p>在介绍完因果图中常见的概念之外，最终的目的是为了能够介绍因果图中的 association 和 causation</p><ul><li>association<ul><li>casual association 按照 direct path 运行</li><li>confounding association 混淆关联</li></ul></li></ul><p>在上述的基础后提出的“后门准则”和“前门准则”中的意义在于</p><ul><li><p>在某些研究中，虽然 DAG 的某些变量不可观测，我们仍然可以从有限的观测数据中估计因果效应</p></li><li><p>有助于鉴别 混杂变量和设计观察性研究</p></li><li><p>后门准则(Fork)：定义变量之间除了直连路径之外的其他 unblocked path 均为后门路径</p><ul><li>Z 中节点不是 X 的后代</li><li>Z 阻断所有指向 X 的路径</li></ul></li><li><p>前门准则(Chain)</p><ul><li>Z  阻断了所有 X 到 Y 的直接路径</li><li>X 到 Z 没有后门路径</li><li>所有 Z 到 Y 的后门路径都被 X 阻断</li></ul></li></ul><blockquote><p><strong>例子：</strong>我们关心吸烟和肺癌之间的因果关系。由于一个潜在的不可观测的基因 的存在，吸烟和肺癌之间有一条“活”的后门路径，因此不借助其他的条件，我们无法识别吸烟与肺癌的因果关系。如果我们有这样的知识“吸烟 仅仅通过肺部烟焦油的含量 来影响肺癌 ”，那么吸烟对肺癌的因果作用就可以估计出来了。不过，这里需要两个条件，也就是在证明中使用的两个条件独立性，他们表明：（1）吸烟  和肺部烟焦油的含量  之间没有“活”的后门路径（或者没有混杂因素）；（2）吸烟 对肺癌 的作用仅仅来源于吸烟 对肺部烟焦油 的作用，或者说，吸烟 对肺癌 没有“直接作用“”</p></blockquote><p>再回到原始的辛普森悖论的 问题，可以发现在背后可以有很多种解释的方法，因此只有基于因果的方式才能更好的描述这个状况</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913155020606.png" alt="辛普森悖论背后的解释方法"></p><h2 id="四、如何得到准确的答案：Casual-Models"><a href="#四、如何得到准确的答案：Casual-Models" class="headerlink" title="四、如何得到准确的答案：Casual Models"></a>四、如何得到准确的答案：Casual Models</h2><h4 id="4-1-do算子和干预输出分布"><a href="#4-1-do算子和干预输出分布" class="headerlink" title="4.1 do算子和干预输出分布"></a>4.1 do算子和干预输出分布</h4><p>因果推断中 do 算子的一些描述</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913160033455.png" alt="可视化描述"></p><h4 id="4-2-核心的假设：模块化-Modularity"><a href="#4-2-核心的假设：模块化-Modularity" class="headerlink" title="4.2 核心的假设：模块化 Modularity"></a>4.2 核心的假设：模块化 Modularity</h4><p>为确保因果图中对于变量 X 的干预只会改变 X 本身的概率，等价于移除所有指向 X 的因果边</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913161235590.png" alt="模块化的定义"></p><p>例如从观察数据中，和对其施加干预之后的干预分布定义</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913161333239.png" alt="分布示意图"></p><p>被定义为反事实规则或者是干预规则</p><h4 id="4-3-因果图分解-truncated-factorization"><a href="#4-3-因果图分解-truncated-factorization" class="headerlink" title="4.3 因果图分解 truncated factorization"></a>4.3 因果图分解 truncated factorization</h4><p>在因果图的局部假设的基础上，可以提出因果图的分解方法</p><ul><li><p>贝叶斯网络分解公式</p><ul><li>对其进行分解</li></ul></li><li><p>马尔可夫假设 或者 模块化假设</p><ul><li>对其进行分解</li></ul><p>通过三个步骤计算出因果统计量</p></li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913161443725.png" alt="因果图分解"></p><h4 id="4-4-后门调整-backdoor-adjustment"><a href="#4-4-后门调整-backdoor-adjustment" class="headerlink" title="4.4 后门调整 backdoor adjustment"></a>4.4 后门调整 backdoor adjustment</h4><p>如果在上述的到完整的后门准则调整的集合之后，在给定的数据集后则可以识别出对应的因果效应</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/image-20240913161836120.png" alt="后门调整"></p><p>前提是需要满足 正值假设、后门准则（充分集合）来求解出对应的因果效应</p><h4 id="4-5-结构因果模型-SCM"><a href="#4-5-结构因果模型-SCM" class="headerlink" title="4.5 结构因果模型 SCM"></a>4.5 结构因果模型 SCM</h4><h4 id="4-6-后门调整的应用"><a href="#4-6-后门调整的应用" class="headerlink" title="4.6 后门调整的应用"></a>4.6 后门调整的应用</h4><h4 id="4-7-回顾假设"><a href="#4-7-回顾假设" class="headerlink" title="4.7 回顾假设"></a>4.7 回顾假设</h4><h2 id="五、自然对照实验和非参数识别"><a href="#五、自然对照实验和非参数识别" class="headerlink" title="五、自然对照实验和非参数识别"></a>五、自然对照实验和非参数识别</h2><h2 id="六、如何估计因果效应-Casual-effect"><a href="#六、如何估计因果效应-Casual-effect" class="headerlink" title="六、如何估计因果效应 Casual effect"></a>六、如何估计因果效应 Casual effect</h2><h2 id="七、-未观察到的混杂因子"><a href="#七、-未观察到的混杂因子" class="headerlink" title="七、 未观察到的混杂因子"></a>七、 未观察到的混杂因子</h2><h2 id="八、工具变量-IV"><a href="#八、工具变量-IV" class="headerlink" title="八、工具变量 IV"></a>八、工具变量 IV</h2><h2 id="九、时间差异-DID"><a href="#九、时间差异-DID" class="headerlink" title="九、时间差异 DID"></a>九、时间差异 DID</h2><h2 id="十、观察数据中的因果发现"><a href="#十、观察数据中的因果发现" class="headerlink" title="十、观察数据中的因果发现"></a>十、观察数据中的因果发现</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>《Introduction to casual inference from a machine learning perspective》 by Brady Neal</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>因果推断</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>盛夏之后，苦秋之前 ｜ 致求职</title>
    <link href="/posts/21152.html"/>
    <url>/posts/21152.html</url>
    
    <content type="html"><![CDATA[<p>从未想象过南方的夏天会如此的丰富多彩，阳光和暴雨的展开姿势是内陆从未见过的大开大合。当天气明媚的时候，仿佛广州塔都在微笑，而当暴雨倾城，突然分不清天地之间的边界。很幸运可以在六月底背负着被航班取消，踏着十个小时的高铁来到羊城，开始了自己的“无问西东“的暑期实习，也希望自己的秋招可以顺利。</p><blockquote><p>“无问西东” 是这个暑假的旋律</p></blockquote><p>朴实竟然是我对腾讯的第一眼，T.I.T 创意园中穿插着矮楼和高树，这些树儿不知何时初见行人，又不知送走了多少代观众。艺术氛围在这里是不缺的，但没想到在游客之中也穿插 B1～B10 的工区，这边是办公地点的，不问西东是未来两个月的主旋律。</p><p>人总是需要追踪点什么才能跑过时间的流失，有的时候是什么都不说的骑着摩托车去体验地面的温度，也有的时候是被赶鸭子上架上手工作。前者是感受生命的禅意，后者可能是感受生命的跳动。初入实习的时候很难适应快节奏：不会写 OKR，看不懂周报黑话，甚至连 SQL 都不熟练，但可能知道自己缺点的时候才是成长最快的时候。当知道自己眼里有梁木之后，时间成为最宝贵的东西。</p><p>天而复始的起床-冥想-工作-下班的飞轮是盛夏的核心，吃掉十几个西瓜、经历过多次暴雨，不知道听过多少蝉鸣，当度过答辩，满下脚步确发现这个夏天已经过去了，如果等站在行刑队面前，一定忘不了这个夏天的坚持。感谢实习中遇到的人让我自己成长，见识到了一个更深层次的世界，也明白追求完美再实际中就要动手去做，“无问西东”是我学到最宝贵的教训。</p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/DSC00223.JPG" alt="DSC00223" style="zoom:10%;" /><blockquote><p> “ 我是谁？我从哪里来？我向哪里去？“ 是无论何时都需要回答的问题</p></blockquote><p>但我好像还是不知道自己应该选择什么样的职业。在绿皮书中，博士爆发时说“我不够黑，我也不够白，告诉我我是谁？”，同样的悖论在日漫中也有体现，无论是金木还是艾伦，当系统中出现变化的人，无法找到自我认同是最大的障碍，筚路蓝缕的孤独只有初代人才能体会。当踏破铁鞋，行半百却蓦然发现不知道自己在哪里，也不知道自己需要什么。</p><p>本人的专业是可笑的交通信息，是个问题非常多的专业。可以说是跨越再文科、工科、理科、商科之外的专业，曾经去过工地见过道路施工，也上过铁路搬过道岔机，了解机场的飞机起飞，也写过公交的排班算法，既要会出彩的报告能力，也要过硬的代码素质，还要探讨群体的管理。仿佛这个专业告诉过我们世界上所有的难题，唯一不告诉的是怎么去解决这些问题。但也幸运的是在这个四不像的专业中，找到了自己喜欢并热爱的东西 – 数据和代码。</p><p>热爱代码是自己的第一步，第一次接触代码是在高中的信息科上，唯一的工具是家里一台 03 年生产的奔腾笔记本，第一次安装 Python，在交互式命令行敲击 print（‘hello world’）,就觉得这是非常酷的事情。上大学最喜欢的一门课是嵌入式系统（交通也要学这个，是不是很神奇），当看到自己写的代码经历烧录和串口传输可以操控硬件，是一件非常有成就感的事情，内心一直希望可以和乔布斯一样可以造出自己的盗版电话机，直到本科的信息论上老师说不同频段使用有规范，才发现条条框框早已梳在虚拟世界之中，接下来唯一有趣的便是数据了。</p><p>机器学习是数据处理中非常热门的话题，但是它到底解决了什么问题？从另外一个角度，当前社会生产力已经经历了很多次的分类，庞大甚至恐怖的信息分发机制笼罩着人们的生活：新闻、生活、外卖、购物、出行… 也由此产生了大量需要决策的需求，而如何将人力解放来使用模型解决它可以解决的问题，让人来确定模型去哪里便是机器学习的意义所在。而幸运的是交通中从不缺难题，因此机器学习、深度学习、强化学习中一些模型也慢慢被尝试动手下来，仿佛自己学会了很多。</p><p>打击来自日常实习的寻找，还记得字节的面试官问我“你知道辛普森悖论”吗？感觉自己就像骄傲的地球人被水滴戳破了虚浮的外表，好像自己从来没有认真的做过什么呢，模型也是只会调包，推导的公式也是复述的，所幸的是通过了滴滴的面试，让我得以认识到真实公司的运作，第一次见识如何处理大规模数据，第一次认真的魔改模型和调参，第一次因为模型效果不够好而焦虑，也第一次因为模型评估通过而高兴。明白了自己的优势并不是刷榜式的改模型，而是针对业务需求是分析问题、应用工具，这边是我需要往哪去。</p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/blog/53c6bb3ed9e27fc57f283b56c0aaadba.jpg" alt="53c6bb3ed9e27fc57f283b56c0aaadba" style="zoom:10%;" /><blockquote><p>心中有信仰，脚下才有力量</p></blockquote><p>人总是需要信仰的，这样才能保持前进的步伐。</p><ul><li>信仰可以是金钱，这是人类社会最通用的等价值交换物，也是个体耐以生存的必要，保持对金钱的渴望才能对现在有更多的动力，但也会专注与六便士而忽略远方的月亮。</li><li>信仰可以是兴趣，因为喜欢代码，旁边的开发小哥会参加 icpc，也会在闲暇时间刷题提升能力，这样的生活是愉悦的，是不自知的。</li><li>信仰也可以来自外部，当失去力量的时候往往需要外部的力量来激励自己，或者说“情绪价值”，保持对代码的热爱可以让人更快速的学习，保持对总结的热爱，可以规范人的行为</li></ul><p>所以在秋招的时候，由于不信任自己擅长什么，在过去一周都过的浑浑噩噩的。所以在此希望自己能勉励自己，在看见自己不足的情况下才是应该成长的时候，去前进而不是去逃避。也希望可以避免成为乌合之众，在牛马之中也可以有天命人的激情。</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活杂谈</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>电脑键盘背后的故事 feat. 宁芝 Plum</title>
    <link href="/posts/4529.html"/>
    <url>/posts/4529.html</url>
    
    <content type="html"><![CDATA[<p>最近购买了一款静电容键盘 Niz68 有线双模，首先面临的最大的问题是这款键盘没有功能键，在对功能键的摸索中发现了自定义键盘的海洋，有一种第一次看到 vim的使用方式的敬畏感油然而生。更重要的激发对于键盘，这个在电脑诞生就陪伴人们的工具，产生了好奇：它是如何一步一步发展成为现在这个样子？它之后是否还会改变？从实体键盘到无线键盘，从电脑键盘到手机虚拟键盘，本文搜集材料来介绍键盘的发展史。</p><h2 id="电报-打字机"><a href="#电报-打字机" class="headerlink" title="电报 打字机"></a>电报 打字机</h2><p>19 世纪初，英国、法国和美国逐渐出现了电报，一种通过电信号传递信息的手段，为了将信息从 文本&#x2F;语言 转换成为 电信号，打字机 作为转换装置自然而然的诞生的，1866 年，“打字机之父” <a href="https://en.wikipedia.org/wiki/Christopher_Latham_Sholes">Christopher  Sholes</a>被认为是 QWERTY 键盘的发明家第一，</p><p>参考 wiki 百科，Sholes 在成为一家报纸的编辑者以后，尝试在印刷机排字工人罢工之后尝试制造一台用于排版的机器，之后它希望通过创建一台机器来实现对书籍、门票的页码编号，在 1866 年 11 月 13 日获取编号机的专利，之后与另一位机械车间的发明者发明一种可以敲击字母的机器，科学美国人将其成为“文字钢琴”</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"> <span class="hljs-attribute">3</span> <span class="hljs-number">5</span> <span class="hljs-number">7</span> <span class="hljs-number">9</span> NOPQRSTUVWXYZ <br><span class="hljs-attribute">2</span> <span class="hljs-number">4</span> <span class="hljs-number">6</span> <span class="hljs-number">8</span> 。 ABCDEFGHIJKLM<br></code></pre></td></tr></table></figure><p>之后他们意识到速记员可能是最需要的用户，但是以最快的速度敲击了薄弱的机械结构，让其成为 可以使用的产品，在 1870 年代不断改进打字机，其中 James Densmore 建议将常用的字符组合分开以解决因敲击恢复速度缓慢造成的干扰问题。由此产生的布局至今在计算机键盘中使用，尽管干扰问题不再存在。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/Sholes_typewriter.jpg" alt="Sholes 打字机"></p><h2 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h2><p>键盘是我们接触电子设备最直接可靠的工具，它简直创造了一种新的交流方式。同时对于键盘本身，从面向对象的角度可以对ta 进行不同层次的抽象，主要包括硬件电路的物理实体和软件定义的快捷键， 由于键盘天生具有物流按键的含义，因此为了之后的修改，其天然的就具备足够多的功能修饰键来管理对应的信息，</p><h3 id="机械键盘？"><a href="#机械键盘？" class="headerlink" title="机械键盘？"></a>机械键盘？</h3><p>机械键盘的物理按键结构主要分为：</p><ol><li>防尘罩：保护内部零件、原理灰尘</li><li>机械轴（Switch）：舒适手感、按键反馈优化</li><li>黄金十字触点：灵敏反映、快速响应</li><li>强力弹簧：敲击寿命</li></ol><p>根据不同的轴体区分处<a href="https://www.zhihu.com/question/20973635">青、红、茶、黑四大轴体</a>，不同的轴体区分主要影响不同的手感。手感可以从两个方面判断：</p><ol><li>段落感，类似于圆珠笔的感觉，主要是青轴和茶轴</li><li>线性轴体，直上直下，干脆利弱，红轴为 45g、黑轴为 55g</li></ol><h3 id="薄膜键盘？"><a href="#薄膜键盘？" class="headerlink" title="薄膜键盘？"></a>薄膜键盘？</h3><p>薄膜键盘内部存在三层线路板，上下两层有导电线路，中是绝缘层，当键盘依靠按键下的硅胶按下到底，让键盘第一层与第三层接触，判断响应量</p><h3 id="静电容键盘？"><a href="#静电容键盘？" class="headerlink" title="静电容键盘？"></a>静电容键盘？</h3><p>利用电容量的变化来实现触发</p><h2 id="键盘是如何工作的"><a href="#键盘是如何工作的" class="headerlink" title="键盘是如何工作的"></a>键盘是如何工作的</h2><p>键盘在当今的计算机时代主要充当 <strong>输入设备</strong>的角色，通过键盘我们可以键入文档、使用快捷键、访问菜单、游戏等等，键盘的不同按键取决于不同的操作系统，通常键盘包括 60～100 键，功能包括</p><ol><li>打字键，存在不同的布局形式，包括 QQWER、Dvorak、ABCDE、XPeRT 等等</li><li>数字键盘，随着商业环境中计算机使用的增加，对于快速数据输入的需求随之增加</li><li>功能、控制，1986 年，IBM 进一步扩充基础键盘，增加功能键和控制键来让操作系统或应用程序可以将特定命令分配给不同的按键。之后 window 、Apple 都为键盘增加不同的按键</li></ol><p>但是从键盘本身可以看作是一台微型计算器，通过自己的矩阵形处理器电路将信息传入和传出该处理器，并通过存储器中的字符映射比较（maybe 查找表）确定输出的值。</p><p>其中键盘矩阵是按键下方的电路网络，在键盘中电路在每个键下方的某个点处断开，当您按下一个键时，它会按下一个开关从而完成电路并允许少量电流通过。在这个过程中往往出现的问题是机械机构会出现反弹的问题，通常需要滤波装置来消除这种振动。</p><p>但是，电容按键可以认为是非机械的，电容式键盘中电流不断流过按键矩阵的所有部分，每个按键均存在弹簧，底部存在一个小板，当您按下某个按键后，该板会移动到其下方的板，当两块板靠近时流过矩阵的电流量发生变化，进而解释为该按键的位置，电容键盘的好处在于两者未接触进而增加键盘的寿命。</p><p>除了传统的键盘之外，在针对人体工程学设计 的键盘往往将键盘分隔开来让人嗯的双手分开的公园，使得手腕与前臂对齐。</p><p>在键盘处理器确定需要将那些字符发送给计算机之后，可以将字符保留在内存缓冲区中，然后通过信息传送装置，如有线的 rs232、sub 等，无线装置包括蓝牙、射频（RF）等等发送给键盘，并由系统分析是否为系统级别命令 or 操作系统级别命令。</p><h2 id="软件定义键盘-｜-karabiner"><a href="#软件定义键盘-｜-karabiner" class="headerlink" title="软件定义键盘 ｜ karabiner"></a>软件定义键盘 ｜ karabiner</h2><p>在 Mac 端一款非常强大的键位映射工具，官网的描述是“A powerful and stable keyboard customizer for macOS”，其核心功能包括</p><ol><li>简单映射，一对一的按键映射关系</li><li>复杂映射，包括一对多的映射，外加条件的映射关系</li></ol><p><a href="https://today.line.me/hk/v2/article/1P1BXm">电脑键盘进化史</a></p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apple</tag>
      
      <tag>macOS</tag>
      
      <tag>静电容键盘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面试经验 vol3 ｜ 从 AB 实验到因果推断</title>
    <link href="/posts/77fa14ff.html"/>
    <url>/posts/77fa14ff.html</url>
    
    <content type="html"><![CDATA[<p>这里参考很多前人的经验：</p><p><a href="https://cloud.tencent.com/developer/article/1913932?areaId=106001">因果推断笔记——入门学习因果推断在智能营销、补贴的通用框架（十一）-腾讯云开发者社区-腾讯云</a></p><h2 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h2><h3 id="0-相关性和因果性"><a href="#0-相关性和因果性" class="headerlink" title="0. 相关性和因果性"></a>0. 相关性和因果性</h3><p>在日常的生活和数据分析中，我们可以得到大量和相关性的杰伦，通常这些相关性和因果性存在对称性，“小偷多的地方，警察多” or “警察多的地方，小偷也会多”，那么是否需要对这个城市增加警力？</p><p>因果性往往是存在单箭头的，比如辛普森悖论</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194410328.png" alt="image-20240429194410328"></p><p>吃药的意愿收到 X 的印象，由此 T 和 X 共同会影响最终的 Y，这和时候需要做分层的解释才能得到最终的结果</p><h3 id="1-解决问题"><a href="#1-解决问题" class="headerlink" title="1. 解决问题"></a>1. 解决问题</h3><p>推理问题的层次包括：</p><ol><li><strong>关联 association</strong>：纯粹的统计关系，在 X 条件下 Y 的输出会怎么样？eg. 购买儿童纸尿裤的人有较大概率会购买啤酒</li><li><strong>介入 intervention</strong>：如果做了某件事情，会发生什么样的效果？eg. 如果我们将地铁票价翻倍会如何？如果吃了药是否会变好？</li><li><strong>反事实推断 counterfactual inference</strong>：如果我们希望 Y 变化，我们希望对 x 做出什么样的改变？eg. 如果地铁人数减少，我们希望知道是否是由票价提高引起的？我们头疼变好是否是因为吃了药</li></ol><h3 id="2-内生性问题"><a href="#2-内生性问题" class="headerlink" title="2. 内生性问题"></a>2. 内生性问题</h3><p><strong>定义：</strong></p><p>在线性回归模型中可以定义四个假设：</p><ol><li>响应变量 y 和解释变量 x 之间存在线性随机函数关系</li><li>严格外生假定: 当所有时期的解释变量给定时，每一期的随机干扰项的均值都为 0</li><li>球形扰动假定</li><li>无完全共线性假定，也就是变量之间不存在线性关系</li></ol><p>由此可以得到对应权重变量的OLS 的最优线性无偏估计量，外生性值得是：</p><p>E(误差 ｜ 变量) &#x3D; 0 ，E（误差 t ｜ 变量 t） &#x3D; 0</p><p>如果外生性假定不满足，则会产生内生性问题，常见的问包括</p><ol><li>遗漏解释变量，存在un observation variable</li><li>X 和 Y 之间互为因果</li><li>自选择问题</li><li>测量偏差问题</li></ol><p>常见的解决方案包括</p><ol><li>自然实验法，将其看作是一种实验组和对照组</li><li>双重差分法 DID，双重差分法，倘若出现一次外部冲击，影响了一部分样本，对另外一部分样本无影响，如果如何外部冲击的影响？可以使用收影响样本做差、未受影响样本做差，差再做差得到最终的结果</li><li>工具变量 IV，找到影响内在变量的外生变量，连同其他的变量得到最终的结果</li><li>动态面板回归法，将解释变量和被解释变量的滞后项作为工具变量的方法来进行使用，但是不认可这种处理方法</li><li>断点回归</li><li>倾向得分匹配模型 PSM：加权、分层、匹配</li></ol><p>关于 PS 问题存在几个误区：</p><ol><li>没有从根本上介绍因为选择偏差或者遗漏变量造成的内生性问题</li><li>不能成为 准实验，无法提供模拟实验条件</li><li>外部有效性条件，在共同支撑假设，PSM 也会排除缺乏对照组的样本进而使得样本代表性变差，影响效果的外部有效性</li></ol><h3 id="3-因果分析的两种框架"><a href="#3-因果分析的两种框架" class="headerlink" title="3. 因果分析的两种框架"></a>3. 因果分析的两种框架</h3><p><strong>潜在结果框架 Rubin potential outcome RCM</strong></p><p>希望估计单个个体或者整体平均意义下的 treatment effect：</p><ol><li>识别（identification）将因果关系从关联中分割</li><li>估计（estimation）计算因果关系的大小</li><li>检验（hypo test）我们有多大信心结果是正确的</li><li>置信（conf int）结果会存在多大的扰动</li></ol><p>最重要的是需要找到合适的：对照组和实验组</p><p>keywords：</p><ol><li>匹配方法</li><li>双重差分模型 DID</li><li>合成控制法 Synthetic control method</li><li>断点回归</li></ol><p><strong>因果图结果框架 Pearl causal graph CGM</strong></p><p>有向图用来描述变量之间的因果关系，通过计算因果图之间的条件分布，获得变量之间的因果关系，希望使用条件分布来消除估计偏差，最核心的估计检验分布，消除其他变量带来的偏差</p><p>侧重识别</p><p>keywords：后门准则 和 前门准则</p><h3 id="4-调节效应于中介效应"><a href="#4-调节效应于中介效应" class="headerlink" title="4. 调节效应于中介效应"></a>4. 调节效应于中介效应</h3><ol><li>中介效应：X —&gt; M —&gt; Y</li><li>调节效用：X —&gt; Y， M —&gt; Y</li></ol><p>比如，性别 X 和录取 Y 之间的关系，其中性别 X 会影响 M 学历，进而影响 Y，这个是中介效用，这个时候我们需要消除这个学历（confounders）的影响来保证学历不变的情况下，衡量性别于录取结果之间的关系</p><p>比如，收入 I 会影响M，这个就是调节作用</p><h3 id="5-常见假设"><a href="#5-常见假设" class="headerlink" title="5. 常见假设"></a>5. 常见假设</h3><p><strong>无混淆性 unconfounder ｜ 可忽略假设 Ignorability ｜ CIA 假设</strong></p><p>不存在未被观察到的可以影响 T 分布的混杂因子</p><p><strong>正值假设 positivity</strong></p><p>至少存在一项干预导致的结果是无法被观察饿的</p><p><strong>一致性假设 consistency ｜ 稳定单元干预值假设 Stable unit treatment value assumption SUTVA</strong></p><p>任意单元的潜在结果都不会因为其他单元的干预发生改变而改变，且对于每个单元其所接受的每种干预不存在不同的形式或版本导致不同的潜在结果</p><p>干预水平对于所有的个体一致</p><h2 id="常见模型"><a href="#常见模型" class="headerlink" title="常见模型"></a>常见模型</h2><h3 id="1-PS-based-方法"><a href="#1-PS-based-方法" class="headerlink" title="1. PS based 方法"></a>1. PS based 方法</h3><p>Match 匹配方法，寻找对照组</p><p>IPW 逆倾向性打分的方法</p><p>缺点：1. 需要对 PS 估计足够准确 2. 过去趋近于 会导致权重过高</p><p>DR or （AIPW）增强 IPW 方法结合倾向性打分和结果回顾模型来得样本权重</p><p>Stratification PSS 方法</p><h3 id="2-数据驱动的方法"><a href="#2-数据驱动的方法" class="headerlink" title="2. 数据驱动的方法"></a>2. 数据驱动的方法</h3><p>D2VD 变量分解方法</p><ol><li>混淆变量 confounders：影响干预变量 T，也会影响结果变量 Y</li><li>调整变量 adjustment： 与干预变量 T 独立，但是会影响到 Y</li><li>无关变量，不会直接影响到干预变量于估计变量</li></ol><h3 id="3-平衡性-and-均衡性检验检查"><a href="#3-平衡性-and-均衡性检验检查" class="headerlink" title="3. 平衡性 and 均衡性检验检查"></a>3. 平衡性 and 均衡性检验检查</h3><p><strong>如何知道匹配的效果？或者实验组对照组是否分布均匀</strong></p><p>最直观的是观察</p><ol><li>PS 分数在匹配前后的分布，或者是</li><li>特征匹配前后的 QQ plot</li><li>量化指标 SMD，用于计算两者之间的差异</li><li>共同支撑检验 common support：主要检验的目的是确定针对每个处理组，都有对照组与之匹配</li></ol><p><strong>反驳 Refute</strong></p><p>使用不同的数据干预方式来进行检验，来验证得到的因果效应的有效性，反驳的基本原理是对于数据进行某种关于之后，对重新生成的数据进行因果效应的估计</p><ol><li>安慰剂数据方法，使用 placebo 的方法来代替真实的处理变量</li><li>随机混淆变量，增加一个随机生成的混淆变量</li><li>子集数据方法：随机删除一部分数据，作为最终数据的随机自己</li></ol><h3 id="4-DID-双重差分法"><a href="#4-DID-双重差分法" class="headerlink" title="4. DID 双重差分法"></a>4. DID 双重差分法</h3><p>在一定程度上减轻了选择偏差和外因带来的影响</p><p>使用的时候需要满足的假设：</p><ol><li>线性关系假设：来自于线性回归，认为因变量和结果变量存在线性关系</li><li>个体处理稳定假设 SUTVA<ol><li>一致性 consistency：个体接受处理所导致的潜在结果是唯一的</li><li>互不干预：个体接受处理的潜在结果不会受到其他个体吃的影响，eg 比如我在淘宝上购买红包后会更愿意买东西，并不会因为我同事也领到了红包而降低意愿</li></ol></li><li>平行趋势假设：结果的趋势是一只的</li></ol><h3 id="5-工具变量法-Instrumental-variable"><a href="#5-工具变量法-Instrumental-variable" class="headerlink" title="5. 工具变量法 Instrumental variable"></a>5. 工具变量法 Instrumental variable</h3><p>工具变量需要存在以下三个条件</p><ol><li>Z 和 T 相关</li><li>Z 只能通过T 影响 Y</li><li>Z 和 Y 没有共同诱因</li></ol><h3 id="6-Double-machine-learning"><a href="#6-Double-machine-learning" class="headerlink" title="6. Double machine learning"></a>6. Double machine learning</h3><p>消除精准预测，使用任意的 ML 模型给出特征对于目标的无偏估计</p><h2 id="应用细节"><a href="#应用细节" class="headerlink" title="应用细节"></a>应用细节</h2><h3 id="1-双重机器学习-Double-machine-learning"><a href="#1-双重机器学习-Double-machine-learning" class="headerlink" title="1. 双重机器学习 Double machine learning"></a>1. 双重机器学习 Double machine learning</h3><p>问题例子：分析直播推荐多样性（D）对用户活跃度（Y）的影响，都收到用户自身的画像、用户过去的直播浏览历史 X 有关</p><p>存在的问题：</p><ol><li>X 的纬度太高怎么办？</li><li>不知道 X 和 D 的关系</li></ol><p>方法思路：</p><ol><li>CIA 假设，所有混淆变量都可以被观测</li><li>ML 自带的正则化来达到高纬度变量的选择的目的</li></ol><p>估计 X 对 Y 的影响，再估计 X 对 D 的影响，提出这两部分之后，取残差进行估计，来分析直播多样性和输出之间的关系</p><h3 id="2-因果随机森林模型"><a href="#2-因果随机森林模型" class="headerlink" title="2. 因果随机森林模型"></a>2. 因果随机森林模型</h3><p>相当于训练一个 treatment 的树，然后在树的节点做因果头对岸，类似于match or subclass很相似</p><h3 id="3-meta-learner-对于-uplift-modeling"><a href="#3-meta-learner-对于-uplift-modeling" class="headerlink" title="3. meta learner 对于 uplift modeling"></a>3. meta learner 对于 uplift modeling</h3><p>S-learner</p><p>T-learner</p><p>X-learner：利用 weight function 来得到两种不同 ATE 估计的求和</p><h3 id="4-快手的因果推断和实验估计"><a href="#4-快手的因果推断和实验估计" class="headerlink" title="4. 快手的因果推断和实验估计"></a>4. 快手的因果推断和实验估计</h3><p>场景：直播是一个双边网络</p><p>网络设计的难点：网络效应的检测和应对</p><ol><li>流量扶持情况下，主播是否存在流量基站</li><li>主播侧传播到用户侧的双边网络效用</li><li>用户侧传播到主播侧的双边网络效用</li></ol><p>更微观一点的：</p><p>问题挂件是否会存在刺激消费？</p><p>主播：control 不上挂件、treatment 上挂件</p><p>用户：control 看不到挂件、treatment 看得到挂件</p><ol><li>挂件可能会导致 treatment用户在 treatment 和 control 的用户之间转移直播消费</li><li>treatment 主播可能会更加卖力直播，影响 treatment 和 control 组观众</li></ol><h3 id="5-淘宝中的PSM-DID"><a href="#5-淘宝中的PSM-DID" class="headerlink" title="5. 淘宝中的PSM+DID"></a>5. 淘宝中的PSM+DID</h3><p>实验组和对照组是否同质</p><p><a href="https://mp.weixin.qq.com/s/pjgYKfFICqzvFwhHwHBNow">倾向得分匹配（PSM）的原理以及应用</a></p><p>PSM： 根据倾向性打分来进行匹配，适用于样本属性很高，并且不好做切断的离散变量</p><p>CEM：广义精确匹配，使用核心混杂因子进行匹配，每个实验用户匹配到的 N 个同特征用户作为对照组，取 N 个同特征用户的核心指标均值作为实验用户的对照</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194428562.png" alt="image-20240429194428562"></p><h3 id="6-淘宝3D-化价值分析"><a href="#6-淘宝3D-化价值分析" class="headerlink" title="6. 淘宝3D 化价值分析"></a>6. 淘宝3D 化价值分析</h3><p><a href="https://mp.weixin.qq.com/s/etMqiZ0oKH9BtrGwLdJu2g">mp.weixin.qq.com</a></p><h3 id="7-腾讯因果推断框架总结"><a href="#7-腾讯因果推断框架总结" class="headerlink" title="7 腾讯因果推断框架总结"></a>7 腾讯因果推断框架总结</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194435623.png" alt="image-20240429194435623"></p><h3 id="8-滴滴国际化外卖团队的因果推断"><a href="#8-滴滴国际化外卖团队的因果推断" class="headerlink" title="8. 滴滴国际化外卖团队的因果推断"></a>8. 滴滴国际化外卖团队的因果推断</h3><p>因果推断建模和 auuc</p><p>ROI &#x3D; （指标｜干预 - 指标｜不干预）&#x2F; （补贴｜干预 - 补贴｜不干预）</p><p>ROI 不一定会是一个好的指标，同时 GMV 和成本分层下对应的ROI 提升也是有效的</p><h3 id="9-QQ-浏览器-push-优化实践"><a href="#9-QQ-浏览器-push-优化实践" class="headerlink" title="9. QQ 浏览器 push 优化实践"></a>9. QQ 浏览器 push 优化实践</h3><p><a href="https://zhuanlan.zhihu.com/p/451884908">安全验证 - 知乎</a></p><h2 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194445924.png" alt="image-20240429194445924">个人总结</h2><p>离散选择或者一类的回归分析、基于特征重要性的树模型是从 结果来分析对应的原因，这个其实和因果推断中得出 AB 实验如何来分析背后的差异是一致的，本质上解决的问题是从结果到原因</p><ul><li>改不改发优惠卷</li></ul><p>在因果推断方法中，我们希望解决what if 的问题，也就是从如果我们希望得到什么样的结果，我们应该如何来做出自己的策略的选择，这个是从“原因来分析结果”，所以通常有因果关系发现和因果效用估计两种方式</p><ul><li>线路关闭受到哪些影响因素</li><li>指标异常分析、流失分析、DAU 分析</li></ul><p>那么为什么要做因果推断？这个表现在强化学习中是可能会造成智能体学习的策略估计，通常最简单的方式是DQN 的 Q 值会过高轨迹，为了解决这个 on policy 的问题，我们通常会使用 off policy 的方法来分离探索智能体和目标智能体来得到教优估计。</p>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>因果推断</tag>
      
      <tag>AB 实验</tag>
      
      <tag>观察数据建模</tag>
      
      <tag>选择偏差</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面试经验 Vol2 ｜ 损失函数和评价指标总结</title>
    <link href="/posts/d29c87a5.html"/>
    <url>/posts/d29c87a5.html</url>
    
    <content type="html"><![CDATA[<p>Mitchell 在《机器学习》中对其的定义是：</p><blockquote><p><strong>一个计算机程序被认为能够从经验 E 中学习,去完成任务 T,达到性能度量 P,当且仅当,通过经验 E 的学习,其在任务 T 上的性能度量 P 得到了提升。</strong></p></blockquote><p>假设我们有一个模型 $f(x;θ)$,其中 x 是输入,θ 是模型的参数。给定一组训练数据  $D&#x3D;{(x_i,y_i)}_{i&#x3D;1}^N$,其中 $x_i$是输入,$y_i$是对应的目标输出。我们的目标是找到一组参数 θ,使得模型 f(x;θ) 在任务 T 上的性能度量 P 最大化。</p><p>这里,损失函数 L(f(x;θ),y) 的作用就是衡量模型的预测输出 f(x;θ) 与真实目标输出 y 之间的差异。我们希望通过最小化损失函数,来找到最优的模型参数 θ。因此,机器学习的任务可以表示为一个优化问题:</p><p>$$ min_\theta \frac{1}{N} L(f(x_i;\theta),y_i) $$</p><p>通过最小化训练数据上的平均损失,我们得到了一组最优参数 θ,使得模型 $f(x;\theta)$在任务 $T$上的性能度量 P 得到提升,从而实现了从经验 E 中学习的目标。</p><p>在学习链路的整个过程中，数据（E）、模型（计算机程序）、度量（L）、训练（T） 都是非常重要的部分，本文主要针对学习过程中针对不同的任务设计不同的损失函数，来实现模型效果的最优化。</p><h2 id="0x00-如何设计损失函数"><a href="#0x00-如何设计损失函数" class="headerlink" title="0x00 如何设计损失函数"></a>0x00 如何设计损失函数</h2><h3 id="1-1-ERM-输出～标签"><a href="#1-1-ERM-输出～标签" class="headerlink" title="1.1 ERM 输出～标签"></a>1.1 ERM 输出～标签</h3><p>如果把数据看作是历史观察的经验，那么经验风险就是模型在训练数据上的平均误差：</p><p>$$ R_{erm} &#x3D; \frac{1}{N}\Sigma L(f(x_i),y_i) $$</p><h3 id="1-2-SRM-模型本身"><a href="#1-2-SRM-模型本身" class="headerlink" title="1.2 SRM 模型本身"></a>1.2 SRM 模型本身</h3><p>由于模拟复杂度和问题复杂度之间不匹配会造成《欠拟合或过拟合》的问题，因此通过对模型的复杂度进行限制有有效的避免过拟合</p><p>$$ R_{srm} &#x3D; R_{erm}+\lambda \Omega(f) $$</p><p>初次之外，我们可能需要根据不同的任务来区分不同的损失函数设计方式，比如：</p><ol><li>GAN 的损失函数设计，将生成器看作负样本，将真实样本看作正样本，训练过程</li><li>SVM 的损失函数设计，利用最大间隔来区分正负样本</li><li>交叉熵或 KL 散度，利用信息论来衡量模型预测概率和真实概率分布的差异</li><li>排序学习损失函数设计，包括 list wise、Pair wise、List wise 等方式计算</li></ol><table><thead><tr><th>分类</th><th>Cross entropy</th><th>回归</th><th>Square loss</th><th>LTR</th><th>pairwise CE</th></tr></thead><tbody><tr><td></td><td>hinge loss</td><td></td><td>absolute loss</td><td></td><td>pairwise Hinge</td></tr><tr><td></td><td>Focal loss</td><td></td><td>Huber loss</td><td></td><td>pairwise Square</td></tr><tr><td></td><td>weighted CE</td><td></td><td>Log loss</td><td></td><td>lambdaRANK loss</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>ListNet loss</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>对比</td><td>infoNCE</td><td>多任</td><td>Linear weighted</td><td>多标</td><td>soft margin</td></tr><tr><td>学习</td><td>Triplet</td><td>务学</td><td>Mean squared</td><td>签学</td><td>ranking</td></tr><tr><td></td><td>NCE</td><td>习</td><td>uncertain loss</td><td>习</td><td></td></tr><tr><td></td><td>CPC</td><td></td><td>gradient norm</td><td></td><td></td></tr></tbody></table><p>初次之外还包括很多其他的任务，比如分类、回归、学习排序、搜推广、强化学习、生成学习、对比学习、度量学习、多任务学习、异常检测损失、few-shot 学习、零样本学习、多标签学习、非监督学习损失、半监督学习损失、持续学习损失函数、因果推断损失等等。</p><h2 id="0x01-分类损失函数"><a href="#0x01-分类损失函数" class="headerlink" title="0x01 分类损失函数"></a>0x01 分类损失函数</h2><h3 id="1-1-Cross-Entropy-损失"><a href="#1-1-Cross-Entropy-损失" class="headerlink" title="1.1 Cross Entropy 损失"></a>1.1 Cross Entropy 损失</h3><p>利用0-1 分布可以推导出对应的损失函数，注意这里的y_hat 是概率，y是 0-1 标签</p><p>$$ L(y,y_{hat}) &#x3D;\Sigma ylog(y_{hat})+(1-y)log(1-y_{hat})) $$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194055099.png" alt="image-20240429194055099"></p><h3 id="1-2-KL散度"><a href="#1-2-KL散度" class="headerlink" title="1.2 KL散度"></a>1.2 KL散度</h3><p>KL 散度 &#x3D; 交叉熵 - 信息熵，用于策略两个概率分布差异的方法，其数学表示为；注意通过这个可以看出两者具有非负性、非对称性。</p><p>$$ KL(P||Q) &#x3D; \Sigma_xP(x)\frac{P(x)}{Q(x)} $$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194102381.png" alt="image-20240429194102381"></p><h3 id="1-3-指数损失"><a href="#1-3-指数损失" class="headerlink" title="1.3 指数损失"></a>1.3 指数损失</h3><p>同号时两者的损失较小，当两者sign 不一致后导致惩罚急剧增大</p><p>$$ L &#x3D; \Sigma exp(-y_if(x_i)) $$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194112167.png" alt="image-20240429194112167"></p><h3 id="1-4-Hinge-los"><a href="#1-4-Hinge-los" class="headerlink" title="1.4 Hinge los"></a>1.4 Hinge los</h3><p>确定性较强时损失为 0，否则损失不为 0</p><ol><li>间隔最大化：Hinge loss 通过惩罚位于正确分类一边但距离决策边界太近的点，以及位于错误分类一边的点，从而鼓励模型找到一个能够最大化正负样本间隔的决策边界。</li><li>对正确分类的容忍度：如果一个样本被正确分类，并且其预测值与实际标签的乘积大于1（即，它们之间的间隔超过了1），那么这个样本对损失函数的贡献为0。这意味着模型不会因为正确分类的样本而受到惩罚，只要这些样本距离决策边界有足够的间隔。</li><li>对错误分类的惩罚：对于错误分类的样本（即，预测值与实际标签的乘积小于1），损失值随着分类错误的程度增加而增加。这促使模型减少这些错误。</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194121832.png" alt="image-20240429194121832"></p><h3 id="1-5-Focal-loss"><a href="#1-5-Focal-loss" class="headerlink" title="1.5 Focal loss"></a>1.5 Focal loss</h3><p>通常用于解决分类任务中的类别不平衡和难易样本的问题。它是一种针对二分类问题设计的损失函数，旨在减轻易分类样本对模型训练的影响，使得模型更加关注难分类样本，从而提高整体分类性能。因此，Focal Loss属于分类任务中的损失函数。</p><p>$$ FL(p_t )&#x3D;−α_t (1−p_t )^γ log(p_t ) $$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194129471.png" alt="image-20240429194129471"></p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs jsx"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.<span class="hljs-property">nn</span> <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.<span class="hljs-property">nn</span>.<span class="hljs-property">functional</span> <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FocalLoss</span>(nn.<span class="hljs-property">Module</span>):<br>    def <span class="hljs-title function_">__init__</span>(self, gamma=<span class="hljs-number">2</span>, weight=<span class="hljs-title class_">None</span>, reduction=<span class="hljs-string">&#x27;mean&#x27;</span>):<br>        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">FocalLoss</span>, self).<span class="hljs-title function_">__init__</span>()<br>        self.<span class="hljs-property">gamma</span> = gamma<br>        self.<span class="hljs-property">weight</span> = weight<br>        self.<span class="hljs-property">reduction</span> = reduction<br>        <br>    def <span class="hljs-title function_">forward</span>(self, input, target):<br>        # 计算交叉熵<br>        ce_loss = F.<span class="hljs-title function_">cross_entropy</span>(input, target, reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>        <br>        # 计算pt<br>        pt = torch.<span class="hljs-title function_">exp</span>(-ce_loss)<br>        <br>        # 计算focal loss<br>        focal_loss = ((<span class="hljs-number">1</span>-pt)**self.<span class="hljs-property">gamma</span> * ce_loss).<span class="hljs-title function_">mean</span>()<br>        <br>        <span class="hljs-keyword">return</span> focal_loss<br></code></pre></td></tr></table></figure><h2 id="0x02-回归损失函数"><a href="#0x02-回归损失函数" class="headerlink" title="0x02 回归损失函数"></a>0x02 回归损失函数</h2><h3 id="2-1-MSE-均方差损失函数"><a href="#2-1-MSE-均方差损失函数" class="headerlink" title="2.1 MSE 均方差损失函数"></a>2.1 MSE 均方差损失函数</h3><p>用于衡量样本真实值和预测值之间的差距，利用概率可以推导出背后的公式</p><h3 id="2-2-RMSE-均方根误差损失函数"><a href="#2-2-RMSE-均方根误差损失函数" class="headerlink" title="2.2 RMSE 均方根误差损失函数"></a>2.2 RMSE 均方根误差损失函数</h3><p>保证与样本量纲保持一致</p><h3 id="2-3-MAE-绝对值损失"><a href="#2-3-MAE-绝对值损失" class="headerlink" title="2.3 MAE 绝对值损失"></a>2.3 MAE 绝对值损失</h3><h2 id="0x03-对比学习（contrastive-learning）中的损失函数"><a href="#0x03-对比学习（contrastive-learning）中的损失函数" class="headerlink" title="0x03 对比学习（contrastive learning）中的损失函数"></a>0x03 对比学习（contrastive learning）中的损失函数</h2><p><a href="https://zhuanlan.zhihu.com/p/346686467">对比学习（Contrastive Learning）综述</a></p><p>这里总结一下对比学习的基本思想，主要是通过度量两者分布的距离，保证同类别度量尽可能小，不同类型之间的距离尽可能的大</p><h3 id="3-1-contrastive-loss"><a href="#3-1-contrastive-loss" class="headerlink" title="3.1 contrastive loss"></a>3.1 contrastive loss</h3><p>对比损失中其中最困难的应该是如何在无标签数据中定义标签来进行学习，其中 d 代表的是(x1,x2)之间的样本距离，其中 y 表示两者是否为同类型的样本，类比弹簧势能 的定义：</p><ol><li>对于模型预测为同类别，1，但是样本实际的距离越远，两者之间的损失越大</li><li>对于模型预测为非同类，0，但是样本实际的距离越远，两者的损失函数越小；至少保证在一定 margin 外不会出现 0 的情况</li></ol><p>对比损失属于 Pair wise 的一种，在通常的情况下并不需要计算所有样本下的损失，而是计算正样本和负样本之间的距离可以作为方式，或者增加一些随机样本、负采样、软采样的方式。</p><p>$CL &#x3D; \frac{1}{2N}\Sigma [y_i ⋅d^2 +(1−y_i )⋅max(margin−d,0)^2 ]$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194139259.png" alt="image-20240429194139259"></p><h3 id="3-2-Triplet-los-三元组损失"><a href="#3-2-Triplet-los-三元组损失" class="headerlink" title="3.2 Triplet los 三元组损失"></a>3.2 Triplet los 三元组损失</h3><p>但是在实际的训练过程中，我们并不确定所有的样本都可以计算，同时假设所有的样本无限拉近可能会导致过拟合的风险。因此希望通过对比的方式将三元组分为（x 正，x 负，x）;也就是将数据集划分为：Anchor、positive、negative</p><p>$$ L&#x3D; max(d(x^+,x)-d(x^-,x)+\alpha,0) $$</p><p>FaceNet: A Unified Embedding for Face Recognition and Clustering</p><p>在搜索推荐场景中：</p><ol><li>anchor：用户当前搜索查询的浏览记录</li><li>postive：用户点击或者互动过的相关商品和内容</li><li>negative：用户未点击或互动的不相关的商品</li></ol><h3 id="3-3-NCE-loss"><a href="#3-3-NCE-loss" class="headerlink" title="3.3 NCE loss"></a>3.3 NCE loss</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194150982.png" alt="image-20240429194150982"></p><h3 id="3-4-Info-NCE-loss"><a href="#3-4-Info-NCE-loss" class="headerlink" title="3.4 Info NCE loss"></a>3.4 Info NCE loss</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194208664.png" alt="image-20240429194208664"></p><h3 id="3-5-Center-loss"><a href="#3-5-Center-loss" class="headerlink" title="3.5 Center loss"></a>3.5 Center loss</h3><p>A Discriminative Feature Learning Approach for Deep Face Recognition</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194216806.png" alt="image-20240429194216806"></p><h3 id="3-6-Circle-loss"><a href="#3-6-Circle-loss" class="headerlink" title="3.6 Circle loss"></a>3.6 Circle loss</h3><h2 id="0x04-排序学习（learning-to-rank）中的损失函数"><a href="#0x04-排序学习（learning-to-rank）中的损失函数" class="headerlink" title="0x04 排序学习（learning to rank）中的损失函数"></a>0x04 排序学习（learning to rank）中的损失函数</h2><p><a href="https://www.xiemingzhao.com/posts/IntroductionofLTR.html">LTR(Learning to Rank)概述</a></p><p>了解排序问题首先需要明确目的，比如在搜索问题中，最关心的是返回 N 个样本的相关顺序关系，而不是每个样本的预测值最准确，属于 ranking 问题（另一类是 rate 问题）</p><p>对于训练样本的标注主要包括以下两种：</p><ol><li>人工标注，针对搜索结果进行不同等级的划分</li><li>历史数据，通过搜索历史来获取对应的标注，对应这种观察数据存在选择偏差</li></ol><p>根据 Point wise、Pair wise、List wise，</p><ol><li>Pointwise: Subset Ranking, McRank, Prank, OC SVM</li><li>Pairwise: Ranking SVM, RankBoost, RankNet, GBRank, IR SVM, Lambda Rank, LambdaMart</li><li>Listwise: ListNet, ListMLE, AdaRank, SVM MAP, Soft Rank</li></ol><h2 id="0x05-生成学习（Generative）中的损失函数"><a href="#0x05-生成学习（Generative）中的损失函数" class="headerlink" title="0x05 生成学习（Generative）中的损失函数"></a>0x05 生成学习（Generative）中的损失函数</h2><p><a href="https://github.com/hindupuravinash/the-gan-zoo">https://github.com/hindupuravinash/the-gan-zoo</a></p><h2 id="0x06-多任务学习（Multi-task-learning）中的损失函数"><a href="#0x06-多任务学习（Multi-task-learning）中的损失函数" class="headerlink" title="0x06 多任务学习（Multi-task learning）中的损失函数"></a>0x06 多任务学习（Multi-task learning）中的损失函数</h2><ol><li>uncertainty weighting 不确定性</li><li>Grad normalization 梯度均衡</li><li>Multi-objective optimization</li><li>Geometric loss</li><li>HydaLearn 存在的问题</li><li>Cov - weighting 动态计算</li><li>Scaled loss approximate weighting</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429194227076.png" alt="image-20240429194227076"></p>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>损失函数</tag>
      
      <tag>评价指标</tag>
      
      <tag>模型评估</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面试经验 vol1 ｜ 计算机基础知识</title>
    <link href="/posts/632147e0.html"/>
    <url>/posts/632147e0.html</url>
    
    <content type="html"><![CDATA[<p>总结常见的诞生于 <strong>计算机组成原理，操作系统</strong>、 <strong>计算机网络</strong>、<strong>算法设计基础</strong>中的常见的知识，在这个过程中回顾快速入门计算机知识中，从纸条到硬盘、从继电器到二极管；这门学科的目的是实现自动化的操作，帮助我们高效的完成既定任务。在这个过程要克服死记硬背的指令，来解决盲目寻找答案的过程。</p><h1 id="操作系统相关"><a href="#操作系统相关" class="headerlink" title="操作系统相关"></a>操作系统相关</h1><h2 id="1-1-Shell和脚本"><a href="#1-1-Shell和脚本" class="headerlink" title="1.1 Shell和脚本"></a>1.1 Shell和脚本</h2><p>使用字符串和计算机内核进行交互。其核心功能是允许执行程序，输入并获取某种结构化的输出。shell和所有的编程语言，同样包括变量、函数、条件、循环等一些系列操作，同时会询问环境变量 $PATH</p><h3 id="Shell-中常见的函数有哪些？"><a href="#Shell-中常见的函数有哪些？" class="headerlink" title="Shell 中常见的函数有哪些？"></a>Shell 中常见的函数有哪些？</h3><p>常见的函数：</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs jsx">date # 查看的当前日期<br>pwd # 阅读当前的工作目录<br>echo hello # 输出对应的值<br>echo $PATH # 查看环境变量<br>cd # 进入目录<br>ls # 列表<br>mv # 重命名或者移动文件<br>cp # 拷贝文件<br>mkdir #创建文件夹<br>find # 查找文件<br>fd pattern<br> locate<br></code></pre></td></tr></table></figure><h3 id="Shell文本处理三巨头grep、sed、awk的案例"><a href="#Shell文本处理三巨头grep、sed、awk的案例" class="headerlink" title="Shell文本处理三巨头grep、sed、awk的案例"></a>Shell文本处理三巨头grep、sed、awk的案例</h3><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs jsx">grep # <span class="hljs-variable language_">global</span> regular expression print ，用于搜索文本内容中匹配特定模式 <br>sed #  stream editor 流编辑器，用于执行文本文件中的文本替换、删除、插入操作<br>awk # 文本和数据的提取和报告生成<br><br># 一些案例<br>grep <span class="hljs-string">&#x27;error&#x27;</span> log.<span class="hljs-property">txt</span><br>grep -r <span class="hljs-string">&#x27;functionName&#x27;</span> /path/to/directory/<br>awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span> file.<span class="hljs-property">txt</span><br>awk <span class="hljs-string">&#x27;END &#123;print NR&#125;&#x27;</span> file.<span class="hljs-property">txt</span><br>sed <span class="hljs-string">&#x27;s/oldText/newText/g&#x27;</span> file.<span class="hljs-property">txt</span><br>sed <span class="hljs-string">&#x27;3d&#x27;</span> file.<span class="hljs-property">txt</span>‘<br></code></pre></td></tr></table></figure><h3 id="Shell-中函数的常见参数"><a href="#Shell-中函数的常见参数" class="headerlink" title="Shell 中函数的常见参数"></a>Shell 中函数的常见参数</h3><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jsx">-h # 帮助<br>-- version# 查看版本<br></code></pre></td></tr></table></figure><h3 id="管道的使用方式Pipeline"><a href="#管道的使用方式Pipeline" class="headerlink" title="管道的使用方式Pipeline"></a>管道的使用方式Pipeline</h3><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jsx">&gt; file  &lt; file # 将程序的输入输出重定向<br>｜ # 使用pipes来连接输入和输出<br></code></pre></td></tr></table></figure><h3 id="从shell命令到脚本"><a href="#从shell命令到脚本" class="headerlink" title="从shell命令到脚本"></a>从shell命令到脚本</h3><p>从shell到脚本，可以实现变量、控制流和语法，同时加强了pipelines、保存到文件、标准输入输出原生操作的优势。</p><ol><li>$Var 是变量，$0 脚本名、$1~9 参数名、$@所有参数、$# 参数个数、$? 前一个命令的饭绘制、$$ 当前脚本的进程识别码、!! 上一次命令、$_ 上一条命令最后一层参数</li><li>‘ 和 “” 是不一样的</li><li>同样支持 if case while for的控制流关键字的操作</li><li>命令替换（command substitution）、进程替换（process substitution）</li><li></li></ol><h3 id="常用的正则表达式"><a href="#常用的正则表达式" class="headerlink" title="常用的正则表达式"></a>常用的正则表达式</h3><p>必须要熟悉正则表达式：</p><ol><li>. 除去换行符之外的任意单个字符</li><li>*匹配前面字符零次或者多次</li><li>‘+’ 匹配前面字符一次或者多次</li><li>[abc]匹配abc其中的任何一个</li><li>(rx1|rx2)任何能够匹配Rx1或者rx2的结果</li><li>^ 行首</li><li>$ 行尾</li></ol><p><a href="https://regexone.com/">RegexOne - Learn Regular Expressions - Lesson 1: An Introduction, and the ABCs</a></p><h3 id="配置文件的格式-dotfile"><a href="#配置文件的格式-dotfile" class="headerlink" title="配置文件的格式 dotfile"></a>配置文件的格式 dotfile</h3><p>相关程序的配置问价 DOTFILE 是通过这类隐藏的配置文件实现的</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jsx">bash ~/.<span class="hljs-property">bashrc</span><br>git /.<span class="hljs-property">gitconfig</span><br>vim ~/.<span class="hljs-property">vimrc</span> <br>ssh ~<span class="hljs-regexp">/.ssh/</span>config<br></code></pre></td></tr></table></figure><h3 id="SSH-使用教程"><a href="#SSH-使用教程" class="headerlink" title="SSH 使用教程"></a>SSH 使用教程</h3><p>ssh 使用过程，只需要向服务器证明客户端的私钥即可，等效于密码。</p><ul><li>ssh-keygen可以生成密钥</li><li>查询 .ssh&#x2F;authorized_keys</li><li>ssh-copy-id -i **pub username@ip</li></ul><h2 id="1-2-构建系统"><a href="#1-2-构建系统" class="headerlink" title="1.2 构建系统"></a>1.2 构建系统</h2><h3 id="调试器是什么以及其对应的作用？"><a href="#调试器是什么以及其对应的作用？" class="headerlink" title="调试器是什么以及其对应的作用？"></a><strong>调试器是什么以及其对应的作用？</strong></h3><p>是一种可以允许我们和正在执行的程序进行交互的程序，可以做到一条一条执行并且在程序崩溃之后查看变量的值；对于python的调试器包括：</p><ol><li>l 显示当前行附近11行或继续执行之前的提示</li><li>s 执行当前行</li><li>n 继续执行到下一个条遇见</li><li>b 设置断点</li><li>p 打印，pprint对于表达式求值并输出结果</li><li>r继续执行</li><li>q推出</li></ol><p>除了代码调试之外，通常还包括专<strong>门工具、静态分析、性能分析、事件分析等等</strong></p><h3 id="元编程-（meta-programming）是什么？"><a href="#元编程-（meta-programming）是什么？" class="headerlink" title="元编程 （meta programming）是什么？"></a>元编程 （meta programming）是什么？</h3><p>不是关于代码、高效工作，而是如何在大型工程中提升代码的鲁棒性，这些包括构建系统、持续集成、代码测试、依赖管理等等内容</p><h3 id="构建系统-（Make-system）"><a href="#构建系统-（Make-system）" class="headerlink" title="构建系统 （Make system）"></a>构建系统 （Make system）</h3><p>构建系统在多次可以见到，包括嵌入式中将语言烧录成为hex格式并传送至小车、包括latex中构建pdf的过程等等了这个步骤包括不同的分支。通常这些工具需要 定义 依赖（dependence）、目标（target）和规则（rule）</p><ul><li>依赖：系统的任务是找到构建这些目标所需要的依赖</li><li>目标：告诉构建系统的具体目标</li><li>规则：构建规则</li></ul><p>常见的make是最常用的构建系统之一，包括makefile</p><h3 id="依赖管理（Dependency-management）"><a href="#依赖管理（Dependency-management）" class="headerlink" title="依赖管理（Dependency management）"></a>依赖管理（Dependency management）</h3><p>对于项目来说，依赖于其他的项目、其他的程序、系统包、语言库。这些依赖往往需要特定的方式来进行管理。每一种仓库、每一种工具的运行机制都不太一样，也就是版本控制（Version control）方法的不同，常见的规则包括</p><ol><li>如果新的版本没有改变API，则补丁递增</li><li>如果添加api，且向后兼容，则次版本号递增</li><li>如果修改api，但并不兼容，则主版本好递增</li></ol><h3 id="持续集成（Continuous-integration）"><a href="#持续集成（Continuous-integration）" class="headerlink" title="持续集成（Continuous integration）"></a>持续集成（Continuous integration）</h3><p>随着项目规模的扩大，往往除了修改代码本身还需要有额外的操作。包括但不限于 <strong>代码风格检查、上传文档、编译文件、发不到pypi、执行测试套件</strong>等等。持续集成通常指的是“当您的代码变动的时候、自动运行的东西”，这些工具包括GitHub actions、Azure pipelines等等。工作原理为</p><p>您需要在代码仓库中添加一个文件，描述当前仓库发生任何修改时，应该如何应对。目前为止，最常见的规则是：如果有人提交代码，执行测试套件。当这个事件被触发时，CI 提供方会启动一个（或多个）虚拟机，执行您制定的规则，并且通常会记录下相关的执行结果。您可以进行某些设置，这样当测试套件失败时您能够收到通知或者当测试全部通过时，您的仓库主页会显示一个徽标。</p><h3 id="测试简介（Code-test）"><a href="#测试简介（Code-test）" class="headerlink" title="测试简介（Code test）"></a>测试简介（Code test）</h3><p>测试套件：所有测试的统称</p><p>单元测试：一种微型测试</p><p>集成测试：针对系统的某个部分进行测试来观察是否可以协同工作</p><p>回归测试：特定模式测试、用于保证之前引起问题的bug不会出现</p><p>模拟：使用一个假的实现来替换函数、模块或类型，匹配那些和测试不相关的内容</p><h3 id="Linux文件管理"><a href="#Linux文件管理" class="headerlink" title="Linux文件管理"></a>Linux文件管理</h3><ul><li><p><code>/bin</code> - 基本命令二进制文件</p></li><li><p><code>/sbin</code> - 基本的系统二进制文件，通常是root运行的</p></li><li><p><code>/dev</code> - 设备文件，通常是硬件设备接口文件</p></li><li><p><code>/etc</code> - 主机特定的系统配置文件</p></li><li><p><code>/home</code> - 系统用户的主目录</p></li><li><p><code>/lib</code> - 系统软件通用库</p></li><li><p><code>/opt</code> - 可选的应用软件</p></li><li><p><code>/sys</code> - 包含系统的信息和配置(<a href="https://missing-semester-cn.github.io/2020/course-shell/">第一堂课</a>介绍的)</p></li><li><p><code>/tmp</code> - 临时文件( <code>/var/tmp</code> ) 通常重启时删除</p></li><li><pre><code class="hljs">/usr/</code></pre><p> - 只读的用户数据</p><ul><li><code>/usr/bin</code> - 非必须的命令二进制文件</li><li><code>/usr/sbin</code> - 非必须的系统二进制文件，通常是由root运行的</li><li><code>/usr/local/bin</code> - 用户编译程序的二进制文件</li></ul></li><li><p><code>/var</code> -变量文件 像日志或缓存</p></li></ul><h2 id="1-3-操作系统相关"><a href="#1-3-操作系统相关" class="headerlink" title="1.3 操作系统相关"></a>1.3 操作系统相关</h2><h3 id="什么是操作系统（Operation-system）"><a href="#什么是操作系统（Operation-system）" class="headerlink" title="什么是操作系统（Operation system）"></a>什么是操作系统（Operation system）</h3><p>用于管理计算机硬件和软件资源，对下屏蔽硬件的复杂性、对上屏蔽资源调度的复杂性。所处于的内核（kernel）用用户提供应用（Application）、对下管理各种设备（device）</p><h3 id="操作系统的主要功能"><a href="#操作系统的主要功能" class="headerlink" title="操作系统的主要功能"></a>操作系统的主要功能</h3><ol><li>进程和线程的管理，包括增、删、阻塞、唤醒、通信等等</li><li>存储管理</li><li>文件管理：读写删除等</li><li>设备管理：输入输出设备的请求和释放</li><li>网络管理</li><li>安全管理等等</li></ol><h3 id="用户态和内核态切换到额方式"><a href="#用户态和内核态切换到额方式" class="headerlink" title="用户态和内核态切换到额方式"></a>用户态和内核态切换到额方式</h3><ol><li>系统调用</li><li>中断 interrupt</li><li>异常 exception</li></ol><h3 id="堆和栈原理"><a href="#堆和栈原理" class="headerlink" title="堆和栈原理"></a>堆和栈原理</h3><p>栈：栈是一种后进先出（Last In, First Out，LIFO）的数据结构，它只允许在一端（栈顶）进行数据的添加和删除操作。栈通常用于管理方法调用的执行顺序、局部变量的存储等。应用包括函数调用、表达式求值</p><p>堆：堆是一种动态分配的内存区域，程序在运行时可以从堆中分配或释放内存。不同于栈，堆的内存分配和释放是不连续的，因此它更灵活，但管理成本更高。应用包括动态数据结构、大对象存储</p><h3 id="进程和线程的区别（Process和thread）"><a href="#进程和线程的区别（Process和thread）" class="headerlink" title="进程和线程的区别（Process和thread）"></a>进程和线程的区别（Process和thread）</h3><p>进程是APP、线程是APP中的一个功能。具体来说进程是操作系统进行资源分配和调度的独立单位，是程序执行的实体，每个进程都有自己独立的地址空间、内存、数据站和辅助数据；线程是进程执行的最小单位，可以利用进程中的资源，在同一个进程内多个线程共享其地址空间和资源</p><h3 id="进程之间的通信方式（Inter-process-communicating）"><a href="#进程之间的通信方式（Inter-process-communicating）" class="headerlink" title="进程之间的通信方式（Inter- process communicating）"></a>进程之间的通信方式（Inter- process communicating）</h3><ol><li><strong>管道（Pipes）</strong>，允许一个进程和另一个进程之间的单向数据流。管道可以是匿名的，也可以是具名的（也称为命名管道或FIFO）。</li><li><strong>信号（Signals）</strong>，种用于进程间通信的有限制的方式，允许进程给另一个进程发送简单的消息。信号是一种异步的通信方式，用于处理诸如终止请求（SIGTERM）、中断请求（SIGINT）等事件。</li><li><strong>消息队列（Message queues）</strong>，允许不同的进程读写一个消息队列，这是一种比管道更灵活的通信方式，因为它允许消息的随机访问，不仅仅是FIFO（先进先出）。</li><li><strong>共享内存（Shared memory）</strong>，允许多个进程访问同一块内存空间。这是一种非常高效的通信方式，因为数据不需要在进程间复制，但它需要同步机制来防止并发访问问题。</li><li><strong>信号量（Semaphore）</strong>，主要用于同步多个进程对共享资源的访问，虽然它们本身不传递数据，但信号量可以用来同步数据访问，因此是进程间通信的重要部分。</li><li><strong>套接字（Sockets）</strong>，提供了在不同主机上运行的进程之间通信的能力。套接字可以是基于流的（TCP），也可以是基于数据报的（UDP）。</li><li><strong>文件映射（Memory- mapped files）</strong>，通过将文件或设备映射到内存，实现进程间的共享数据。这种方式允许文件或设备的内容直接加载到进程的地址空间，从而实现快速访问和修改。</li></ol><h3 id="为什么需要线程和多线程？"><a href="#为什么需要线程和多线程？" class="headerlink" title="为什么需要线程和多线程？"></a>为什么需要线程和多线程？</h3><p>进程切换开销大，线程开销小。同时一个进程可以创建多个线程，线程可以并发处理不同的任务，有效的利用多处理器和多核计算机。而进程只能在一个时间内干一件事情</p><h3 id="线程同步方式有哪些？"><a href="#线程同步方式有哪些？" class="headerlink" title="线程同步方式有哪些？"></a>线程同步方式有哪些？</h3><ol><li>**互斥锁(Mutex)**：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 <code>synchronized</code> 关键词和各种 <code>Lock</code> 都是这种机制。</li><li><strong>读写锁（Read-Write Lock）</strong>：允许多个线程同时读取共享资源，但只有一个线程可以对共享资源进行写操作。</li><li>**信号量(Semaphore)**：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。</li><li><strong>屏障（Barrier）</strong>：屏障是一种同步原语，用于等待多个线程到达某个点再一起继续执行。当一个线程到达屏障时，它会停止执行并等待其他线程到达屏障，直到所有线程都到达屏障后，它们才会一起继续执行。比如 Java 中的 <code>CyclicBarrier</code> 是这种机制。</li><li><strong>事件(Event)</strong> :Wait&#x2F;Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作。</li></ol><hr><h3 id="进程有哪些状态？"><a href="#进程有哪些状态？" class="headerlink" title="进程有哪些状态？"></a>进程有哪些状态？</h3><ol><li>创建进程 new</li><li>就绪状态 ready</li><li>运行</li><li>阻塞 或者等待</li><li>结束</li></ol><h3 id="如何实现多线程？"><a href="#如何实现多线程？" class="headerlink" title="如何实现多线程？"></a>如何实现多线程？</h3><h1 id="计算机组成相关"><a href="#计算机组成相关" class="headerlink" title="计算机组成相关"></a>计算机组成相关</h1><h3 id="如何实现高性能"><a href="#如何实现高性能" class="headerlink" title="如何实现高性能"></a>如何实现高性能</h3><ol><li>CDN</li><li>负载均衡</li><li>数据库优化</li><li>消息队列</li></ol><h3 id="如何实现高可用"><a href="#如何实现高可用" class="headerlink" title="如何实现高可用"></a>如何实现高可用</h3><ol><li>冗余设计</li><li>服务限流</li><li>降级</li><li>超时</li><li>性能测试入门</li></ol><h1 id="计算机网络相关"><a href="#计算机网络相关" class="headerlink" title="计算机网络相关"></a>计算机网络相关</h1><h3 id="OSI-七层模型"><a href="#OSI-七层模型" class="headerlink" title="OSI 七层模型"></a>OSI 七层模型</h3><ol><li>物理层：RS232、RS485、RJ45、SDH、DSL、SPI、CAN、802.11，考虑的 是如何传输比特</li><li>数据链路层：Ethernet、PPP、ATM、MAC，考虑如何纠错，实现局域网的实现</li><li>网络层：IP、ARP、ICMP 实现网络之间传输NAT</li><li>传输层：TCP、UDP、SSL、TLS 实现进程和进程之间的通信</li><li>会话层、表示层、应用层：DNS、HTTP、P2P、EMAIL、Telnet、FTP、SSH</li></ol><h3 id="从输入URL到页面展示的过程"><a href="#从输入URL到页面展示的过程" class="headerlink" title="从输入URL到页面展示的过程"></a>从输入URL到页面展示的过程</h3><ol><li>利用DNS查询（浏览器缓存、系统缓存、路由器缓存、DNS缓存）找到IP</li><li>浏览器向服务器发送http请求<ol><li>建立TCP、IP连接</li><li>OPSF 在路由器之间使用</li><li>ARP 转换成为mac</li><li>HTTP 连接之后使用访问网页</li></ol></li><li>服务器处理需求</li></ol><h3 id="HTTP-和-HTTPS的区别（pass）"><a href="#HTTP-和-HTTPS的区别（pass）" class="headerlink" title="HTTP 和 HTTPS的区别（pass）"></a>HTTP 和 HTTPS的区别（pass）</h3><ul><li><strong>端口号</strong>：HTTP 默认是 80，HTTPS 默认是 443。</li><li><strong>URL 前缀</strong>：HTTP 的 URL 前缀是 <code>http://</code>，HTTPS 的 URL 前缀是 <code>https://</code>。</li><li><strong>安全性和资源消耗</strong>：HTTP 协议运行在 TCP 之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS 是运行在 SSL&#x2F;TLS 之上的 HTTP 协议，SSL&#x2F;TLS 运行在 TCP 之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS 高，但是 HTTPS 比 HTTP 耗费更多服务器资源。</li><li><strong>SEO（搜索引擎优化）</strong>：搜索引擎通常会更青睐使用 HTTPS 协议的网站，因为 HTTPS 能够提供更高的安全性和用户隐私保护。使用 HTTPS 协议的网站在搜索结果中可能会被优先显示，从而对 SEO 产生影响。</li></ul><p>关于 HTTP 和 HTTPS 更详细的对比总结，可以看我写的这篇文章：<a href="notion://www.notion.so/cs-basics/network/http-vs-https.html">HTTP vs HTTPS（应用层）</a> 。</p><h3 id="HTTP-1-0-和HTTP1-1-的区别（pass）"><a href="#HTTP-1-0-和HTTP1-1-的区别（pass）" class="headerlink" title="HTTP 1.0 和HTTP1.1 的区别（pass）"></a>HTTP 1.0 和HTTP1.1 的区别（pass）</h3><ul><li><strong>连接方式</strong> : HTTP&#x2F;1.0 为短连接，HTTP&#x2F;1.1 支持长连接。HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接。</li><li><strong>状态响应码</strong> : HTTP&#x2F;1.1 中新加入了大量的状态码，光是错误响应状态码就新增了 24 种。比如说，<code>100 (Continue)</code>——在请求大资源前的预热请求，<code>206 (Partial Content)</code>——范围请求的标识码，<code>409 (Conflict)</code>——请求与当前资源的规定冲突，<code>410 (Gone)</code>——资源已被永久转移，而且没有任何已知的转发地址。</li><li><strong>缓存机制</strong> : 在 HTTP&#x2F;1.0 中主要使用 Header 里的 If-Modified-Since,Expires 来做为缓存判断的标准，HTTP&#x2F;1.1 则引入了更多的缓存控制策略例如 Entity tag，If-Unmodified-Since, If-Match, If-None-Match 等更多可供选择的缓存头来控制缓存策略。</li><li><strong>带宽</strong>：HTTP&#x2F;1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP&#x2F;1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是 206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。</li><li><strong>Host 头（Host Header）处理</strong> :HTTP&#x2F;1.1 引入了 Host 头字段，允许在同一 IP 地址上托管多个域名，从而支持虚拟主机的功能。而 HTTP&#x2F;1.0 没有 Host 头字段，无法实现虚拟主机</li></ul><h3 id="HTTP1-1-和HTTP2的区别"><a href="#HTTP1-1-和HTTP2的区别" class="headerlink" title="HTTP1.1 和HTTP2的区别"></a>HTTP1.1 和HTTP2的区别</h3><ul><li><strong>多路复用（Multiplexing）</strong>：HTTP&#x2F;2.0 在同一连接上可以同时传输多个请求和响应（可以看作是 HTTP&#x2F;1.1 中长链接的升级版本），互不干扰。HTTP&#x2F;1.1 则使用串行方式，每个请求和响应都需要独立的连接，而浏览器为了控制资源会有 6-8 个 TCP 连接都限制。。这使得 HTTP&#x2F;2.0 在处理多个请求时更加高效，减少了网络延迟和提高了性能。</li><li><strong>二进制帧（Binary Frames）</strong>：HTTP&#x2F;2.0 使用二进制帧进行数据传输，而 HTTP&#x2F;1.1 则使用文本格式的报文。二进制帧更加紧凑和高效，减少了传输的数据量和带宽消耗。</li><li><strong>头部压缩（Header Compression）</strong>：HTTP&#x2F;1.1 支持<code>Body</code>压缩，<code>Header</code>不支持压缩。HTTP&#x2F;2.0 支持对<code>Header</code>压缩，使用了专门为<code>Header</code>压缩而设计的 HPACK 算法，减少了网络开销。</li><li><strong>服务器推送（Server Push）</strong>：HTTP&#x2F;2.0 支持服务器推送，可以在客户端请求一个资源时，将其他相关资源一并推送给客户端，从而减少了客户端的请求次数和延迟。而 HTTP&#x2F;1.1 需要客户端自己发送请求来获取相关资源。</li></ul><h3 id="HTTP2和HTTP3区别（Pass）"><a href="#HTTP2和HTTP3区别（Pass）" class="headerlink" title="HTTP2和HTTP3区别（Pass）"></a>HTTP2和HTTP3区别（Pass）</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193915277.png" alt="image-20240429193915277"></p><h3 id="TCP三次握手"><a href="#TCP三次握手" class="headerlink" title="TCP三次握手"></a>TCP三次握手</h3><ol><li>客户端发送syn数据给服务端</li><li>服务端发送 syn ack给客户端</li><li>客户端发送ack给服务端</li></ol><h3 id="TCP四次回收"><a href="#TCP四次回收" class="headerlink" title="TCP四次回收"></a>TCP四次回收</h3><h1 id="数据结构相关"><a href="#数据结构相关" class="headerlink" title="数据结构相关"></a>数据结构相关</h1><h3 id="面向象中的继承、多态、封装"><a href="#面向象中的继承、多态、封装" class="headerlink" title="面向象中的继承、多态、封装"></a>面向象中的继承、多态、封装</h3><p>封装（Encapsulation）：封装是面向对象编程的基本特征之一，它隐藏了对象的内部状态，并且只通过对象的方法来访问和修改。这样可以使代码更加安全和易于管理。</p><p>继承（Inheritance）：继承是从已有的类派生出新的类，新的类继承了原有类的属性和方法，并且可以添加新的属性和方法。继承可以提高代码的重用性，并且使代码更加清晰和易于维护。</p><p>多态（Polymorphism）：多态是指一个接口可以有多种实现方式，或者一个对象可以表现出多种形态。多态可以提高代码的灵活性和可扩展性，并且使代码更加易于理解和维护。</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>计算机组成原理</tag>
      
      <tag>面试经验</tag>
      
      <tag>程序设计基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北京西二旗租房攻略</title>
    <link href="/posts/564f7d50.html"/>
    <url>/posts/564f7d50.html</url>
    
    <content type="html"><![CDATA[<h2 id="0x01-租房目标的确定"><a href="#0x01-租房目标的确定" class="headerlink" title="0x01 租房目标的确定"></a>0x01 租房目标的确定</h2><p>租房是希望在一定的成本范围内找到满足自己最大效用的方式。可以从货币成本、个人喜好、社会经济因素来观察。每个人的效用具有不同个人差异性，但是租房经验往往是可以互相参考的，这也是互联网平台传播的意义所在。分享你所看到的，来铸造集体的记忆。</p><h2 id="0x02-租房平台"><a href="#0x02-租房平台" class="headerlink" title="0x02 租房平台"></a>0x02 租房平台</h2><p>NOTE：租房平台越规范，信息获取越容易，但是价格也会更高</p><p>租房平台按照熵增的顺序可以分为：</p><ol><li>基于互联网和算法：自如、安租客、贝壳链家等</li><li>基于人工操作运转：房产中介、托管公司、我爱我家</li><li>有一定租房聚集力：闲鱼、豆瓣等</li><li>依靠主观搜索：小红书、闲鱼等</li></ol><p>往往同样的房子自上而下价格越来越低；但是看到房子的影子成本会越来越高。这里的影子成本可以看作房子的维修难度、隐藏不确定性、暗病等等因素。</p><h2 id="0x03-租房考虑因素"><a href="#0x03-租房考虑因素" class="headerlink" title="0x03 租房考虑因素"></a>0x03 租房考虑因素</h2><p>首先需要考虑经济因素，房子的租金主要包括：</p><ol><li>房间空间属性：大小、地理位置等等</li><li>租房的时间属性：短租 or 长租</li></ol><p>其次需要考虑个人因素</p><ol><li>是否有隔音</li><li>是否有独卫</li><li>房间是否足够大</li><li>电梯 or 楼梯</li><li>是否有供暖，集中供暖 or 自采暖</li></ol><p>之后考虑社会经济因素</p><ol><li>到商圈的距离</li><li>到地铁站的距离</li><li>到自己目标点位的交通便捷性，包括步行 or 电车 or 公交 or 地铁等</li><li>小区的破旧程度，有无电梯、有无保安</li><li>小区的属性和基本设施，高档小区、拆迁房</li><li>房间软配问题，水电老化</li></ol><p>额外因素</p><ol><li>合租的舍友</li><li>中介、闲聊的靠谱程度</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193731853.png" alt="image-20240429193731853"></p><h2 id="0x04-租房技巧"><a href="#0x04-租房技巧" class="headerlink" title="0x04 租房技巧"></a>0x04 租房技巧</h2><p>首先根据地图确定周边点，以及周边店地铁沿线服务的备选小区</p><p>在规范平台中查看这些小区的大致属性，得到筛选之后的备选小区 2</p><p>在社交媒体中寻找备选小区 2 的额外信息，这些是平台、中介无法提供给你的真实评价</p><p>最后按照自己的成本，选择平台 or 中介的房子，或者是花费自己的时间成本去 闲鱼、豆瓣、小红书等平台寻求转租的价格较低的房子。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193741663.png" alt="image-20240429193741663"></p><p>PS：</p><p>最后的最后，还是需要自己体验过才知道房子是否合适自己！</p><p>强烈推荐经济允许的情况下，优先选择独卫而不是通勤距离</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>租房</tag>
      
      <tag>西二旗</tag>
      
      <tag>实习生</tag>
      
      <tag>短租</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Meta quest3 使用折腾指南</title>
    <link href="/posts/2d990b59.html"/>
    <url>/posts/2d990b59.html</url>
    
    <content type="html"><![CDATA[<p>Quest3作为一台头戴眼镜内设备，针对现在依赖手机、电脑的工作方式，天然具有“反链接“的特点，同时在国内难以魔法上网和校园网奇葩的路由器的情况，如何顺利的享受虚拟世界的乐趣是一件非常困难的工作。因此在本文总结自己踩过的坑作为记录。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>对于一些教程用一些未见过的语言来描述 Quest3 的使用过程，实际上是对小白自信心的一次打击，此时针对拿到手，需要了解的事情做一个总结，也就是“拿到 Quest3 我可能需要做哪些工作”：</p><ol><li>机内游戏下载<ol><li>目的：下载一体机内部使用的app、游戏等</li><li>方法：1️⃣ 注册官方账号，在官方商店中充值并购买 2️⃣ 破解软件下载安装</li><li>问题：1️⃣ 此时面临魔法、账号、购买问题 2️⃣ 仅支持PC、文件传输、安装复杂等问题</li></ol></li><li>串流<ol><li>目的：通过串流来查看电脑屏幕，实现办公、多媒体观看、游戏等目的</li><li>方法：通信app上可以分为官方自带的quest link， steam VR 或者virtual desktop。通信方式分为有线串流和无线串流</li><li>问题：有线对于线材要求较高，无线对于路由器要求较高</li></ol></li><li>视频观看<ol><li>目的：在尽可能不折腾的情况下观看视频</li><li>方法：1️⃣ 传输到机器内部本地观看 2️⃣使用局域网共享的方式来查看高码率视频</li><li>问题：无线传输过程对于路由器要求较高</li></ol></li><li>头戴配重<ol><li>目的：由于quest3本体仅作为一个简陋的设备，无法满足使用者对于舒适度的要求</li><li>方法：可以通过头戴、配重、面罩等方面来考虑</li><li>问题：市场上东西比较多</li></ol></li></ol><p>在过程平台硬件注释：</p><ol><li>window主机</li><li>一根type-typec的线</li><li>校园网（局域网共享困难）</li></ol><p>购买的服务包括：</p><table><thead><tr><th>Virtual desktop</th><th>画质好的串流软件</th><th>$16.99</th></tr></thead><tbody><tr><td>Steam VR</td><td>免费串流软件</td><td>0</td></tr><tr><td>Quest link</td><td>免费串流软件</td><td>0</td></tr><tr><td>半衰期  Alyx</td><td>PC端的VR游戏</td><td>￥79</td></tr><tr><td>破解网站</td><td>一体机游戏</td><td>0</td></tr><tr><td>Clash</td><td>不解释</td><td>不解释</td></tr><tr><td>Skybox</td><td>一体机内部播放器</td><td>$ 7.99</td></tr><tr><td>Air screen</td><td>电脑端播放服务器</td><td>0</td></tr></tbody></table><h2 id="第一部分：魔法、账号、充值"><a href="#第一部分：魔法、账号、充值" class="headerlink" title="第一部分：魔法、账号、充值"></a>第一部分：魔法、账号、充值</h2><p>《如何在window下使用clash》略过</p><p>如果使用quest link进行链接，首先需要在官方上下载对应的PC软件（免费）</p><p><a href="https://www.meta.com/zh-hk/help/quest/articles/headsets-and-accessories/oculus-rift-s/install-app-for-link/">為 Meta Quest Link 安裝 Oculus 電腦版應用程式</a></p><p>但是在这个过程中面临 Facebook 账号浏览器认证失败的问题，这里需要修改host加速：</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs jsx"><span class="hljs-number">20.205</span><span class="hljs-number">.243</span><span class="hljs-number">.166</span> github.<span class="hljs-property">com</span><br><span class="hljs-number">157.240</span><span class="hljs-number">.11</span><span class="hljs-number">.49</span> graph.<span class="hljs-property">oculus</span>.<span class="hljs-property">com</span><br><span class="hljs-number">157.240</span><span class="hljs-number">.11</span><span class="hljs-number">.49</span> www2.<span class="hljs-property">oculus</span>.<span class="hljs-property">com</span><br><span class="hljs-number">157.240</span><span class="hljs-number">.8</span><span class="hljs-number">.49</span> scontent.<span class="hljs-property">oculuscdn</span>.<span class="hljs-property">com</span><br><span class="hljs-number">157.240</span><span class="hljs-number">.8</span><span class="hljs-number">.49</span> securecdn.<span class="hljs-property">oculus</span>.<span class="hljs-property">com</span><br></code></pre></td></tr></table></figure><p>同时注意，Meta的账号是分为 Facebook+Instagram+Meta；而 meta quest 对应的是 meta 账号。</p><p>之后的问题是如何讲window的代理分享给quest3，这里由于在校园网环境，未直接使用 clash 的allow for lan 来进行，而是使用 “window代理热点 + clash tunnel共享“的方式来分享代理：</p><ol><li>打开clash的tunnel功能</li><li>代开window的代理热点功能，此时 WLAN为共享、WLAN2为链接校园网</li><li>在 控制面板-网络与连接-适配器管理 中，点击clash tunnel共享，选择WLAN来共享代理</li><li>在 quest3中链接WINDOW热点，成功完成魔法。</li></ol><p>之后的账号和充值可以在PC端、手机端、Quest3端中完成</p><p>对于破解软件下载的通常方式，在破解助手中下载软件安装包，之后利用文件传输工具传送到quest3本地，完成安装。相关免费教程可以查看这类游戏网站</p><p><a href="https://www.vrmoo.net/tools">Oculus Quest教程 – VR魔趣网</a></p><h2 id="第二部分：串流"><a href="#第二部分：串流" class="headerlink" title="第二部分：串流"></a>第二部分：串流</h2><p>第一部分代理热点安装好之后，串流不成问题</p><h2 id="第三部分：视频观看"><a href="#第三部分：视频观看" class="headerlink" title="第三部分：视频观看"></a>第三部分：视频观看</h2><ol><li>使用SMB协议共享window文件夹</li><li>推荐购买 skybox，配合window中免费的air screen中完成文件夹共享</li></ol><h2 id="第四部分：配件购买"><a href="#第四部分：配件购买" class="headerlink" title="第四部分：配件购买"></a>第四部分：配件购买</h2><ol><li>quest3 头戴</li><li>quest3 开放式面罩</li><li>头戴充电宝</li><li>冰丝海绵替换装</li><li>串流线</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193602735.png" alt="image-20240429193602735"></p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>quest3</tag>
      
      <tag>AR 眼镜</tag>
      
      <tag>科学上网</tag>
      
      <tag>新奇硬件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 绘图完整教程</title>
    <link href="/posts/7f325577.html"/>
    <url>/posts/7f325577.html</url>
    
    <content type="html"><![CDATA[<h2 id="0x00-Basic-setting"><a href="#0x00-Basic-setting" class="headerlink" title="0x00 Basic setting"></a>0x00 Basic setting</h2><blockquote><p>What is matplotlib? Matplotlib is a comprehensive library for creating static, animated, interactive visualization in Python. We can as follows:</p></blockquote><ol><li>创建高质量的图片</li><li>创建可以交互的、放大、拖动、更新的图片</li><li>定制化风格和尺寸</li><li>导出为不同的格式</li><li>在Jupyter notebook或其他应用中使用</li><li>丰富的第三方库</li></ol><blockquote></blockquote><p>在使用中图片可以按照A4比例来进行设置，同时设置对应的字体和颜色</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">mm = 1/25.4<br><span class="hljs-comment"># plt.figure(figsize=(10*mm,20*mm)</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>]= <span class="hljs-string">&quot;Times New Roman&quot;</span><br>mpl.rcParams[<span class="hljs-string">&#x27;xtick.labelsize&#x27;</span>] = 10<br>mpl.rcParams[<span class="hljs-string">&#x27;ytick.labelsize&#x27;</span>] = 10<br>plt.rcParams[<span class="hljs-string">&#x27;mathtext.fontset&#x27;</span>]=<span class="hljs-string">&#x27;cm&#x27;</span><br>plt.style.use(<span class="hljs-string">&#x27;tableau-colorblind10&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="0x01-Intro-to-Matplotlib-pyplot"><a href="#0x01-Intro-to-Matplotlib-pyplot" class="headerlink" title="0x01 Intro to Matplotlib pyplot"></a>0x01 Intro to Matplotlib pyplot</h2><h3 id="1-1-两种绘图方式"><a href="#1-1-两种绘图方式" class="headerlink" title="1.1 两种绘图方式"></a>1.1 两种绘图方式</h3><ol><li>面向对象（Objetc- oriented style）的绘图方式将图片的fig和axis区分开来，使用plt.subplots( )来创建figure对象</li><li>函数编程，直接使用封装好的plt.figure() plt.plot()来创建figure和绘图对象</li></ol><h3 id="1-2-图片对象包括哪些属性"><a href="#1-2-图片对象包括哪些属性" class="headerlink" title="1.2 图片对象包括哪些属性"></a>1.2 图片对象包括哪些属性</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429192904205.png" alt="image-20240429192904205"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs bash">fig,ax = plt.subplots(figsize=(150*mm,100*mm))<br><br>ax.set_title(<span class="hljs-string">&#x27;The title stays&#x27;</span>,fontsize=10)<br>ax.set_xlabel(<span class="hljs-string">&#x27;xlabel stays&#x27;</span>,fontsize=10)<br>ax.set_ylabel(<span class="hljs-string">&#x27;ylabel stays&#x27;</span>,fontsize=10)<br>ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(<span class="hljs-string">&#x27;%.1f&#x27;</span>))<br>ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(<span class="hljs-string">&#x27;%.1f&#x27;</span>))<br>ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))<br>ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))<br>ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.5))<br>ax.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.5))<br>ax.set_xscale(<span class="hljs-string">&#x27;linear&#x27;</span>)<br>ax.set_yscale(<span class="hljs-string">&#x27;linear&#x27;</span>)<br>ax.set_xticks(np.linspace(0,10,11))<br>ax.set_yticks(np.linspace(0,10,11))<br><br>ax.plot(np.linspace(0,10,100),np.linspace(0,10,100),label=<span class="hljs-string">&#x27;1:1&#x27;</span>,color=<span class="hljs-string">&#x27;k&#x27;</span>,linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br>ax.scatter(np.linspace(0,10,20),np.sin(np.linspace(0,10,20)+1),label=<span class="hljs-string">&#x27;1:2&#x27;</span>,color=<span class="hljs-string">&#x27;g&#x27;</span>,marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>ax.legend(loc=<span class="hljs-string">&#x27;upper left&#x27;</span>,fontsize=10,frameon=False)<br>ax.grid(<span class="hljs-string">&#x27;major&#x27;</span>,color=<span class="hljs-string">&#x27;k&#x27;</span>,linestyle=<span class="hljs-string">&#x27;--&#x27;</span>,linewidth=0.1)<br>ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;None&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;None&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((&#x27;data&#x27;,<span class="hljs-number">1</span>)) <span class="hljs-comment">#将x轴移到y=0处</span><br>ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((&#x27;data&#x27;,<span class="hljs-number">1</span>)) <span class="hljs-comment">#将x轴移到x=0处</span><br>ax.text(6,9,<span class="hljs-string">&quot;function:y=x&quot;</span>,size=15,color=<span class="hljs-string">&#x27;r&#x27;</span>,style=<span class="hljs-string">&#x27;italic&#x27;</span>,weight=<span class="hljs-string">&#x27;light&#x27;</span>,bbox=dict(facecolor=<span class="hljs-string">&#x27;w&#x27;</span>,alpha=0.5))<br><span class="hljs-keyword">for</span> coord <span class="hljs-keyword">in</span> np.linspace(3,7,10):<br>    ax.text(coord,coord+1,<span class="hljs-string">&#x27;%.0f&#x27;</span>%coord,ha=<span class="hljs-string">&#x27;center&#x27;</span>,va=<span class="hljs-string">&#x27;bottom&#x27;</span>)<br><br>ax.annotate(<span class="hljs-string">&#x27;annotate&#x27;</span>,xy=(9,9),xytext=(7,6),arrowprops=dict(arrowstyle=<span class="hljs-string">&#x27;-&gt;&#x27;</span>,connectionstyle=<span class="hljs-string">&#x27;arc3,rad=0.2&#x27;</span>))<br><br>ax2 = ax.twinx()<br>ax2.set_ylabel(<span class="hljs-string">&#x27;ylabel stays&#x27;</span>,fontsize=10)<br>ax2.set_yscale(<span class="hljs-string">&#x27;linear&#x27;</span>)<br>ax2.set_yticks(np.linspace(0,10,11))<br>ax2.set_ylim(0,10)<br>X,Y = np.meshgrid(np.linspace(0,10,11),np.linspace(0,10,11))<br>Z = np.sin(X)+np.cos(Y)<br>pc = ax2.pcolormesh(X,Y,Z,cmap=<span class="hljs-string">&#x27;RdBu_r&#x27;</span>,vmin=-1, vmax=1,alpha=0.3)<br>fig.colorbar(pc,ax=ax2,shrink=0.8,pad=0.1)<br>ax2.set_ylabel(<span class="hljs-string">&#x27;colorbar&#x27;</span>,fontsize=10)<br></code></pre></td></tr></table></figure><h3 id="1-3-Pyplot-设置"><a href="#1-3-Pyplot-设置" class="headerlink" title="1.3 Pyplot 设置"></a>1.3 Pyplot 设置</h3><p>上面的图形选项很多，从学习的角度可以全而广的了解，从实际使用的角度可能只会用到其中的几个。</p><p>color：pink。lightblue；</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">ax.set_prop_cycle<span class="hljs-punctuation">(</span>color<span class="hljs-punctuation">=</span>[&#x27;#1f77b4&#x27;<span class="hljs-punctuation">,</span> &#x27;#aec7e8&#x27;<span class="hljs-punctuation">,</span> &#x27;#ff7f0e&#x27;<span class="hljs-punctuation">,</span> &#x27;#ffbb78&#x27;<span class="hljs-punctuation">,</span> &#x27;#2ca02c&#x27;<span class="hljs-punctuation">,</span> &#x27;#98df8a&#x27;<span class="hljs-punctuation">,</span>&#x27;#d<span class="hljs-number">6272</span>8&#x27;<span class="hljs-punctuation">,</span> &#x27;#ff<span class="hljs-number">9896</span>&#x27;<span class="hljs-punctuation">,</span> &#x27;#<span class="hljs-number">9467</span>bd&#x27;<span class="hljs-punctuation">,</span> &#x27;#c5b0d5&#x27;<span class="hljs-punctuation">,</span> &#x27;#8c564b&#x27;<span class="hljs-punctuation">,</span> &#x27;#c49c94&#x27;<span class="hljs-punctuation">,</span>&#x27;#e377c2&#x27;<span class="hljs-punctuation">,</span> &#x27;#f7b6d2&#x27;<span class="hljs-punctuation">,</span> &#x27;#7f7f7f&#x27;<span class="hljs-punctuation">,</span> &#x27;#c7c7c7&#x27;<span class="hljs-punctuation">,</span> &#x27;#bcbd22&#x27;<span class="hljs-punctuation">,</span> &#x27;#dbdb8d&#x27;<span class="hljs-punctuation">,</span>&#x27;#17becf&#x27;<span class="hljs-punctuation">,</span> &#x27;#9edae5&#x27;]<span class="hljs-punctuation">)</span><br></code></pre></td></tr></table></figure><p>colormap：RdBu</p><p>linestyle：<a href="https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html#">https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html#</a></p><p>marker：<a href="https://matplotlib.org/stable/gallery/lines_bars_and_markers/marker_reference.html">https://matplotlib.org/stable/gallery/lines_bars_and_markers/marker_reference.html</a></p><p>等等，除此之外还需要设置针对不同的图增加不同的表达方式。这个就需要从图的种类开始学习。</p><h3 id="1-4-Image展示设置"><a href="#1-4-Image展示设置" class="headerlink" title="1.4 Image展示设置"></a>1.4 Image展示设置</h3><p>可以通过PIL的Image.open实现图像转换为numpy.array，之后使用imshow方式来可视化图片。同时可以图像进行通道遮盖、色彩映射、添加colorbar等展示，并计算色彩范围，重塑图片大小、插值等操作</p><h2 id="0x02-Figure-from-the-Matplotlib"><a href="#0x02-Figure-from-the-Matplotlib" class="headerlink" title="0x02 Figure from the Matplotlib"></a>0x02 Figure from the Matplotlib</h2><p>选择官方<a href="https://matplotlib.org/stable/gallery/index.html">https://matplotlib.org/stable/gallery/index.html</a> 中一些比较有意思的作为参考</p><h3 id="2-1-直线（Lines）、条形图（Bar）、标记（Markers）"><a href="#2-1-直线（Lines）、条形图（Bar）、标记（Markers）" class="headerlink" title="2.1 直线（Lines）、条形图（Bar）、标记（Markers）"></a>2.1 直线（Lines）、条形图（Bar）、标记（Markers）</h3><ul><li>ax.bar_label 给bar添加标签</li><li>ax.bar(…,xerr&#x3D;error,…) 添加误差线</li><li>通过添加upperlimits来设置误差线的箭头</li><li>fill_between 填充，可以设置where参数</li><li>dash style的设置。set_dash间隔</li><li>steps 和 stair设计</li></ul><h3 id=""><a href="#" class="headerlink" title=""></a><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429192918364.png" alt="image-20240429192918364"></h3><h3 id="2-2-图片（Images），轮廓（Contours）和平面（Filed"><a href="#2-2-图片（Images），轮廓（Contours）和平面（Filed" class="headerlink" title="2.2 图片（Images），轮廓（Contours）和平面（Filed)"></a><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429192938968.png" alt="image-20240429192938968">2.2 图片（Images），轮廓（Contours）和平面（Filed)</h3><ul><li>添加水印 fig.figimage</li><li>绘制轮廓</li><li>matshow显示二维数组</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429192950571.png" alt="image-20240429192950571"></p><h3 id="2-3-子图（Subplot），坐标轴（Axes）和图（Figures）"><a href="#2-3-子图（Subplot），坐标轴（Axes）和图（Figures）" class="headerlink" title="2.3 子图（Subplot），坐标轴（Axes）和图（Figures）"></a>2.3 子图（Subplot），坐标轴（Axes）和图（Figures）</h3><ul><li>在axes中增加add_axes()</li><li>创建多张axes之间的链接</li><li>放大效果，axins插入一个框，同时设置set_xlim和set_ylim</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193000016.png" alt="image-20240429193000016"></p><h3 id="2-4-统计（Statistics）"><a href="#2-4-统计（Statistics）" class="headerlink" title="2.4 统计（Statistics）"></a>2.4 统计（Statistics）</h3><ul><li>箱图boxplot，可以选择vert垂直标记，path_artist是否填充颜色，labels</li><li>箱型图hist，可以返回cumulative的数据</li></ul><h3 id="2-5-饼图（Pie）和极坐标图（Polar-charts"><a href="#2-5-饼图（Pie）和极坐标图（Polar-charts" class="headerlink" title="2.5 饼图（Pie）和极坐标图（Polar charts)"></a><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193009837.png" alt="image-20240429193009837">2.5 饼图（Pie）和极坐标图（Polar charts)</h3><ul><li>有pie绘制图，用bar绘制堆叠图，之后使用connectionPatch来进行两者的链接</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193021170.png" alt="image-20240429193021170"></p><h3 id="2-6-文本（Text）、标签（Label）和标记（Annotation）"><a href="#2-6-文本（Text）、标签（Label）和标记（Annotation）" class="headerlink" title="2.6 文本（Text）、标签（Label）和标记（Annotation）"></a>2.6 文本（Text）、标签（Label）和标记（Annotation）</h3><ul><li>不玩花的</li></ul><h3 id="2-7-pyplot-module"><a href="#2-7-pyplot-module" class="headerlink" title="2.7 pyplot module"></a>2.7 pyplot module</h3><ul><li>辅助线 axhlines，axvhline</li></ul><h3 id="2-8-颜色（Color）"><a href="#2-8-颜色（Color）" class="headerlink" title="2.8 颜色（Color）"></a>2.8 颜色（Color）</h3><ul><li>colormap，<a href="https://matplotlib.org/stable/gallery/color/colormap_reference.html#sphx-glr-gallery-color-colormap-reference-py">https://matplotlib.org/stable/gallery/color/colormap_reference.html#sphx-glr-gallery-color-colormap-reference-py</a></li><li>colors，<a href="https://matplotlib.org/stable/gallery/color/named_colors.html#sphx-glr-gallery-color-named-colors-py">https://matplotlib.org/stable/gallery/color/named_colors.html#sphx-glr-gallery-color-named-colors-py</a></li></ul><h3 id="2-9-形状和（Shapes）和集合（Collections）"><a href="#2-9-形状和（Shapes）和集合（Collections）" class="headerlink" title="2.9 形状和（Shapes）和集合（Collections）"></a>2.9 形状和（Shapes）和集合（Collections）</h3><ul><li>不玩花的</li><li>通常在matplotlib patch中的形状或者路径<a href="https://blog.csdn.net/qq_27825451/article/details/82967904">https://blog.csdn.net/qq_27825451/article/details/82967904</a></li></ul><h3 id="2-10-风格指南（Style-sheet）"><a href="#2-10-风格指南（Style-sheet）" class="headerlink" title="2.10 风格指南（Style sheet）"></a>2.10 风格指南（Style sheet）</h3><ul><li>集合 <a href="https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py">https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py</a></li><li>选择多了很难呀</li></ul><h3 id="2-11-坐标轴网格"><a href="#2-11-坐标轴网格" class="headerlink" title="2.11 坐标轴网格"></a>2.11 坐标轴网格</h3><ul><li>略</li></ul><h3 id="2-12-坐标轴设计"><a href="#2-12-坐标轴设计" class="headerlink" title="2.12 坐标轴设计"></a>2.12 坐标轴设计</h3><ul><li>略</li></ul><h3 id="2-13-特殊例子-Show-case"><a href="#2-13-特殊例子-Show-case" class="headerlink" title="2.13 特殊例子 Show case"></a>2.13 特殊例子 Show case</h3><ul><li>xkcd非常好玩</li><li>好看的颜色</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193032886.png" alt="image-20240429193032886"></p><h3 id="2-14-动画-Animation"><a href="#2-14-动画-Animation" class="headerlink" title="2.14 动画 Animation"></a>2.14 动画 Animation</h3><ul><li>调用animation.FuncAnimation就可以实现动图</li><li>不玩花的</li></ul><h3 id="2-15-事件句柄-（Event-handling）"><a href="#2-15-事件句柄-（Event-handling）" class="headerlink" title="2.15 事件句柄 （Event handling）"></a>2.15 事件句柄 （Event handling）</h3><ul><li>动态交互，不玩花的</li></ul><h3 id="2-16-混杂的（Miscellaneous）"><a href="#2-16-混杂的（Miscellaneous）" class="headerlink" title="2.16 混杂的（Miscellaneous）"></a>2.16 混杂的（Miscellaneous）</h3><ul><li>给图添加table，可以使用plt.table对象</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193045568.png" alt="image-20240429193045568"></p><h3 id="2-17-3D绘图（3D-plotting）"><a href="#2-17-3D绘图（3D-plotting）" class="headerlink" title="2.17 3D绘图（3D plotting）"></a>2.17 3D绘图（3D plotting）</h3><h3 id="2-18-缩放（Scales）"><a href="#2-18-缩放（Scales）" class="headerlink" title="2.18 缩放（Scales）"></a>2.18 缩放（Scales）</h3><h3 id="2-19-特殊绘图（Specialty-plots）"><a href="#2-19-特殊绘图（Specialty-plots）" class="headerlink" title="2.19 特殊绘图（Specialty plots）"></a>2.19 特殊绘图（Specialty plots）</h3><h3 id="2-20-坐标框（Splines）"><a href="#2-20-坐标框（Splines）" class="headerlink" title="2.20 坐标框（Splines）"></a>2.20 坐标框（Splines）</h3><h3 id="2-21-标签（Ticks）"><a href="#2-21-标签（Ticks）" class="headerlink" title="2.21 标签（Ticks）"></a>2.21 标签（Ticks）</h3><ul><li>坐标</li></ul><p><a href="https://matplotlib.org/stable/gallery/ticks/tick-formatters.html#sphx-glr-gallery-ticks-tick-formatters-py">https://matplotlib.org/stable/gallery/ticks/tick-formatters.html#sphx-glr-gallery-ticks-tick-formatters-py</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193055908.png" alt="image-20240429193055908"></p><h3 id="2-22-单位（Units）"><a href="#2-22-单位（Units）" class="headerlink" title="2.22 单位（Units）"></a>2.22 单位（Units）</h3><ul><li>我超！可以从basic_unit中导入cm</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193104816.png" alt="image-20240429193104816"></p><h3 id="2-23-嵌入（Embedding-into-GUI）"><a href="#2-23-嵌入（Embedding-into-GUI）" class="headerlink" title="2.23 嵌入（Embedding into GUI）"></a>2.23 嵌入（Embedding into GUI）</h3><ul><li>略</li></ul><h3 id="2-24-组件（Widgets）"><a href="#2-24-组件（Widgets）" class="headerlink" title="2.24 组件（Widgets）"></a>2.24 组件（Widgets）</h3><ul><li>指针 cursor添加一些操作，tracking position</li><li>按钮 button</li><li>确认 check</li><li>选择 selector</li><li>选择器 menu</li><li>鼠标 mouse</li><li>区间选择 rangeslider</li><li></li></ul><p>似乎没这个必要哈哈哈</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20240429193114203.png" alt="image-20240429193114203"></p><h3 id="2-25-用户例子（Userdemo）"><a href="#2-25-用户例子（Userdemo）" class="headerlink" title="2.25 用户例子（Userdemo）"></a>2.25 用户例子（Userdemo）</h3><p>- </p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>文档阅读</tag>
      
      <tag>Python 可视化</tag>
      
      <tag>绘图教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GEATPY 文档阅读 ｜ 面向过程解决 TSP 问题</title>
    <link href="/posts/1f5ef6b4.html"/>
    <url>/posts/1f5ef6b4.html</url>
    
    <content type="html"><![CDATA[<p>各种启发式方法很复杂，当时一般不需要自己手动实现，python中找到GeatPy是个还不错的包，geatpy 是 Python 中的一个进化算法库，专门用于解决复杂的优化问题。它基于遗传算法和进化策略，提供了一个高效、灵活的工具集，用于处理各种优化问题。本文以TSP问题为基础，介绍面向过程，这种最基本也是最直观的想法来介绍库中的基本API。</p><h3 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h3><p>TSP问题需要从起始点出发，经过每个点一次，最后要返回原点。要求整个过程经历的路径最短。重点是考虑每个点之间的顺序。</p><h3 id="二、问题建模"><a href="#二、问题建模" class="headerlink" title="二、问题建模"></a>二、问题建模</h3><h3 id="2-1-启发式建模"><a href="#2-1-启发式建模" class="headerlink" title="2.1 启发式建模"></a>2.1 启发式建模</h3><p>我们知道我们输出的结果必然是1xN的向量，那我们的决策变量的大小为N，求解空间为NxN，在这个空间中启发式的生成决策顺序，利用目标函数来决策生成的优劣来寻找最优解。</p><h3 id="2-2-整数规划建模"><a href="#2-2-整数规划建模" class="headerlink" title="2.2 整数规划建模"></a>2.2 整数规划建模</h3><p>这个领域有一些标准的整数规划建模方法,具体这里不在展开。重要是避免出现子环图的问题</p><h3 id="三、启发式方法-GeatPy"><a href="#三、启发式方法-GeatPy" class="headerlink" title="三、启发式方法&#x2F;GeatPy"></a>三、启发式方法&#x2F;GeatPy</h3><p>按照<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="http://geatpy.com/index.php/2019/07/28/第一章：概述/">[1]</span></a></sup>中介绍的方法对启发式算法的过程进行简介。个人总结启发式算法的思想：</p><blockquote><p> 个人愚见 对于NP-Hard问题，既然我们不能自下而上的求解它、但是我们可以快速证明是否为可行解（Feasible- solution）。那我们可以随机生成初始解（1个或者多个），然后计算解对于问题的优劣程度（适应度），之后再通过超现实方式（蚁群、模拟退化、进化算法….)来对解进行重组和重生成，最终收敛得到最终的较优解</p></blockquote><p>所以在进化算法中往往会存在一些基本的概念，包括：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs css">graph<br><span class="hljs-selector-tag">B</span><span class="hljs-selector-attr">[开始]</span><br><span class="hljs-selector-tag">G</span><span class="hljs-selector-attr">[初始种群]</span><br>C<span class="hljs-selector-attr">[计算种群中的适应度]</span><br>E<span class="hljs-selector-attr">[进化操作，选择-重组-变异]</span><br>N<span class="hljs-selector-attr">[下一代种群]</span><br><span class="hljs-attribute">Y</span>&#123;终止条件&#125;<br>E2<span class="hljs-selector-attr">[结束]</span><br><span class="hljs-selector-tag">B</span>--&gt;<span class="hljs-selector-tag">G</span>--&gt;C--&gt;E--&gt;N--&gt;<span class="hljs-attribute">Y</span><br><span class="hljs-attribute">Y</span>--&#123;no&#125;--&gt;C<br><span class="hljs-attribute">Y</span>--&gt;E2<br></code></pre></td></tr></table></figure><p>启发式算法相比较传统的搜索和优化算法的优势在于：</p><ol><li>具有天然的并行性</li><li>进化算法使用的是概率转换规则，而不是非确定转化规则</li><li>进化算法使用的是目标函数和适应度条件即可影响搜索方向</li></ol><p>其中重要的地方在于</p><ol><li>针对特定问题的决策变量确定，也就是编码</li><li>适应度函数的确定</li><li>选择 selection、重组Reorganization和变异Variation算子</li></ol><h3 id="3-1-变量编码-2-Encoding"><a href="#3-1-变量编码-2-Encoding" class="headerlink" title="3.1 变量编码[2] Encoding"></a>3.1 变量编码<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="http://geatpy.com/index.php/2019/07/28/第二章：编码/ ↩">[2]</span></a></sup> Encoding</h3><p>和传统的搜索问题的方式并不一样，启发式算法的搜索问题是针对求解空间的，那么需要将原始问题转换成为一个$f(x)$, 其中$x$的值域辨识搜索空间，也就是编码空间（有点绕口），编码过程中需要的基本原则</p><ol><li>完备性：问题所有可能的解可以fully map到编码空间</li><li>可靠性：编码空间需要对应一个潜在解</li><li>非冗余性：一一对应</li></ol><p>编码的方式对启发式搜索过程十分重要，常见的方式包括：</p><ol><li>一维或二维度的数据列</li><li>二进制数据列的字符串</li><li>简单映射编码</li><li>多重映射编码</li><li>排列编码</li><li>树编码等等</li></ol><h3 id="3-2-适应度编码-3"><a href="#3-2-适应度编码-3" class="headerlink" title="3.2 适应度编码[3]"></a>3.2 适应度编码<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="http://geatpy.com/index.php/2019/07/28/第三章：适应度/">[3]</span></a></sup></h3><p>适应度表示的是生成解和问题之间的匹配程度，它的计算是使用遗传算法的关键要素，制定了后续选择、变异和重组的搜索方向，直接关系到搜索效率和最终求解的质量。</p><p>适应度的常见来源使用规划模型的目标函数作为适应度，其需要遵循的规则通常有：</p><ol><li>遵循单调原则</li><li>适当的变换来影响收敛速度。比如线性变换、指数变换、幂指数变换、截断处理、罚函数、基于等级的适应度分配计算。</li></ol><h3 id="3-3-选择-Selection-4"><a href="#3-3-选择-Selection-4" class="headerlink" title="3.3 选择 Selection[4]"></a>3.3 选择 Selection<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="http://geatpy.com/index.php/2019/07/28/第四章：选择/">[4]</span></a></sup></h3><p>选择分为两个阶段，第一个阶段是筛选现有解来生成下一个过程的解；第二个阶段是生成的下一个过程的解再进过一次“环境选择”</p><p>第一阶段，这里是选择参与进化操作的解。为了保证“探索-利用”的均衡。一部份是适应度较高的解、另一种是随机筛选的个体。没有被选中的解不会进入变异、重组的进化操作中。</p><p>第二阶段，个体经过进化操作之后子代保留到下一代的过程。常见的方案包括全局重插入（Global reinsertion）和本地重插入（Local reinsertion）</p><p><strong>在选择的过程中我们需要考虑如何选择才是最合理的？这里的评价标准主要包括：最优解选择概率（selective pressure）、选择偏差（bias）、多样性损失（loss of diversity）、重复选择上限（spread）、选择强度（selection intensity）、选择方差（selection variance）</strong></p><p>经典的选择算子包括：</p><ol><li>轮盘赌操作（roulette wheel selection），有回放的随机抽样选择</li><li>随机抽样选择（stochastic universal sampling）</li><li>锦标赛选择（tournament selection），随机选择一定规模的解来找到最优</li><li>截断选择（truncation selection）</li><li>本地搜索（local selection）</li><li>基变量选择</li><li>一对一生存竞争选择</li></ol><p><a href="http://geatpy.com/index.php/%E9%80%89%E6%8B%A9%E7%AE%97%E5%AD%90/">选择算子API地址</a></p><h3 id="3-4-重组-5-Recreation"><a href="#3-4-重组-5-Recreation" class="headerlink" title="3.4 重组[5] Recreation"></a>3.4 重组<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="http://geatpy.com/index.php/2019/07/28/第五章：重组/">[5]</span></a></sup> Recreation</h3><p>重组指的是通过结合交配群体的遗传信息来产生新的个体。通常其和问题编码的二进制编码、实值编码、排列编码、树编码方式有关，需要根据不同的编码来确定不同的重组算法。</p><p>重组包括重组片段概率和重组概率：</p><ol><li>重组片段概率，操作的片段长度</li><li>重组概率，两个是否发生重组的概率</li></ol><p><a href="http://geatpy.com/index.php/%E9%87%8D%E7%BB%84%E7%AE%97%E5%AD%90/">重组算子API地址</a></p><h3 id="3-5-变异-6-Mutation"><a href="#3-5-变异-6-Mutation" class="headerlink" title="3.5 变异[6] Mutation"></a>3.5 变异<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="http://geatpy.com/index.php/2019/07/28/第六章：变异/">[6]</span></a></sup> Mutation</h3><p>变异指的是通过改变原始解来生成新解的过程。参考对于自然界的观察，这个过程的基本原则是保持多样性，来避免陷入局部最优解的风险。以下给出常见的变异算法：</p><ol><li>二进制染色变异，在二进制中选择某些变异为0，某些变异为1</li><li>实值染色变异</li><li>均匀变异</li><li>高斯变异</li><li>多项式变异</li><li>差分变异</li><li>整数值变异</li><li>采用实值变异之后四舍五入</li><li>互换突变，在组合优化问题中染色体中的每个值都是独一无二的，因此常见的采用互换操作的方式来进行突变</li></ol><p><a href="http://geatpy.com/index.php/%E5%8F%98%E5%BC%82%E7%AE%97%E5%AD%90/">变异算子API地址</a></p><h3 id="四、GeatPy入门"><a href="#四、GeatPy入门" class="headerlink" title="四、GeatPy入门"></a>四、GeatPy入门</h3><p><a href="http://geatpy.com/">GeatPy</a>（Genetic and Evolutionary Algorithm Toolbox）是一个为Python设计的高性能进化算法框架和工具箱，能够快速解决单目标、多目标、多目标和组合优化问题。</p><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><p>GeatPy开发的角度主要四个抽象类，Problem、Population、PsyPopulation、Algorithm：</p><ol><li>问题类<strong>Problem</strong>，定义问题的名称（name）、优化目标维度（M）、决策变量维度（Dim）、决策变量范围（ranges）、决策变量边界（borders）、等式方向（maxormins）、变量类型（varTypes）、目标函数（aimFunc())、计算或读取目标函数参考值(calReferObjV())</li><li>寻解种群（Population、PsyPopulation），包括染色体（Chrom）、编码方式（Encoding）、译码矩阵（Field）</li><li>算法<strong>Algorithm</strong>，求解问题相关，包括之前抽象的各种算子</li></ol><h3 id="4-2-数据结构"><a href="#4-2-数据结构" class="headerlink" title="4.2 数据结构"></a>4.2 数据结构</h3><p>种群染色体<strong>chrom</strong>：默认一行chrom对应一条染色体，因此维度为： </p><p>基因表现型<strong>Phen</strong>：对于染色体解码之后的表达的变量，在实值编码下两者是一致的</p><p>目标函数值<strong>ObjV</strong>：单目标函数 </p><p>个体适应度<strong>FitnV</strong>:最小适应度为0，维度为 </p><p>违反约束程度<strong>CV</strong>: 存储每个种群违反约束条件的程度，维度为.Note: GeatPy处理约束的方式包括罚函数法和可行性法则</p><h3 id="4-3-快速入门-Get-start"><a href="#4-3-快速入门-Get-start" class="headerlink" title="4.3 快速入门 Get start"></a>4.3 快速入门 Get start</h3><ol><li>直接使用工具箱提供的库函数，通过编写脚本来实现进化算法并且求解问题</li><li>使用GeatPy提供的面向对象的算法框架，使用内置的算法模版求解问题</li><li>使用GeatPy提供的面向对象的算法框架，使用自定义进化算法实现进化算法并且求解问题</li></ol><h3 id="五、实现TSP问题（原始方法）"><a href="#五、实现TSP问题（原始方法）" class="headerlink" title="五、实现TSP问题（原始方法）"></a>五、实现TSP问题（原始方法）</h3><h3 id="5-1-初始化问题"><a href="#5-1-初始化问题" class="headerlink" title="5.1 初始化问题"></a>5.1 初始化问题</h3><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs inform7"># 利用脚本编程的方式来一步一步实现调用GAEATpy求解单目标优化问题<br>import numpy as np<br>import geatpy as ea # 导入geatpy库<br>import time <br>import matplotlib.pyplot as plt<br><br># TSP 坐标点<br>places=<span class="hljs-comment">[<span class="hljs-comment">[35.0, 35.0]</span>, <span class="hljs-comment">[41.0, 49.0]</span>, <span class="hljs-comment">[35.0, 17.0]</span>, <span class="hljs-comment">[55.0, 45.0]</span>, <span class="hljs-comment">[55.0, 20.0]</span>, <span class="hljs-comment">[15.0, 30.0]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[25.0, 30.0]</span>, <span class="hljs-comment">[20.0, 50.0]</span>, <span class="hljs-comment">[10.0, 43.0]</span>, <span class="hljs-comment">[55.0, 60.0]</span>, <span class="hljs-comment">[30.0, 60.0]</span>, <span class="hljs-comment">[20.0, 65.0]</span>, </span><br><span class="hljs-comment">        <span class="hljs-comment">[50.0, 35.0]</span>, <span class="hljs-comment">[30.0, 25.0]</span>, <span class="hljs-comment">[15.0, 10.0]</span>, <span class="hljs-comment">[30.0, 5.0]</span>, <span class="hljs-comment">[10.0, 20.0]</span>, <span class="hljs-comment">[5.0, 30.0]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[20.0, 40.0]</span>, <span class="hljs-comment">[15.0, 60.0]</span>]</span><br>places = np.array(places)<br>distance = np.array(<span class="hljs-comment">[<span class="hljs-comment">[np.linalg.norm(places<span class="hljs-comment">[i]</span>-places<span class="hljs-comment">[j]</span>,2) for i in range(len(places))]</span> for j in range(len(places))]</span>)<br><br>Phen = <span class="hljs-comment">[np.arange(len(places)) for i in range(10)]</span><br>for i in range(10):<br>    np.random.shuffle(Phen<span class="hljs-comment">[i]</span>)<br>print(len(places))<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-be3a9b533c2a8de09a9acd7d81bb1ef8_1440w.png" alt="img"></p><p>TSP问题</p><h3 id="5-2-定义目标函数"><a href="#5-2-定义目标函数" class="headerlink" title="5.2 定义目标函数"></a>5.2 定义目标函数</h3><p>记得目标函数的输入是，输出是 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">aimFunc</span>(<span class="hljs-params">Phen</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; </span><br><span class="hljs-string">    输入求解之后返回目标值</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    res = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Phen)):<br>        temp = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Phen[i])-<span class="hljs-number">1</span>):<br>            temp += distance[Phen[i][j]-<span class="hljs-number">1</span>,Phen[i][j+<span class="hljs-number">1</span>]-<span class="hljs-number">1</span>]<br>        res.append(temp)<br>    res = np.array(res)<br>    res = res.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><h3 id="5-3-定义种群属性"><a href="#5-3-定义种群属性" class="headerlink" title="5.3 定义种群属性"></a>5.3 定义种群属性</h3><p>需要确定种群的：</p><ol><li>编码方式（决策变量variable和染色体Chromosome）之间的关系</li><li>确定染色体的变化范围，FieldD，具体包括变量类型、变量上下限、上下限等号</li><li>种群数量</li></ol><p>因为TSP问题属于组合优化问题，因此可以采用排序编码，这样情况下“染色体”和“决策变量”是一直的，因此不需要相互转换</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-comment"># 定义约束变量的范围，以及是否包含等号</span><br>Encoding = &#x27;P&#x27; <span class="hljs-comment"># 编码方式，采用二进制编码</span><br><span class="hljs-comment"># 定义变量类型</span><br>varTypes = np.ones((len(places))) <span class="hljs-comment"># 0表示实数，1表示整数</span><br>x1 = np.zeros((len(places)))<br>b1 = np.zeros((len(places)))<br>x2 = np.ones((len(places)))*(len(places)+1)<br>b2 = np.zeros((len(places)))<br><br>ranges = np.vstack([x1,x2])<br>borders = np.vstack([b1,b2])<br><br>FieldD = ea.crtfld(Encoding,<br>                   varTypes,<br>                   ranges,<br>                   borders)<br><br><span class="hljs-comment"># 创建种群</span><br>NIND = 40<br>MAXGEN = 200<br>maxormins = [1] <span class="hljs-comment"># 1表示求最大值，-1表示求最小值</span><br>Lind = int(np.sum(FieldD[0,:])) <span class="hljs-comment"># 计算染色体长度</span><br></code></pre></td></tr></table></figure><h3 id="5-4-定义进化过程"><a href="#5-4-定义进化过程" class="headerlink" title="5.4 定义进化过程"></a>5.4 定义进化过程</h3><p>进化过程主要需要确定</p><ol><li>选择算子，这个一般是通用的；可以参考API文档来看</li><li>交叉算子，这个需要根据具体的编码方式来确定；可以参考API文档来看</li><li>变异算子，这个也需要根据具体的编码方式来确定；可以参考API文档来看</li></ol><p>这里有一个坑就是算子的实现GeatPy也在不断更新，这里采用ea.crtpc和ea.Mutpp来构造</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">selectStyle</span> = <span class="hljs-string">&#x27;rws&#x27;</span> <span class="hljs-comment"># 采用轮盘赌选择</span><br><span class="hljs-attr">recStyle</span> = <span class="hljs-string">&#x27;xovox&#x27;</span> <span class="hljs-comment"># 采用两点交叉</span><br><span class="hljs-attr">mutStyle</span> = <span class="hljs-string">&#x27;mutpp&#x27;</span> <span class="hljs-comment"># 采用二进制染色体的变异算子</span><br><br><span class="hljs-attr">pc</span> = <span class="hljs-number">0.7</span> <span class="hljs-comment"># 交叉概率</span><br><span class="hljs-attr">pm</span> = <span class="hljs-number">0.2</span> <span class="hljs-comment"># 变异概率</span><br><span class="hljs-attr">Obj_trace</span> = np.zeros((MAXGEN,<span class="hljs-number">4</span>)) <span class="hljs-comment"># 定义目标函数值记录器</span><br><span class="hljs-attr">var_trace</span> = np.zeros((MAXGEN,Lind)) <span class="hljs-comment"># 定义变量记录器</span><br><br><span class="hljs-attr">start_time</span> = time.time() <span class="hljs-comment"># 开始计时</span><br><span class="hljs-attr">Chrom</span> = ea.crtpc(Encoding, NIND, FieldD) <span class="hljs-comment"># 根据编码格式、编码范围、种群数量构建初始种群染色体</span><br><span class="hljs-attr">variable</span> = Chrom <span class="hljs-comment"># 根据染色体编码形式得到初始种群变量值</span><br><span class="hljs-attr">ObjV</span> = aimFunc(variable) <span class="hljs-comment"># 计算种群的目标函数 or 种群的适应度函数</span><br><span class="hljs-attr">mutOper</span> = ea.Mutpp(Pm=<span class="hljs-number">0.2</span>,MutN=<span class="hljs-number">2</span>)<br><span class="hljs-attr">best_ind</span> = np.argmax(ObjV) <span class="hljs-comment"># 计算当代最优个体的序号</span><br></code></pre></td></tr></table></figure><h3 id="5-5-求解过程"><a href="#5-5-求解过程" class="headerlink" title="5.5 求解过程"></a>5.5 求解过程</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># 开始进化</span><br><span class="hljs-keyword">for</span> gen <span class="hljs-keyword">in</span> range(MAXGEN):<br>    FitnV = ea.ranking(maxormins*ObjV) <span class="hljs-comment"># 计算种群对应的适应度函数值</span><br><br>    <span class="hljs-comment"># ---------- 进化 -----------------#</span><br>    SelCh = Chrom[ea.selecting(selectStyle, FitnV, NIND<span class="hljs-number">-1</span>), :] <span class="hljs-comment"># 选择selection</span><br>    SelCh = ea.recombin(recStyle, SelCh, pc) <span class="hljs-comment"># 重组recreation</span><br>    SelCh = mutOper.<span class="hljs-built_in">do</span>(Encoding,SelCh,FieldD)<br>    <span class="hljs-comment"># SelCh = ea.mutate(mutStyle, Encoding, SelCh) # 变异 mutation</span><br>    Chrom = np.vstack([Chrom[best_ind, :], SelCh]) <span class="hljs-comment"># 更新 update</span><br>    <span class="hljs-comment"># ---------- 结束 -----------------#</span><br><br>    Phen = Chrom <span class="hljs-comment"># 新种群解码操作</span><br>    ObjV = aimFunc(Phen) <span class="hljs-comment"># 新种群的目标函数</span><br><br>    <span class="hljs-comment"># 日志</span><br>    best_ind = np.argmin(ObjV) <span class="hljs-comment"># 计算当代最优个体的序号</span><br>    Obj_trace[gen,<span class="hljs-number">0</span>] = np.<span class="hljs-built_in">sum</span>(ObjV)/ObjV.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># 记录当代种群的目标函数均值</span><br>    Obj_trace[gen,<span class="hljs-number">1</span>] = ObjV[best_ind] <span class="hljs-comment"># 记录当代种群最优个体目标函数值</span><br>    Obj_trace[gen,<span class="hljs-number">2</span>] = ObjV.mean() <span class="hljs-comment"># 记录当代种群最优个体目标函数值</span><br>    Obj_trace[gen,<span class="hljs-number">3</span>] = ObjV.std() <span class="hljs-comment"># 记录当代种群目标函数值的标准差</span><br>    var_trace[gen,:] = Chrom[best_ind, :] <span class="hljs-comment"># 记录当代种群最优个体的变量值</span><br>end_time = <span class="hljs-built_in">time</span>.<span class="hljs-built_in">time</span>() <span class="hljs-comment"># 结束计时</span><br>times = end_time - start_time <span class="hljs-comment"># 计算耗时</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9b888d87c1cfde5330473c3a2fd5607e_1440w.png" alt="img"></p><p>TSP求解过程</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5047fcd732a121742e8ec0d8b67122f0_1440w.png" alt="img"></p><p>结果示意图</p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="http://geatpy.com/index.php/2019/07/28/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E6%A6%82%E8%BF%B0/">http://geatpy.com/index.php/2019/07/28/第一章：概述/</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="http://geatpy.com/index.php/2019/07/28/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E7%BC%96%E7%A0%81/">http://geatpy.com/index.php/2019/07/28/第二章：编码/</a> ↩<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="http://geatpy.com/index.php/2019/07/28/%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E9%80%82%E5%BA%94%E5%BA%A6/">http://geatpy.com/index.php/2019/07/28/第三章：适应度/</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="http://geatpy.com/index.php/2019/07/28/%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E9%80%89%E6%8B%A9/">http://geatpy.com/index.php/2019/07/28/第四章：选择/</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="http://geatpy.com/index.php/2019/07/28/%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E9%87%8D%E7%BB%84/">http://geatpy.com/index.php/2019/07/28/第五章：重组/</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="http://geatpy.com/index.php/2019/07/28/%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E5%8F%98%E5%BC%82/">http://geatpy.com/index.php/2019/07/28/第六章：变异/</a><a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>交通工程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>运筹优化</tag>
      
      <tag>启发式算法</tag>
      
      <tag>TSP 问题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法基础 Vol9</title>
    <link href="/posts/c6120946.html"/>
    <url>/posts/c6120946.html</url>
    
    <content type="html"><![CDATA[<p>对于图的基础认识和问题描述，进而给出基本的图遍历求解问题的思路</p><h3 id="0x01-对于图的基本认识"><a href="#0x01-对于图的基本认识" class="headerlink" title="0x01 对于图的基本认识"></a>0x01 对于图的基本认识</h3><p>对于数据结构的存储无非就是“增删改查”，所以对于图（Graph）的认识也可以从这个角度出发。</p><p>不过与基本的数组和链表不同的是，数组作为顺序存储、链表作为链接存储更多的是代表计算机存储的一种方式，是更为适合计算理解而对人类不友好的；图作为一个存在日常生活中的数据结构，可以很容易的被人理解，但是往往在计算机存储中存在困难。所以对于图的理解可以从图的本身，以及从图对应的计算机存储方式入手，之后再给出图中一些经典问题。</p><h3 id="1-1-图的本身含义"><a href="#1-1-图的本身含义" class="headerlink" title="1.1 图的本身含义"></a>1.1 图的本身含义</h3><p>图（Graph）作为一种描述节点（Vertex or node）和节点之间连接的边（Edge）的抽象数据结构，在现实场景中存在很多的应用，比如说具体的交通网络流量、水管布设、计算机网络流量、电路图等等，在抽象中包括人与人之间的社交关系、车与车之间的安全关系等等。同时我们可以节点根据连接边的数量可以增加几个重要的特征：</p><ol><li>度（Degree），其中出度表示多少边是从节点发出的，入度表示多少边是从节点接受的；</li><li>通路（Path）表示有m+1顶点和m条边组成的序列</li><li>首尾相同的通路则为环路（Cycle），同时特殊的环路包括欧拉环路、哈密尔顿环路</li></ol><p>根据对节点和边我们可以对图进行简单的分类：</p><ol><li>根据边是否有权可以将其分为有权图（Weighted graph）和无权图。这里的区别是图中的边是代表固定的连接和非连接，还是代表一定的数值（比如流量）</li><li>根据边是否有方向可以将其划分为有向图（Directed Graph）和无向图（Undirected graph），表示边是表示两端是否连接，还是表示从A点指向B点</li><li>根据图中是否包含自环可以将其划分为简单图（Simple Graph）和复杂图，特别的如果其为有向图可以称为有向无环图（DAG，Directed acrylic graph）。</li></ol><h3 id="1-2-图的计算机理解"><a href="#1-2-图的计算机理解" class="headerlink" title="1.2 图的计算机理解"></a>1.2 图的计算机理解</h3><p>上述主要是从图本身增加了对图的理解，那么我们如何在计算机中表示图呢？考虑到之前的“增删改差”的特点，我们往往希望一种在空间存储上消耗低、在操作时间上开销少的方式来表达Graph这种高级数据结构，因此我们通常会有以下五种基本方法，在评价每个方法的时候要谨记其存储和操作开销，这里我们定义</p><ol><li>节点数量为 n</li><li>边的数量为 m</li><li>D表示节点的度</li></ol><h3 id="1-2-1-邻接矩阵（Adjacency-Matrix）"><a href="#1-2-1-邻接矩阵（Adjacency-Matrix）" class="headerlink" title="1.2.1 邻接矩阵（Adjacency Matrix）"></a>1.2.1 邻接矩阵（Adjacency Matrix）</h3><p>很直接的我们会想到点是点，边是边，因此可以通过定义一个点的数组，同时在定义点和点之间边的数组来表示一个图Graph。这样带来的缺点是空间复杂度过高：</p><blockquote><p> 新建空间：O($n^2$) 查、增、删：O(1) 新增顶点：O(n) 遍历操作：O($n^2$)</p></blockquote><h3 id="1-2-2-边集数组（Edgeset-Array）"><a href="#1-2-2-边集数组（Edgeset-Array）" class="headerlink" title="1.2.2 边集数组（Edgeset Array）"></a>1.2.2 边集数组（Edgeset Array）</h3><p>由于图中的边已经表示起点和终点，因此如果可以记录所有的边也可以得到对应图的结果</p><blockquote><p> 新建空间：O(m) 查询（是否存在某条边）：O(m) 遍历某个点的所有边：O（m） 遍历整张图：O（mn）</p></blockquote><p>边集数组适合对于边进行操作的运算，而不适合对于顶点的运算和任何一条边的运算</p><h3 id="1-2-3-邻接表（Adjacency-Link-List）"><a href="#1-2-3-邻接表（Adjacency-Link-List）" class="headerlink" title="1.2.3 邻接表（Adjacency Link List）"></a>1.2.3 邻接表（Adjacency Link List）</h3><p>是一种顺序存储和链式存储结合的存储方式来存储图的节点和边。其中数据结构中数组存放节点的信息，每个数组的元素为链表，用于记录和当前节点链接其他节点的信息。</p><blockquote><p> 初始化操作：O(m+n) 查询是否存在边：O（TD（）） 遍历某个点的所有边：O（TD） 遍历整张图：O（n+m）</p></blockquote><h3 id="1-2-4-链式前向星（Linked-forward-Start）"><a href="#1-2-4-链式前向星（Linked-forward-Start）" class="headerlink" title="1.2.4 链式前向星（Linked forward Start）"></a>1.2.4 链式前向星（Linked forward Start）</h3><p>将上述邻接表中的节点的数组转换成为链表，同时和边集数组结合的方式，可以大幅度的提高建图和遍历效率的额方式，具体的实现方式没看懂～</p><blockquote><p> 如果说邻接表是不好写但效率好，邻接矩阵是好写但效率低的话，前向星就是一个相对中庸的数据结构。前向星固然好写，但效率并不高。而在优化为链式前向星后，效率也得到了较大的提升。虽然说，世界上对链式前向星的使用并不是很广泛，但在不愿意写复杂的邻接表的情况下，链式前向星也是一个很优秀的数据结构。  – 百度百科 初始化操作：O(m+n) 空间复杂度：O(m+n) 遍历效率:O(m+n)</p></blockquote><h3 id="1-2-5-哈希表"><a href="#1-2-5-哈希表" class="headerlink" title="1.2.5 哈希表"></a>1.2.5 哈希表</h3><p>这个比较好理解，主要包括两个哈希表：</p><ol><li>第一个哈希表以节点作为key，以下一个终端节点哈希表作为value</li><li>终端哈希表中终端阶段为key，边的权重作为 value</li></ol><blockquote><p> 图的初始化：O(m+n) 查询是否存在：O(1) 遍历某个点的所有边:O(TD) 遍历整张图：O(m+n)</p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4998558e4046be922ca21daa3e43267e_1440w.png" alt="img"></p><p>图（Graph）的计算机存储结构</p><h3 id="0x02-图的经典问题"><a href="#0x02-图的经典问题" class="headerlink" title="0x02 图的经典问题"></a>0x02 图的经典问题</h3><p>既然有图这种数据结构，我们往往会需要什么要的操作呢？这也是我们在现实场景中需要面对的问题，下面将以<strong>交通网络</strong>为基础，介绍图中的一些典型问题，希望可以在没有实际操作图的基础上，可以对图问题的理解。</p><p>首先最基本的是对图的遍历，在通过对节点的遍历的基础上，我们可以使用结合对应的算法来实现最短路径的计算、最小生成树的实现，但是这些依旧是属于点与点之间的问题，我们往往会需要探索图中整体的性质，因此图中的连通性，包括重要的节点和边是需要搜索的对象，在此基础上我们遇到探索对应的网络流问题，来寻找对应的网络流量，</p><h3 id="2-1-遍历问题"><a href="#2-1-遍历问题" class="headerlink" title="2.1 遍历问题"></a>2.1 遍历问题</h3><p>和其他所有的数据结构一样，我们肯定希望存在一种操作可以获取图中所有的数据。所以最基本的是在知道图的数据结构之后，可以获取所有图的节点。</p><blockquote><p> 当你处于世界上任何一个地方，你需要某种方式来知道去其他任何地方的路径</p></blockquote><p>常见的DFS、BFS</p><h3 id="2-2-最短路径问题"><a href="#2-2-最短路径问题" class="headerlink" title="2.2 最短路径问题"></a>2.2 最短路径问题</h3><p>如果边的权重表示一定的物理含义（运输费用、运输距离等等），我们希望知道节点中A点和B点之间的最短路径的值</p><blockquote><p> 在知道如何去一个地方的时候，需要知道那条路径是最低的</p></blockquote><p>常见的有Djikstra算法和Bellman-Ford算法</p><h3 id="2-3-生成树问题"><a href="#2-3-生成树问题" class="headerlink" title="2.3 生成树问题"></a>2.3 生成树问题</h3><p>我们定义图的树：是包含图G中所有顶点V得树，那么该树就是子图的生成图。由此衍生出最小生成树、次小生成树等问题。</p><blockquote><p> 如果一个个体知道点到点之间的路径，从整体的角度我们需要知道遍历所有顶点需要的最小代价，也就是最小生成树</p></blockquote><p>常见的问题方法有prim算法和kruskal算法</p><h3 id="2-4-连通图问题"><a href="#2-4-连通图问题" class="headerlink" title="2.4 连通图问题"></a>2.4 连通图问题</h3><p>连通指的是图中的点和点之间是否存在链接。但是自然而然的我们会想到图中不同边的属性是不一样的，如果有的边或者节点中断往往会造成网络的瘫痪。</p><blockquote><p> 对应交通网络中的主动脉，或者是关键枢纽</p></blockquote><h3 id="2-5-网络流问题"><a href="#2-5-网络流问题" class="headerlink" title="2.5 网络流问题"></a>2.5 网络流问题</h3><p>在讨论连通和遍历所有节点的基础上，我们给网络的边增加限制：流量上限，在这种情况下，群体的属性会发生改变，如何讨论边具有流量上限下网络的属性非常有意思</p><blockquote><p> 如何计算点与点之间的OD流量的最大流量，以及如何在每条边具有最大流量限制的情况下，计算流量分配来是的整体的费用最少。</p></blockquote><p>常见的概念有最小割、最大流、网络流问题</p><h3 id="2-6-二分图问题"><a href="#2-6-二分图问题" class="headerlink" title="2.6 二分图问题"></a>2.6 二分图问题</h3><p>讨论是两个二分图之间匹配的问题，属于典型的网络应用</p><blockquote><p> 经典的骑手和商家的问题，如果在同一个时间点有n个商家，有n个骑手，如何将骑手分配给商家 打车问题，加入同时有n个需求点，有n个出租车，如何将出租车分配给不同的需求点，来是的整体效益最高</p></blockquote><p>常见的算法有匈牙利算法、KM算法</p><h3 id="0x03-图的遍历岛屿数量"><a href="#0x03-图的遍历岛屿数量" class="headerlink" title="0x03 图的遍历岛屿数量"></a>0x03 图的遍历岛屿数量</h3><p>如果将树看作是自上而下的规则，那么一步一步的我们必然可以得到一个结果，所以我们可以将节点作为状态，将节点的分枝作为动作，这样我们在不同的状态具有不同的状态转移到下一个状态，如果我们需要遍历所有的状态，我们最好的方式是通过搜索的方式（search strategy），这里参考CS188对于搜索问题的解决方式，有两种最基本的方式BFS和DFS。</p><p>终点在于：</p><ol><li>根据分枝判断下一步的转移方向</li><li>能够遍历所有的分枝</li><li>记录遍历的过程</li><li>到达终点后返回</li></ol><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs crmsh"><span class="hljs-comment"># node with different actions</span><br>path = []<br>def dfs(<span class="hljs-keyword">node</span><span class="hljs-title">):</span><br><span class="hljs-title">    path</span>.append(node.val)<br>    for action <span class="hljs-keyword">in</span> node.actions:<br>        dfs(<span class="hljs-keyword">node</span><span class="hljs-title">+action</span>)<br></code></pre></td></tr></table></figure><p>所以虽然图看上去形式和树并不一样，我们对于图的遍历方式依旧可以使用DFS或者BFS</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8eb886aa199b57a255d9c9b7d06a2ae5_1440w.png" alt="img"></p><p>T200岛屿数量</p><p>首先分析问题是需要寻找所有的岛屿，那么从岛屿的定义中可以达到遍历的条件，也就是找到0的点，遍历的分枝也就是上下左右，同时对于我们寻找的地方我们需要标记为结束。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numIslands</span>(<span class="hljs-params">self, grid: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-variable language_">self</span>.m = <span class="hljs-built_in">len</span>(grid)<br>        <span class="hljs-variable language_">self</span>.n = <span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>])<br>        count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.m):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n):<br>                <span class="hljs-keyword">if</span> grid[i][j] == <span class="hljs-string">&#x27;1&#x27;</span>:<br>                    <span class="hljs-variable language_">self</span>.dfs(grid,i,j)<br>                    count += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> count <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">dfs</span>(<span class="hljs-params">self,grid,i,j</span>):<br>        <span class="hljs-comment"># 从grid i 和 j 进行遍历分析</span><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> i &gt;= <span class="hljs-variable language_">self</span>.m <span class="hljs-keyword">or</span> j &lt; <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> j &gt;= <span class="hljs-variable language_">self</span>.n <span class="hljs-keyword">or</span> grid[i][j] == <span class="hljs-string">&#x27;0&#x27;</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        grid[i][j] = <span class="hljs-string">&#x27;0&#x27;</span><br>        <span class="hljs-variable language_">self</span>.dfs(grid,i+<span class="hljs-number">1</span>,j)<br>        <span class="hljs-variable language_">self</span>.dfs(grid,i-<span class="hljs-number">1</span>,j)<br>        <span class="hljs-variable language_">self</span>.dfs(grid,i,j+<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.dfs(grid,i,j-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h3 id="0x04-扫雷问题"><a href="#0x04-扫雷问题" class="headerlink" title="0x04 扫雷问题"></a>0x04 扫雷问题</h3><p>这个属于遍历问题的升级版，唯一的区别是岛屿终点的判断并不仅仅是0，而是包括未观测出的地雷的数字。所有从约束编程求解的思路来看，如果一个问题我们求解是NP-hard，但是通常我们验证其正确性只需要P得时间，因此我们可以先得到地图数据的mask，通过点击位置和mask来判断需要展示的区别，进而修改棋盘的状态</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ed9533446ac16f0375305aa98c8df59d_1440w.png" alt="img"></p><p>T529 扫雷游戏</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs prolog">class <span class="hljs-symbol">Solution</span>:<br>    def updateBoard(self, board, click):<br>        # 将原始棋盘转换成为 recover的形式<br>        # 同时计算点击下的连通图作为mask<br>        # 利用mask recovery的合体来实现最终的结果<br>        self.dirs = [[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">-1</span>],<br>                [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">-1</span>], [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>]]<br>        self.board = board<br>        m = len(self.board)<br>        n = len(self.board[<span class="hljs-number">0</span>])<br>        dp = [[<span class="hljs-number">0</span> for <span class="hljs-symbol">_</span> in range(n)] for <span class="hljs-symbol">_</span> in range(m) ]<br>        for i in range(m):<br>            for j in range(n):<br>                if self.board[i][j] == <span class="hljs-string">&#x27;M&#x27;</span>:<br>                    dp[i][j] = <span class="hljs-number">-1</span><br>                    for dir in self.dirs:<br>                        if i+dir[<span class="hljs-number">0</span>] &gt;=<span class="hljs-number">0</span> and i+dir[<span class="hljs-number">0</span>] &lt; m and j+dir[<span class="hljs-number">1</span>] &gt;= <span class="hljs-number">0</span> and j+dir[<span class="hljs-number">1</span>] &lt; n:<br>                            if dp[i+dir[<span class="hljs-number">0</span>]][j+dir[<span class="hljs-number">1</span>]] != <span class="hljs-number">-1</span>:<br>                                dp[i+dir[<span class="hljs-number">0</span>]][j+dir[<span class="hljs-number">1</span>]]+=<span class="hljs-number">1</span>   <br><br>        # 从click开始进行dfs，三种情况<br>        # <span class="hljs-number">1.</span> 如果是dp 地雷，将board中的<span class="hljs-symbol">M</span>改为<span class="hljs-symbol">X</span>，返回<br>        # <span class="hljs-number">2.</span> 如果是dp 数字，将board中的<span class="hljs-symbol">E</span>改为数字，返回<br>        # <span class="hljs-number">3.</span> 如果是dp 中的<span class="hljs-number">0</span>，将board中的<span class="hljs-symbol">E</span>改为<span class="hljs-symbol">B</span>，然后递归的将其相邻的方块改为<span class="hljs-symbol">B</span><br>        # loguru.logger.info(dp)<br>        if dp[click[<span class="hljs-number">0</span>]][click[<span class="hljs-number">1</span>]] == <span class="hljs-number">-1</span>:<br>            self.board[click[<span class="hljs-number">0</span>]][click[<span class="hljs-number">1</span>]] = <span class="hljs-string">&#x27;X&#x27;</span><br>            return self.board<br>        if dp[click[<span class="hljs-number">0</span>]][click[<span class="hljs-number">1</span>]] &gt; <span class="hljs-number">0</span>:<br>            self.board[click[<span class="hljs-number">0</span>]][click[<span class="hljs-number">1</span>]] = str(dp[click[<span class="hljs-number">0</span>]][click[<span class="hljs-number">1</span>]])<br>            return self.board<br>        if dp[click[<span class="hljs-number">0</span>]][click[<span class="hljs-number">1</span>]] == <span class="hljs-number">0</span>:<br>            # 说明修改的是<span class="hljs-symbol">E</span><br>            self.dfs(dp,click[<span class="hljs-number">0</span>],click[<span class="hljs-number">1</span>])<br>            return self.board<br><br>    def dfs(self,dp,i,j):<br>        # loguru.logger.info(<span class="hljs-string">&#x27;dfs&#x27;</span>)<br>        if i&lt;<span class="hljs-number">0</span> or i&gt;=len(dp) or j&lt;<span class="hljs-number">0</span> or j&gt;=len(dp[<span class="hljs-number">0</span>]):<br>            # loguru.logger.info(<span class="hljs-string">&#x27;out of range&#x27;</span>)<br>            return <br>        if dp[i][j] != <span class="hljs-number">0</span>:<br>            # loguru.logger.info(<span class="hljs-string">&#x27;not zero&#x27;</span>)<br>            self.board[i][j] = str(dp[i][j])<br>            return<br>        if dp[i][j] == <span class="hljs-number">0</span> and self.board[i][j] == <span class="hljs-string">&#x27;E&#x27;</span>:<br>            # loguru.logger.info(<span class="hljs-string">&#x27;zero&#x27;</span>)<br>            self.board[i][j] = <span class="hljs-string">&#x27;B&#x27;</span><br>            for dir in self.dirs:<br>                self.dfs(dp,i+dir[<span class="hljs-number">0</span>],j+dir[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>图</tag>
      
      <tag>遍历</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据分析和算法基础 Vol9</title>
    <link href="/posts/c0d90fda.html"/>
    <url>/posts/c0d90fda.html</url>
    
    <content type="html"><![CDATA[<p>堆栈 Stack是一种线性表数据，只允许在表的一端进行插入（push）和删除（pop）操作。堆栈的好处在于操作时间为线性的。因此可以极大的提供效率。最基本的题目是《T20 有效的大括号》，而在其中更加复杂的是单调栈：</p><blockquote><p> 单调栈（Monotone stack）：一种特殊的栈，在先进先出的基础上，要求从top到bottom的元素是单调的（单调递增 or 单调递减）</p></blockquote><h3 id="0x01-理解单调栈"><a href="#0x01-理解单调栈" class="headerlink" title="0x01 理解单调栈"></a>0x01 理解单调栈</h3><p>维护一个单调栈的过程如图：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f3ae981069363b72a821a5e84f810160_1440w.png" alt="img"></p><p>单调栈过程</p><p>单调栈可以解决的问题包括：</p><ol><li>寻找左侧第一个比当前元素大的元素</li><li>寻找左侧第一个比当前元素小的元素</li><li>寻找右侧第一个比当前元素大的元素</li><li>寻找右侧第一个从当前元素小的元素</li></ol><p>单调栈思路模版</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp">stack = []<br><span class="hljs-keyword">for</span> num in nums:<br>    <span class="hljs-keyword">while</span> stack <span class="hljs-keyword">and</span> <span class="hljs-built_in">rule</span>(stack[<span class="hljs-number">-1</span>],num):<br>        <span class="hljs-meta"># rule should be defined</span><br>        stack.<span class="hljs-built_in">pop</span>()<br>    stack.<span class="hljs-built_in">push</span>(<span class="hljs-built_in">gen</span>(num))<br>    <span class="hljs-meta"># gen should be related to the num</span><br></code></pre></td></tr></table></figure><h3 id="0x02-单调解决问题"><a href="#0x02-单调解决问题" class="headerlink" title="0x02 单调解决问题"></a>0x02 单调解决问题</h3><h3 id="2-1-T901-股票价格跨度"><a href="#2-1-T901-股票价格跨度" class="headerlink" title="2.1 T901 股票价格跨度"></a>2.1 <a href="https://leetcode.cn/problems/online-stock-span/">T901 股票价格跨度</a></h3><p>典型的单调栈记录中值下标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">StockSpanner</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.stack = []<br>        <span class="hljs-variable language_">self</span>.index = [<span class="hljs-number">0</span>]<br>        <span class="hljs-variable language_">self</span>.record = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">next</span>(<span class="hljs-params">self, price: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-variable language_">self</span>.record += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-variable language_">self</span>.stack <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.stack[-<span class="hljs-number">1</span>]&lt;=price:<br>            <span class="hljs-variable language_">self</span>.stack.pop()<br>            <span class="hljs-variable language_">self</span>.index.pop()<br>        <span class="hljs-variable language_">self</span>.stack.append(price)<br>        <span class="hljs-variable language_">self</span>.index.append(<span class="hljs-variable language_">self</span>.record)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.index[-<span class="hljs-number">1</span>]-<span class="hljs-variable language_">self</span>.index[-<span class="hljs-number">2</span>]<br></code></pre></td></tr></table></figure><h3 id="2-2-T853-车队"><a href="#2-2-T853-车队" class="headerlink" title="2.2 T853 车队"></a>2.2 <a href="https://leetcode.cn/problems/car-fleet/">T853 车队</a></h3><p>严格来说并不是完全的单调栈，遍历即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">carFleet</span>(<span class="hljs-params">self, target, position, speed</span>):<br>        cars = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">zip</span>(position, speed))<br>        times = [<span class="hljs-built_in">float</span>(target - p) / s <span class="hljs-keyword">for</span> p, s <span class="hljs-keyword">in</span> cars]<br>        ans = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(times) &gt; <span class="hljs-number">1</span>:<br>            lead = times.pop()<br>            <span class="hljs-keyword">if</span> lead &lt; times[-<span class="hljs-number">1</span>]: ans += <span class="hljs-number">1</span>  <span class="hljs-comment"># if lead arrives sooner, it can&#x27;t be caught</span><br>            <span class="hljs-keyword">else</span>: times[-<span class="hljs-number">1</span>] = lead <span class="hljs-comment"># else, fleet arrives at later time &#x27;lead&#x27;</span><br><br>        <span class="hljs-keyword">return</span> ans + <span class="hljs-built_in">bool</span>(times) <span class="hljs-comment"># remaining car is fleet (if it exists)</span><br></code></pre></td></tr></table></figure><h3 id="2-3-T907-子数组的最小值之和"><a href="#2-3-T907-子数组的最小值之和" class="headerlink" title="2.3  T907 子数组的最小值之和"></a>2.3  <a href="https://leetcode.cn/problems/sum-of-subarray-minimums/">T907 子数组的最小值之和</a></h3><p>典型的利用单调栈作为一种工具，来记录自己左边和右边的第一个发现的元素。这题更重要的是要明白重复是如何计算的</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs vim">MOD = <span class="hljs-number">10</span> ** <span class="hljs-number">9</span> + <span class="hljs-number">7</span><br>class Solution:<br>    def sumSubarrayMins(self, arr: List[<span class="hljs-keyword">int</span>]) -&gt; <span class="hljs-keyword">int</span>:<br>        # # stage 为连续子数组长度为<span class="hljs-keyword">k</span>的长度，<span class="hljs-built_in">max</span> <span class="hljs-keyword">k</span> 为 <span class="hljs-built_in">len</span>(arr)<br>        # <span class="hljs-keyword">res</span> = []<br>        # <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(arr)-<span class="hljs-number">1</span>):<br>        #     <span class="hljs-keyword">res</span>.<span class="hljs-keyword">append</span>(<span class="hljs-built_in">min</span>(arr[i],arr[i+<span class="hljs-number">1</span>]))<br>        # <span class="hljs-keyword">if</span> <span class="hljs-keyword">res</span>:<br>        #     <span class="hljs-keyword">return</span> (sum(arr)+self.sumSubarrayMins(<span class="hljs-keyword">res</span>)) % MOD<br>        # <span class="hljs-keyword">else</span>:<br>        #     <span class="hljs-keyword">return</span> arr[<span class="hljs-number">0</span>]<br><br>        # 根据元素的最小值程度来计算<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(arr) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(arr) == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> arr[<span class="hljs-number">0</span>]<br><br>        ans = <span class="hljs-number">0</span><br>        left_1 = [<span class="hljs-number">1</span>]*<span class="hljs-built_in">len</span>(arr) # 每个元素左边小于自己的值<br>        right_1 = [<span class="hljs-number">1</span>]*<span class="hljs-built_in">len</span>(arr) # 每个元素右边小于自己的值<br><br>        def singleStack(arr,<span class="hljs-keyword">left</span>):<br>            stack = []<br>            <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(arr)):<br>                <span class="hljs-keyword">while</span> stack <span class="hljs-built_in">and</span> arr[stack[-<span class="hljs-number">1</span>]]&gt;=arr[i]:<br>                    # 当存在的时候<br>                    stack.<span class="hljs-keyword">pop</span>()<br>                stack.<span class="hljs-keyword">append</span>(i)<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(stack)==<span class="hljs-number">1</span>:<br>                    <span class="hljs-keyword">left</span>[i] = stack[<span class="hljs-number">0</span>]+<span class="hljs-number">1</span><br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">left</span>[i] = stack[-<span class="hljs-number">1</span>]-stack[-<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">left</span><br><br>        def singleStack2(arr,<span class="hljs-keyword">left</span>):<br>            stack = []<br>            <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(arr)):<br>                <span class="hljs-keyword">while</span> stack <span class="hljs-built_in">and</span> arr[stack[-<span class="hljs-number">1</span>]]&gt;arr[i]:<br>                    # 当存在的时候<br>                    stack.<span class="hljs-keyword">pop</span>()<br>                stack.<span class="hljs-keyword">append</span>(i)<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(stack)==<span class="hljs-number">1</span>:<br>                    <span class="hljs-keyword">left</span>[i] = stack[<span class="hljs-number">0</span>]+<span class="hljs-number">1</span><br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">left</span>[i] = stack[-<span class="hljs-number">1</span>]-stack[-<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">left</span><br><br>        # how ever, this will <span class="hljs-keyword">make</span> the duplicated <br>        # <span class="hljs-keyword">only</span> one way stack <span class="hljs-keyword">is</span> accpted<br>        <span class="hljs-keyword">left</span> = singleStack(arr,left_1)<br>        <span class="hljs-keyword">right</span> = singleStack2(arr[::-<span class="hljs-number">1</span>],right_1)[::-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">res</span> = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(arr)):<br>            <span class="hljs-keyword">res</span>+=arr[i]*<span class="hljs-keyword">left</span>[i]*<span class="hljs-keyword">right</span>[i]<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">res</span>%MOD<br></code></pre></td></tr></table></figure><h3 id="2-4-T456-132模式"><a href="#2-4-T456-132模式" class="headerlink" title="2.4 T456 132模式"></a>2.4 T456 132模式</h3><p>首先根据模式我们可以很简单的判断可以设计单调递升的栈作为方法，但是如何判断2是否存在？也就是只要存在一个在13之间的数字即可，而这个正好是每次单调栈所不需要的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">find132pattern</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        ak = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>)<br>        stack = []<br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(nums):<br>            <span class="hljs-keyword">if</span> ak &gt; num: <br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">while</span> stack <span class="hljs-keyword">and</span> stack[-<span class="hljs-number">1</span>] &lt; num:<br>                ak = stack.pop()<br>            stack.append(num)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><h3 id="2-5-T2865-美丽塔"><a href="#2-5-T2865-美丽塔" class="headerlink" title="2.5 T2865 美丽塔"></a>2.5 T2865 美丽塔</h3><p>引入presum的气息，在计算每个元素左边和右边相关元素之前，先计算从左向右的所有数组，再计算自右向左的所有数组，最后遍历得到自己想要的值</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs inform7">class Solution:<br>    def maximumSumOfHeights(self, maxHeights: List<span class="hljs-comment">[int]</span>) -&gt; int:<br>        # 自左向右建立单调栈<br>        n = len(maxHeights)<br>        suffix = <span class="hljs-comment">[0]</span>*(n+1)<br>        presum = <span class="hljs-comment">[0]</span>*(n+1)<br>        stack = <span class="hljs-comment">[]</span><br>        # 根据单调栈求前缀和<br>        for i in range(n):<br>            while stack and maxHeights<span class="hljs-comment">[stack<span class="hljs-comment">[-1]</span>]</span> &gt; maxHeights<span class="hljs-comment">[i]</span>:<br>                stack.pop()<br>            j = stack<span class="hljs-comment">[-1]</span> if stack else -1<br>            # i 目标前缀和的下标<br>            # j 是左侧第一个比目标值小的元素，也就是下山的元素<br>            presum<span class="hljs-comment">[i+1]</span> = presum<span class="hljs-comment">[j+1]</span>+(i-j)*maxHeights<span class="hljs-comment">[i]</span><br>            stack.append(i)<br><br>        stack = <span class="hljs-comment">[]</span><br>        # 根据单调栈求后缀和<br>        for i in range(n-1,-1,-1):<br>            while stack and maxHeights<span class="hljs-comment">[stack<span class="hljs-comment">[-1]</span>]</span> &gt; maxHeights<span class="hljs-comment">[i]</span>:<br>                stack.pop()<br>            j = stack<span class="hljs-comment">[-1]</span> if stack else n<br>            suffix<span class="hljs-comment">[i]</span> = suffix<span class="hljs-comment">[j]</span>+(j-i)*maxHeights<span class="hljs-comment">[i]</span><br>            stack.append(i)<br><br>        # 合并<br>        res = 0<br>        for i in range(n):<br>            res = max(res,presum<span class="hljs-comment">[i+1]</span>+suffix<span class="hljs-comment">[i]</span>-maxHeights<span class="hljs-comment">[i]</span>)<br>        return res<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>单调栈</tag>
      
      <tag>复习</tag>
      
      <tag>力扣刷题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229 机器学习 Vol14｜ GBDT 与 Kaggle</title>
    <link href="/posts/96d2f3ef.html"/>
    <url>/posts/96d2f3ef.html</url>
    
    <content type="html"><![CDATA[<p>回顾说明是基于树的集成学习，同时给出基本的随机森林和Adaboost解释，以及GBDT的两种工程实现XGboost和lightGBM的介绍，并用实际的Kaggle例子解释使用。</p><h3 id="0x01-树（Tree）与集成学习（Ensemble-learning）"><a href="#0x01-树（Tree）与集成学习（Ensemble-learning）" class="headerlink" title="0x01 树（Tree）与集成学习（Ensemble learning）"></a>0x01 树（Tree）与集成学习（Ensemble learning）</h3><h3 id="1-1-什么是决策树"><a href="#1-1-什么是决策树" class="headerlink" title="1.1 什么是决策树"></a>1.1 什么是决策树</h3><p>在之前的文章已经提到过 <a href="https://blog.tjdata.site/posts/a32aa49a.html">决策树CART与手撕代码</a>，决策树是一类基于分治思想的非参数的机器学习方法。通过自上而下的树形拆分对数据进行分类，作为一类最简单的最基础的监督学习模型可以被用于分类和回归问题。决策的过程分为</p><ol><li>特征选择</li><li>树的构造</li><li>树的建枝，为了避免过拟合</li></ol><p>所谓构造准则是在选择分支的时候如何确定规则取值？我们可以使用某种不纯度（Impurity）度量方式，来比较分支前的集合和分支之后子集度量之和来选择不纯度下降最大的特征作为区分。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4afc2dc91d34afcd40f07efa8660d7cf_1440w.png" alt="img"></p><p>特征选择过程</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>ID3</td><td>最大信息增益</td><td>倾向于选择取值多的特征；对样本缺失值敏感</td><td>离散，分类</td></tr><tr><td>C4.5</td><td>最大信息增益比</td><td>对于ID3的正则化，避免选择取值多的特征</td><td>离散连续，分类</td></tr><tr><td>CART</td><td>Gini或者最小平方差</td><td>最终形成二叉树</td><td>离散连续，分类和回归</td></tr></tbody></table><h3 id="1-2-什么是集成学习"><a href="#1-2-什么是集成学习" class="headerlink" title="1.2 什么是集成学习"></a>1.2 什么是集成学习</h3><p>在之前的 <a href="https://blog.tjdata.site/posts/2cac3df3.html">学习理论 — 集成学习</a>中我们介绍了我们可以尝试训练不同的模型并将他们共同输出来得到最终的结果，潜在的假设在于集成多个训练的结果可以达到超过原有单个模型的效果。其中集中方式主要分为：</p><ol><li>使用算法训练得到不同的模型</li><li>将不同模型输出的结果通过平均、投票或者学习（Stacking）的方式来得到（MOE）</li><li>在学习规则中使用不同的策略集中，Boosting（串行）、Bagging（并行）</li></ol><p>这里主要解释两种不同的策略</p><p>Bagging，主要是设计不同的基训练模型，对于同一个训练集我们如何训练得到不同的模型？一种简单的方式是分割数据集，但是这样对于样本的利用率并不高。是否存在一种方式可以重复利用样本但是不同学习器之间存在训练分布的差异。这个时候可以使用之前在模型评价中的自助采样（bootstrap）方法来选择不同的数据样本或者选择不同的特征。</p><p>Boosting，主要是将上一模型的输出继续进行学习，有点类似残差学习的概念，或者resent的想法来弥补之前算法的缺陷，在其中通常是设计如何选择之后采样的权重。</p><h3 id="1-3-随机森林和AdaBoost"><a href="#1-3-随机森林和AdaBoost" class="headerlink" title="1.3 随机森林和AdaBoost"></a>1.3 随机森林和AdaBoost</h3><p>给出两种不同的例子，一种是基于Bagging的随机森林</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-745dce26ece095f986302d2fbc1e48f7_1440w.png" alt="img"></p><p>随机森林</p><p>一种是基于boosting的Adaboost，通过输出错误分类数据的权重来影响下一个决策树的学习</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6a9423b6abec15579c7c5b8d6a91a8ee_1440w.png" alt="img"></p><p>Adaboost</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-bf2a3f8490d6f2949b112d655babc749_1440w.png" alt="img"></p><p>Adaboost伪代码</p><h3 id="0x02-梯度提升树（Gradient-boosting-tree）"><a href="#0x02-梯度提升树（Gradient-boosting-tree）" class="headerlink" title="0x02 梯度提升树（Gradient boosting tree）"></a>0x02 梯度提升树（Gradient boosting tree）</h3><h3 id="2-1-什么是梯度提升树"><a href="#2-1-什么是梯度提升树" class="headerlink" title="2.1 什么是梯度提升树"></a>2.1 什么是梯度提升树</h3><p>梯度提升（Gradient boosting）是一类方法，思想主要是使用残差进行下一段的学习，最终的结果为所有的树的结果加和。其是基于经验损失函数的负梯度来构建新的决策树，同时在决策树构建完成之后进行剪枝</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-25d2edaa2b27d841f0fb55655183652e_1440w.png" alt="img"></p><p>Boosting模型：GBDT原理介绍| 机器学习算法技术分享</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a5e065000e17e239404a67aaf63a98d9_1440w.png" alt="img"></p><p>Gradient boost</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">from sklearn<span class="hljs-selector-class">.ensemble</span> import AdaBoostClassifier<br>clf = <span class="hljs-built_in">AdaBoostClassifier</span>(n_estimators=<span class="hljs-built_in">len</span>(X_train),random_state=<span class="hljs-number">42</span>)<br>clf<span class="hljs-selector-class">.fit</span>(X_train, y_train)<br>y_pred = clf<span class="hljs-selector-class">.predict</span>(X_test)<br>from sklearn<span class="hljs-selector-class">.metrics</span> import accuracy_score<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;Accuracy: &#x27;</span>, accuracy_score(y_test, y_pred)</span></span>)<br></code></pre></td></tr></table></figure><h3 id="2-2-Xgboost"><a href="#2-2-Xgboost" class="headerlink" title="2.2 Xgboost"></a>2.2 Xgboost</h3><p>作为GBDT工程化的行驶，相比较GBDT直接使用差分，Xgboost对损失函数进行了二阶泰勒展开，并加入正则化项目来保证整体的最优解，用来权衡目标函数的下降和模型的复杂度来避免过拟合</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import xgboost as xgb<br>xgb_model = xgb.XGBClassifier(<span class="hljs-attribute">objective</span>=<span class="hljs-string">&quot;binary:logistic&quot;</span>, <span class="hljs-attribute">random_state</span>=42)<br>xgb_model.fit(X_train, y_train)<br>y_pred = xgb_model.predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Accuracy: &#x27;</span>, accuracy_score(y_test, y_pred))<br></code></pre></td></tr></table></figure><h3 id="2-3-LightGBM"><a href="#2-3-LightGBM" class="headerlink" title="2.3 LightGBM"></a>2.3 LightGBM</h3><p>作为另外一种GBDT工程化实现的方式，其具有更快的训练速度、效率、低内存的使用和更好的性能，更容易训练在大型文件夹中。其主要用到了两种非常独特的方法</p><ul><li>单边梯度采样，Gradient-based on side sampling，利用数学证明是可行的</li></ul><p>不同的数据实例在计算信息增益时具有不同的功能。梯度大的实例（即培训不足的示例）将为信息获取做出更多贡献。GOSS保留具有大梯度（例如，大于预定义阈值或顶部百分位数）的实例，并且仅随机删除具有小梯度的实例，以保持信息增益估计的准确性。这种处理可以导致比均匀随机抽样更准确的增益估计，具有相同的目标采样率，特别是当信息增益值范围很大时。</p><ul><li>特征捆绑，Exclusive feature bundling technique</li></ul><p>高维数据通常非常稀疏，这为我们提供了设计一种几乎无损的方法来减少特征数量的可能性。具体来说，在稀疏特征空间中，许多特征是相互排斥的，即它们从不同时使用非零值。独家功能可以安全地捆绑到单个功能中（称为独家功能包）。因此，直方图构建的复杂性从O（数据×特征）变为O（数据×捆绑），而bundle&lt;&lt;特征。因此，在不影响准确性的情况下，提高了培训框架的速度。</p><p>怎么说呢？这些在使用的过程中通常并不会使用到。其中非常有意思的为了压缩信息，会使用直方图的方式来聚集数据和离散数据。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8bdf6f18fd668f7b93010da6b95ce304_1440w.png" alt="img"></p><p>lightGBM伪代码</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> sklearn.metrics import log_loss, roc_auc_score<br><span class="hljs-comment"># Set parameters</span><br>params = &#123;<br>    <span class="hljs-string">&#x27;boosting_type&#x27;</span>: <span class="hljs-string">&#x27;gbdt&#x27;</span>,<br>    <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;binary&#x27;</span>,<br>    <span class="hljs-string">&#x27;metric&#x27;</span>: &#123;<span class="hljs-string">&#x27;binary_logloss&#x27;</span>, <span class="hljs-string">&#x27;auc&#x27;</span>&#125;,<br>    <span class="hljs-string">&#x27;num_leaves&#x27;</span>: 31,<br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: 0.05,<br>    <span class="hljs-string">&#x27;feature_fraction&#x27;</span>: 0.9,<br>    <span class="hljs-string">&#x27;bagging_fraction&#x27;</span>: 0.8,<br>    <span class="hljs-string">&#x27;bagging_freq&#x27;</span>: 5,<br>    <span class="hljs-string">&#x27;verbose&#x27;</span>: 0<br>&#125;<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Start training...&#x27;</span>)<br><span class="hljs-comment"># train</span><br>gbm = lgb.train(params, train_data, <span class="hljs-attribute">num_boost_round</span>=20, <span class="hljs-attribute">valid_sets</span>=test_data, <span class="hljs-attribute">early_stopping_rounds</span>=5)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Save model...&#x27;</span>)<br><span class="hljs-comment"># save model to file</span><br>gbm.save_model(<span class="hljs-string">&#x27;model.txt&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Start predicting...&#x27;</span>)<br><span class="hljs-comment"># predict</span><br>y_pred = gbm.predict(X_test, <span class="hljs-attribute">num_iteration</span>=gbm.best_iteration)<br><span class="hljs-comment"># eval</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The logloss of prediction is:&#x27;</span>, log_loss(y_test, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The auc of prediction is:&#x27;</span>, roc_auc_score(y_test, y_pred))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Plot feature importances...&#x27;</span>)<br><span class="hljs-comment"># feature importances</span><br>ax = lgb.plot_importance(gbm)<br>ax = lgb.plot_importance(gbm,<span class="hljs-attribute">title</span>=<span class="hljs-string">&#x27;Feature importance&#x27;</span>, <span class="hljs-attribute">xlabel</span>=<span class="hljs-string">&#x27;Feature importance&#x27;</span>, <span class="hljs-attribute">ylabel</span>=<span class="hljs-string">&#x27;Features&#x27;</span>, figsize=(10, 10))<br>plt.show()<br></code></pre></td></tr></table></figure><h3 id="2-4-解释性"><a href="#2-4-解释性" class="headerlink" title="2.4 解释性"></a>2.4 解释性</h3><p>对于tree-based模型最直观的是可以输出树作为最终结果，在树的节点上我们可以给出计算结果，包括：</p><ol><li>Feature importance，基于之前的不纯度指标，或者特征节点的分裂次数来计算</li><li>Shap，是可以计算出Tree-based模型的系数的一种方法，但是比较麻烦</li><li>Null importance，参考统计学中的一个基本假设。如果一个特征被随机数代替之后结果并没有出现明显差异，说明这个特征并没有什么用。称为洗牌（permutation）</li></ol><h3 id="0x03-实例：M5-Forecasting-Accuracy"><a href="#0x03-实例：M5-Forecasting-Accuracy" class="headerlink" title="0x03 实例：M5-Forecasting-Accuracy"></a>0x03 实例：M5-Forecasting-Accuracy</h3><h3 id="3-1-问题描述"><a href="#3-1-问题描述" class="headerlink" title="3.1 问题描述"></a>3.1 问题描述</h3><p>Kaggle 比赛地址：<a href="https://www.kaggle.com/competitions/m5-forecasting-accuracy/data?select=sales_train_evaluation.csv">M5-Forecasting-Accuracy</a></p><p>原始数据项目地址：<a href="https://mofc.unic.ac.cy/m5-competition/">MOFC</a></p><p>数据主要为美国三个州、十个商店、3490个商品的销售数据，数据描述包括：</p><ol><li>日历文件（Calendar），给出week下标、星期、日、月、年、事件1名称、事件1类型、事件2名称、事件2类型、三个州是否允许商品券购买</li><li>销售数据（Sales-train），给出每个产品每天的销售量、分类、商品、状态和日期</li><li>销售价格（Sell-price），对应日期商品的价格</li></ol><p>最终的考核指标除了单个商品销售数量的预测，还包括不同层级聚合后的结果评价（比如三个州商品总和预测精度等等。</p><h3 id="3-2-特征工程"><a href="#3-2-特征工程" class="headerlink" title="3.2 特征工程"></a>3.2 特征工程</h3><p>首先对于单个商品来看，是一个时间序列处理问题，主要是采用原有数据特征来分析。从销售事件序列上来看我们可以采集滞后项目（Lag）、季节特征（season）；从日历中我们可以设计天特征、事件类型等特征；从价格中我们设计价格特征等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-comment"># 日历包括：日期、week id、weekday、month、year、event name、type、second event name、type、是否允许购买CA、TX、WI</span><br><span class="hljs-comment">#  Supply nutrition assistance program SNAP 是否允许食品券购买</span><br>df_calendar = pd.read_csv(<span class="hljs-string">&#x27;../../data/m5-forecasting-accuracy/calendar.csv&#x27;</span>)<br><span class="hljs-comment"># 30490个产品，每个产品每天的销售量，departure id、分类id、商店id、状态id、日期、销售量</span><br><span class="hljs-comment"># 总共3090类产品、10个商店、3个州、每个商店的类别</span><br>df_sales_train_validation = pd.read_csv(<span class="hljs-string">&#x27;../../data/m5-forecasting-accuracy/sales_train_validation.csv&#x27;</span>)<br><span class="hljs-comment"># sell prices 包括：商店id、产品id、日期、价格</span><br>df_sell_prices = pd.read_csv(<span class="hljs-string">&#x27;../../data/m5-forecasting-accuracy/sell_prices.csv&#x27;</span>)<br>df_sample_submission = pd.read_csv(<span class="hljs-string">&#x27;../../data/m5-forecasting-accuracy/sample_submission.csv&#x27;</span>)<br><br><span class="hljs-comment"># 构建特征工程</span><br><span class="hljs-comment"># 1. 价格特征</span><br>d_cols = [c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> df_sales_train_validation.columns <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;d_&#x27;</span> <span class="hljs-keyword">in</span> c] <span class="hljs-comment"># sales data columns</span><br>x = df_sales_train_validation[d_cols].copy()<br><br>target_day = <span class="hljs-number">1914</span><br><br><span class="hljs-comment">#使用历史数据中最后的7天构造特征</span><br>local_range = <span class="hljs-number">7</span><br><br><span class="hljs-comment"># 由于使用前1913天的数据预测第1914天，历史数据与预测目标的距离只有1天，因此predict_distance=1</span><br>predict_distance = <span class="hljs-number">1</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_local_features</span>(<span class="hljs-params">target_day, predict_distance</span>):<br>    local_features = pd.DataFrame()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(local_range):<br>        local_features[<span class="hljs-string">&#x27;l_&#x27;</span>+<span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>)] = x[<span class="hljs-string">&#x27;d_&#x27;</span>+<span class="hljs-built_in">str</span>(target_day - i - predict_distance)].astype(<span class="hljs-built_in">float</span>)<br>    l_cols = [<span class="hljs-string">&#x27;l_&#x27;</span>+<span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(local_range)]<br>    <span class="hljs-keyword">return</span> local_features[l_cols]<br><br><span class="hljs-built_in">print</span>(get_local_features(target_day,predict_distance))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_local_accumulated_feature</span>(<span class="hljs-params">target_day, predict_distance</span>):<br>    local_accumulated_feature = pd.DataFrame()<br>    local_accumulated_feature[<span class="hljs-string">&#x27;la_1&#x27;</span>] = x[<span class="hljs-string">&#x27;d_&#x27;</span>+<span class="hljs-built_in">str</span>(target_day - predict_distance)].astype(<span class="hljs-built_in">float</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, local_range):<br>        local_accumulated_feature[<span class="hljs-string">&#x27;la_&#x27;</span>+<span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>)] = x[<span class="hljs-string">&#x27;d_&#x27;</span>+<span class="hljs-built_in">str</span>(target_day - i - predict_distance)].astype(<span class="hljs-built_in">float</span>) +\<br>            local_accumulated_feature[<span class="hljs-string">&#x27;la_&#x27;</span> + <span class="hljs-built_in">str</span>(i)]<br>    la_cols = [<span class="hljs-string">&#x27;la_&#x27;</span>+<span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(local_range)]<br>    <span class="hljs-keyword">return</span> local_accumulated_feature[la_cols]<br><br>get_local_accumulated_feature(target_day, predict_distance)<br><br>tx = x[d_cols[::-<span class="hljs-number">1</span>]].cumsum(axis=<span class="hljs-number">1</span>)<br>used_history_distances = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">14</span>, <span class="hljs-number">21</span>, <span class="hljs-number">28</span>, <span class="hljs-number">42</span>, <span class="hljs-number">56</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_accumulated_features</span>(<span class="hljs-params">target_day, predict_distance</span>):<br>    long_term_features = pd.DataFrame()<br>    <span class="hljs-keyword">for</span> distance <span class="hljs-keyword">in</span> used_history_distances:<br>        long_term_features[<span class="hljs-string">&#x27;la_&#x27;</span> + <span class="hljs-built_in">str</span>(distance)] = tx[<span class="hljs-string">&#x27;d_&#x27;</span> + <span class="hljs-built_in">str</span>(target_day - distance - predict_distance + <span class="hljs-number">1</span>)].astype(<span class="hljs-built_in">float</span>)<br>    la_cols = [<span class="hljs-string">&#x27;la_&#x27;</span> + <span class="hljs-built_in">str</span>(distance) <span class="hljs-keyword">for</span> distance <span class="hljs-keyword">in</span> used_history_distances]<br>    <span class="hljs-keyword">return</span> long_term_features[la_cols]<br><br>get_accumulated_features(target_day, predict_distance)<br><br><span class="hljs-comment"># 构建周期特征</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_period_sale</span>(<span class="hljs-params">target_day, predict_distance</span>):<br>    period = <span class="hljs-number">7</span><br>    i_start = (predict_distance + period - <span class="hljs-number">1</span>) // period<br>    period_sale = pd.DataFrame()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">8</span>):<br>        cur_day = target_day - (i + i_start) * period<br>        period_sale[<span class="hljs-string">&#x27;p_&#x27;</span>+<span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>)] = x[<span class="hljs-string">&#x27;d_&#x27;</span> + <span class="hljs-built_in">str</span>(cur_day)].astype(<span class="hljs-built_in">float</span>)<br>    <span class="hljs-keyword">return</span> period_sale<br><br>get_period_sale(target_day, predict_distance)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_period_features</span>(<span class="hljs-params">target_day, predict_distance</span>):<br>    tx_period = get_period_sale(target_day, predict_distance)<br>    tx_period = tx_period.cumsum(axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> tx_period<br><br>get_period_features(target_day, predict_distance)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_history_features</span>(<span class="hljs-params">target_day, predict_distance</span>):<br>    <span class="hljs-keyword">return</span> pd.concat([get_accumulated_features(target_day, predict_distance),<br>                      get_period_features(target_day, predict_distance)], axis=<span class="hljs-number">1</span>)<br><br>get_history_features(target_day, predict_distance)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f9ad5490f92279f0e4363cd736756c02_1440w.png" alt="img"></p><p>特征构造思路</p><h3 id="3-3-训练与调参"><a href="#3-3-训练与调参" class="headerlink" title="3.3 训练与调参"></a>3.3 训练与调参</h3><p>默认参数包括</p><p>估计类型boosting: ‘gbdt’</p><p>学习模版objective：‘regression‘</p><p>评价指标metric：’rmse‘</p><p>由于集成模型中的参数比较多，调参力度可能比较大。以lightGBM的params为例子，参考知乎中的文章，可以使用</p><ol><li>经验参数：学习率（learning rate）、集成的迭代次数（n_estimator）、节点分裂的最小增益(min_split_gain)、叶子上最小数据量(min_child_sample)、叶子上最小的hessian和(min_child_weight)</li><li>模型最大的深度(max_depth)、叶子节点数量(num_leaves)、正则化参数(reg_alpha,reg_lambda)，数据采样（subsample or bagging_fraction，colsample_bytree or feature_fraction)</li></ol><p>具体调整参数过程可以参考【6】</p><h3 id="0x04-总结"><a href="#0x04-总结" class="headerlink" title="0x04 总结"></a>0x04 总结</h3><h3 id="4-1-经典偏差方差权衡"><a href="#4-1-经典偏差方差权衡" class="headerlink" title="4.1 经典偏差方差权衡"></a>4.1 经典偏差方差权衡</h3><p>对于模型的效果评价分为偏差（Bias）和方差（Variance），其中偏差反映的是模型的准确程度，比如说GT是1，偏差预测的结果为（2.1，2.2，1.9），这种属于高偏差，低方差，说明模型是欠拟合，此时模型还没有学习到模型背后的假设。而当GT是（1.2，1.3），模型的预测结果为（1.2，1.3）说明模型具有低偏差但是具有高方差，说明此时模型不仅学习到了假设同时也包含了噪声，导致过拟合</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1242c47a37d093bed5faee17ab25164c_1440w.png" alt="img"></p><p>modelcomplexity</p><p>在集成学习如何解决 bias- variance tradeoff</p><p>bagging and variance：</p><p>通过学习的方式，可以看出bagging方式训练了很多模型，但是并没有显著的提高模型的效果，所以有可能所有的模型的效果并不是很好，但是通过大数定律来提高集成模型的效果，也就是减少模型的variance</p><p>boosting and bias：</p><p>boosting的方式也就是在不断的学习残差或者梯度的过程，是在不断的减少模型的bias</p><h3 id="4-2-如何学习"><a href="#4-2-如何学习" class="headerlink" title="4.2 如何学习"></a>4.2 如何学习</h3><p>获取模型算法的设计过程无关，但是前期的特征工程和对比分析也非常有意思！</p><p>[<a href="https://microsoft.github.io/ai-edu/%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/B16-%E5%9F%BA%E4%BA%8ELightGBM%E7%9A%84%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/">1]基于LightGBM的时间序列预测</a></p><p>[<a href="https://zhuanlan.zhihu.com/p/531784901">2]LightGBM回归预测模型构建</a></p><p>[<a href="https://www.kaggle.com/discussions/general/264327">3What is Light GBM? Advantages &amp; Disadvantages? Light GBM vs XGBoost?]</a></p><p>[<a href="https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/">4]LightGBM</a></p><p>[<a href="https://zhuanlan.zhihu.com/p/376485485">5]lightGBM调参教程</a></p><p>[<a href="https://www.cnblogs.com/bjwu/p/9307344.html">6]非常棒的调参过程</a></p><p>[<a href="https://www.section.io/engineering-education/ensemble-bias-var/">7]Bias and variance</a></p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>梯度提升树</tag>
      
      <tag>面试技巧</tag>
      
      <tag>Kaggle 比赛</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法基础 vol8</title>
    <link href="/posts/112796ee.html"/>
    <url>/posts/112796ee.html</url>
    
    <content type="html"><![CDATA[<p>重新回顾二分查找必要的细节</p><h3 id="0x01-算法想法"><a href="#0x01-算法想法" class="headerlink" title="0x01 算法想法"></a>0x01 算法想法</h3><p>二分查找（Binary search algorithm），是在数组章节第一次体会到算法精神的算法。其基本思想是：先确定待查元素的范围，之后以某种方式缩小范围，直到到达某种条件为支。很while</p><p>其基本的算法思想是“减而治之”，和分而治之类似，但是更加的巧妙的是在于如何减小问题的规模或者排除问题。二分查找算法流程为：</p><ol><li>每次查找从数组的中间元素开始，如果中奖元素正好是要查找的元素，则搜索过程结束</li><li>如果中间元素大于或者小于特定元素，那么选择对应的左边或者右边查找</li><li>如果为空则说明找不到</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9ea36a5a54d4336159f9925a192540bc_1440w.png" alt="img"></p><p>二分查找过程</p><h3 id="0x02-魔鬼细节"><a href="#0x02-魔鬼细节" class="headerlink" title="0x02 魔鬼细节"></a>0x02 魔鬼细节</h3><p>想法并不困难，以最基本的方式从一个升序的数组中查找某一元素：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># arr = [1,2,3,4,5,6,7,8]</span><br><span class="hljs-comment"># target = 3</span><br>left = <span class="hljs-number">0</span><br><span class="hljs-literal">right</span> = <span class="hljs-built_in">len</span>(arr)<span class="hljs-number">-1</span><br><span class="hljs-keyword">while</span> left&lt;=<span class="hljs-literal">right</span>:<br>  <span class="hljs-keyword">mid</span> = left+(<span class="hljs-literal">right</span>-left<span class="hljs-comment">)//2</span><br>  <span class="hljs-keyword">if</span> arr[<span class="hljs-keyword">mid</span>]&gt;target:<br>    <span class="hljs-literal">right</span> = <span class="hljs-keyword">mid</span><span class="hljs-number">-1</span><br>  <span class="hljs-keyword">if</span> arr[<span class="hljs-keyword">mid</span>]&lt;target:<br>    left = <span class="hljs-keyword">mid</span>+<span class="hljs-number">1</span><br>  <span class="hljs-keyword">if</span> arr[<span class="hljs-keyword">mid</span>]==target:<br>    <span class="hljs-literal">return</span> <span class="hljs-keyword">mid</span><br>  <span class="hljs-literal">return</span> None<br></code></pre></td></tr></table></figure><p>同样也可以这样写</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># arr = [1,2,3,4,5,6,7,8]</span><br><span class="hljs-comment"># target = 3</span><br>left = <span class="hljs-number">0</span><br><span class="hljs-literal">right</span> = <span class="hljs-built_in">len</span>(arr)<br><span class="hljs-keyword">while</span> left&lt;<span class="hljs-literal">right</span>:<br>  <span class="hljs-keyword">mid</span> = left+(<span class="hljs-literal">right</span>-left<span class="hljs-comment">)//2</span><br>  <span class="hljs-keyword">if</span> arr[<span class="hljs-keyword">mid</span>]&gt;target:<br>    <span class="hljs-literal">right</span> = <span class="hljs-keyword">mid</span><br>  <span class="hljs-keyword">if</span> arr[<span class="hljs-keyword">mid</span>]&lt;target:<br>    left = <span class="hljs-keyword">mid</span>+<span class="hljs-number">1</span><br>  <span class="hljs-keyword">if</span> arr[<span class="hljs-keyword">mid</span>]==target:<br>    <span class="hljs-literal">return</span> <span class="hljs-keyword">mid</span><br>  <span class="hljs-literal">return</span> None<br></code></pre></td></tr></table></figure><p>这里魔鬼的地方在于</p><ol><li>选择的初始状态的选择，left还是right</li><li>以及最终条件的判断，left小于right 还是 left 小于等于 right</li><li>以及每一步比较之后的选择，left &#x3D; mid-1 还是 left &#x3D; mid</li></ol><p>其实如果刷题的话可以发现二分查找主要分为三种搜索：</p><ol><li>搜索给定值的下标</li><li>搜索最左边的值</li><li>搜索最右边的值（可以转换为第二种）</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4e4f91968d576d445747d4f52bea3135_1440w.png" alt="img"></p><p>常见搜索</p><p>在二分查找缩小空间的方式中，我们唯一能做到的只有排除我们确定没有的空间。比如对于第一类寻找目标值，如果arr[mid]确定小于target，我们相信一定不在这个区间，那么我们可以设计为left &#x3D; mid+1，因为包含mid的区间我们知道一定不存在。而对于第二类寻找最左边的值，如果arr[mid]大于等于target我们也不能确定是否一定在mid-1的区间中，因此我们这个时候需要选择开区间，且设置right &#x3D; mid而不是right&#x3D;mid-1</p><h3 id="0x03-反思"><a href="#0x03-反思" class="headerlink" title="0x03 反思"></a>0x03 反思</h3><p>搜索解空间其实只有两种，一种是确定我们剩下的，另外一种是排除不存在的。</p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>二分查找</tag>
      
      <tag>数组</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习 Vol8 ｜ transformer 总结和解释</title>
    <link href="/posts/c2dee8e1.html"/>
    <url>/posts/c2dee8e1.html</url>
    
    <content type="html"><![CDATA[<p>这次详细介绍一下transformer框架，以及潜在的想法。在我心中这个是和Alexnet一样的文章。主要从基于RNN的encoder-decoder缺陷中引入attention，并从并行的角度完全引入attention。在第二章介绍attention的机制，在第三章介绍相关的论文细节。在第四章介绍基于torch的实现过程。参考了很多文章！</p><h3 id="0x01-从RNN到Attention的Encoder-Decoder结构"><a href="#0x01-从RNN到Attention的Encoder-Decoder结构" class="headerlink" title="0x01 从RNN到Attention的Encoder- Decoder结构"></a>0x01 从RNN到Attention的Encoder- Decoder结构</h3><p>在这里还没有介绍Attention之前，请把它看作是一种计算权重的方式！第二章会详细介绍，这里不了解attention不会本章的理解。</p><p>在时序信息处理中RNN、GRU、LSTM均体现出卓越的优势，通过隐藏状态可以捕获之前的信息，通过时间迁移（Time shift）来不断的预测下一个时间，通过时间梯度反向传播来弥补其中梯度的传播。同时在RNN系列的基础上，Encoder-Decoder架构完善了序列生成的模型框架，在DNA预测、机器翻译转录中取得非常好的效果。</p><p>那么什么是Encoder-Decoder框架呢？最基本、最原始的形式沐神在《动手学深度学习》中已经给出</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-cd7f24a0c38046ebcf917098c5bf3863_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>但是如果直接从基本知识跳跃到《Attention is all you need》论文中会发现存在一个GAP，因为在Transformer提出之前，已经有Attention应用在编码器-解码器框架之中，我们会发现上述编码器和解码器之间的状态太过简短，如何提供一种高效的状态传递方式是值得被研究的。于是在机器翻译中Attention被引入，同是存储对应的隐藏状态</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0977700629d0a5d848c3bf2c1d31c30e_1440w.png" alt="img"></p><p>Encoder- Decoder+attention</p><p>（这里是我的想法）虽然这样引入Attention可以保证信息由Encoder传递给Decoder，但是由于RNN时间步进的限制，无法并行计算导致模型依旧训练缓慢。</p><p>那么是否有一种方式：可以得到隐藏表示向量，同时可以并行计算？第一种想法是CNN，感觉应该有相关方面的研究，第二种方法是Attention，也就是本文提出的Transformer结构。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-fac942f68181c809efc96cc4ce340e86_1440w.png" alt="img"></p><p>Transformer</p><h3 id="0x02-从B站搜索到Attention维度分析"><a href="#0x02-从B站搜索到Attention维度分析" class="headerlink" title="0x02 从B站搜索到Attention维度分析"></a>0x02 从B站搜索到Attention维度分析</h3><h3 id="2-1-视频检索到Attention"><a href="#2-1-视频检索到Attention" class="headerlink" title="2.1 视频检索到Attention"></a>2.1 视频检索到Attention</h3><p>在介绍Attention中需要包括三个部分：查询（Query）、键（Key）、值（Value）。这里以B站的视频为例子，假设这里只有seq-len&#x3D;5个视频，是我们的value，每一个视频会对应不同的key，当我们给出我们的query，对于不同的query会分别与这些key进行计算得到一个权重，我们由这些权重作为相关度进行排序。（如有错误可以指出）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-40c22e33946b72dd4eae89bba923b947_1440w.png" alt="img"></p><p>视频检索Attention</p><h3 id="2-2-Attention维度分析"><a href="#2-2-Attention维度分析" class="headerlink" title="2.2 Attention维度分析"></a>2.2 Attention维度分析</h3><p>虽然这样迷迷糊糊的，最终计算的过程中都是一样的，在基于Torch的attention计算中，我们当然不能手动实现QKV的计算，因此我们简单的将原始数据输入，由内部确认权重来转换成为QKV即可。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-812829faabb3450bc9e11c5b5d769ada_1440w.png" alt="img"></p><p>维度分析</p><h3 id="2-3-Multi-head-attention"><a href="#2-3-Multi-head-attention" class="headerlink" title="2.3 Multi-head attention"></a>2.3 Multi-head attention</h3><p>通过上述算法我们得到一个向量，其形状一般为原始维度（N，D），这里和CNN的单通道卷积很相似，但是我们知道多通道卷积在卷积神经网络中有一定的效果，这样可以一定程度上以ensemble的思路来提高结果的准确性，因此我们可以设置Multi-head attention作为多通道输入。其具体实现是将上述的QKV拆分为按照nhead的数量来拆分，分别计算attention之后在concat</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1d6ec25bdd4945788a19cf588ef34fa3_1440w.png" alt="img"></p><p>multi-head attention</p><h3 id="2-4-Attention-mask"><a href="#2-4-Attention-mask" class="headerlink" title="2.4 Attention mask"></a>2.4 Attention mask</h3><p>在我们计算最后的输出的时候，在得到权重矩阵的时候要知道在decoder情况下并不能获取之后的信息，也就是在我们没有看到的数据中我们是不能计算权重的，因此我们可以设置一个可选的（Optional）来作为计算。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1862876f7fd714fd287e1e7fffa68fa1_1440w.png" alt="img"></p><p>Attention Maskpng</p><h3 id="2-5-Attention-layer实现"><a href="#2-5-Attention-layer实现" class="headerlink" title="2.5 Attention layer实现"></a>2.5 Attention layer实现</h3><p>我是手撕不了，网上有很多大神手撕的案例。但是我还是推荐观察Torch的官方代码。</p><h3 id="0x03-魔鬼细节：Embedding，LayerNorm，Add"><a href="#0x03-魔鬼细节：Embedding，LayerNorm，Add" class="headerlink" title="0x03 魔鬼细节：Embedding，LayerNorm，Add"></a>0x03 魔鬼细节：Embedding，LayerNorm，Add</h3><p>上述已经大体介绍Transformer的结构，但是这里依旧存在一些细节。</p><p>Embedding：将原始变量转换成为可以训练的向量，通常需要做一层语义embedding，但是除此之外，因为Transformer无法感知时序信息，通常需要Position Embedding来覆盖两者之间的顺序</p><p>LayerNorm：相比较BatchNorm可以更加容易推理，同时主要是为了模型训练的时候可以收敛。具体原因有相关论文的梯度分析</p><p>Add：参考Resnet的分析！yyds</p><h3 id="0x04-基于Torch实现Transformer"><a href="#0x04-基于Torch实现Transformer" class="headerlink" title="0x04 基于Torch实现Transformer"></a>0x04 基于Torch实现Transformer</h3><p>主要是实现encoder layer、decoder layer，encoder、decoder</p><h3 id="4-1-Embedding"><a href="#4-1-Embedding" class="headerlink" title="4.1 Embedding"></a>4.1 Embedding</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> math<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">TokenEmbedding</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>, <span class="hljs-title">vocab_size</span>,<span class="hljs-title">embedding_size</span>) -&gt; <span class="hljs-type">None</span>:</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.embedding = nn.<span class="hljs-type">Embedding</span>(<span class="hljs-title">vocab_size</span>,<span class="hljs-title">embedding_size</span>)</span><br><span class="hljs-class">        self.embedding_size = embedding_size</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        return self.embedding(<span class="hljs-title">x</span>.<span class="hljs-title">long</span>())*math.sqrt(<span class="hljs-title">self</span>.<span class="hljs-title">embedding_size</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">PositionalEncoding</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>,<span class="hljs-title">dimen</span>,<span class="hljs-title">dropout</span>=0.1,<span class="hljs-title">max_len</span>=5000):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.dropout = nn.<span class="hljs-type">Dropout</span>(<span class="hljs-title">p</span>=<span class="hljs-title">dropout</span>)</span><br><span class="hljs-class">        <span class="hljs-type">PE</span> = torch.zeros(<span class="hljs-title">max_len</span>,<span class="hljs-title">dimen</span>)</span><br><span class="hljs-class">        position = torch.arange(0,<span class="hljs-title">max_len</span>,<span class="hljs-title">dtype</span>=<span class="hljs-title">torch</span>.<span class="hljs-title">float</span>).unsqueeze(1)</span><br><span class="hljs-class">        div_term = torch.exp(<span class="hljs-title">torch</span>.<span class="hljs-title">arange</span>(0,<span class="hljs-title">dimen</span>,2).float() * (-<span class="hljs-title">math</span>.<span class="hljs-title">log</span>(10000.0) / dimen))</span><br><span class="hljs-class">        <span class="hljs-type">PE</span>[:,0::2] = torch.sin(<span class="hljs-title">position</span> * <span class="hljs-title">div_term</span>)</span><br><span class="hljs-class">        <span class="hljs-type">PE</span>[:,1::2] = torch.cos(<span class="hljs-title">position</span> * <span class="hljs-title">div_term</span>)</span><br><span class="hljs-class">        <span class="hljs-type">PE</span> = <span class="hljs-type">PE</span>.unsqueeze(0).transpose(0,1)</span><br><span class="hljs-class">        self.register_buffer(&#x27;<span class="hljs-type">PE</span>&#x27;,<span class="hljs-type">PE</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        x = x + self.<span class="hljs-type">PE</span>[:x.size(0),:]</span><br><span class="hljs-class">        return self.dropout(<span class="hljs-title">x</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">x = torch.tensor([[1,2,3,4,5],[5,6,7,8,9]],<span class="hljs-title">dtype</span>=<span class="hljs-title">torch</span>.<span class="hljs-title">float</span>)</span><br><span class="hljs-class">x = x.reshape(5,2) # note that the batch size is the second dimension</span><br><span class="hljs-class">vocab_size = 10</span><br><span class="hljs-class">embedding_size = 512</span><br><span class="hljs-class"></span><br><span class="hljs-class">token_embedding = <span class="hljs-type">TokenEmbedding</span>(<span class="hljs-title">vocab_size</span>,<span class="hljs-title">embedding_size</span>)</span><br><span class="hljs-class">x = token_embedding(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">print(&#x27;<span class="hljs-title">token</span> <span class="hljs-title">embedding</span> <span class="hljs-title">size</span> <span class="hljs-title">is</span> &#x27;,<span class="hljs-title">x</span>.<span class="hljs-title">shape</span>)</span><br><span class="hljs-class">pos_embedding = <span class="hljs-type">PositionalEncoding</span>(<span class="hljs-title">embedding_size</span>)</span><br><span class="hljs-class">x = pos_embedding(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">print(&#x27;<span class="hljs-type">Position</span> <span class="hljs-title">embedding</span> <span class="hljs-title">size</span> <span class="hljs-title">is&#x27;</span>,<span class="hljs-title">x</span>.<span class="hljs-title">shape</span>)</span><br><span class="hljs-class">print(&#x27;<span class="hljs-type">The</span> <span class="hljs-title">size</span> <span class="hljs-title">must</span> <span class="hljs-title">be</span> [<span class="hljs-title">seq_len</span>,<span class="hljs-title">batchsize</span>,<span class="hljs-title">embedding_size</span>]&#x27;)</span><br></code></pre></td></tr></table></figure><h3 id="4-2-Encode-layer-and-encoder"><a href="#4-2-Encode-layer-and-encoder" class="headerlink" title="4.2 Encode layer and encoder"></a>4.2 Encode layer and encoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerEncoderLayerStractch</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,dimen,nhead,dim_forward=<span class="hljs-number">2048</span>,dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.self_attn = nn.MultiheadAttention(dimen,nhead,dropout=dropout)<br>        <span class="hljs-variable language_">self</span>.dropoutAttn = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.norm1 = nn.LayerNorm(dimen)<br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(dimen,dim_forward)<br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(dim_forward,dimen)<br>        <span class="hljs-variable language_">self</span>.activation = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.norm2 = nn.LayerNorm(dimen)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x,mask=<span class="hljs-literal">None</span>,src_key_padding_mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        x: [seq_len,batch_size,embedding_size]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        x2,_ = <span class="hljs-variable language_">self</span>.self_attn(x,x,x,attn_mask=mask,key_padding_mask=src_key_padding_mask)<br>        <span class="hljs-comment"># x2: [seq_len,batch_size,embedding_size*nhead]</span><br>        x = x + <span class="hljs-variable language_">self</span>.dropoutAttn(x2)<br>        x = <span class="hljs-variable language_">self</span>.norm1(x)<br><br>        x2 = <span class="hljs-variable language_">self</span>.activation(<span class="hljs-variable language_">self</span>.linear1(x))<br>        x2 = <span class="hljs-variable language_">self</span>.linear2(<span class="hljs-variable language_">self</span>.dropout1(x2))<br>        x = x + <span class="hljs-variable language_">self</span>.dropout2(x2)<br><br>        x = <span class="hljs-variable language_">self</span>.norm2(x)<br>        <span class="hljs-keyword">return</span> x <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test the transformer encoder layer&#x27;</span>)<br>nhead = <span class="hljs-number">8</span><br>layer = TransformerEncoderLayerStractch(embedding_size,nhead)<br>x = layer(x)<br><span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_copy</span>(<span class="hljs-params">module,N</span>):<br>    <span class="hljs-keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N)])<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerEncoderStractch</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,encoder_layer,num_layers,norm=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.layers = nn.ModuleList([encoder_layer <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)])<br>        <span class="hljs-variable language_">self</span>.norm = norm<br>        <span class="hljs-variable language_">self</span>.num_layers = num_layers<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x,mask=<span class="hljs-literal">None</span>,src_key_padding_mask=<span class="hljs-literal">None</span></span>):<br>        output = x<br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.layers:<br>            output = layer(output,mask=mask,src_key_padding_mask=src_key_padding_mask)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.norm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            output = <span class="hljs-variable language_">self</span>.norm(output)<br>        <span class="hljs-keyword">return</span> output <br><br>encoder = TransformerEncoderStractch(layer,<span class="hljs-number">6</span>)<br>x = encoder(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test the transformer encoder&#x27;</span>)<br>loss = nn.MSELoss()<br><span class="hljs-built_in">print</span>(loss(x,encoder(x)))<br></code></pre></td></tr></table></figure><h3 id="4-3-Decoder-layer-和Decoder"><a href="#4-3-Decoder-layer-和Decoder" class="headerlink" title="4.3 Decoder layer 和Decoder"></a>4.3 Decoder layer 和Decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerDecoderLayerStractch</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,dimen,nhead,dim_forward=<span class="hljs-number">2048</span>,dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.self_attn = nn.MultiheadAttention(dimen,nhead,dropout=dropout)<br>        <span class="hljs-variable language_">self</span>.dropoutAttn = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.norm1 = nn.LayerNorm(dimen)<br>        <span class="hljs-variable language_">self</span>.multihead_attn = nn.MultiheadAttention(dimen,nhead,dropout=dropout)<br>        <span class="hljs-variable language_">self</span>.dropoutAttn2 = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.norm2 = nn.LayerNorm(dimen)<br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(dimen,dim_forward)<br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(dim_forward,dimen)<br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.norm3 = nn.LayerNorm(dimen)<br>        <span class="hljs-variable language_">self</span>.activation = nn.ReLU()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,tgt,memory,tgt_mask=<span class="hljs-literal">None</span>,memory_mask=<span class="hljs-literal">None</span>,tgt_key_padding_mask=<span class="hljs-literal">None</span>,memory_key_padding_mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27; </span><br><span class="hljs-string">        tgt = [tgt_len,batch_size,embedding_size]</span><br><span class="hljs-string">        memory = [seq_len,batch_size,embedding_size]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        tgt2,_ = <span class="hljs-variable language_">self</span>.self_attn(tgt,tgt,tgt,attn_mask=tgt_mask,key_padding_mask=tgt_key_padding_mask)<br>        tgt = tgt + <span class="hljs-variable language_">self</span>.dropoutAttn(tgt2)<br>        tgt = <span class="hljs-variable language_">self</span>.norm1(tgt)<br>        <span class="hljs-comment"># tgt = [tgt_len,batch_size,embedding_size]</span><br><br>        tgt2,_ = <span class="hljs-variable language_">self</span>.multihead_attn(tgt,memory,memory,attn_mask=memory_mask,key_padding_mask=memory_key_padding_mask)<br>        tgt = tgt + <span class="hljs-variable language_">self</span>.dropoutAttn2(tgt2)<br>        tgt = <span class="hljs-variable language_">self</span>.norm2(tgt)<br>        <span class="hljs-comment"># tgt = [tgt_len,batch_size,embedding_size]</span><br><br>        tgt2 = <span class="hljs-variable language_">self</span>.activation(<span class="hljs-variable language_">self</span>.linear1(tgt))<br>        tgt2 = <span class="hljs-variable language_">self</span>.linear2(<span class="hljs-variable language_">self</span>.dropout1(tgt2))<br>        tgt = tgt + <span class="hljs-variable language_">self</span>.dropout2(tgt2)<br><br>        tgt = <span class="hljs-variable language_">self</span>.norm3(tgt)<br>        <span class="hljs-comment"># tgt = [tgt_len,batch_size,embedding_size]</span><br>        <span class="hljs-keyword">return</span> tgt<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test the transformer decoder layer&#x27;</span>)<br>decoder_layer = TransformerDecoderLayerStractch(embedding_size,nhead)<br>x = decoder_layer(x,layer(x))<br><span class="hljs-built_in">print</span>(loss(x,decoder_layer(x,layer(x))))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerDecoderStractch</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,decoder_layer,num_layers,norm=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.layers = nn.ModuleList([copy.deepcopy(decoder_layer) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)])<br>        <span class="hljs-variable language_">self</span>.norm = norm<br>        <span class="hljs-variable language_">self</span>.num_layers = num_layers<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,tgt,memory,tgt_mask=<span class="hljs-literal">None</span>,memory_mask=<span class="hljs-literal">None</span>,tgt_key_padding_mask=<span class="hljs-literal">None</span>,memory_key_padding_mask=<span class="hljs-literal">None</span></span>):<br>        output = tgt<br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.layers:<br>            output = layer(output,memory,tgt_mask=tgt_mask,memory_mask=memory_mask,tgt_key_padding_mask=tgt_key_padding_mask,memory_key_padding_mask=memory_key_padding_mask)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.norm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            output = <span class="hljs-variable language_">self</span>.norm(output)<br>        <span class="hljs-keyword">return</span> output<br>encoder = TransformerEncoderStractch(layer,<span class="hljs-number">6</span>)<br>decoder = TransformerDecoderStractch(decoder_layer,<span class="hljs-number">6</span>)<br></code></pre></td></tr></table></figure><h3 id="4-4-完整的Transformer"><a href="#4-4-完整的Transformer" class="headerlink" title="4.4 完整的Transformer"></a>4.4 完整的Transformer</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># Bulid the transformer</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">MyTransformer</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>,<span class="hljs-title">dimen</span>=512,<span class="hljs-title">nhead</span>=8,<span class="hljs-title">num_encoder_layers</span>=6,<span class="hljs-title">num_decoder_layers</span>=6,<span class="hljs-title">dim_forward</span>=2048,<span class="hljs-title">dropout</span>=0.1):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        encoder_layer = <span class="hljs-type">TransformerEncoderLayerStractch</span>(<span class="hljs-title">dimen</span>,<span class="hljs-title">nhead</span>,<span class="hljs-title">dim_forward</span>,<span class="hljs-title">dropout</span>)</span><br><span class="hljs-class">        encoder_norm = nn.<span class="hljs-type">LayerNorm</span>(<span class="hljs-title">dimen</span>)</span><br><span class="hljs-class">        self.encoder = <span class="hljs-type">TransformerEncoderStractch</span>(<span class="hljs-title">encoder_layer</span>,<span class="hljs-title">num_encoder_layers</span>,<span class="hljs-title">encoder_norm</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        decoder_layer = <span class="hljs-type">TransformerDecoderLayerStractch</span>(<span class="hljs-title">dimen</span>,<span class="hljs-title">nhead</span>,<span class="hljs-title">dim_forward</span>,<span class="hljs-title">dropout</span>)</span><br><span class="hljs-class">        decoder_norm = nn.<span class="hljs-type">LayerNorm</span>(<span class="hljs-title">dimen</span>)</span><br><span class="hljs-class">        self.decoder = <span class="hljs-type">TransformerDecoderStractch</span>(<span class="hljs-title">decoder_layer</span>,<span class="hljs-title">num_decoder_layers</span>,<span class="hljs-title">decoder_norm</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        self._reset_parameters()</span><br><span class="hljs-class">        self.dimen = dimen</span><br><span class="hljs-class">        self.nhead = nhead</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">src</span>,<span class="hljs-title">tgt</span>,<span class="hljs-title">src_mask</span>=<span class="hljs-type">None</span>,<span class="hljs-title">tgt_mask</span>=<span class="hljs-type">None</span>,<span class="hljs-title">memory_mask</span>=<span class="hljs-type">None</span>,<span class="hljs-title">src_key_padding_mask</span>=<span class="hljs-type">None</span>,<span class="hljs-title">tgt_key_padding_mask</span>=<span class="hljs-type">None</span>,<span class="hljs-title">memory_key_padding_mask</span>=<span class="hljs-type">None</span>):</span><br><span class="hljs-class">        memory = self.encoder(<span class="hljs-title">src</span>,<span class="hljs-title">mask</span>=<span class="hljs-title">src_mask</span>,<span class="hljs-title">src_key_padding_mask</span>=<span class="hljs-title">src_key_padding_mask</span>)</span><br><span class="hljs-class">        output = self.decoder(<span class="hljs-title">tgt</span>,<span class="hljs-title">memory</span>,<span class="hljs-title">tgt_mask</span>=<span class="hljs-title">tgt_mask</span>,<span class="hljs-title">memory_mask</span>=<span class="hljs-title">memory_mask</span>,<span class="hljs-title">tgt_key_padding_mask</span>=<span class="hljs-title">tgt_key_padding_mask</span>,<span class="hljs-title">memory_key_padding_mask</span>=<span class="hljs-title">memory_key_padding_mask</span>)</span><br><span class="hljs-class">        return output</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def _reset_parameters(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        for p in self.parameters():</span><br><span class="hljs-class">            if p.dim() &gt; 1:</span><br><span class="hljs-class">                nn.init.xavier_uniform_(<span class="hljs-title">p</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def generate_square_subsequent_mask(<span class="hljs-title">self</span>,<span class="hljs-title">sz</span>):</span><br><span class="hljs-class">        mask = (<span class="hljs-title">torch</span>.<span class="hljs-title">triu</span>(<span class="hljs-title">torch</span>.<span class="hljs-title">ones</span>(<span class="hljs-title">sz</span>,<span class="hljs-title">sz</span>)) == 1).transpose(0,1)</span><br><span class="hljs-class">        mask = mask.float().masked_fill(<span class="hljs-title">mask</span> == 0,<span class="hljs-title">float</span>(&#x27;-<span class="hljs-title">inf&#x27;</span>)).masked_fill(<span class="hljs-title">mask</span> == 1,<span class="hljs-title">float</span>(0.0))</span><br><span class="hljs-class">        return mask</span><br></code></pre></td></tr></table></figure><h3 id="4-5-测试"><a href="#4-5-测试" class="headerlink" title="4.5 测试"></a>4.5 测试</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs stylus">seq_len = <span class="hljs-number">12</span><br>batch_size = <span class="hljs-number">2</span><br>dimen = <span class="hljs-number">128</span><br>tag_len = <span class="hljs-number">10</span><br>nhead = <span class="hljs-number">8</span><br><span class="hljs-selector-tag">input</span> = torch<span class="hljs-selector-class">.randn</span>(seq_len,batch_size,dimen)<br>target = torch<span class="hljs-selector-class">.randn</span>(tag_len,batch_size,dimen)<br><br>model = <span class="hljs-built_in">MyTransformer</span>(dimen=dimen,nhead=nhead,num_decoder_layers=<span class="hljs-number">6</span>,num_encoder_layers=<span class="hljs-number">6</span>,dim_forward=<span class="hljs-number">2048</span>,dropout=<span class="hljs-number">0.1</span>)<br><br>tgt_mask = model<span class="hljs-selector-class">.generate_square_subsequent_mask</span>(tag_len)<br>out = <span class="hljs-built_in">model</span>(<span class="hljs-selector-tag">input</span>,target,tgt_mask=tgt_mask)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;Test the transformer&#x27;</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(loss(out,model(input,target,tgt_mask=tgt_mask)</span></span>))<br></code></pre></td></tr></table></figure><p><a href="https://zhuanlan.zhihu.com/[https://zhuanlan.zhihu.com/p/624740065](https://zhuanlan.zhihu.com/p/624740065)">transform模型的参数量、计算量规模估算</a></p><p><a href="https://zhuanlan.zhihu.com/[https://www.ylkz.life/deeplearning/p10553832/](https://www.ylkz.life/deeplearning/p10553832/)">The post is all you need —- 月来客栈</a></p><p><a href="https://zhuanlan.zhihu.com/[https://huggingface.co/blog/zh/time-series-transformers](https://huggingface.co/blog/zh/time-series-transformers)">使用Transformer进行概率时间序列预测</a></p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>attention、</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习 Vol7 ｜ 如何理解 rnn 中的循环</title>
    <link href="/posts/3af655a9.html"/>
    <url>/posts/3af655a9.html</url>
    
    <content type="html"><![CDATA[<p>相比较MLP和CNN，RNN结构仿佛尤其的神秘。相比较更加直观的可视化，实现端到端的输出，包含隐藏状态（Hidden state）的时序建模方式对于RNN代码的理解真是尤其的困难。虽然Torch为我们提供封装好函数，但是我更加希望去探索其封装借口背后的结构，来提高自己的理解能力。</p><h3 id="0x01-缘起"><a href="#0x01-缘起" class="headerlink" title="0x01 缘起"></a>0x01 缘起</h3><p>相比较MLP和CNN，RNN结构仿佛尤其的神秘。相比较更加直观的可视化，实现端到端的输出，包含隐藏状态（Hidden state）的时序建模方式对于RNN代码的理解真是尤其的困难。虽然Torch为我们提供封装好函数，但是我更加希望去探索其封装借口背后的结构，来提高自己的理解能力。</p><h3 id="0x02-数学公式"><a href="#0x02-数学公式" class="headerlink" title="0x02 数学公式"></a>0x02 数学公式</h3><p>对于神经网络的计算方式可以考虑层层递进，包括单个神经元的前向传播、单个神经元的反向梯度下降、批量梯度下降。由此来理解它是如何使用的、如何学习的以及如何更好的工程化学习。</p><h3 id="2-1-基本思想"><a href="#2-1-基本思想" class="headerlink" title="2.1 基本思想"></a>2.1 基本思想</h3><p>RNN、LSTM、GRU等等这类循环神经网络更多的是参考如何应用在序列的数据中，参考沐神的解释，这一类网络结构的特点优势包括：</p><ol><li>跨时间权重共享</li><li>时间平移不对称</li><li>适用不同长度</li></ol><p>这里有点太晦涩了。当我们以时间序列预测（Time series forecasting）的方式来理解一个问题，我们如何从t的状态来预测t+1，经过分析我们会知道t+1状态并不完全取决于t。比如同一支股票在相同的价格均会存在下跌和上涨的状态，因此我们会希望另外一个隐藏值H来表示之前所有序列的状态，但是这个隐藏状态可能不能太长，导致隐含信息被平滑，也不能太短到无法提供信息。因此我们需要选择合适的WINDOW-SIZE。由此我们的估计值应该包括</p><p>𝑦𝑡&#x3D;𝑓(𝑥𝑡,ℎ𝑡−1) </p><p>所以存在部分量一直在被网络输入和输出，一直在循环的迭代。如果我们再抽离整个网络结构，我们会发现我们似乎在对每个步骤重复一些值的计算，只是我们并没有将其作为输出。这一类的基本思想为</p><blockquote><p> 我们坚信，在相当长的序列中存在不同类别的形式。这样的参数共享可以使得模型在不同输入样本下得到更好的泛化。</p></blockquote><h3 id="2-2-前向传播"><a href="#2-2-前向传播" class="headerlink" title="2.2 前向传播"></a>2.2 前向传播</h3><p>这个时候我们先定义我们的数据集，是长度为N的维度为D的序列组成$X&#x3D;(x_1,x_2,…,x_t,..)R^{N*D}$,由此我们根据当前状态和上一状态的隐藏值更新隐藏状态，同时也根据隐藏状态和当前输入得到预测输出。</p><p>𝐻𝑡&#x3D;𝜙(𝑥𝑡𝑊𝑥ℎ+𝐻𝑡−1𝑊ℎℎ+𝑏ℎ) </p><p>𝑌𝑡&#x3D;𝜙(𝐻𝑡𝑊𝑜ℎ+𝑏𝑜) </p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1eb2577019631a9391a365d83257f6b4_1440w.png" alt="img"></p><p>Untitled</p><h3 id="2-3-反向梯度下降"><a href="#2-3-反向梯度下降" class="headerlink" title="2.3 反向梯度下降"></a>2.3 反向梯度下降</h3><p>这个网络结构看上去可以实现，看上去也非常的诱人。但是如何构造一种学习的方式来对其参数更新并且有效呢？梯度下降（Gradient Decent）已经被证明在MLP和CNN中有效，由此我们可以尝试对RNN的梯度进行分析，来判断我们可以如何来接近真值（Ground-truth）。</p><p>但是要知道隐藏状态是会循环的，我们发现在计算梯度的时候，似乎它并不会消失，而是会一直计算下去，它可能会迭代消失，但是更多情况下是会迭代爆炸。因为这种无限可能性的存在，导致我们很容易焦虑因此在循环神经网络中往往会有的技巧（Trick）是对其梯度进行裁剪，或者是增加新的控制单元来解决正则化。具体的实现方式并不在这里赘述。</p><h3 id="2-4-批量梯度下降"><a href="#2-4-批量梯度下降" class="headerlink" title="2.4 批量梯度下降"></a>2.4 批量梯度下降</h3><p>如果说上述是理论分析部分，下面希望是实践的角度来说明，如何确定网络构建的参数。首先定义两个基本的原则：</p><ol><li>要明白Tensor的维度规则，是自外向内的。同时可以通过折叠维度的方式来升维和降维</li><li>矩阵乘法的过程中，M<em>N和N</em>Q的结果是M*Q</li><li>利用矩阵乘法广播和利用批量乘法是不一样的，因为有的三维数组并不能广播。B<em>M</em>N和B<em>N</em>Q想乘得到B<em>M</em>Q</li></ol><p>下文中变量的维度D为输入维度 input_size，其中隐藏状态维度为hidden_size,输出维度D(与输入维度相同）为output_size</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d666ef1e4ab0b80d3ffb14261652f233_1440w.png" alt="img"></p><p>Untitled</p><p>在使用批量梯度下降的过程中，因为同一个batch中的数据是不同的序列，因此隐藏状态并不是1<em>h的广播，而必须是batchsize</em>h。因此这里并不像MLP可以使用广播。同时这里的重点也不是想得到多么好的隐藏状态h，而是具有一种可以反应正常状态的规则，也就是权重，这才是真正希望学习的参数</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-82caef534ab1f798c929c78285e7a181_1440w.png" alt="img"></p><p>Untitled</p><p>疑问：但是在训练过程中隐藏状态都是随机产生的，虽然可能在一定时间预热（Warm up ）之后得到一个比较好的效果，但是依旧很难反应他是真实有效的反应隐藏状态的？这里的梯度下降似乎没有理论依据</p><p>注意这里还会有batch是否设置为first，这样对最终的矩阵的形状还会有不同的影响。</p><h3 id="0x03-时间序列预测任务"><a href="#0x03-时间序列预测任务" class="headerlink" title="0x03 时间序列预测任务"></a>0x03 时间序列预测任务</h3><p>降雨量数据预测</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import torch <br>import torch<span class="hljs-selector-class">.nn</span> as nn<br>import pandas as pd<br>import matplotlib<span class="hljs-selector-class">.pyplot</span> as plt<br>import numpy as np<br>df = pd<span class="hljs-selector-class">.read_csv</span>(<span class="hljs-string">&#x27;../input/Rainfall_data.csv&#x27;</span>)<br>df<span class="hljs-selector-class">.info</span>()<br><br>plt<span class="hljs-selector-class">.subplot</span>(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>plt<span class="hljs-selector-class">.plot</span>(df<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Specific Humidity&#x27;</span>]</span>)<br>plt<span class="hljs-selector-class">.title</span>(<span class="hljs-string">&#x27;Specific Humidity&#x27;</span>)<br>plt<span class="hljs-selector-class">.subplot</span>(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br>plt<span class="hljs-selector-class">.plot</span>(df<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Relative Humidity&#x27;</span>]</span>)<br>plt<span class="hljs-selector-class">.title</span>(<span class="hljs-string">&#x27;Relative Humidity&#x27;</span>)<br>plt<span class="hljs-selector-class">.subplot</span>(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)<br>plt<span class="hljs-selector-class">.plot</span>(df<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Temperature&#x27;</span>]</span>)<br>plt<span class="hljs-selector-class">.title</span>(<span class="hljs-string">&#x27;Temperature&#x27;</span>)<br>plt<span class="hljs-selector-class">.subplot</span>(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)<br>plt<span class="hljs-selector-class">.plot</span>(df<span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Precipitation&#x27;</span>]</span>)<br>plt<span class="hljs-selector-class">.title</span>(<span class="hljs-string">&#x27;Precipitation&#x27;</span>)<br>plt<span class="hljs-selector-class">.tight_layout</span>()<br>plt<span class="hljs-selector-class">.show</span>()<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-764f951c5572d18050c026742a4dc30b_1440w.png" alt="img"></p><p>Untitled</p><p>使用自回归进行预测</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs vim">from statsmodels.tsa.ar_model import AutoReg<br><br># Train the model <span class="hljs-built_in">and</span> gets the params<br><span class="hljs-keyword">x</span> = df[<span class="hljs-string">&#x27;Precipitation&#x27;</span>].<span class="hljs-built_in">values</span><br>train, test = <span class="hljs-keyword">x</span>[:<span class="hljs-built_in">len</span>(<span class="hljs-keyword">x</span>)-<span class="hljs-number">50</span>], <span class="hljs-keyword">x</span>[<span class="hljs-built_in">len</span>(<span class="hljs-keyword">x</span>)-<span class="hljs-number">50</span>:]<br><span class="hljs-keyword">p</span> = <span class="hljs-number">12</span><br>model = AutoReg(train, lags = <span class="hljs-keyword">p</span>)<br>model_fit = model.fit()<br><span class="hljs-keyword">print</span>(<span class="hljs-string">&#x27;Coefficients: %s&#x27;</span> % model_fit.params)<br>params = model_fit.params<br><br># Walk forward over time steps in test<br><span class="hljs-keyword">history</span> = train[<span class="hljs-built_in">len</span>(train)-<span class="hljs-keyword">p</span>:]<br><span class="hljs-keyword">history</span> = [<span class="hljs-keyword">history</span>[i] <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-keyword">history</span>))]<br>predictions = <span class="hljs-keyword">list</span>()<br><span class="hljs-keyword">for</span> t in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(test)):<br>    length = <span class="hljs-built_in">len</span>(<span class="hljs-keyword">history</span>)<br>    lag = [<span class="hljs-keyword">history</span>[i] <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(length-<span class="hljs-keyword">p</span>,length)]<br>    yhat = params[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> d in <span class="hljs-built_in">range</span>(<span class="hljs-keyword">p</span>):<br>        yhat += params[d+<span class="hljs-number">1</span>] * lag[<span class="hljs-keyword">p</span>-d-<span class="hljs-number">1</span>]<br>    obs = test[t]<br>    predictions.<span class="hljs-keyword">append</span>(yhat)<br>    <span class="hljs-keyword">history</span>.<span class="hljs-keyword">append</span>(obs)<br>    <span class="hljs-keyword">print</span>(<span class="hljs-string">&#x27;predicted=%f, expected=%f&#x27;</span> % (yhat, obs))<br><br># Evaluate forecasts<br>from sklearn.metrics import mean_squared_error<br>error = mean_squared_error(test, predictions)<br><span class="hljs-keyword">print</span>(<span class="hljs-string">&#x27;Test MSE: %.3f&#x27;</span> % error)<br>plt.plot(test)<br>plt.plot(predictions, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3334cd286b13c31caad3d5d151903c62_1440w.png" alt="img"></p><p>Untitled</p><p>利用RNN进行序列预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The length of the dataset is:&#x27;</span>,<span class="hljs-built_in">len</span>(train))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The length of the test set is:&#x27;</span>,<span class="hljs-built_in">len</span>(test))<br>window_size = <span class="hljs-number">12</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data</span>(<span class="hljs-params">seq,ws</span>):<br>    output = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(seq)-ws):<br>        output.append(seq[i:i+ws])<br>    <span class="hljs-keyword">return</span> np.array(output)<br><br>train_set = input_data(train,window_size)<br>test_set = input_data(test,window_size)<br><span class="hljs-built_in">print</span>(train_set)<br><br>input_tensor = torch.Tensor(train_set)<br>test_set = torch.Tensor(test_set)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, train_set</span>):<br>        <span class="hljs-variable language_">self</span>.train_set = train_set<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.train_set[index,:<span class="hljs-number">11</span>], <span class="hljs-variable language_">self</span>.train_set[index,-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.train_set)<br><br>train_dataset = MyDataset(input_tensor)<br>train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">for</span> x,y <span class="hljs-keyword">in</span> train_loader:<br>    <span class="hljs-built_in">print</span>(x.shape,y.shape)<br>    <span class="hljs-keyword">break</span><br><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyRnn</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size,hidden_size,num_layers,output_size</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        input_size: number of features</span><br><span class="hljs-string">        hidden_size: number of hidden units</span><br><span class="hljs-string">        num_layers: number of layers</span><br><span class="hljs-string">        output_size: number of output</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.hidden_size = hidden_size<br>        <span class="hljs-variable language_">self</span>.num_layers = num_layers<br>        <span class="hljs-variable language_">self</span>.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.fc = nn.Linear(hidden_size, output_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        h0 = torch.zeros(<span class="hljs-variable language_">self</span>.num_layers, x.size(<span class="hljs-number">0</span>), <span class="hljs-variable language_">self</span>.hidden_size).to(device)<br>        out, _ = <span class="hljs-variable language_">self</span>.rnn(x, h0)<br>        out = <span class="hljs-variable language_">self</span>.fc(out[:, -<span class="hljs-number">1</span>, :])<br>        <span class="hljs-keyword">return</span> out<br><br>input_size = <span class="hljs-number">1</span><br>hidden_size = <span class="hljs-number">128</span><br>num_layers = <span class="hljs-number">2</span><br>output_size = <span class="hljs-number">1</span><br><br>model = MyRnn(input_size, hidden_size, num_layers, output_size).to(device)<br>criterion = nn.MSELoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">0.001</span>)<br><br>num_epochs = <span class="hljs-number">5000</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> x,y <span class="hljs-keyword">in</span> train_loader:<br>        x.unsqueeze_(-<span class="hljs-number">1</span>)<br>        x = x.to(device)<br>        y = y.to(device)<br>        output = model(x)<br>        loss = criterion(output, y.unsqueeze(<span class="hljs-number">1</span>))<br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch: &#123;&#125;, Loss: &#123;:.5f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch, loss.item()/<span class="hljs-built_in">len</span>(x)))<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7b921b5a98bd507118332c5c0c9591b6_1440w.png" alt="img"></p><p>Untitled</p><p>利用RNN进行多变量时序预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = df.iloc[:,<span class="hljs-number">3</span>:<span class="hljs-number">7</span>].values<br>train_set = dataset[:<span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">50</span>]<br>test_set = dataset[<span class="hljs-built_in">len</span>(dataset)-<span class="hljs-number">50</span>:]<br><br>WINDOWS_SIZE = <span class="hljs-number">12</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data</span>(<span class="hljs-params">seq,ws</span>):<br>    output = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(seq)-ws):<br>        output.append(seq[i:i+ws])<br>    <span class="hljs-keyword">return</span> np.array(output)<br><br>train_set = input_data(train_set,WINDOWS_SIZE)<br>test_set = input_data(test_set,WINDOWS_SIZE)<br><br>input_tensor = torch.Tensor(train_set)<br>test_set = torch.Tensor(test_set)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, train_set</span>):<br>        <span class="hljs-variable language_">self</span>.train_set = train_set<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.train_set[index,:<span class="hljs-number">11</span>], <span class="hljs-variable language_">self</span>.train_set[index,-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.train_set)<br><br>train_dataset = MyDataset(input_tensor)<br>train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br>test_dataset = MyDataset(test_set)<br>test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=<span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>input_size = <span class="hljs-number">4</span><br>model = MyRnn(input_size, hidden_size, num_layers, output_size).to(device)<br>epochs = <span class="hljs-number">5000</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-keyword">for</span> x,y <span class="hljs-keyword">in</span> train_loader:<br>        x = x.to(device)<br>        y = y.to(device)<br>        output = model(x)<br>        loss = criterion(output, y.unsqueeze(<span class="hljs-number">1</span>))<br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>RNN</tag>
      
      <tag>循环神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol6 ｜ 从 gym 开始自己的环境</title>
    <link href="/posts/1e270005.html"/>
    <url>/posts/1e270005.html</url>
    
    <content type="html"><![CDATA[<p>openAI的gym中提供了很多封装好的环境，在此基础上我们可以使用其来跑通深度强化学习的代码，但是更多的时候我们希望调用算法来解决一个实际问题，因此尝试为定制化的问题转换成为MDP六元组《变量、状态、动作、奖励、状态转移、终止条件》后编程为可以交互的环境即可。本文介绍学习gymnasium和stable- baseline3的学习思路并手动实现一个MyCar的环境。</p><h3 id="0x01-巨人的肩膀-：调库"><a href="#0x01-巨人的肩膀-：调库" class="headerlink" title="0x01 巨人的肩膀 ：调库"></a>0x01 巨人的肩膀 ：调库</h3><p>根据MDP过程，环境和智能体两个抽象类主要需要包括几个API操作：</p><ol><li>环境：参数设置（init），初始化环境（reset），状态更新（step），关闭（closed），显示（render）</li><li>智能体：深度学习参数（net），学习行为（learn），生成行为（predict）</li></ol><p>所以抽象来看</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import gymnasium as gym<br>import stable_baselines3 as sb3<br>env = gym.make(<span class="hljs-string">&#x27;[env_name]&#x27;</span>,<span class="hljs-attribute">render_mode</span>=<span class="hljs-string">&#x27;human&#x27;</span>)<br><span class="hljs-comment"># 定义好参数，已经学习</span><br>agent = sb3.load(<span class="hljs-string">&#x27;[saved_model_dir]&#x27;</span>) <br><br>observation,<span class="hljs-built_in">info</span> = env.reset(<span class="hljs-attribute">seed</span>=42)<br>terminated = <span class="hljs-literal">false</span><br><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> terminated:<br>    action = agent.predict(observation)<br>    observation,reward,terminated,truncated,<span class="hljs-built_in">info</span> = env.<span class="hljs-keyword">step</span>(action)<br>    <span class="hljs-keyword">if</span> truncated:<br>            env.reset()<br>env.closed<br></code></pre></td></tr></table></figure><h3 id="1-1-环境库-gymnasium-env"><a href="#1-1-环境库-gymnasium-env" class="headerlink" title="1.1 环境库 gymnasium.env"></a>1.1 环境库 gymnasium.env</h3><p>目前主流的强化学习环境主要是基于<a href="https://www.gymlibrary.dev/index.html">openai-gym</a>，主要介绍为</p><blockquote><p> Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym’s API has become the field standard for doing this. Gym是一个开源的Python库，通过提供标准API在学习算法和环境之间进行通信，以及一组符合该API的标准环境，来开发和比较强化学习算法。自发布以来，Gym的API已成为这样做的现场标准。</p></blockquote><p>但是代码安装有点太屎山了，现有另外的一个fork版本<a href="https://gymnasium.farama.org/">gymnasium</a>更加的简单和包容</p><blockquote><p><strong>Gymnasium is a maintained fork of OpenAI’s Gym library.</strong> The Gymnasium interface is simple, pythonic, and capable of representing general RL problems, and has a <a href="https://gymnasium.farama.org/content/gym_compatibility/">compatibility wrapper</a> for old Gym environments Gymasium是OpenAI gym library的一个维护分支。Gymnasium界面简单，pythonic，能够表示一般的RL问题，并具有旧gym环境的兼容性warp器</p></blockquote><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> gymnasium<br></code></pre></td></tr></table></figure><h3 id="1-2-强化学习算法库-stable-baselines3"><a href="#1-2-强化学习算法库-stable-baselines3" class="headerlink" title="1.2 强化学习算法库 stable_baselines3"></a>1.2 强化学习算法库 stable_baselines3</h3><p><a href="https://stable-baselines3.readthedocs.io/en/master/">Stable_baseline3</a>是基于OpenAI baselines改进的实现，类似gymnasium和gym的关系，主要实现的修改为：</p><ol><li>统一算法结构</li><li>实现PEP8兼容</li><li>文档化函数和类</li><li>更多的测试和代码覆盖</li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> stable_baseline3<br></code></pre></td></tr></table></figure><p>另外在stable_baseline3的基础上包括预训练好的智能体平台<a href="https://stable-baselines.readthedocs.io/en/master/guide/rl_zoo.html">RL Baseline zoo</a>，同时也提供训练、评估智能体行为、微调超参数和录屏的功能，具体的使用可以参考官方文档。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">apt-get <span class="hljs-keyword">install </span><span class="hljs-keyword">swig </span>cmake libopenmpi-dev zlib1g-dev ffmpeg<br>pip <span class="hljs-keyword">install </span>stable-<span class="hljs-keyword">baselines </span><span class="hljs-keyword">box2d </span><span class="hljs-keyword">box2d-kengz </span>pyyaml pybullet optuna pytablewriter<br></code></pre></td></tr></table></figure><p>PS: 如果不习惯用conda管理环境，或者有迁移环境的需求可以参考使用docker创建镜像</p><p>另外还有一些其他优秀的RL库，比如<a href="https://github.com/datawhalechina/joyrl">蘑菇书-joyrl</a>、<a href="https://github.com/tensorforce/tensorforce">Tensorforce</a></p><h3 id="0x02-优秀环境欣赏"><a href="#0x02-优秀环境欣赏" class="headerlink" title="0x02 优秀环境欣赏"></a>0x02 优秀环境欣赏</h3><p>在gymnasium的<a href="https://gymnasium.farama.org/environments/classic_control/">官网环境</a>中给出一些典型的环境，可以分类为：</p><ol><li><strong>经典控制（Classic control）</strong>，比如杂技演员（Acrobat）、单臂摆（Cart pole）、小车上山（Mountain car）、钟摆（Pendulum）</li><li><strong>二维环境（Box2D）</strong>，双足行走（Bipedal walker）、赛车（Car racing）、登月（Lunar lander）</li><li><strong>文本游戏（Toy Text）</strong>，二十一点（Blackjack）、悬崖寻路（Cliff walking）、冰湖（Frozen lake）、出租车（Taxi）</li><li><strong>多关节接触动力学（Multi Joint Dynamics with Contact，MoJoCo）</strong></li><li><strong>雅达利（Atari）</strong>，是的就是被任天堂打败的“雅达利大崩溃”的雅达利</li><li><a href="https://gymnasium.farama.org/environments/third_party_environments/"><strong>第三方环境</strong></a>，<a href="https://github.com/robertoschiavone/flappy-bird-env">flappy-bird-env</a>，<a href="https://github.com/eleurent/highway-env">highway-env</a>，<a href="https://github.com/LucasAlegre/sumo-rl">sumo-rl</a>，等等</li></ol><h3 id="0x03-gymnasium-env-详细介绍"><a href="#0x03-gymnasium-env-详细介绍" class="headerlink" title="0x03 gymnasium.env 详细介绍"></a>0x03 gymnasium.env 详细介绍</h3><p>关于基类的介绍，在<a href="https://gymnasium.farama.org/api/env/">gymnasium.env</a>中很清楚，但是一堆英文可能看着有点累，这里主要介绍作为一个抽象类它的外部接口和基本常见属性：</p><p>方法：</p><ol><li>Step() ，根据agent的action更新state，同时返回五元组(更新状态obs，奖励信号reward，是否结束terminated，是否中断truncated，信息info)，注意这里对gym.env中的done更加细致</li><li>reset()，重置环境到初始状态</li><li>Render() 图形引擎，用于可视化过程，不要也可以</li><li>close（） 关闭环境</li></ol><p>属性</p><ol><li>actions_space 定义动作环境</li><li>observation_space 定义状态环境</li><li>Reward_range 奖励范围</li><li>spec </li><li>metadata</li><li>np.random</li></ol><p>Stable_baseline3也提供一些<a href="https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html">教程给出自定义类的属性</a>并且提供了一个<a href="https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb">colab-GoLeftEnv</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium <span class="hljs-keyword">as</span> gym<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> gymnasium <span class="hljs-keyword">import</span> spaces<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomEnv</span>(gym.Env):<br>    <span class="hljs-string">&quot;&quot;&quot;Custom Environment that follows gym interface.&quot;&quot;&quot;</span><br><br>    metadata = &#123;<span class="hljs-string">&quot;render_modes&quot;</span>: [<span class="hljs-string">&quot;human&quot;</span>], <span class="hljs-string">&quot;render_fps&quot;</span>: <span class="hljs-number">30</span>&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, arg1, arg2, ...</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># Define action and observation space</span><br>        <span class="hljs-comment"># They must be gym.spaces objects</span><br>        <span class="hljs-comment"># Example when using discrete actions:</span><br>        <span class="hljs-variable language_">self</span>.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)<br>        <span class="hljs-comment"># Example for using image as input (channel-first; channel-last also works):</span><br>        <span class="hljs-variable language_">self</span>.observation_space = spaces.Box(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">255</span>,<br>                                            shape=(N_CHANNELS, HEIGHT, WIDTH), <br>                                            dtype=np.uint8)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, action</span>):<br>        ...<br>        <span class="hljs-keyword">return</span> observation, reward, terminated, truncated, info<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self, seed=<span class="hljs-literal">None</span>, options=<span class="hljs-literal">None</span></span>):<br>        ...<br>        <span class="hljs-keyword">return</span> observation, info<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render</span>(<span class="hljs-params">self</span>):<br>        ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close</span>(<span class="hljs-params">self</span>):<br>        ...<br></code></pre></td></tr></table></figure><h3 id="0x04-从零开始的MyCar"><a href="#0x04-从零开始的MyCar" class="headerlink" title="0x04 从零开始的MyCar"></a>0x04 从零开始的MyCar</h3><p>假设我们现在希望训练一个智能体，可以在出现下列的网格中出现时都会向原点前进，在定义的环境时可以使用gymnaisum.env定义自己的环境类MyCar，之后使用stable_baselines3中的check_env对环境的输入和输出做检查：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ce9776d28d3070177014e89dac7de05d_1440w.png" alt="img"></p><p>MyCar env</p><p>由此分析环境中的属性：</p><p>状态空间：二维的空间和问题的size有关</p><p>动作空间：离散的五种动作，暂停和上下左右</p><p>是否结束：到达原点</p><p>是否中止：跑到环境之外</p><p>奖励：当前状态距离原点的距离</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gymnasium<br><span class="hljs-keyword">from</span> gymnasium <span class="hljs-keyword">import</span> spaces<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-comment"># Path: modelTimetable/DRL/myEnv.ipynb</span><br><span class="hljs-comment"># Implementing the environment</span><br><span class="hljs-comment"># Reproduction of the cartpole environment</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># Discription: </span><br><span class="hljs-comment"># Create a car in a two-dimensional plane with a width of 20, and the coordinates of </span><br><span class="hljs-comment"># the center point are the destination of the car to reach.</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># State:</span><br><span class="hljs-comment"># The state of the car is represented by the coordinates of the center point of the car.(x,y)</span><br><span class="hljs-comment"># Action:</span><br><span class="hljs-comment"># The action of the car is represented by the speed of the car.(vx,vy)</span><br><span class="hljs-comment"># Reward:</span><br><span class="hljs-comment"># The reward is the distance between the car and the destination.</span><br><span class="hljs-comment"># Termination:</span><br><span class="hljs-comment"># The car reaches the destination.(0,0)</span><br><span class="hljs-comment"># truncation:</span><br><span class="hljs-comment"># The car is out of the screen.</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">gymnasium is the main class that we will use to create our environment.</span><br><span class="hljs-string"></span><br><span class="hljs-string">The gymnasium class has the following methods:</span><br><span class="hljs-string">__init__(): This method is used to initialize the environment. It takes the following parameters:</span><br><span class="hljs-string"></span><br><span class="hljs-string">step(): This method is used to take an action and return the next state, reward, and whether the episode is over. </span><br><span class="hljs-string">Physical engine</span><br><span class="hljs-string">- input: action</span><br><span class="hljs-string">- output: observation, reward,terminated,truncated,info</span><br><span class="hljs-string"></span><br><span class="hljs-string">reset(): This method is used to reset the environment to its initial state.</span><br><span class="hljs-string">- input: None</span><br><span class="hljs-string">- output: observation</span><br><span class="hljs-string"></span><br><span class="hljs-string">render(): This method is used to render the environment:</span><br><span class="hljs-string">Image engine</span><br><span class="hljs-string">- input: mode(default=&#x27;human&#x27;,&#x27;human&#x27;,&#x27;rgb_array&#x27;,&#x27;ansi&#x27;,&#x27;rgb_array_list)</span><br><span class="hljs-string">- output: None</span><br><span class="hljs-string">eg:gymnasium.make(&#x27;CartPole-v0&#x27;,render_mode=&#x27;human&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">close(): This method is used to close the environment.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyCar</span>(gymnasium.Env):<br>    metadata = &#123;<br>        <span class="hljs-string">&#x27;render.modes&#x27;</span>: [<span class="hljs-string">&#x27;human&#x27;</span>, <span class="hljs-string">&#x27;rgb_array&#x27;</span>],<br>        <span class="hljs-string">&#x27;video.frames_per_second&#x27;</span>: <span class="hljs-number">2</span><br>        &#125;<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.target_x = <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.target_y = <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.size = <span class="hljs-number">10</span><br>        <span class="hljs-variable language_">self</span>.action_space = spaces.Discrete(<span class="hljs-number">5</span>) <span class="hljs-comment"># 0:stop, 1:up, 2:down, 3:left, 4:right</span><br>        <span class="hljs-variable language_">self</span>.observation_space = spaces.Box(np.array([-<span class="hljs-variable language_">self</span>.size,-<span class="hljs-variable language_">self</span>.size]), np.array([<span class="hljs-variable language_">self</span>.size,<span class="hljs-variable language_">self</span>.size]))<br>        <span class="hljs-variable language_">self</span>.state = <span class="hljs-literal">None</span><br>        <span class="hljs-variable language_">self</span>.info = &#123;&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, action</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.action_space.contains(action), <span class="hljs-string">&quot;%r (%s) invalid&quot;</span>%(action, <span class="hljs-built_in">type</span>(action))<br>        <span class="hljs-comment"># update the state by the action</span><br>        x,y = <span class="hljs-variable language_">self</span>.state<br>        <span class="hljs-keyword">if</span> action == <span class="hljs-number">0</span>:<br>            x += <span class="hljs-number">0</span><br>            y += <span class="hljs-number">0</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">1</span>:<br>            x += <span class="hljs-number">0</span><br>            y += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">2</span>:<br>            x += <span class="hljs-number">0</span><br>            y += -<span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">3</span>:<br>            x += -<span class="hljs-number">1</span><br>            y += <span class="hljs-number">0</span><br>        <span class="hljs-keyword">elif</span> action == <span class="hljs-number">4</span>:<br>            x += <span class="hljs-number">1</span><br>            y += <span class="hljs-number">0</span><br>        <span class="hljs-comment"># the next state</span><br>        <span class="hljs-variable language_">self</span>.state = np.array([x,y])<br>        <span class="hljs-variable language_">self</span>.state = <span class="hljs-variable language_">self</span>.state.astype(np.float32)<br>        reward = <span class="hljs-variable language_">self</span>._get_reward()<br>        terminated = <span class="hljs-variable language_">self</span>._get_terminated()<br>        terminated = <span class="hljs-built_in">bool</span>(terminated)<br>        truncated = <span class="hljs-variable language_">self</span>._get_truncated()<br>        truncated = <span class="hljs-built_in">bool</span>(truncated)<br>        info = &#123;&#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.state, reward, terminated,truncated, info<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self,seed=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-variable language_">self</span>.state = np.ceil(np.random.rand(<span class="hljs-number">2</span>)*<span class="hljs-number">2</span>*<span class="hljs-variable language_">self</span>.size)-<span class="hljs-variable language_">self</span>.size<br>        <span class="hljs-variable language_">self</span>.state = <span class="hljs-variable language_">self</span>.state.astype(np.float32)<br>        <span class="hljs-variable language_">self</span>.counts = <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.info = &#123;&#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.state,<span class="hljs-variable language_">self</span>.info<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render</span>(<span class="hljs-params">self, mode=<span class="hljs-string">&#x27;human&#x27;</span></span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-variable language_">self</span>.state)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().close()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_reward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> -np.sqrt(<span class="hljs-variable language_">self</span>.state[<span class="hljs-number">0</span>]**<span class="hljs-number">2</span>+<span class="hljs-variable language_">self</span>.state[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_terminated</span>(<span class="hljs-params">self</span>):<br>        x,y = <span class="hljs-variable language_">self</span>.state<br>        <span class="hljs-keyword">return</span> x==<span class="hljs-variable language_">self</span>.target_x <span class="hljs-keyword">and</span> y==<span class="hljs-variable language_">self</span>.target_y<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_truncated</span>(<span class="hljs-params">self</span>):<br>        x,y = <span class="hljs-variable language_">self</span>.state<br>        <span class="hljs-keyword">return</span> x&lt;-<span class="hljs-variable language_">self</span>.size <span class="hljs-keyword">or</span> x&gt;<span class="hljs-variable language_">self</span>.size <span class="hljs-keyword">or</span> y&lt;-<span class="hljs-variable language_">self</span>.size <span class="hljs-keyword">or</span> y&gt;<span class="hljs-variable language_">self</span>.size<br><br><span class="hljs-keyword">from</span> stable_baselines3.common.env_checker <span class="hljs-keyword">import</span> check_env<br>env = MyCar()<br>check_env(env, warn=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>测试它的输出输出</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pf">env = MyCar()<br>env.reset()<br><span class="hljs-keyword">state</span>,reward,terminated,truncated,info = env.step(env.action_space.sample())<br><span class="hljs-keyword">log</span> = <span class="hljs-number">0</span><br>while not terminated:<br>    env.render()<br>    <span class="hljs-keyword">state</span>,reward,terminated,truncated,info = env.step(env.action_space.sample())<br>    if truncated:<br>        env.reset()<br>    <span class="hljs-keyword">log</span> += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-dea3f22ab732cf33b0dc59889230d22c_1440w.png" alt="img"></p><p>output</p><h3 id="0x05-开始训练"><a href="#0x05-开始训练" class="headerlink" title="0x05 开始训练"></a>0x05 开始训练</h3><p>这里只是调用stable_baselines的最简单的DQN库，没有调整参数和网络结构</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> stable_baselines3 <span class="hljs-keyword">import</span> DQN<br><span class="hljs-keyword">from</span> stable_baselines3.common <span class="hljs-keyword">import</span> logger<br># Train the agent <span class="hljs-keyword">by</span> the stable_baselines3<br><span class="hljs-keyword">import</span> os<br>models_dir = <span class="hljs-string">&#x27;./models/DQN&#x27;</span><br>logdir = <span class="hljs-string">&#x27;./logs&#x27;</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.<span class="hljs-keyword">exists</span>(models_dir):<br>    os.makedirs(models_dir)<br><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.<span class="hljs-keyword">exists</span>(logdir):<br>    os.makedirs(logdir)<br><br>env = MyCar()<br>agent = DQN(<span class="hljs-string">&#x27;MlpPolicy&#x27;</span>, env, <span class="hljs-keyword">verbose</span>=<span class="hljs-number">1</span>,tensorboard_log=logdir)<br>agent.learn(total_timesteps=<span class="hljs-number">100000</span>, log_interval=<span class="hljs-number">100</span>,tb_log_name=<span class="hljs-string">&#x27;DQN&#x27;</span>)<br>agent.save(&quot;DQN_MyCar&quot;)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5cabe79b310b2dcbf3ebb053b135f096_1440w.png" alt="img"></p><p>DQN训练结果</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-57b50edd23ae8580d0786a2bbf069997_1440w.png" alt="img"></p><p>训练最终结果</p><p>之后可以通过保存的环境来测试结果</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">env = MyCar()<br>obs = env.reset()<br>agent = DQN.load(<span class="hljs-string">&#x27;deepq_cartpole.zip&#x27;</span>,<span class="hljs-attribute">env</span>=env)<br>terminated = <span class="hljs-literal">False</span><br><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> terminated:<br>    action,_state = agent.predict(obs)<br>    obs,rew,terminated,truncated,<span class="hljs-built_in">info</span> = env.<span class="hljs-keyword">step</span>(action)<br>    <span class="hljs-built_in">print</span>(env.state)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-04ef1ce7869e37f9d1d9fc38f612a33c_1440w.png" alt="img"></p><p>解决方案</p><p>并使用</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf">tensorboard --logdir <span class="hljs-operator">=</span> logs<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6bc3eb407754d0f10ac2eb12dae6446c_1440w.png" alt="img"></p><p>参数</p><h3 id="0x06-总结"><a href="#0x06-总结" class="headerlink" title="0x06 总结"></a>0x06 总结</h3><p>最感动的是stable_baselines3提供的custom_gym_env.ipynb中最后给出的be creative！</p><p>建立环境又何尝不是一种创造。</p><p>参考链接很多，感谢互联网～</p>]]></content>
    
    
    <categories>
      
      <category>算法设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>gym</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol5 ｜ 智能体与搜索</title>
    <link href="/posts/9af6a934.html"/>
    <url>/posts/9af6a934.html</url>
    
    <content type="html"><![CDATA[<p>行为流派01_智能体与搜索blog.tjdata.site&#x2F;posts&#x2F;9c6b1465.html</p><p>根据CS188《Intro to AI》来对搜索策略进行一定的总结</p><h3 id="0x01-智能体"><a href="#0x01-智能体" class="headerlink" title="0x01 智能体"></a>0x01 智能体</h3><p>在人工智能中，核心问题是创建一个理性的<strong>智能体(Rational agent)<strong>，是一个实体通过一些系列的</strong>动作(Actions)<strong>来实现目标或者是</strong>喜好(Goal or perferences),<strong>它存在于</strong>环境中（Environment）</strong>，智能体通过<strong>传感器（sensors）</strong>获取信息并驱动自己的<strong>动作（Actuators）</strong>。</p><p>Reflex agent：仅仅根据现有来选择动作</p><p>Planning agent：根据一系列的动作来确定现有状态下的动作</p><p>定义一个任务通常会用到PEAS（评价指标，环境，动作，传感器）</p><p>Performance Measure：指的是智能体需要提高的衡量指标</p><p>Environment：总结影响智能体的因素</p><p>Actions and Sensors：说明智能体如何对环境造成改变，同时如何获取信息</p><p>特定的我们确定几类不同特征的任务：</p><ol><li>部分观察的任务(Partially observable environments) 或者 完全观察(Fully observation)</li><li>随机过程(Stochastic) 或者 确定过程（Deterministic）</li><li>多智能体环境（multi-agent）</li><li>静态环境（static）或者动态环境（Dynamic）</li></ol><h3 id="0x02-搜索问题定义"><a href="#0x02-搜索问题定义" class="headerlink" title="0x02 搜索问题定义"></a>0x02 搜索问题定义</h3><p>搜索问题包括以下几个部分：</p><ol><li>State 在所给案例中的所有状态</li><li>Action 在每个状态可取动作的集合</li><li>Transition 在某一状态下采用动作得到的下一个状态</li><li>Cost 从一个状态到另外一个状态的成本</li><li>Start state 初始状态</li><li>Goal test 判断是否到达最终的状态</li></ol><p>例如吃豆人Pathing和吃掉所有豆子Eat-all-dots问题之间的区别：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-66e0dba2c604a7b4d0aba42f7b2fe3e7_1440w.png" alt="img"></p><p>搜索问题举例</p><p>常见的表现形式为状态空间图和搜索树</p><p>state space graphs and search trees</p><p>状态空间图：用不同状态之间的转移关系来表示</p><p>搜索树：以状态为节点，动作作为边，但是每个状态和节点不仅编码状态本身，还编码路径</p><ul><li>getNextState</li><li>get Action</li><li>get Action Cost</li></ul><p>我们只存储立即处理的state，并计算最新的状态</p><h3 id="0x03-基本搜索（normal-search）"><a href="#0x03-基本搜索（normal-search）" class="headerlink" title="0x03 基本搜索（normal search）"></a>0x03 基本搜索（normal search）</h3><p>一个标准的方式来寻找到轨迹（Plan），是从初始状态开始(getStartState)来到达最终状态（Goal state），同时维护可能的过程（Frontier），同时拓展我们的当前过程通过删除节点state为state- action- state。（称作strategy），当我们删除缩手的frontiers，我们得到最终的路径（path）</p><p>这些搜索中具有一些基本属性：</p><ol><li>搜索的完整性 completeness</li><li>搜索的最优性 optimality</li><li>分枝因素</li><li>最大深度</li><li>最浅解的深度</li></ol><p>常见的无信息搜索包括：DFS、BFS、UCS</p><p>常见的信息搜索包括；Greedy search and A star search</p><p>后者相比较前者主要增加代价，代价的含义包括两个方面</p><ol><li>Estimation of distance to goal state，as the cost function may be the lower bound for the problem, the heuristics are typically solutions to relaxed problem(where some of the constraint  of the original problem have been removed)</li></ol><p>比如利用启发式的方法可以让PACMAN更加倾向于到达最终的目的地</p><ol><li>Estimation of the empirical cost function</li></ol><h3 id="0x04-约束问题（Constraint-satisfied-problem）和回溯（Backtracking-search）和local-search"><a href="#0x04-约束问题（Constraint-satisfied-problem）和回溯（Backtracking-search）和local-search" class="headerlink" title="0x04 约束问题（Constraint satisfied problem）和回溯（Backtracking search）和local search"></a>0x04 约束问题（Constraint satisfied problem）和回溯（Backtracking search）和local search</h3><p>之前的文章中介绍什么是规划问题（Planing problem），可以使用搜索策略（Search strategy）进行求解。对于另外一种问题可以描述为约束满足问题（Constraint satisfaction problems，CSP）是一类识别（Identification problem）</p><blockquote><p><strong>识别问题（Identification problem）：我们必须简单的识别最终的结果是否为目标状态</strong> 这个术语通常指在机器学习和人工智能中,系统需要从一组潜在的候选项中识别出正确的项目的任务。这类问题通常涉及模式识别,即从输入的数据中提取有意义的信息模式。 一些常见的识别问题示例包括: </p></blockquote><ul><li>图像识别:从图片中识别出特定对象或场景</li><li>语音识别:从语音输入中识别出说话内容</li><li>面部识别:从图片或视频中识别出特定人脸</li><li>手写识别:识别手写输入的文字内容</li><li>生物识别:通过指纹、声纹等生物特征识别个体身份</li><li>文档分类:将文本文档分类到预定义的类别中</li></ul><p> 识别问题通常需要构建合适的机器学习模型,然后使用大量带标签的训练数据对模型进行训练,使其能够对新输入进行可靠的识别与分类。同时还需要处理各种困难情况,如光线、角度、遮挡等对图像识别的影响,以提高模型的鲁棒性。</p><p>三元组：</p><ol><li>variables，一系列的变量集合</li><li>domain，不同变量可能存在的取值范围</li><li>constraints，不同变量之间的约束关系</li></ol><p>例如：八皇后问题</p><p>约束编程问题的求解方式是NP-hard问题，随着问题规模的扩大时间呈现指数型增加。我们可以将<strong>约束满足问题转换成为搜索问题：</strong></p><p>state 为部分赋值（对CSP的可变赋值，其中一些变量被分配了值，而另一些变量没有）。</p><p>successor function 输出所有状态，并分配一个新变量，目标测试验证所有变量都被分配，并在其测试的状态中满足所有约束。</p><p>约束问题比之前的搜索问题具有更复杂的框架，因此可以尝试将上述公示和合适的启发式方法结合起来</p><p>之前的文章中介绍什么是规划问题（Planing problem），可以使用搜索策略（Search strategy）进行求解。对于另外一种问题可以描述为约束满足问题（Constraint satisfaction problems，CSP）是一类识别（Identification problem）</p><blockquote><p><strong>识别问题（Identification problem）：我们必须简单的识别最终的结果是否为目标状态</strong> 这个术语通常指在机器学习和人工智能中,系统需要从一组潜在的候选项中识别出正确的项目的任务。这类问题通常涉及模式识别,即从输入的数据中提取有意义的信息模式。 一些常见的识别问题示例包括: </p></blockquote><ul><li>图像识别:从图片中识别出特定对象或场景</li><li>语音识别:从语音输入中识别出说话内容</li><li>面部识别:从图片或视频中识别出特定人脸</li><li>手写识别:识别手写输入的文字内容</li><li>生物识别:通过指纹、声纹等生物特征识别个体身份</li><li>文档分类:将文本文档分类到预定义的类别中</li></ul><p> 识别问题通常需要构建合适的机器学习模型,然后使用大量带标签的训练数据对模型进行训练,使其能够对新输入进行可靠的识别与分类。同时还需要处理各种困难情况,如光线、角度、遮挡等对图像识别的影响,以提高模型的鲁棒性。</p><p>三元组：</p><ol><li>variables，一系列的变量集合</li><li>domain，不同变量可能存在的取值范围</li><li>constraints，不同变量之间的约束关系</li></ol><p>例如：八皇后问题</p><p>约束编程问题的求解方式是NP-hard问题，随着问题规模的扩大时间呈现指数型增加。我们可以将<strong>约束满足问题转换成为搜索问题：</strong></p><p>state 为部分赋值（对CSP的可变赋值，其中一些变量被分配了值，而另一些变量没有）。</p><p>successor function 输出所有状态，并分配一个新变量，目标测试验证所有变量都被分配，并在其测试的状态中满足所有约束。</p><p>约束问题比之前的搜索问题具有更复杂的框架，因此可以尝试将上述公示和合适的启发式方法结合起来</p><p>map coloring problem : 给定一堆颜色，要求相邻的地区之间的颜色不能相等</p><p>通常我们可以对约束进行分类：</p><ol><li>unary constraints 专有约束，在约束图中表示 点</li><li>binary constraint：两个变量，在约束图中表示 边</li><li>higher-order constraint：非常规</li></ol><p>另一方面，回溯搜索仅在变量值不违反任何约束的情况下为变量分配值，从而显著减少回溯。虽然回溯搜索是比深度优先搜索的野蛮强化的巨大改进，但通过过滤、变量&#x2F;值排序和结构拓宽的进一步改进，我们仍然可以获得更多的速度收益。</p><ul><li>变量的取值范围已经是一个较大的进步</li><li>Tricks1: 过滤（Filter）：</li><li>Tricks2:变量排序（Variable or value ordering）</li><li>minimum remaining values，选择分配有效剩余值最少的变量</li><li>least constraining value，从剩余未份配值的domain中选择prunes最少的值</li><li>Tricks3: 结构拓宽（Structural exploration）</li></ul><p>另外一种方法：本地搜索通过迭代的改进对一些值的随机分配开始，然后迭代的选择一个随机冲突变量，并对变量重新分配给违反约束最小的变量，直到不再存在约束违规，称为一种具有最小冲突启发式的策略</p><ol><li>爬山搜索 Hill-Climbing Search</li><li>模拟退火搜索 Simulate annealing search</li><li>遗传搜索 generic algorithm</li></ol><h3 id="0x05-游戏（Game）-和对抗搜索（Adversarial-search-problem）"><a href="#0x05-游戏（Game）-和对抗搜索（Adversarial-search-problem）" class="headerlink" title="0x05 游戏（Game） 和对抗搜索（Adversarial search problem）"></a>0x05 游戏（Game） 和对抗搜索（Adversarial search problem）</h3><p>在之前的搜索问题中，我们尝试解决他们高效并且最优的，利用不同的搜索方式。以及更加具有启发式的强大的搜索算法。但是在面临一些问题中搜索算法中会遇到对方的阻碍，这种情况下单纯的利用搜索策略。这一类新的问题通常可以被称为adversarial search problem，或者叫做游戏games。</p><p>不同于传统搜索问题返回直接的答案，对抗搜索由于problem一直在发生变化，这样复杂的场景中往往要求我们返回一个策略（Strategy or policy），一个简单的对抗搜索问题包括一下策略：</p><ol><li>Initial state</li><li>players ：每一轮属于谁的player</li><li>actions，player可以实现的动作</li><li>transition model 状态转移的结果</li><li>terminal test</li><li>terminal values</li></ol><p>可以使用</p><ol><li>Min-max搜索</li><li>Alpha-beta搜索</li><li>evaluation function</li></ol><h3 id="0x06-动态决策过程（MDP）和强化学习（Reinforcement-learning）"><a href="#0x06-动态决策过程（MDP）和强化学习（Reinforcement-learning）" class="headerlink" title="0x06 动态决策过程（MDP）和强化学习（Reinforcement learning）"></a>0x06 动态决策过程（MDP）和强化学习（Reinforcement learning）</h3><p>～ to be continued</p><p>参考链接：</p><p><a href="https://oi-wiki.org/search/">OI wiki：搜索部分简介</a></p><p><a href="https://inst.eecs.berkeley.edu/~cs188/fa22/">《CS188: Intro to atrificial intelligent》</a></p><p>伯克利人工智能入门课按经典教材章节顺序授课,内容涵盖各主要领域;课程笔记通俗易懂,作业可在Gradescope平台完成实时测评,项目复现经典游戏,运用所学知识实现算法,使吃豆人在迷宫自由行动。</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>搜索问题</tag>
      
      <tag>人工智能</tag>
      
      <tag>search problem</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>工科生研 1 使用MacBook Pro14 深度感受</title>
    <link href="/posts/c61c1ac7.html"/>
    <url>/posts/c61c1ac7.html</url>
    
    <content type="html"><![CDATA[<p>在研究生使用半年的Macbook Air（2020M1）之后由于存储、屏幕等原因，转向具有诸多优点的MacbookPro14（2021），在深度磨合半年之后介绍这台设备的优点。从工业设计、外部接口、硬件配置、屏幕素质介绍使用感受，并从文件处理、知识管理、系统工具、娱乐角度介绍软件资源，最后给出文件管理和待办管理的工作流。</p><p>本文主要介绍2021款MacBook Pro 14inch, 具体的<a href="https://www.apple.com/hk/en/macbook-pro-14-and-16/">参数规格</a></p><h3 id="0x01-使用感受"><a href="#0x01-使用感受" class="headerlink" title="0x01 使用感受"></a>0x01 使用感受</h3><p>一下从工业设计、外部接口、硬件配置、屏幕素质角度介绍使用感受。</p><p>从<strong>工业设计</strong>角度，外观上虽然失去了Macbook Air楔形设计的优美，但是坚实的底座和方正的设计会让人觉得它很踏实。但是真的很重，相比Air的重真的需要加一个档次的心理准备。另外这次的键盘相比air更重，按键也更大，特别是非常大的esc键，非常舒服。</p><p>从<strong>外部接口</strong>来看尤其是 3thunderbolt4+MagSafe3+headphone jack +HDMI+SDXC 真是非常实用，所以直接抛弃贝尔金七合一的拓展坞。HDMI在外出接投影仪的时候真的非常好用，SD卡在导出相机照片的时候非常方便。美中不足的有两点：一是没有USB-A接口，在日常使用移动硬盘和连接安卓手机不方便 二是HDMI和SD卡的速度有一定的限制。</p><blockquote><p> 观点1：外设永远没有自带方便</p></blockquote><p>从<strong>硬件配置</strong>角度，后续我会详细结合软件操作来介绍。这里总的体验感受是M1pro相比M1从速度上真的会快很多，但是功耗也会快很多。但是菜狗很少会遇到性能瓶颈（doge），日常使用的续航大概只有6～12小时不等（之前的Air大概有12～18小时）。另外对于电脑来说，256G还是太小了，虽然我有一台4T的windows台式机，但还是感觉移动办公的电脑存储也不能太低（点名微信）。同时虽然移动硬盘很便宜，但是使用建议参考观点1。外放的感觉超级赞！远超iPhone 13pro，略超iPad pro12.9inch。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b85d1c52f6c460eaf0c07c3ca6f31d20.jpg" alt="img"></p><p>广告</p><p>罗技 MX Keys 多设备键盘</p><p>知乎</p><p>¥549.00</p><p>去购买</p><p>从<strong>屏幕素质</strong>角度，3024*1964的254ppi的屏幕日常使用非常的惊艳，同时基于miniLED观看HDR视频可以高达1600nit。但是总感觉屏幕太亮很刺眼，看时间长了屏幕有点头晕。但是理论上基于miniLED的LCD面板频闪很低，具体原因未知。将其显示从XDR调整为Display，同时开启True Tone和Night shift会稍微好一点。刘海没有Face ID很让人失望，但是刘海正好处于菜单栏和状态栏之间的交界处，这样的设计可以正好将显示上移，在屏下技术不完善的今天，日常使用的关注点还是很少的。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-66e44a1b58a249005322bef19c65d7d1_1440w.png" alt="img"></p><p>显示选项</p><h3 id="0x02-软件资源库"><a href="#0x02-软件资源库" class="headerlink" title="0x02 软件资源库"></a>0x02 软件资源库</h3><p>这里主要介绍具有GUI的应用程序，通常可以分为以下，推荐下载软件的网站：</p><p><a href="https://www.macyy.cn/">macyy</a>、<a href="https://xclient.info/">xclient</a>、<a href="https://appnee.com/category/mac-software/">AppNee</a>以及相关telegram群聊</p><h3 id="2-1-文件处理"><a href="#2-1-文件处理" class="headerlink" title="2.1 文件处理"></a>2.1 文件处理</h3><ul><li>Office 365三件套：Word、Excel、PowerPoint</li><li>macOS自带：Keynote、iMovie</li><li>Typora：markdown的最佳工具</li><li>PDF expert和预览：PDF处理工具</li><li>Tex Studio：不会用VScode来编辑Latex，还是用这个</li><li>nPlayer：相比IINA打开HDR视频会更有优势</li><li>VS code：最强的文本编辑器，sublime text有点麻烦的。</li><li>GifSki：压缩视频生成Gif</li><li>SVG view：查看SVG</li><li>R studio：处理R文件</li></ul><h3 id="2-2-知识管理"><a href="#2-2-知识管理" class="headerlink" title="2.2 知识管理"></a>2.2 知识管理</h3><ul><li>Notion：all in one的笔记软件</li><li>Zotero7: 文献管理软件</li><li>Marginnote3: 桌面级的读书笔记摘要工具</li><li>微信读书：macOS可以直接下载！</li><li>xmind：生成思维导图很好看，但是现在逐渐转向PPT</li><li>One drive：拼车的Office365非常香，iCloud太贵了！</li><li>Dash：API管理</li><li>Cubox：真的知识管理软件，稍后读。</li><li>Note+Reminder+Calendar：系统自带的GTD管理系统</li></ul><h3 id="2-3-系统功能"><a href="#2-3-系统功能" class="headerlink" title="2.3 系统功能"></a>2.3 系统功能</h3><ul><li>iStat Menus Status，状态栏系统状态检测，可以显示现有功率！</li><li>Alfred，相比spotlight而言功能更多，但是文件检索的效率略低。有的时候one drive文件搜索不到</li><li>Bob：翻译软件，配合自带的翻译使用非常流畅，同时可以下载插件</li><li>Clash X：网络增强</li><li>Terminus：终端增强</li><li>Oh-My-Zsh：终端增强</li><li>Barender4：状态栏增强</li><li>Rectangle：窗口管理</li><li>iShot pro：截图</li><li>Better zip：解压缩</li><li>cheetsheet：快捷键提醒</li><li>超级右键：右键增强，但是有的时候增强也并不是好事</li><li>Monitor Control：管理显示器亮度</li></ul><blockquote><p> 观点2：局部便捷化会破坏系统一致性，而产生更大的混乱。</p></blockquote><h3 id="2-4-娱乐"><a href="#2-4-娱乐" class="headerlink" title="2.4 娱乐"></a>2.4 娱乐</h3><ul><li>微信、telegram：即时通讯</li><li>Chrome、Safari、Arc：浏览器，通往互联网的窗口</li><li>Steam：有一部分Mac可以玩的，比如人类一败涂地、胡闹厨房等等</li><li>Music：自带的yyds</li><li>百度网盘+阿里云盘</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b85d1c52f6c460eaf0c07c3ca6f31d20.jpg" alt="img"></p><p>广告</p><p>罗技 MX Keys 多设备键盘</p><p>知乎</p><p>¥549.00</p><p>去购买</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b85d1c52f6c460eaf0c07c3ca6f31d20.jpg" alt="img"></p><p>广告</p><p>罗技 MX Keys 多设备键盘</p><p>知乎</p><p>¥549.00</p><p>去购买</p><h3 id="0x03-工作流分享"><a href="#0x03-工作流分享" class="headerlink" title="0x03 工作流分享"></a>0x03 工作流分享</h3><p>在上述的软件基础上，我们给出自己的工作流，主要包括3.1文件管理、3.2待办管理（GTD）、3.3 同步管理等等</p><h3 id="3-1-文件管理"><a href="#3-1-文件管理" class="headerlink" title="3.1 文件管理"></a>3.1 文件管理</h3><p>经过长时间的摸索，给出<strong>结合领域知识+标签的Finder文件管理方式</strong>。</p><p>树状的文件资源管理器在Linux、macOS、Windows中几乎达成共识。但是macOS依旧保留“标签”这个重要属性，标签可以将文件之间的关系从<strong>树的节点</strong>转换成为平权的<strong>图的节点</strong>，这样带来的好处是自由行的增加，坏处是管理的困难。通常在最终管理只会记住<strong>度比较大的节点</strong>。我们会面临的困境是，树状分类的文件夹不足以代表内部文件的所有特征，而标签的文件系统管理困难。所以我们需要在<strong>合理文件夹分类和恰当的标签管理中做权衡。</strong>给出的解决方案是：</p><ol><li>领域知识鲜明的设置领域文件夹，否则按照格式管理</li><li>标签仅表示状态，和额外的一次性的元信息</li></ol><p>首先对于第一点，感觉Flomo标签软件传授的笔记理念（虽然我不用flomo），其倡导的卡片分类我发现也适用于文件管理。系统默认会将用户文件分为：文档、音乐、视频、照片这样的格式。如果我们只按照格式来存储这样的文件，我们会发现我们的Project1需要的文档、代码、图片等等文件散落在不同的文件夹，不利于整理。因此我们可以设计另外一个专放领域知识的文件夹，里面可以存储课程1、课程2、项目1、项目2,…的领域文件夹，对于小型文件不易区分的文件存储在不同格式的文件夹，pdf文件存储在文档、png存储在图片文件夹。充分利用不同文件的<strong>强关系</strong></p><p>对于第二点，受益于spotlight、everything这样的文件，我们可以用哈希的方式来搜索我们的文件。因此我设计<strong>标签</strong>并不是对文件属性的强分类，而是对文件的信息补充，包括状态信息、所属成员、以及独一无二的meta信息，标签可以加的足够多来保证后续可以搜索到。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-bf0eeb0c17df744be0d2e56661d1fdba_1440w.png" alt="img"></p><p>领域知识和标签的Finder文件管理</p><h3 id="3-2-待办管理（GTD）"><a href="#3-2-待办管理（GTD）" class="headerlink" title="3.2 待办管理（GTD）"></a>3.2 待办管理（GTD）</h3><p>《Get thing done》参考这本书可以给对于日常任务管理提升一个级别。在研究生之前的任务可能就是简单的复习，写题等等。回想生活那么简单就是因为自己是个单线程生物。但是研究生中不同任务的并行，有的需要跑代码，有的时候需要写文档，还会有偶发的中断任务去处理，由于我们并不是机器，任务也是死板的输入输出。因此在混乱的任务中找到自己的工作流是GTD需要做的事情。</p><p>之前尝试过滴答清单，最好发现还是多端同步的苹果自带的日历Calendar、提醒事项Reminder、笔记Note好用。</p><p>首先我们可以对任务分为三类：Event、Todo、DDL。这里的命名和书里面的可能不一样～</p><ol><li>Event：通常放在Calendar中，<strong>指的是有确定时间发生的，不需要check是否完成的</strong>。比如上课（不去也没关系）、比如演唱会抢票（抢不到也没关系）、比如开组会（是否完成也不需要确认）等等；</li><li>Todo：通常放在Reminder中，可能叫Task比较好，指的是<strong>需要check完成的，但是一般没有时间</strong>。比如买袜子、看一本书等等。这一类任务通常需要根据<strong>优先级来排序</strong>；</li><li>DDL：放在Reminder中，<strong>指的是需要check完成的，一般有截止日期。</strong>比如6月1号之前提交论文，比如下一月十号之前完成一项汇报。这一类任务需要<strong>根据截止日期来排序</strong>。</li></ol><p>分类好之后我们需要包括输入，通常即时可以用Siri、Alfred收集自己的任务，或者以周或月为单位整理自己的任务（用备忘录），之后再整理送入自己的系统。</p><blockquote><p> 观点3: 友好的系统设计可以保证它易用，但是维护一个系统是需要持之以恒的用下去，并从中得到正反馈。</p></blockquote><h3 id="3-3-知识管理"><a href="#3-3-知识管理" class="headerlink" title="3.3 知识管理"></a>3.3 知识管理</h3><p>基于Notion实现All in one并不现实，不同的软件具有应对不同场景的优势。Cubox用于稍后阅读、Notion适用于笔记梳理、Note适用于即刻笔记、Marginnote适用于文档阅读等等，自己路需要自己去摸索！</p><p><a href="https://blog.tjdata.site/posts/3be9bc5f.html">Mermaid使用教程</a></p><p><a href="https://blog.tjdata.site/posts/d49191c7.html">知识管理流程</a></p><p><a href="https://blog.tjdata.site/posts/cc1a6fba.html">网络代理使用</a></p><p><a href="https://blog.tjdata.site/posts/de0c5bf1.html">预览工具使用</a></p><p>更多的参考</p><p><a href="https://blog.tjdata.site/posts/9148fb62.html">工科生研0的Macbook Air M1 13inch（16+256）深度使用感受</a></p><h3 id="One-more-thing"><a href="#One-more-thing" class="headerlink" title="One more thing"></a>One more thing</h3><blockquote><p> Mac终究只是连接到网络世界的工具，屏幕背后的人是最重要的！</p></blockquote><p>在使用Mac的过程中，我可能陷入追求机器性能极致迷茫，似乎M1pro的H.264编码很厉害，似乎视频剪辑很厉害。但是如果你不需要这些功能，你不需要用这些功能来证明你确实需要M1pro，而是要在实现自己本身目的的前提下最大化的利用它并判断是否合适！</p><p>希望能启发你的一些思考和讨论！</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>apple</tag>
      
      <tag>MacBook</tag>
      
      <tag>产品测评</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229 机器学习Vol13 ｜ EM 算法与 HMM 和 CRF</title>
    <link href="/posts/8b4f9c0c.html"/>
    <url>/posts/8b4f9c0c.html</url>
    
    <content type="html"><![CDATA[<p>本文主要介绍EM算法，作为统计学习中可以通过迭代的方式估计含有隐变量模型的方法。同时给出两个典型的例子作为注释并给出相关代码的分析作为注解。</p><h3 id="0x01-摘要"><a href="#0x01-摘要" class="headerlink" title="0x01 摘要"></a>0x01 摘要</h3><p>（个人感悟）对于人工智能三大流派的分析，统计学习、神经网络和行为学习的一些新的感悟。首先统计学习依靠概率统计的知识建立起的模型和神经网络的范式并不完全一样，可能在模型、推理、优化等方面有名称的雷同但两者是完全不同的概念。同时以约束编程或者强化学习的代表的行为学习是处于这两者之上层的控制决策手段。</p><p>针对统计学习而言，其基本的任务分为参数估计和假设检验。这里的参数估计更多的是使用统计的方法来进行，最常见的MLE为极大似然估计（通过最大化likelihood function）来得到对应的估计。这次介绍的EM算法可以看作是一种对含隐变量的新估计方法。</p><h3 id="0x02-EM算法"><a href="#0x02-EM算法" class="headerlink" title="0x02 EM算法"></a>0x02 EM算法</h3><p>首先回顾一下，对于不含隐变量的方法我们会如何建模，{𝑥𝑖,𝑦𝑖}，最常见的是可以建立起判别式模型（Discriminate）比如逻辑回归、支持向量机、决策树等进行学习得到𝑃(𝑌|𝑋)。但是有的时候这样太直接了，我们可以从另外生成式的角度从每个y进行建模得到𝑎𝑟𝑔𝑚𝑎𝑥𝑦𝑃(𝑋|𝑌)𝑃(𝑌),来实现类似的效果。</p><p>但是加入对于没有标签的数据，我们通常会使用非监督学习(unsupervised learning)的方式来得到答案，比如Kmeans的方式。而EM算法是从另外一个角度来对其进行建模。其中心思想分为两步：</p><ol><li>随机初始化参数</li><li>E步 即然不知道隐藏状态，那么计算所有隐藏状态下的概率。也就是计算出不同隐藏状态下，对应的概率 𝑃(𝑋,𝑍;𝜃) </li><li>M步 优化参数，最大化似然值来得到答案</li></ol><p>假设坐标轴中分为两类点 {𝑥𝑖,𝑧𝑖},其中相同类型点的分布满足高斯分布，也就是𝑥|𝑧&#x3D;𝑗:𝑁(𝑢𝑗,Σ𝑗) </p><p>，同时假设点的隐藏变量为二项分布，则𝑧:𝐵(𝜙),由此我们可以写出：</p><p>𝑝(𝑥𝑖,𝑧𝑖)&#x3D;𝑝(𝑥𝑖|𝑧𝑖;𝑢,Σ)𝑝(𝑧𝑖,𝜙) </p><p>这样可以很轻松的写出含有隐变量的似然函数</p><p>𝑙(𝑢,Σ,𝜙)&#x3D;𝑙𝑜𝑔∏𝑖𝑝(𝑥𝑖,𝑧𝑖)&#x3D;Σ𝑖𝑙𝑜𝑔Σ𝑗𝑝(𝑥𝑖|𝑧𝑖&#x3D;𝑗;𝑢𝑗,Σ𝑗)𝑝(𝑧𝑖&#x3D;𝑗,𝜙) </p><p>如果我们知道隐变量取值,就可以使用GDA轻松求出上述的变量梯度，并开始下降。</p><p>但是我们不知道，因此我们可以先计算m个样本隐变量k中情况下的概率：</p><p>𝑄𝑖(𝑍𝑖)&#x3D;𝑝(𝑧𝑖|𝑥𝑖,𝜙,𝑢,Σ)∈𝑅(𝑚,𝑘) </p><p>之后我们通过琴生不等式的证明来优化上述含有隐变量的似然函数下界</p><p>𝑙(𝑢,Σ,𝜙)≥Σ𝑖Σ𝑗𝑄𝑖(𝑧&#x3D;𝑗)𝑙𝑜𝑔(𝑃(𝑥𝑖,𝑧&#x3D;𝑗;𝜃)𝑄𝑖(𝑧&#x3D;𝑗)) </p><p>因为这里的隐变量的分布是已知的，所以必然是可以被优化的。</p><p>𝑢^,Σ^,𝜙^&#x3D;𝑎𝑟𝑔𝑚𝑎𝑥(Σ𝑙𝑜𝑔(𝑃(𝑥𝑖,𝑧&#x3D;𝑗;𝜃)𝑄𝑖(𝑧&#x3D;𝑗)) </p><p>例如GMM的标准的求导之后的已经有人解析过：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3af821a567b7471d91e5d90a8d3b9a3d_1440w.png" alt="img"></p><p>GMM中的M步</p><p>由此不断的迭代求解得到答案，或者是解析解得到答案</p><p>到这一步其实发现EM算法并不是一个单纯的模型，而更多是一种解决手段，它是类似MLE一样单纯作为估计的方式，如何将其应用在不同的建模场景中可以得到不同的模型，比如应用在序列标注中的HMM，CRF。个人的直觉上觉得它很多的代表一种类似蒙特卡洛的思想，这里的猜测或者掷骰子是一种没有先验知识而去探索exploration的感觉，在GAN、知识蒸馏、强化学习、序列预测中感觉都会用到相关的概念。</p><h3 id="0x03-隐马尔可夫模型-HMM"><a href="#0x03-隐马尔可夫模型-HMM" class="headerlink" title="0x03 隐马尔可夫模型 HMM"></a>0x03 隐马尔可夫模型 HMM</h3><p>隐马尔可夫模型是用于标记问题的统计学习模型，比如说给照片流进行标记，同时在这类随机过程的研究中主要问题有：1. 如何计算概率  2. 如何根据观测值确定参数 3. 如何根据给定的观测值确定背后的状态</p><h3 id="3-1-HMM定义与模拟"><a href="#3-1-HMM定义与模拟" class="headerlink" title="3.1 HMM定义与模拟"></a>3.1 HMM定义与模拟</h3><p>隐马尔可夫模型是关于时序 的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各状态生成不同观测随机序列的过程，其中常见的参数和集合的描述如下：</p><p>状态序列 State&#x2F;Track : state sequence(which is latent)</p><p>观察序列 Observation : observation sequence</p><p>状态空间 Q: state spcae 1,..,N</p><p>观测空间 V: observation spae 1,…,M</p><p>I: state sequence</p><p>O: observation sequence</p><p>转移矩阵 A: transition matrix P(I|I) R(N,N)</p><p>状态观测矩阵B: observation matrix P(O|I) R(N,M)</p><p>初始概率分布 Pi: init state distribution Pi(I) R(1,N])</p><p>在这之中包含两个重要的基础假设（随机过程 is all you need）：</p><ol><li>齐次马尔可夫假设，假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也和时刻无关</li><li>观测独立性假设，假设任意时刻表的观测只依赖于该时刻的马尔可夫链的状态，与其他的观测和状态无关</li></ol><p>下面介绍一个例子，我们将 “Health”和“Fever“作为状态，以不同的表现形式”normal“，‘cold’，‘dizzy”作为观测，同时定义转移概率和转移矩阵等参数，注意这里给出的map形式后面用了函数调整，让解释性变差：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># state space</span><br>Q = &#123;<span class="hljs-string">&#x27;Healthy&#x27;</span>,<span class="hljs-string">&#x27;Fever&#x27;</span>&#125;<br><br><span class="hljs-comment"># observation space</span><br>V = &#123;<span class="hljs-string">&#x27;normal&#x27;</span>,<span class="hljs-string">&#x27;cold&#x27;</span>,<span class="hljs-string">&#x27;dizzy&#x27;</span>&#125;<br><br><span class="hljs-comment"># init state distribution</span><br>Pi = &#123;<br>    <span class="hljs-string">&#x27;Healthy&#x27;</span>:<span class="hljs-number">0.6</span>,<br>    <span class="hljs-string">&#x27;Fever&#x27;</span>:<span class="hljs-number">0.4</span><br>&#125;<br><br><span class="hljs-comment"># state transition</span><br>A = &#123;<br>    <span class="hljs-string">&#x27;Healthy&#x27;</span>: &#123;<span class="hljs-string">&#x27;Healthy&#x27;</span>: <span class="hljs-number">0.7</span>, <span class="hljs-string">&#x27;Fever&#x27;</span>: <span class="hljs-number">0.3</span>&#125;,<br>    <span class="hljs-string">&#x27;Fever&#x27;</span>: &#123;<span class="hljs-string">&#x27;Healthy&#x27;</span>: <span class="hljs-number">0.4</span>, <span class="hljs-string">&#x27;Fever&#x27;</span>: <span class="hljs-number">0.6</span>&#125;,<br>&#125;<br><br><span class="hljs-comment"># observation transition</span><br>B = &#123;<br>    <span class="hljs-string">&#x27;Healthy&#x27;</span>: &#123;<span class="hljs-string">&#x27;normal&#x27;</span>: <span class="hljs-number">0.5</span>, <span class="hljs-string">&#x27;cold&#x27;</span>: <span class="hljs-number">0.4</span>, <span class="hljs-string">&#x27;dizzy&#x27;</span>: <span class="hljs-number">0.1</span>&#125;,<br>    <span class="hljs-string">&#x27;Fever&#x27;</span>: &#123;<span class="hljs-string">&#x27;normal&#x27;</span>: <span class="hljs-number">0.1</span>, <span class="hljs-string">&#x27;cold&#x27;</span>: <span class="hljs-number">0.3</span>, <span class="hljs-string">&#x27;dizzy&#x27;</span>: <span class="hljs-number">0.6</span>&#125;,<br>&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_index_map</span>(<span class="hljs-params">labels</span>):<br>    index2label = &#123;&#125;<br>    label2index = &#123;&#125;<br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels:<br>        index2label[i] = label<br>        label2index[label] = i<br>        i += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> index2label,label2index<br><br>Qlabel,Qindex = generate_index_map(Q)<br>Vlabel,Vindex = generate_index_map(V)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Qlabel,Qindex,Vlabel,Vindex:&#x27;</span>)<br><span class="hljs-built_in">print</span>(Qlabel,Qindex)<br><span class="hljs-built_in">print</span>(Vlabel,Vindex)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gen_Pi</span>(<span class="hljs-params">pi_dict,Qlabel</span>):<br>    v = np.zeros(<span class="hljs-built_in">len</span>(Qlabel),dtype=<span class="hljs-built_in">float</span>)<br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> pi_dict:<br>        v[Qlabel[key]] = pi_dict[key]<br>    <span class="hljs-keyword">return</span> v <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gen_matrix</span>(<span class="hljs-params">trans_dict,Qlabel,Vlabel</span>):<br>    res = np.zeros((<span class="hljs-built_in">len</span>(Qlabel),<span class="hljs-built_in">len</span>(Vlabel)),dtype=<span class="hljs-built_in">float</span>)<br>    <span class="hljs-keyword">for</span> si <span class="hljs-keyword">in</span> trans_dict:<br>        <span class="hljs-keyword">for</span> sj <span class="hljs-keyword">in</span> trans_dict[si]:<br>            res[Qlabel[si]][Vlabel[sj]] =  trans_dict[si][sj]<br>    <span class="hljs-keyword">return</span> res <br><br>A = gen_matrix(A,Qindex,Qindex)<br>B = gen_matrix(B,Qindex,Vindex)<br>pi = gen_Pi(Pi,Qindex)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state transition matrix&#x27;</span>)<br><span class="hljs-built_in">print</span>(A)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state-observation matrix&#x27;</span>)<br><span class="hljs-built_in">print</span>(B)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state prob distribution&#x27;</span>)<br><span class="hljs-built_in">print</span>(pi)<br></code></pre></td></tr></table></figure><p>之后我们来模拟生成</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs inform7">def simulate(T,A,B,pi):<br>    def draw_from(state_prob_distribution):<br>        return np.where(np.random.multinomial(1,state_prob_distribution)==1)<span class="hljs-comment">[0]</span><span class="hljs-comment">[0]</span><br>    observations = np.zeros(T,dtype=int)<br>    states = np.zeros(T,dtype=int)<br>    states<span class="hljs-comment">[0]</span> = draw_from(pi)<br>    observations<span class="hljs-comment">[0]</span> = draw_from(B<span class="hljs-comment">[states<span class="hljs-comment">[0]</span>,:]</span>)<br>    for t in range(1, T):<br>        states<span class="hljs-comment">[t]</span> = draw_from(A<span class="hljs-comment">[states<span class="hljs-comment">[t-1]</span>,:]</span>)<br>        observations<span class="hljs-comment">[t]</span> = draw_from(B<span class="hljs-comment">[states<span class="hljs-comment">[t]</span>,:]</span>)<br>    return observations, states<br><br># simulate the data <br>O,I = simulate(10,A,B,pi)<br>print(<span class="hljs-comment">[Qlabel<span class="hljs-comment">[i]</span> for i in I]</span>)<br>print(<span class="hljs-comment">[Vlabel<span class="hljs-comment">[j]</span> for j in O]</span>)<br></code></pre></td></tr></table></figure><h3 id="3-2-HMM概率计算"><a href="#3-2-HMM概率计算" class="headerlink" title="3.2 HMM概率计算"></a>3.2 HMM概率计算</h3><p>如何计算一条轨迹的概率 其实是一件非常复杂的事情，这个看上去其实可以使用一个R(m^t)来描述，这里参考李航老师的书中给出前向概率的计算和后向概率,之后引申地给出和的计算方式，注意后者都是未来后面问题2用的</p><p>其实前向传播计算和后向传播计算的逻辑很简单，</p><p>在给定的观察轨迹情况下：指的是在时刻t的时候状态s&#x3D;q的概率，所有 R(K,T)</p><p>指的是在给定的观察轨迹情况下，时刻t到T给给定轨迹下，t时刻s&#x3D;q的概率，大小同样为R(K,T)</p><p>由此可以得到</p><p> 表示给定模型和观测值的情况下，时刻表t处于状态q的概率为：</p><p>表示给定模型和观察情况下在，在时刻表t处于状态i，在时刻表t+1处于状态j的概率</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-72b267e75679db40d6f187296881442e_1440w.jpg" alt="img"></p><p>概率计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># In the study of HMM, there are three main problems</span><br><span class="hljs-comment"># Track Probability: give the O=&#123;o1,o2,...,oT&#125;,P(O)</span><br><span class="hljs-comment"># Learning problem: give the O, estimate the parameters</span><br><span class="hljs-comment"># Forecasting problem: (decoding problem also),given the observation track, </span><br><span class="hljs-comment">#                       show the most likely state track</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">obs_seq,A,B,pi</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;cal the probability from the observation sequence with forward&#x27;&#x27;&#x27;</span> <br>    state_len = A.shape[<span class="hljs-number">0</span>]<br>    track_len = <span class="hljs-built_in">len</span>(obs_seq)<br>    F = np.zeros((state_len,track_len))<br><br>    <span class="hljs-comment"># init the state with the pi</span><br>    F[:,<span class="hljs-number">0</span>] = pi*B[:,obs_seq[<span class="hljs-number">0</span>]]<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,track_len):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(state_len):<br>            F[n,t] = np.dot(F[:,t-<span class="hljs-number">1</span>],(A[:,n]))*B[n,obs_seq[t]]<br>    <span class="hljs-keyword">return</span> F<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">obs_seq,A,B,pi</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;cal the probability from the observation sequence with backward&#x27;&#x27;&#x27;</span> <br>    N = A.shape[<span class="hljs-number">0</span>]<br>    T = <span class="hljs-built_in">len</span>(obs_seq)<br>    <span class="hljs-comment"># X保存后向概率矩阵</span><br>    X = np.zeros((N,T))<br>    X[:,-<span class="hljs-number">1</span>:] = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(T-<span class="hljs-number">1</span>)):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>            X[n,t] = np.<span class="hljs-built_in">sum</span>(X[:,t+<span class="hljs-number">1</span>] * A[n,:] * B[:, obs_seq[t+<span class="hljs-number">1</span>]])<br><br>    <span class="hljs-keyword">return</span> X<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;track probability forward&#x27;</span>)<br><span class="hljs-built_in">print</span>(forward(O,A,B,pi))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;track probability backward&#x27;</span>)<br><span class="hljs-built_in">print</span>(backward(O,A,B,pi))<br></code></pre></td></tr></table></figure><h3 id="3-3-HMM参数估计"><a href="#3-3-HMM参数估计" class="headerlink" title="3.3 HMM参数估计"></a>3.3 HMM参数估计</h3><p>这里使用学习方法，将EM用在HMM中得到Baum Welch算法</p><p>首先确定E步中的Q函数</p><p>&#x3D;  </p><p>之后在M步中更新参数，这里有人求解过了</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># Based on the EM algrithm, the HMM learning process can be solved by Baum-Weich algorithm<br>def baum_weich(obs_seq,A,B,pi,criterion=<span class="hljs-number">0.05</span>):<br>    &#x27;&#x27;&#x27;<br>    Unsupervised learning algorithm<br>    &#x27;&#x27;&#x27;<br>    n_states = A.shape[<span class="hljs-number">0</span>]<br>    n_samples = len(obs_seq)<br><br>    done = False<br><br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>        # Cal the <span class="hljs-built_in">track</span> probability<br>        # alpha(i) = p(obs_seq | q0=s_i,hmm)<br>        alpha = forward(obs_seq,A,B,pi)<br>        # <span class="hljs-built_in">beta</span>(i)=p(obs_seq | q0=s_i,hmm)<br>        <span class="hljs-built_in">beta</span> = backward(obs_seq,A,B,pi)<br><br>        # xi: <span class="hljs-keyword">in</span> the timestamp t, the t state <span class="hljs-built_in">is</span> i <span class="hljs-keyword">and</span> the t+<span class="hljs-number">1</span> state <span class="hljs-built_in">is</span> j<br>        xi = <span class="hljs-built_in">np</span>.zeros((n_states,n_states,n_samples-<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples-<span class="hljs-number">1</span>):<br>            <span class="hljs-built_in">denom</span> = <span class="hljs-built_in">np</span>.dot(<span class="hljs-built_in">np</span>.dot(alpha[:,t].T, A) * B[:,obs_seq[t+<span class="hljs-number">1</span>]].T, <span class="hljs-built_in">beta</span>[:,t+<span class="hljs-number">1</span>])<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_states):<br>                <span class="hljs-built_in">numer</span> = alpha[i,t] * A[i,:] * B[:,obs_seq[t+<span class="hljs-number">1</span>]].T * <span class="hljs-built_in">beta</span>[:,t+<span class="hljs-number">1</span>].T<br>                xi[i,:,t] = <span class="hljs-built_in">numer</span> / <span class="hljs-built_in">denom</span><br><br>        # <span class="hljs-built_in">gamma</span>, <span class="hljs-keyword">in</span> the timestamp t ,the t state <span class="hljs-built_in">is</span> i<br>        <span class="hljs-built_in">gamma</span> = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(xi,axis=<span class="hljs-number">1</span>)<br>        # prod <span class="hljs-keyword">for</span> what<br>        prod = (alpha[:,n_samples-<span class="hljs-number">1</span>]*<span class="hljs-built_in">beta</span>[:,n_samples-<span class="hljs-number">1</span>]).reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>        <span class="hljs-built_in">gamma</span> = <span class="hljs-built_in">np</span>.hstack((<span class="hljs-built_in">gamma</span>,prod/<span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(prod)))<br><br>        # M<br>        newpi = <span class="hljs-built_in">gamma</span>[:,<span class="hljs-number">0</span>]<br>        newA = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(xi,<span class="hljs-number">2</span>) / <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">gamma</span>[:,:-<span class="hljs-number">1</span>],axis=<span class="hljs-number">1</span>).reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br>        newB = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">copy</span>(B)<br>        num_levels = B.shape[<span class="hljs-number">1</span>]<br>        sumgamma = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">gamma</span>,axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">for</span> lev <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_levels):<br>            mask = obs_seq == lev<br>            newB[:,lev] = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">gamma</span>[:,mask],axis=<span class="hljs-number">1</span>) / sumgamma<br>        # 检查是否满足阈值<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(<span class="hljs-built_in">abs</span>(pi - newpi)) &lt; criterion <span class="hljs-keyword">and</span> \<br>                        <span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(<span class="hljs-built_in">abs</span>(A - newA)) &lt; criterion <span class="hljs-keyword">and</span> \<br>                        <span class="hljs-built_in">np</span>.<span class="hljs-built_in">max</span>(<span class="hljs-built_in">abs</span>(B - newB)) &lt; criterion:<br>            done = <span class="hljs-number">1</span><br>        A[:], B[:], pi[:] = newA, newB, newpi<br>    <span class="hljs-built_in">return</span> newA, newB, newpi<br><br>A = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>],[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>]])<br>B = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>],[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.3</span>]])<br>pi = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([<span class="hljs-number">0.6</span>, <span class="hljs-number">0.4</span>])<br><br>observations_data, states_data = simulate(<span class="hljs-number">100</span>,A,B,pi)<br>newA, newB, newpi = baum_weich(observations_data, A, B, pi)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;newA: &quot;</span>, newA)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;newB: &quot;</span>, newB)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;newpi: &quot;</span>, newpi)<br></code></pre></td></tr></table></figure><h3 id="3-4-预测-维特比算法Viterbi-algorithm"><a href="#3-4-预测-维特比算法Viterbi-algorithm" class="headerlink" title="3.4 预测 维特比算法Viterbi algorithm"></a>3.4 预测 维特比算法Viterbi algorithm</h3><p>本质上是一个用动态规划求解最优路径的问题，很简单～只是和时刻表有点相似,这里不在详细的介绍。参考连接</p><p><a href="https://applenob.github.io/machine_learning/HMM/">https://applenob.github.io/machine_learning/HMM/</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">viterbi</span>(<span class="hljs-params">obs_seq, A, B, pi</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    V : numpy.ndarray</span><br><span class="hljs-string">        V [s][t] = Maximum probability of an observation sequence ending</span><br><span class="hljs-string">                   at time &#x27;t&#x27; with final state &#x27;s&#x27;</span><br><span class="hljs-string">    prev : numpy.ndarray</span><br><span class="hljs-string">        Contains a pointer to the previous state at t-1 that maximizes</span><br><span class="hljs-string">        V[state][t]</span><br><span class="hljs-string"></span><br><span class="hljs-string">    V对应δ，prev对应ψ</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    N = A.shape[<span class="hljs-number">0</span>]<br>    T = <span class="hljs-built_in">len</span>(obs_seq)<br>    prev = np.zeros((T - <span class="hljs-number">1</span>, N), dtype=<span class="hljs-built_in">int</span>)<br><br>    <span class="hljs-comment"># DP matrix containing max likelihood of state at a given time</span><br>    V = np.zeros((N, T))<br>    V[:,<span class="hljs-number">0</span>] = pi * B[:,obs_seq[<span class="hljs-number">0</span>]]<br><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, T):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>            seq_probs = V[:,t-<span class="hljs-number">1</span>] * A[:,n] * B[n, obs_seq[t]]<br>            prev[t-<span class="hljs-number">1</span>,n] = np.argmax(seq_probs)<br>            V[n,t] = np.<span class="hljs-built_in">max</span>(seq_probs)<br><br>    <span class="hljs-keyword">return</span> V, prev<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_viterbi_path</span>(<span class="hljs-params">prev, last_state</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Returns a state path ending in last_state in reverse order.</span><br><span class="hljs-string">    最优路径回溯</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    T = <span class="hljs-built_in">len</span>(prev)<br>    <span class="hljs-keyword">yield</span>(last_state)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">yield</span>(prev[i, last_state])<br>        last_state = prev[i, last_state]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">observation_prob</span>(<span class="hljs-params">obs_seq</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; P( entire observation sequence | A, B, pi ) &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(forward(obs_seq)[:,-<span class="hljs-number">1</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">state_path</span>(<span class="hljs-params">obs_seq, A, B, pi</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    V[last_state, -1] : float</span><br><span class="hljs-string">        Probability of the optimal state path</span><br><span class="hljs-string">    path : list(int)</span><br><span class="hljs-string">        Optimal state path for the observation sequence</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    V, prev = viterbi(obs_seq, A, B, pi)<br>    <span class="hljs-comment"># Build state path with greatest probability</span><br>    last_state = np.argmax(V[:,-<span class="hljs-number">1</span>])<br>    path = <span class="hljs-built_in">list</span>(build_viterbi_path(prev, last_state))<br><br>    <span class="hljs-keyword">return</span> V[last_state,-<span class="hljs-number">1</span>], <span class="hljs-built_in">reversed</span>(path)<br></code></pre></td></tr></table></figure><h3 id="0x04-条件随机场CRF"><a href="#0x04-条件随机场CRF" class="headerlink" title="0x04 条件随机场CRF"></a>0x04 条件随机场CRF</h3><p>在之前的分析中HMM中的马尔可夫假设非常强，但是很多时候标准存在序列相关的情况，因此常见的是需要参考前面多个时刻表的隐藏状态作为后续的分析，因此这种马尔可夫随机场的方式给出一种新的求解方。这里看理论过于深奥了，参考Torch官方文档中给出的BiLSTM-CRF的例子，可能会比较好理解一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.autograd <span class="hljs-keyword">as</span> autograd<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn <br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br>torch.manual_seed(<span class="hljs-number">2023</span>)<br><br><span class="hljs-comment"># Define the helper functions to make the code more readable</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">argmax</span>(<span class="hljs-params">vector</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    return the argmax index </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    _, idx = torch.<span class="hljs-built_in">max</span>(vector,<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> idx.item()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_sequence</span>(<span class="hljs-params">seq,to_ix</span>):<br>    idxs = [to_ix[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq]<br>    <span class="hljs-keyword">return</span> torch.tensor(idxs,dtype=torch.long)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_sum_exp</span>(<span class="hljs-params">vector</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    return the log sum exp in a numerically stable way fot the forward algorithm </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    max_score = vector[<span class="hljs-number">0</span>,argmax(vector)]<br>    max_score_broadcast = max_score.view(<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>).expand(<span class="hljs-number">1</span>,vector.size()[<span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">return</span> max_score+torch.log(torch.<span class="hljs-built_in">sum</span>(torch.exp(vector-max_score_broadcast)))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BiLSTM_CRF</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, tag_to_ix, embedding_dim, hidden_dim</span>):<br>        <span class="hljs-built_in">super</span>(BiLSTM_CRF, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.embedding_dim = embedding_dim<br>        <span class="hljs-variable language_">self</span>.hidden_dim = hidden_dim<br>        <span class="hljs-variable language_">self</span>.vocab_size = vocab_size<br>        <span class="hljs-variable language_">self</span>.tag_to_ix = tag_to_ix<br>        <span class="hljs-variable language_">self</span>.tagset_size = <span class="hljs-built_in">len</span>(tag_to_ix)<br><br>        <span class="hljs-variable language_">self</span>.word_embeds = nn.Embedding(vocab_size, embedding_dim)<br>        <span class="hljs-variable language_">self</span>.lstm = nn.LSTM(embedding_dim, hidden_dim // <span class="hljs-number">2</span>,<br>                            num_layers=<span class="hljs-number">1</span>, bidirectional=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># Maps the output of the LSTM into tag space.</span><br>        <span class="hljs-variable language_">self</span>.hidden2tag = nn.Linear(hidden_dim, <span class="hljs-variable language_">self</span>.tagset_size)<br><br>        <span class="hljs-comment"># Matrix of transition parameters.  Entry i,j is the score of</span><br>        <span class="hljs-comment"># transitioning *to* i *from* j.</span><br>        <span class="hljs-variable language_">self</span>.transitions = nn.Parameter(<br>            torch.randn(<span class="hljs-variable language_">self</span>.tagset_size, <span class="hljs-variable language_">self</span>.tagset_size))<br><br>        <span class="hljs-comment"># These two statements enforce the constraint that we never transfer</span><br>        <span class="hljs-comment"># to the start tag and we never transfer from the stop tag</span><br>        <span class="hljs-variable language_">self</span>.transitions.data[tag_to_ix[START_TAG], :] = -<span class="hljs-number">10000</span><br>        <span class="hljs-variable language_">self</span>.transitions.data[:, tag_to_ix[STOP_TAG]] = -<span class="hljs-number">10000</span><br><br>        <span class="hljs-variable language_">self</span>.hidden = <span class="hljs-variable language_">self</span>.init_hidden()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_hidden</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> (torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.hidden_dim // <span class="hljs-number">2</span>),<br>                torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.hidden_dim // <span class="hljs-number">2</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_alg</span>(<span class="hljs-params">self, feats</span>):<br>        <span class="hljs-comment"># Do the forward algorithm to compute the partition function</span><br>        init_alphas = torch.full((<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.tagset_size), -<span class="hljs-number">10000.</span>)<br>        <span class="hljs-comment"># START_TAG has all of the score.</span><br>        init_alphas[<span class="hljs-number">0</span>][<span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]] = <span class="hljs-number">0.</span><br><br>        <span class="hljs-comment"># Wrap in a variable so that we will get automatic backprop</span><br>        forward_var = init_alphas<br><br>        <span class="hljs-comment"># Iterate through the sentence</span><br>        <span class="hljs-keyword">for</span> feat <span class="hljs-keyword">in</span> feats:<br>            alphas_t = []  <span class="hljs-comment"># The forward tensors at this timestep</span><br>            <span class="hljs-keyword">for</span> next_tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.tagset_size):<br>                <span class="hljs-comment"># broadcast the emission score: it is the same regardless of</span><br>                <span class="hljs-comment"># the previous tag</span><br>                emit_score = feat[next_tag].view(<br>                    <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).expand(<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.tagset_size)<br>                <span class="hljs-comment"># the ith entry of trans_score is the score of transitioning to</span><br>                <span class="hljs-comment"># next_tag from i</span><br>                trans_score = <span class="hljs-variable language_">self</span>.transitions[next_tag].view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>                <span class="hljs-comment"># The ith entry of next_tag_var is the value for the</span><br>                <span class="hljs-comment"># edge (i -&gt; next_tag) before we do log-sum-exp</span><br>                next_tag_var = forward_var + trans_score + emit_score<br>                <span class="hljs-comment"># The forward variable for this tag is log-sum-exp of all the</span><br>                <span class="hljs-comment"># scores.</span><br>                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="hljs-number">1</span>))<br>            forward_var = torch.cat(alphas_t).view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>        terminal_var = forward_var + <span class="hljs-variable language_">self</span>.transitions[<span class="hljs-variable language_">self</span>.tag_to_ix[STOP_TAG]]<br>        alpha = log_sum_exp(terminal_var)<br>        <span class="hljs-keyword">return</span> alpha<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_lstm_features</span>(<span class="hljs-params">self, sentence</span>):<br>        <span class="hljs-variable language_">self</span>.hidden = <span class="hljs-variable language_">self</span>.init_hidden()<br>        embeds = <span class="hljs-variable language_">self</span>.word_embeds(sentence).view(<span class="hljs-built_in">len</span>(sentence), <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>        lstm_out, <span class="hljs-variable language_">self</span>.hidden = <span class="hljs-variable language_">self</span>.lstm(embeds, <span class="hljs-variable language_">self</span>.hidden)<br>        lstm_out = lstm_out.view(<span class="hljs-built_in">len</span>(sentence), <span class="hljs-variable language_">self</span>.hidden_dim)<br>        lstm_feats = <span class="hljs-variable language_">self</span>.hidden2tag(lstm_out)<br>        <span class="hljs-keyword">return</span> lstm_feats<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_score_sentence</span>(<span class="hljs-params">self, feats, tags</span>):<br>        <span class="hljs-comment"># Gives the score of a provided tag sequence</span><br>        score = torch.zeros(<span class="hljs-number">1</span>)<br>        tags = torch.cat([torch.tensor([<span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]], dtype=torch.long), tags])<br>        <span class="hljs-keyword">for</span> i, feat <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(feats):<br>            score = score + \<br>                <span class="hljs-variable language_">self</span>.transitions[tags[i + <span class="hljs-number">1</span>], tags[i]] + feat[tags[i + <span class="hljs-number">1</span>]]<br>        score = score + <span class="hljs-variable language_">self</span>.transitions[<span class="hljs-variable language_">self</span>.tag_to_ix[STOP_TAG], tags[-<span class="hljs-number">1</span>]]<br>        <span class="hljs-keyword">return</span> score<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_viterbi_decode</span>(<span class="hljs-params">self, feats</span>):<br>        backpointers = []<br><br>        <span class="hljs-comment"># Initialize the viterbi variables in log space</span><br>        init_vvars = torch.full((<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.tagset_size), -<span class="hljs-number">10000.</span>)<br>        init_vvars[<span class="hljs-number">0</span>][<span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># forward_var at step i holds the viterbi variables for step i-1</span><br>        forward_var = init_vvars<br>        <span class="hljs-keyword">for</span> feat <span class="hljs-keyword">in</span> feats:<br>            bptrs_t = []  <span class="hljs-comment"># holds the backpointers for this step</span><br>            viterbivars_t = []  <span class="hljs-comment"># holds the viterbi variables for this step</span><br><br>            <span class="hljs-keyword">for</span> next_tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.tagset_size):<br>                <span class="hljs-comment"># next_tag_var[i] holds the viterbi variable for tag i at the</span><br>                <span class="hljs-comment"># previous step, plus the score of transitioning</span><br>                <span class="hljs-comment"># from tag i to next_tag.</span><br>                <span class="hljs-comment"># We don&#x27;t include the emission scores here because the max</span><br>                <span class="hljs-comment"># does not depend on them (we add them in below)</span><br>                next_tag_var = forward_var + <span class="hljs-variable language_">self</span>.transitions[next_tag]<br>                best_tag_id = argmax(next_tag_var)<br>                bptrs_t.append(best_tag_id)<br>                viterbivars_t.append(next_tag_var[<span class="hljs-number">0</span>][best_tag_id].view(<span class="hljs-number">1</span>))<br>            <span class="hljs-comment"># Now add in the emission scores, and assign forward_var to the set</span><br>            <span class="hljs-comment"># of viterbi variables we just computed</span><br>            forward_var = (torch.cat(viterbivars_t) + feat).view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>            backpointers.append(bptrs_t)<br><br>        <span class="hljs-comment"># Transition to STOP_TAG</span><br>        terminal_var = forward_var + <span class="hljs-variable language_">self</span>.transitions[<span class="hljs-variable language_">self</span>.tag_to_ix[STOP_TAG]]<br>        best_tag_id = argmax(terminal_var)<br>        path_score = terminal_var[<span class="hljs-number">0</span>][best_tag_id]<br><br>        <span class="hljs-comment"># Follow the back pointers to decode the best path.</span><br>        best_path = [best_tag_id]<br>        <span class="hljs-keyword">for</span> bptrs_t <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(backpointers):<br>            best_tag_id = bptrs_t[best_tag_id]<br>            best_path.append(best_tag_id)<br>        <span class="hljs-comment"># Pop off the start tag (we dont want to return that to the caller)</span><br>        start = best_path.pop()<br>        <span class="hljs-keyword">assert</span> start == <span class="hljs-variable language_">self</span>.tag_to_ix[START_TAG]  <span class="hljs-comment"># Sanity check</span><br>        best_path.reverse()<br>        <span class="hljs-keyword">return</span> path_score, best_path<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">neg_log_likelihood</span>(<span class="hljs-params">self, sentence, tags</span>):<br>        <span class="hljs-comment"># Input the sentence with the dirsed tags</span><br>        feats = <span class="hljs-variable language_">self</span>._get_lstm_features(sentence)<br>        <span class="hljs-comment"># Cancaluate the feature from the lsmt layers</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;FEAT&#x27;</span>,feats)<br>        forward_score = <span class="hljs-variable language_">self</span>._forward_alg(feats)<br>        <span class="hljs-comment"># Calculante the forward scoress, based on the feats</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;FORWARD_SCORE&#x27;</span>,forward_score)<br>        gold_score = <span class="hljs-variable language_">self</span>._score_sentence(feats, tags)<br>        <span class="hljs-comment"># calculate the score from the sentence</span><br>        <span class="hljs-keyword">return</span> forward_score - gold_score<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sentence</span>):  <span class="hljs-comment"># dont confuse this with _forward_alg above.</span><br>        <span class="hljs-comment"># Get the emission scores from the BiLSTM</span><br>        lstm_feats = <span class="hljs-variable language_">self</span>._get_lstm_features(sentence)<br><br>        <span class="hljs-comment"># Find the best path, given the features.</span><br>        score, tag_seq = <span class="hljs-variable language_">self</span>._viterbi_decode(lstm_feats)<br>        <span class="hljs-keyword">return</span> score, tag_seq<br><br><span class="hljs-comment"># Training process</span><br>START_TAG = <span class="hljs-string">&#x27;&lt;START&gt;&#x27;</span><br>STOP_TAG = <span class="hljs-string">&#x27;&lt;STOP&gt;&#x27;</span><br>EMBEDDING_DIM = <span class="hljs-number">5</span><br>HIDDEN_DEM = <span class="hljs-number">4</span><br><br><span class="hljs-comment"># Makding up the training data</span><br>training_data = [<br>    ( <span class="hljs-string">&quot;the wall street journal reported today that apple corporation made money&quot;</span>.split(),<br>    <span class="hljs-string">&quot;B I I I O O O B I O O&quot;</span>.split()),<br>    ( <span class="hljs-string">&quot;georgia tech is a university in georgia&quot;</span>.split(),<br>    <span class="hljs-string">&quot;B I O O O O B&quot;</span>.split() )<br>]<br>word_to_ix = &#123;&#125;<br><span class="hljs-keyword">for</span> sentence,tags <span class="hljs-keyword">in</span> training_data:<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence:<br>        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> word_to_ix:<br>            word_to_ix[word] = <span class="hljs-built_in">len</span>(word_to_ix)<br><br>tag_to_ix = &#123;<span class="hljs-string">&quot;B&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;I&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;O&quot;</span>: <span class="hljs-number">2</span>, START_TAG: <span class="hljs-number">3</span>, STOP_TAG: <span class="hljs-number">4</span>&#125;<br>model = BiLSTM_CRF(<span class="hljs-built_in">len</span>(word_to_ix),tag_to_ix,EMBEDDING_DIM,HIDDEN_DEM)<br>optimizer = optim.SGD(model.parameters(),lr=<span class="hljs-number">0.01</span>,weight_decay=<span class="hljs-number">1e-4</span>)<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    precheck_sent = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],word_to_ix)<br>    precheck_tags = torch.tensor([tag_to_ix[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> training_data[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]],dtype=torch.long)<br><br>    <span class="hljs-built_in">print</span>(model.neg_log_likelihood(precheck_sent,precheck_tags))<br><br><span class="hljs-comment"># Make sure prepare_sequence from earlier in the LSTM section is loaded</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">300</span>):  <span class="hljs-comment"># again, normally you would NOT do 300 epochs, it is toy data</span><br>    <span class="hljs-keyword">for</span> sentence, tags <span class="hljs-keyword">in</span> training_data:<br>        <span class="hljs-comment"># Step 1. Remember that Pytorch accumulates gradients.</span><br>        <span class="hljs-comment"># We need to clear them out before each instance</span><br>        model.zero_grad()<br><br>        <span class="hljs-comment"># Step 2. Get our inputs ready for the network, that is,</span><br>        <span class="hljs-comment"># turn them into Tensors of word indices.</span><br>        sentence_in = prepare_sequence(sentence, word_to_ix)<br>        targets = torch.tensor([tag_to_ix[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tags], dtype=torch.long)<br><br>        <span class="hljs-comment"># Step 3. Run our forward pass.</span><br>        loss = model.neg_log_likelihood(sentence_in, targets)<br><br>        <span class="hljs-comment"># Step 4. Compute the loss, gradients, and update the parameters by</span><br>        <span class="hljs-comment"># calling optimizer.step()</span><br>        loss.backward()<br>        optimizer.step()<br><br><span class="hljs-comment"># Check predictions after training</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    precheck_sent = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], word_to_ix)<br>    <span class="hljs-built_in">print</span>(model(precheck_sent))<br></code></pre></td></tr></table></figure><h3 id="0x05-总结"><a href="#0x05-总结" class="headerlink" title="0x05 总结"></a>0x05 总结</h3><p>还是没有看懂EM算法，还在理论推导和代码实现之前做权衡</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
      <tag>EM 算法</tag>
      
      <tag>HMM</tag>
      
      <tag>CRF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch 官方文档阅读和实现</title>
    <link href="/posts/f2248357.html"/>
    <url>/posts/f2248357.html</url>
    
    <content type="html"><![CDATA[<p>Torch文档阅读blog.tjdata.site&#x2F;posts&#x2F;5ef17af5.html</p><p>Torch官方文档中最关键的类torch.tensor，最重要的机制autograd，这里对常见的API进行总结</p><h3 id="Pytorch-基本教程"><a href="#Pytorch-基本教程" class="headerlink" title="Pytorch 基本教程"></a>Pytorch 基本教程</h3><p>从组成元素上来看学习Torch主要需要熟悉以下几个概念，个人总结以下几个概念 1. 什么是Tensor？这个是深度学习框架计算的源泉，参考NumPy的array和Pandas的Dataframe。我们对于基本变量的操作主要分为如何创建？如何生成？如何复制？如何运算？如何索引？如何删除？等等操作 2. 数据的预处理，如何通过读取数据来构建自己的dataset、dataloader，并对数据进行一定的transform、随机抽取等等 3. 深度的八股：网路结构Net、优化器选择Optim、损失函数的选择Loss等等 4. 训练函数和测试函数。如何在epoch训练、如何打印loss、如何计算summary等等</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch<br><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-comment"># 生成torch通常可以包括以下几点</span><br><span class="hljs-comment"># 1. 利用内置的函数 empty.ones.rand.randn.randint randperm linspace等等</span><br><span class="hljs-comment"># 2. 利用现有的数据，list、numpy.array、tensor等</span><br><span class="hljs-comment"># 3. 通过运算broadcasting,cat,join等</span><br><br><span class="hljs-attribute">x</span> =torch.empty(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">x_numpy</span>=np.zeros((<span class="hljs-number">5</span>,<span class="hljs-number">3</span>))<br><br><span class="hljs-comment"># Torch 生成随机数的种类</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>) # 均匀分布<br><span class="hljs-attribute">torch</span>.randint(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)) #整数分布<br><span class="hljs-attribute">torch</span>.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>) # 标准正态分布<br><span class="hljs-attribute">torch</span>.randperm(<span class="hljs-number">10</span>) # 不重复随机数<br><span class="hljs-attribute">torch</span>.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">1</span>) # 线性间距<br><span class="hljs-attribute">torch</span>.arange(<span class="hljs-number">12</span>)<br><span class="hljs-attribute">torch</span>.poisson(torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>)*<span class="hljs-number">5</span>) # poisson分布<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><span class="hljs-attribute">plt</span>.plot(torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,steps=<span class="hljs-number">50</span>),torch.poisson(torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,steps=<span class="hljs-number">50</span>)))<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0fee721b859e9ee5e457c9eaa112fcbb_1440w.png" alt="img"></p><p>image-20230523222823589</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">x</span>=torch.randn(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>,dtype=torch.float32)<br><span class="hljs-attribute">x</span> = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br><span class="hljs-attribute">x</span> = torch.tensor(np.arange(<span class="hljs-number">10</span>))<br><span class="hljs-attribute">x</span> = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">y</span> = torch.rand_like(x)<br><span class="hljs-comment"># Torch之间的运算</span><br><span class="hljs-comment"># torch默认的操作是elements和broadcasting的</span><br><span class="hljs-attribute">X</span> = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">y</span>= torch.rand(<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)<br><br><span class="hljs-comment"># torch.tensor之间的可能会有三种方式：运算符号，运算函数，内在的函数</span><br><span class="hljs-attribute">torch</span>.add(X,y)<br><span class="hljs-attribute">X</span>.add_(y)<br><span class="hljs-attribute">X</span>+y<br><br><span class="hljs-comment"># 对于tensor的操作</span><br><span class="hljs-attribute">x</span>=torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)<br><span class="hljs-attribute">x</span>[:,:,-<span class="hljs-number">1</span>]<br><span class="hljs-attribute">y</span>=x.view(<span class="hljs-number">60</span>)<br><span class="hljs-attribute">z</span>=x.view(<span class="hljs-number">5</span>,<span class="hljs-number">12</span>)<br><span class="hljs-attribute">z</span><br><span class="hljs-attribute">x</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].item()<br><br><span class="hljs-comment"># 对于tensor的reshape操作通常可以分为:名字name、维度shape和类型type</span><br><span class="hljs-comment"># view和reshape是在改变形状</span><br><span class="hljs-comment"># squeeze unsqueeze 增加维度和删减维度</span><br><span class="hljs-comment"># transpose permute 是变换维度</span><br><span class="hljs-comment"># expand repeat 维度拓展</span><br><span class="hljs-attribute">images</span>=torch.randn(<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>) # <span class="hljs-number">4</span>张通道数为<span class="hljs-number">1</span>的<span class="hljs-number">28</span>*<span class="hljs-number">28</span>的图片<br><span class="hljs-attribute">images_batch</span>=images.view(<span class="hljs-number">4</span>,<span class="hljs-number">28</span>*<span class="hljs-number">28</span>)<br><br><span class="hljs-comment"># 在0维度拓展维度unsqueeze，相当于将tensor放入包装盒种</span><br><span class="hljs-attribute">images_batch</span>.unsqueeze(<span class="hljs-number">0</span>).shape<br><br><span class="hljs-comment"># Tensor 的合并和分割主要包括</span><br><span class="hljs-comment"># concatenate 连接，作用是将2个tensor按照特定的维度连接起来，其他维度必须相同</span><br><span class="hljs-comment"># stack 对接起来</span><br><span class="hljs-comment"># split 根据长度拆分tensor</span><br><span class="hljs-comment"># chunk 均等分的split</span><br><span class="hljs-attribute">a</span>=torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">b</span>=torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.cat([a,b],dim=<span class="hljs-number">0</span>)<br><span class="hljs-attribute">a</span>.split([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h3 id="0x02-自动微分-Autograd"><a href="#0x02-自动微分-Autograd" class="headerlink" title="0x02 自动微分 Autograd"></a>0x02 自动微分 Autograd</h3><p>上述虽然给tensor的创建、索引、运算和形状改变，但其实更重要的是Torch基于此实现的自动微分机制。可能需要一些前向推理和反向传播的知识，这里需要结合tensor和gradient来介绍一些基础的知识。初步的需要了解</p><ol><li>tensor中grad的激活、分割、计算等等</li><li>运算</li></ol><p>torch.Tensor是package的核心类，如果将其属性</p><p>.require_grad设置为true，则会开始跟踪tensor的所有操作，在完成计算之后，可以调用</p><p>.backward()来自动就按所有的梯度，累积到</p><p>.grad属性之中</p><p>.detach() 可以停止跟踪历史记录和内存，或者在with torch.no_grad()</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">x = torch.randn(2,2,<span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-attribute">y</span>=x+2<br><span class="hljs-attribute">z</span>=y*y*3<br><span class="hljs-attribute">out</span>=z.sum()<br>out.backward()<br>x.grad # 这里想当于 d(out)/d(x)<br><br><span class="hljs-attribute">x</span>=torch.randn(10,3,requires_grad=True)<br><span class="hljs-attribute">b</span>=torch.randn(3,1,requires_grad=True)<br><span class="hljs-attribute">y</span>=torch.matmul(x,b).sum()<br>y.backward()<br>b<br></code></pre></td></tr></table></figure><h3 id="0x03-神经网络的搭建"><a href="#0x03-神经网络的搭建" class="headerlink" title="0x03 神经网络的搭建"></a>0x03 神经网络的搭建</h3><ol><li>网络内部需要考虑 input 和 output，内部需要考虑layer和forward</li><li>迭代整个输入</li><li>通过神经网络处理输入</li><li>计算损失 loss</li><li>反向传播计算梯度</li><li>更新网络的参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn <br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 假设图片的输入是 1channel,5*5,6channel 输出</span><br>        <span class="hljs-variable language_">self</span>.conv1=nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.conv2=nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,<span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># 这里的16*5*5可以推理应该需要的size</span><br>        <span class="hljs-variable language_">self</span>.fc1=nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2=nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3=nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=F.max_pool2d(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)),(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># If the size is a square you can only specify a single number</span><br>        x = F.max_pool2d(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)), <span class="hljs-number">2</span>)<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.num_flat_features(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">num_flat_features</span>(<span class="hljs-params">self, x</span>):<br>        size = x.size()[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># all dimensions except the batch dimension</span><br>        num_features = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> size:<br>            num_features *= s<br>        <span class="hljs-keyword">return</span> num_features<br>net=Net()<br><span class="hljs-comment"># Question: 这里必须要加上一个梯度</span><br><span class="hljs-built_in">input</span>=torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,requires_grad=<span class="hljs-literal">True</span>)<br><br>output=net(<span class="hljs-built_in">input</span>)<br><br><span class="hljs-comment"># dummy target</span><br>target=torch.rand(<span class="hljs-number">10</span>)<br>target=target.unsqueeze(<span class="hljs-number">0</span>)<br>criterion=nn.MSELoss()<br><br>loss=criterion(target,output)<br><br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br>optimizer=optim.SGD(net.parameters(),lr=<span class="hljs-number">0.01</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;conv1.bias.grad before backward&#x27;</span>)<br><span class="hljs-built_in">print</span>(net.conv1.bias.grad)<br>optimizer.zero_grad()<br>out=net(<span class="hljs-built_in">input</span>)<br>loss=criterion(output,target)<br>loss.backward()<br>optimizer.step()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;conv1.bias.grad after backward&#x27;</span>)<br><span class="hljs-built_in">print</span>(net.conv1.bias.grad)<br><span class="hljs-comment">#用net来训练CIFA10</span><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><br>transform = transforms.Compose(<br>    [transforms.ToTensor(),<br>     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<br><br>trainset=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>,train=<span class="hljs-literal">True</span>,download=<span class="hljs-literal">False</span>,transform=transform)<br><br>trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="hljs-number">4</span>,<br>                                          shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>classes = (<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>,<br>           <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">imshow</span>(<span class="hljs-params">img</span>):<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>     <span class="hljs-comment"># unnormalize</span><br>    npimg = img.numpy()<br>    plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br><br><br><span class="hljs-comment"># get some random training images</span><br>dataiter = <span class="hljs-built_in">iter</span>(trainloader)<br>images, labels = <span class="hljs-built_in">next</span>(dataiter)<br><br><span class="hljs-comment"># show images</span><br>imshow(torchvision.utils.make_grid(images))<br><span class="hljs-comment"># print labels</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)))<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)))<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>net = Net()<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):  <span class="hljs-comment"># loop over the dataset multiple times</span><br><br>    running_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader, <span class="hljs-number">0</span>):<br>        <span class="hljs-comment"># get the inputs</span><br>        inputs, labels = data<br><br>        <span class="hljs-comment"># zero the parameter gradients</span><br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># forward + backward + optimize</span><br>        outputs = net(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>        <span class="hljs-comment"># print statistics</span><br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2000</span> == <span class="hljs-number">1999</span>:    <span class="hljs-comment"># print every 2000 mini-batches</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %<br>                  (epoch + <span class="hljs-number">1</span>, i + <span class="hljs-number">1</span>, running_loss / <span class="hljs-number">2000</span>))<br>            running_loss = <span class="hljs-number">0.0</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="One-more-thing-cheetsheet"><a href="#One-more-thing-cheetsheet" class="headerlink" title="One more thing: cheetsheet"></a>One more thing: cheetsheet</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># creating tensors</span><br><span class="hljs-attribute">x</span> =torch.empty(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.manual_seed(<span class="hljs-number">1998</span>)<br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.rand_like(x)<br><span class="hljs-attribute">torch</span>.tensro([<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br><span class="hljs-attribute">torch</span>.one((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),dtype=torch.int16)<br><br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)+<span class="hljs-number">1</span><br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)*<span class="hljs-number">3</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)**<span class="hljs-number">4</span><br><br><span class="hljs-comment"># broadcasting</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*(torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*<span class="hljs-number">2</span>)<br><span class="hljs-comment"># more math with tensors</span><br><span class="hljs-attribute">torch</span>.abs(a)<br><span class="hljs-attribute">torch</span>.ceil(a)<br><span class="hljs-attribute">torch</span>.floor(a)<br><span class="hljs-attribute">torch</span>.clamp(a,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>)<br><span class="hljs-attribute">torch</span>.sin(a)<br><span class="hljs-attribute">torch</span>.asin(a)<br><span class="hljs-attribute">torch</span>.bitwise_and(a,x)<br><br><span class="hljs-attribute">torch</span>.max()<br><span class="hljs-attribute">torch</span>.mean()<br><span class="hljs-attribute">torch</span>.std()<br><span class="hljs-attribute">torch</span>.prod()<br><br><span class="hljs-attribute">v1</span> = torch.tensor([<span class="hljs-number">1</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.])         # x unit vector<br><span class="hljs-attribute">v2</span> = torch.tensor([<span class="hljs-number">0</span>., <span class="hljs-number">1</span>., <span class="hljs-number">0</span>.])         # y unit vector<br><span class="hljs-attribute">torch</span>.cross(v2,v1)<br><span class="hljs-attribute">torch</span>.matmul(a,x)<br><span class="hljs-attribute">torch</span>.mv(a,v2)<br><span class="hljs-attribute">torch</span>.svd(x)<br><br><span class="hljs-comment"># creating tensors</span><br><span class="hljs-attribute">x</span> =torch.empty(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br><span class="hljs-attribute">torch</span>.manual_seed(<span class="hljs-number">1998</span>)<br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)<br><span class="hljs-attribute">torch</span>.rand_like(x)<br><span class="hljs-attribute">torch</span>.tensro([<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br><span class="hljs-attribute">torch</span>.one((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),dtype=torch.int16)<br><br><span class="hljs-attribute">torch</span>.zeros(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)+<span class="hljs-number">1</span><br><span class="hljs-attribute">torch</span>.ones(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)*<span class="hljs-number">3</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)**<span class="hljs-number">4</span><br><br><span class="hljs-comment"># broadcasting</span><br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*(torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)*<span class="hljs-number">2</span>)<br><span class="hljs-comment"># more math with tensors</span><br><span class="hljs-attribute">torch</span>.abs(a)<br><span class="hljs-attribute">torch</span>.ceil(a)<br><span class="hljs-attribute">torch</span>.floor(a)<br><span class="hljs-attribute">torch</span>.clamp(a,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>,<span class="hljs-number">0</span>.<span class="hljs-number">5</span>)<br><span class="hljs-attribute">torch</span>.sin(a)<br><span class="hljs-attribute">torch</span>.asin(a)<br><span class="hljs-attribute">torch</span>.bitwise_and(a,x)<br><br><span class="hljs-attribute">torch</span>.max()<br><span class="hljs-attribute">torch</span>.mean()<br><span class="hljs-attribute">torch</span>.std()<br><span class="hljs-attribute">torch</span>.prod()<br><br><span class="hljs-attribute">v1</span> = torch.tensor([<span class="hljs-number">1</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.])         # x unit vector<br><span class="hljs-attribute">v2</span> = torch.tensor([<span class="hljs-number">0</span>., <span class="hljs-number">1</span>., <span class="hljs-number">0</span>.])         # y unit vector<br><span class="hljs-attribute">torch</span>.cross(v2,v1)<br><span class="hljs-attribute">torch</span>.matmul(a,x)<br><span class="hljs-attribute">torch</span>.mv(a,v2)<br><span class="hljs-attribute">torch</span>.svd(x)<br><br><span class="hljs-attribute">x</span>=torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,requires_grad=True)<br><span class="hljs-attribute">y</span>=x+<span class="hljs-number">2</span><br><span class="hljs-attribute">y</span>.sum().backward()<br><span class="hljs-attribute">plt</span>.plot(x.detach(),y.detach())<br><span class="hljs-attribute">with</span> torch.no_grad():<br>    <span class="hljs-attribute">y</span>=x+<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>torch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>知识管理的一些想法</title>
    <link href="/posts/d49191c7.html"/>
    <url>/posts/d49191c7.html</url>
    
    <content type="html"><![CDATA[<p><a href="https://blog.tjdata.site/posts/d49191c7.htmlblog.tjdata.site/posts/d49191c7.html">https://blog.tjdata.site/posts/d49191c7.htmlblog.tjdata.site/posts/d49191c7.html</a></p><p>作为一枚INFJ，经常会对效率工具进行反思。今天看了一本和知识管理联系不大的《如何有效阅读一本书》，然后对自己现有的知识管理系统中备忘录+日历+提醒事项，配合ShortCut快速输入、Cubox聚集、Notion整理的一套系统进行整理和反思。给出三个核心的观点。</p><h3 id="0x01-现有工具使用流程"><a href="#0x01-现有工具使用流程" class="headerlink" title="0x01 现有工具使用流程"></a>0x01 现有工具使用流程</h3><p>对于现代互联网的工具的感受是，它无处不在，但是在光鲜亮丽的覆盖率之下反而更加暴露了各种信息交换不流畅的问题。你在Apple日历中设置的事项很难在安卓手机上看到，你在推特发的推文很难被同步到你的备忘录，你在豆瓣收藏的书籍也不会出现在亚马逊的购物中，你在PDF中做出的批注也难导出到你的Notion中…..</p><p>为了解决这些信息交流的问题，我们往往需要确定自己的工作流Workflow。通过自身的控制来更好的使用工具。当然当一个高效的工具出现的时候你也会开心的，比如看到Warp代替iTerm，时代毕竟是在发展的。电子工具的用途主要有三个：1. GTD日程管理、2. 信息收集、3. 信息处理</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs swift">graph<br>classDef <span class="hljs-keyword">some</span><span class="hljs-keyword">class</span> <span class="hljs-title class_">fill</span>:#f96<br>    <span class="hljs-title class_ inherited__">A</span>[<span class="hljs-title class_ inherited__">GTD</span> : <span class="hljs-title class_ inherited__">Get</span> thing done]:::someclass<br>    <span class="hljs-title class_ inherited__">C</span>(<span class="hljs-title class_ inherited__">Calendar</span>:<span class="hljs-title class_ inherited__">Event</span>)<br>    <span class="hljs-title class_ inherited__">R</span>(<span class="hljs-title class_ inherited__">Reminder</span>:<span class="hljs-title class_ inherited__">ToDo</span> <span class="hljs-title class_ inherited__">Schedule</span>)<br>    <span class="hljs-title class_ inherited__">N</span>(<span class="hljs-title class_ inherited__">Notes</span>)<br>    <span class="hljs-title class_ inherited__">C</span>--&gt;<span class="hljs-title class_ inherited__">A</span><br>    <span class="hljs-title class_ inherited__">R</span>--&gt;<span class="hljs-title class_ inherited__">A</span><br>    <span class="hljs-title class_ inherited__">N</span>--&gt;<span class="hljs-title class_ inherited__">A</span><br>    <span class="hljs-title class_ inherited__">N</span>--&gt;<span class="hljs-title class_ inherited__">C</span><br>    <span class="hljs-title class_ inherited__">N</span>--&gt;<span class="hljs-title class_ inherited__">R</span><br><br>    <span class="hljs-title class_ inherited__">B</span>[知识收集 <span class="hljs-title class_ inherited__">Cubox</span>]:::someclass<br>    <span class="hljs-title class_ inherited__">W</span>(<span class="hljs-title class_ inherited__">Wechat</span> <span class="hljs-title class_ inherited__">Helper</span>)<br>    <span class="hljs-title class_ inherited__">API</span>(<span class="hljs-title class_ inherited__">Cubox</span> <span class="hljs-title class_ inherited__">API</span>)<br>    <span class="hljs-title class_ inherited__">ALF</span>(<span class="hljs-title class_ inherited__">Alfred</span>)<br>    <span class="hljs-title class_ inherited__">S</span>(<span class="hljs-title class_ inherited__">Short</span> <span class="hljs-title class_ inherited__">Cut</span>)<br>    <span class="hljs-title class_ inherited__">W</span>--&gt;<span class="hljs-title class_ inherited__">B</span><br>    <span class="hljs-title class_ inherited__">API</span>--&gt;<span class="hljs-title class_ inherited__">B</span><br>    <span class="hljs-title class_ inherited__">ALF</span>--&gt;<span class="hljs-title class_ inherited__">B</span><br>    <span class="hljs-title class_ inherited__">S</span>--&gt;<span class="hljs-title class_ inherited__">B</span><br><br>    <span class="hljs-title class_ inherited__">NO</span>[知识整理 <span class="hljs-title class_ inherited__">Notion</span>]:::someclass<br>    <span class="hljs-title class_ inherited__">DB</span>(<span class="hljs-title class_ inherited__">Database</span>)<br>    <span class="hljs-title class_ inherited__">BD</span>(<span class="hljs-title class_ inherited__">Database2</span>)<br><br>    <span class="hljs-title class_ inherited__">DB</span>--&gt;<span class="hljs-title class_ inherited__">NO</span><br>    <span class="hljs-title class_ inherited__">BD</span>--&gt;<span class="hljs-title class_ inherited__">NO</span><br><br>    <span class="hljs-title class_ inherited__">OUTPUT</span>(<span class="hljs-title class_ inherited__">Blog</span>):::someclass<br>    <span class="hljs-title class_ inherited__">A</span>--&gt;<span class="hljs-title class_ inherited__">B</span>--&gt;<span class="hljs-title class_ inherited__">NO</span>--&gt;<span class="hljs-title class_ inherited__">OUTPUT</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f88065dfe310c1ecc9da114d22ebd533_1440w.png" alt="img"></p><p>总的知识管理</p><p>接下来介绍一些原则：</p><ol><li>一元化原则：如果在这个地方找不到，那么其他地方也找不到</li><li>避免无穷原则：收藏很简单，但是清空很难</li><li>多思考，多输出：一件事情只有我们给别人能够讲清楚的时候，我们自己才能明白</li></ol><h3 id="0x02-一元化原则-–-GTD系统的可靠性"><a href="#0x02-一元化原则-–-GTD系统的可靠性" class="headerlink" title="0x02 一元化原则 – GTD系统的可靠性"></a>0x02 一元化原则 – GTD系统的可靠性</h3><p>GTD是在一本书中提出的概念，用于个人的任务管理的一种方法论。同样这里还会有其他很多的任务管理的方法。最核心的要素是Get thing done。那么如何知道自己做什么？什么时候去做呢？原书给出作者思考的过程，这里介绍我目前摸索的对于日常事务的分类方法和实践规则。</p><p>首先事项根据主观和客观可以分为Calendar和Reminder，这也是两者之间的区别：</p><ol><li>Calendar：日历中发生的往往是一件Events（这也是macOS中的英文），它应该是一种不随个人意志发生的事情。比如国庆节、比如朋友的生日、比如上课、比如上课的DDL。这些事情的最大特点是发生完就结束了，不会有主动拖延这个概念。所以这个通常需要确定一个时间点</li><li>Reminder：这个发生的是我们主观想去做的事情（Item），这些事项可以是“去买个便利贴”“完成某一件作业”“在DDL之前做完PPT”</li></ol><p>但是对于不同的提醒事项也可以进行分类，经过可以经验可以分为</p><ol><li>Schedule：按照时间排序。规划好时间的事情，比如“明天早上9点新建一个论文文件夹”。在之前建立系统的时候，假如schedule设置太多，往往会导致自己完不成而逐渐拖延，所以这里建议计划一个星期会比较好</li><li>To Do List：按照优先级排序。这里是想做的，但是没有规划好时间的事情，比如说“看《奥本海默》”，这类事情往往需要构建一个优先级，当自己的Schedule做完的时候，可以从TODO中做起优先级高的事情</li><li>Deadline：按照时间排序。这里往往是一件事情的结束。代表的是一个过程。比如“下周一交作业”，这个时候需要我们在“下周一”这个时间点完成作业，并提交，我们不可能等到下周一再去完成作业。所以这个建议设置Flag</li></ol><p>到此为止我们的GTD系统已经可以建立起来了，但是这个系统往往会受到冲击：</p><ol><li>不可靠性，收集在Calendar和Reminder中的事项并不完全。导致我们对系统的信任度不高，经常还会依靠自己的大脑来回忆事情。</li><li>懒惰性，假如Schedule的东西太多，而自己不去完成，会导致系统的任务越来越多而崩溃</li><li>缺乏激励性，没有办法给自己正反馈，导致系统维护困难。</li></ol><p>这里给出一个建议：在实践中我们要坚持一元化的原则，也就是在微信或者口头上获取一个事件，可以立马将其添加到自己的inbox，然后再对其分类整理。我们要充分信任自己的系统，因为GTD不仅是记录我们做了什么，还是一个帮助我们大脑记忆的系统。如果我们陷入回忆自己需要做什么的魔咒中，往往会消耗大量的精力和时间。</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs xl">graph<br> cal[Calendar]<br> re[Reminder]<br> E(Event sorted <span class="hljs-keyword">by</span> <span class="hljs-built_in">time</span>)<br> S(Schedule sorted <span class="hljs-keyword">by</span> <span class="hljs-built_in">time</span>)<br> T(To Do List sorted <span class="hljs-keyword">by</span> priority)<br> D(DeadLine sorted <span class="hljs-keyword">by</span> <span class="hljs-built_in">time</span>)<br> <span class="hljs-function"><span class="hljs-title">cal</span> --&gt;</span> E<br> <span class="hljs-function"><span class="hljs-title">re</span>--&gt;</span>S<br> <span class="hljs-function"><span class="hljs-title">re</span>--&gt;</span>T<br> <span class="hljs-function"><span class="hljs-title">re</span>--&gt;</span>D<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-01e5cac834bcf4b4b2b951877e5b159a_1440w.png" alt="img"></p><p>GTD系统</p><h3 id="0x03-避免无穷收藏：Cubox"><a href="#0x03-避免无穷收藏：Cubox" class="headerlink" title="0x03 避免无穷收藏：Cubox"></a>0x03 避免无穷收藏：Cubox</h3><p>互联网将知识串联在一起的后果是我们可以无限的滑动，我们永远看不完Google搜索的结果、看不完微博的帖子、看不完商品的简介…在知识管理也是这样。互联网带来的有用的知识也越来越多，当我们看到一个有意思的微信文章、或者诱人的博客、或者是B站大学的课程，往往会丢到自己的收藏夹中，然后不同软件中的收藏夹并不会互相沟通，唯一的共同点是逐渐增加但是从不清空。</p><p>所以将收集箱填满只是起点，终点是让收集箱回归空的状态。同样为了遵循“一元化”原则，希望可以将所有的知识和自己的想法收集到一个地方。并不想发生在微信收藏看文章、在B站看视频、在微信读书读想读的书、在浏览器看自己的read list，这样的劣势是不能整理。仿佛看完之后就结束了。</p><p>借助Cubox可以通过随机回顾来看自己曾经做过什么批注，可以将看完的东西分类放到架子（文件夹）中日后浏览，也可以将没有营养的归档或者删除。Review才是终点！</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8dc69c1d83d9cbcc26e765ebdf166298_1440w.png" alt="img"></p><p>Cubox 快捷指令</p><h3 id="0x04-输出和All-in-one：Notion"><a href="#0x04-输出和All-in-one：Notion" class="headerlink" title="0x04 输出和All in one：Notion"></a>0x04 输出和All in one：Notion</h3><p>首先Notion作为知识管理的终点站在于其开放，它具有和其他系统联合并开放API，这样的开放性导致其可以综合其他内容。由此得到的后果是其可以作为所有知识收集工具的终点，同样它也是非常好的编辑器。</p><p>Databse是其最具特色的系统，我们可以通过设置不同的属性来对页面进行分类整理，将具有关系的页面根据Filter或Group来集聚，来达到自己系统管理的目的。</p><p>但是同样的在最终，在明白了自己需要做什么、在自己收集很多知识之后，考虑如何将其讲明白，写出一份给别人看的博客才是获取知识的最终途径～</p><p>共勉</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>知识管理</tag>
      
      <tag>笔记软件</tag>
      
      <tag>notion</tag>
      
      <tag>typora</tag>
      
      <tag>cubox</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Combining_Reinforcement_Learning_and Constraint_Programming_for_Combinatorial Optimization</title>
    <link href="/posts/adc401ca.html"/>
    <url>/posts/adc401ca.html</url>
    
    <content type="html"><![CDATA[<p>将RL嵌入到现有的框架当中来保证效率和完全性，非常Fancy！</p><h2 id="https-arxiv-org-abs-2006-01610"><a href="#https-arxiv-org-abs-2006-01610" class="headerlink" title="https://arxiv.org/abs/2006.01610"></a><a href="https://arxiv.org/abs/2006.01610">https://arxiv.org/abs/2006.01610</a></h2><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><a href="https://www.notion.so/Combinatorial-Optimization-0f8d8ad2698643b1abf8e955f2a4e869">Combinatorial Optimization</a> 应用广泛：从航空领域、交通规划甚至到经济学中，它的目标是在有限的解空间中找到最优解（find an optimal solution among a finite set of possibilities），其面临的最严重的挑战为<a href="https://www.notion.so/Q-state-action-d9e8f9603c0a47f895d30615644c41d9">Q-state-action方法</a> 中提到的 状态空间爆炸（State- space explosion problem）：问题的解空间岁问题规模增大成指数型的增大，导致对于一些大型的问题难以求解；另一方面DRL（deep reinforcement learning）在求解NP-hard 组合优化问题过程对设计好的启发式求解算法具有比较好的贡献，但是现有的方法具有两个缺点</p><ol><li>他们仅仅专注于标准的TSP问题，不容易拓展到其他问题</li><li>通常只给出一种近似解（approximate solution），而没有给出一种系统的方法来提高或者证明它的最优性（Optimality）</li></ol><p>在另外一种情况下，<a href="https://www.notion.so/Constraint-programming-05ee88110f8b4c18ac9f9cffe6f65976">Constraint programming</a> 是解决组合优化问题的通用工具，基于一个完整的搜索过程，通常可以在比较大的运行时间中找到最优解。其中决定性的确定搜索的方向，或者说分枝策略，或者说是探索策略。</p><p>在本文中我们提出一种通用和混合的方法（l）的方法，结合CP和DRL来求解COP，主要是基于<a href="https://www.notion.so/Part01-dynamic-programming-8f83f0126c2447e8984b0904e305a33a">运筹学Part01–dynamic programming</a> 作为两者之间的桥梁，并通过实验来证明求解器在两个有挑战性的问题上进行求解：<a href="https://www.notion.so/TSPTW-the-traveling-salesman-problem-with-time-windows-e46e9c09083041af998fa7a2ffe2ee1e">TSPTW the traveling salesman problem with time windows</a> 和 <a href="https://www.notion.so/4POP-4-moments-portfolio-optimization-problem-b67f257853f348c29fd54edd42d79092">4POP 4-moments portfolio optimization problem</a>,实时证明比一些单独的RL或者利用通用求解器效果更好</p><blockquote><p> Brief Conclusion P：具有多项式时间算法，算得很快的问题 NP：不确定需要多长时间，但是我们验证它只需要多项式时间 NP- complete：属于NP问题，同时也属于NP-hard问题 NP-hard：比NP问题都要难的问题</p></blockquote><p>NP问题 指的是非确定性多项式问题（non- deterministic polynomial NP），非确定性指的是可用一定数量的运算去解决多项式时间内可解决的问题，通常解决问题包括：TSP、Hamilton回路问题、最大团问题；但是其可以在多项式时间内可以被验证其正确定的问题</p><p>NP-hard问题 指的是non- deterministic polynomial time hardness，如果所有的NP问题都可以在多项式时间规约到某个问题，则称这个问题为NP困难</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在组合优化中大多数问题为NP-hard问题，对其高效的优化算法的探索由来已久。广泛的可以将其分为两种方法</p><p><a href="https://www.notion.so/exact-algorithm-6f0de3f67ad942299d12b6e6962f9e10">精确算法 exact algorithm</a> 是基于对求解空间的完整并且巧妙的枚举，它的优点是可以选招到最优解，但是在对于一些大型的案例中往往会出现求解时间扩大。但也有另外的说法可以让精确算法在找到最优解之前就终止的方式来探索得到次优解。这种灵活性让精确求解算法更加appealing并且practical，也正是如此构成了现代求解器的核心 CPLEX、Gurobi、Gecode等等。这是一个CP的介绍，通过additional asset 用来求解非常大的复杂性的COP问题，通常<a href="https://www.notion.so/mixed-integer-programming-MIP-fc936c07376b4ef098d1307191c9019e">混合整数规划mixed integer programming MIP</a> 求解器往往只能求解线性问题或者有限的非线形问题。</p><p><a href="https://www.elsevier.com/books/handbook-of-constraint-programming/rossi/978-0-444-52726-4">Handbook of Constraint Programming</a></p><p>而在利用CP进行求解的过程中重要的是branching strategy，非常自然的，好的设计的启发式的探索方法可以很好的发现求解，但是差的启发式分枝策略往往会导致进入没有解的子空间。所以在CP求解问题中br分枝策略的研究是非常热门的话题</p><p><a href="https://www.notion.so/heuristic-algorithm-5887d2ccbbc34a0db4eb5136ab9645ca">启发式算法 heuristic algorithm</a> 是一种非完全的手段来求解得到最终解，但是不能证明解的最优性。他们通常需要特定子问题的知识才能构建他们。在近几年，<a href="https://www.notion.so/Lecture01-Deep-Reinforcement-Learning-Decision-Making-and-Control-38667912b7734364a527d8ae71ff2a40">Lecture01 Deep Reinforcement Learning, Decision Making,and Control</a> 在对一些NP-hard的COP问题的近似求解中取得了非常好的成果。只要一个模型训练好，在实践中的运行时间是可以忽略不计的，利用DRL求解问题的前提是</p><ol><li>我们知道实际问题的例子分布</li><li>我们有足够多的样本，并可以从中采样来训练模型</li></ol><p>但是现有的DRL在COP应用中存在的一些问题表阔</p><ol><li>仅限于求解特定的问题，比如TSP</li><li>其次，它们仅旨在充当建设性的启发式方法，并且与完整的方法（例如CP）不同，没有系统的方法来改进所获得的解决方案。</li></ol><p>上述的<a href="https://www.notion.so/exact-algorithm-6f0de3f67ad942299d12b6e6962f9e10">精确算法 exact algorithm</a> 和基于学习的<a href="https://www.notion.so/heuristic-algorithm-5887d2ccbbc34a0db4eb5136ab9645ca">启发式算法 heuristic algorithm</a> 都具有好处和坏处，很自然的就有一个问题</p><blockquote><p> 我们是否可以将他们的优点联合起来来求解COP</p></blockquote><p>在本文中我们成果将RL与CP利用<a href="https://www.notion.so/Part01-dynamic-programming-8f83f0126c2447e8984b0904e305a33a">运筹学Part01–dynamic programming</a> 结合起来，动态规划在很多场合中都具有很成果的作用，是一种非常关键的建模COP问题的方法，简单来说DP问题将一个问题分解成为子问题并通过递归公式（recursive formulation，比如常见的bellman equation结合起来），其中DP面临的苦难常常是 维度灾难问题：生成的子问题的数量呈指数级增长，以至于将它们全部存储在内存中变得不可行。</p><p>据我们所知是第一次在CP求解器中内置可学习的启发式探索，详细的贡献如下：</p><ol><li>对于COP问题新的DP编码方式来利用RL求解和CP模型</li><li>利用两种常见的RL模型，DQL、PPO，训练数据集是从问题中随机抽样得到的</li><li>与现有的CP求解过程中分枝策结合，形成三种 brand and bound \ iterative limited discrpancy search and restart based search三种</li><li>给出有意义的结果</li><li>代码开源，为了简化后续研究的进程</li></ol><p>通常如果没有内在的假设，比如线形和凸性，DP不能简单的编码然后利用标准的MIP方法求解，这也是驱动我们思考尝试使用CP进行编码的原因之一</p><p>下一章给出混合整数求解器求解的过程</p><p>对两个案例进行实验分析</p><p>最后给出总结</p><h2 id="2-数据表征"><a href="#2-数据表征" class="headerlink" title="2  数据表征"></a>2  数据表征</h2><p>这一部分总结我们提出的完整的框架，一个概括性的框架图在图一中展示，分为三个部分：</p><ol><li>学习部分，利用随机生成的例子通过unifying representation作为RL的环境进行训练，最终训练的智能体是对求解过程中search的启发式分枝策略进行基于价值的选择（或者说是强化学习的探索）</li><li>求解部分，利用CP对模型进行求解，同时对求解的案例进行评估</li><li>联合表示阶段，讲输入的COP问题，结合Dominance pruning rules剪枝策略得到动态规划模型的建模，这样可以作为学习阶段的环境，也可以作为求解节点的输入模型</li></ol><p>绿色表示我们的原创共享，蓝色表示已经有的框架</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1aaaf2e10c6c37bc9a27ae001791deaa_1440w.png" alt="img"></p><p>image-20221218154958762</p><h3 id="2-1-动态规划模型"><a href="#2-1-动态规划模型" class="headerlink" title="2.1 动态规划模型"></a>2.1 动态规划模型</h3><p>动态规划模型是一种将数学建模与计算机编程联系起来求解复杂优化问题的手段，简单来说是将一个大的问题转换成很多小的子问题然后将他们利用recursive formulation的方式联系起来最终求解，其中包括的一些关键的影响因素</p><table><thead><tr><th>Q</th><th>COP问题</th></tr></thead><tbody><tr><td>controls</td><td>C</td></tr><tr><td>domain</td><td>D</td></tr><tr><td>transition</td><td>T</td></tr><tr><td>states</td><td>S</td></tr><tr><td>stage</td><td></td></tr><tr><td>reward</td><td>R</td></tr><tr><td>validity conditions</td><td>V</td></tr><tr><td>dominance rules</td><td>P</td></tr></tbody></table><p>其中validity conditions 和dominance rules之间的区别在于两者之间的区别在于，有效性条件是强制性的，以确保DP模型的正确性（V （s， x） &#x3D; 0 ⇔ T（s， x） &#x3D; ⊥），而优势规则仅用于效率目的（P（s， x） &#x3D; 0 ⇒ T（s， x） &#x3D; ⊥），其中⇔、⇒和⊥表示等价性、含义、 和不可行的状态，分别。其中DP建模可以包括 S，X，T，R，V，P；根据贝尔曼方程我们可以一直计算到开头的状态的价值。</p><p>但是DP收到curse of dimensionality的影响，在解决大的状态或者动作空间下需要的内存很多，一种方法是对其进行剪枝处理，其中需要评估</p><p>此问题的部分解决方案是修剪主导操作 （P（s， x） &#x3D; 0）。如果一个动作根据递归公式是有效的，那么它就是主导的，但（1）要么严格地比另一个动作差，要么（2）它不能导致可行的解决方案。在实践中，修剪这些主导操作会对搜索空间的大小产生巨大影响，但是在实践过程中找到这样的求解方案并不是容易的</p><p>而且就算在DP过程中对上述的子问题进行修建之后仍不能确保解空间足够小</p><h3 id="2-2-强化学习编码"><a href="#2-2-强化学习编码" class="headerlink" title="2.2 强化学习编码"></a>2.2 强化学习编码</h3><p>从DP问题中，结合一个实际COP问题可以定义一个强化学习过程，这里有一个关于强化学习的概述</p><p>介绍强化学习的作用：</p><p>介绍一个基本的过程</p><h3 id="2-2-1-State"><a href="#2-2-1-State" class="headerlink" title="2.2.1 State"></a>2.2.1 State</h3><p>对于DP问题中不同的阶段，我们可以使用（Q，s_i）作为当前状态的定义，通常状态被embed一种张量作为神经网络的输入，也包括action，最终输出是最优的动作</p><h3 id="2-2-2-Action"><a href="#2-2-2-Action" class="headerlink" title="2.2.2 Action"></a>2.2.2 Action</h3><p>给定一个DP问题中的state，其中包括一种一对一关系的，$a_i$可以被采用除非$x_i$是有效的，通常可行的动作可以由domain和valid进行判断</p><h3 id="2-2-3-Transition"><a href="#2-2-3-Transition" class="headerlink" title="2.2.3 Transition"></a>2.2.3 Transition</h3><p>输入当前状态和侗族哦，我们就可以使用DP问题来进行转换就行</p><h3 id="2-2-4-Reward"><a href="#2-2-4-Reward" class="headerlink" title="2.2.4 Reward"></a>2.2.4 Reward</h3><p>最初的想法是使用DP问题中存在的奖励，但是这样可能会存在没有解的轨迹奖励要高于有解方案的奖励，因此在设计奖励的需要考虑COP问题中存在的两个原则</p><p>1， 如果e1比e2更坏，我们需要e1的奖励小于e2的奖励</p><ol><li>所有找到可行解的轨迹需要比没有找到可行解的轨迹高</li></ol><p>UB对应规划问题可能到达的上限，第二个问题强迫智能体去寻找到最佳的可行解，最终的scaling factor 防止最终奖励落入零值周边</p><h3 id="2-3-学习方法"><a href="#2-3-学习方法" class="headerlink" title="2.3 学习方法"></a>2.3 学习方法</h3><p>DQN</p><p>PPO</p><p>有一个非常重要的假设，利用生成器得到instance的时候，得到的分布与实际问题中的问题想等，这也是在RL解决其他问题中常见的假设，强化学习的过程是希望学习出不同状态和动作下得到不同的Q，我们的策略往往是采用最大Q值的东西，利用DQN得到Q值的训练过程有</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs clean"># Trainer——DQN（）<br># <span class="hljs-keyword">class</span> Env，<br># <span class="hljs-keyword">class</span> BrainDQN<br># <span class="hljs-keyword">class</span> TSPTW<br># <span class="hljs-keyword">class</span> state<br></code></pre></td></tr></table></figure><p>展现代码的调用过程</p><ol><li>这一步仅仅是希望训练出DQN的brain，而没有其他的东西</li><li>重点在环境生成，以及训练的整个过程</li></ol><p>首先确保如何生成一个TSPTW问题</p><p>之后生成多个作为训练的集合</p><p>同时在每次训练中提高</p><h3 id="2-4-网络框架"><a href="#2-4-网络框架" class="headerlink" title="2.4 网络框架"></a>2.4 网络框架</h3><p>为了确保框架的通用型和有效性，对于网络框架我们需要确保两件事情</p><ol><li>对于相同的COP问题，可以有不同的输入尺寸</li><li>对输入的排列保持不变</li></ol><p>为了保证这样的架构我们通常会使用一种Transformer架构，对于TSP问题天然的我们可以采用GAT的方式来实现序列到序列的转换</p><p>对于DQN网络中最后一层的输入是不同动作对应的Q- value</p><h3 id="2-5-CP-Encoding"><a href="#2-5-CP-Encoding" class="headerlink" title="2.5 CP Encoding"></a>2.5 CP Encoding</h3><p>现在介绍如何将DP过程编码放入Constraint model</p><table><thead><tr><th></th><th>CP</th><th>DP</th></tr></thead><tbody><tr><td>variable and domains</td><td>X- decision variables，set of the variables，表示动作</td><td>X</td></tr><tr><td></td><td>X-auxiliary variables，表示state</td><td>S</td></tr><tr><td></td><td>D，set of the domain</td><td>V</td></tr><tr><td></td><td>C，set of the constraints</td><td>P</td></tr><tr><td></td><td>O，objective function</td><td>在后面N阶段中的最大值，这里的动态规划方向仅仅和x_a有关</td></tr></tbody></table><h3 id="2-6-搜索策略"><a href="#2-6-搜索策略" class="headerlink" title="2.6 搜索策略"></a>2.6 搜索策略</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-2d7fecd2ea4b073214a51a6800838ea9_1440w.png" alt="img"></p><p>image-20221218155022842</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5f28dc3679f4c16209105fdde7480d7c_1440w.png" alt="img"></p><p>image-20221218155036915</p><p>与DP方式相同，我们用此来构建RL的环境（并固定CO问题）来得到动作的输出，框架的统一性是问题的核心，这一部分展示我们如何将学习的模型嵌入到CP搜索中，我们考虑几种常见的搜索方式</p><ol><li>Depth-first branch-and-bound search</li><li>iterative limited discrepancy search</li><li>restart based search</li></ol><p>这里仅介绍第一种搜索策略与DQN的结合，DFS确保了当一个可行解找到之后，下一个解需要好与当前的，当遇到没有可行的分支会回溯到之前的分支，由此来确保整个解空间都得到遍历，同时最终得到的解可以被证明是最优的optimal，这个过程中需要非常好的value- selection</p><p>在学习之后，DQN的作用是输入当前的状态，以及可行的动作，输入不同动作的最大值，这里可以贪心搜索的方式进行下去，尽管可能不是最优的，需要保证和DP问题中存在的一致性，通常这里会有一些近似变量来作为对DP问题的关键影响，但是这里不是重点</p><p>整个过程如下表所示⬇️</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-221614208a328d30af141b6278eabc32_1440w.png" alt="img"></p><p>image-20221218155108646</p><h2 id="3-Experimental-result"><a href="#3-Experimental-result" class="headerlink" title="3 Experimental result"></a>3 Experimental result</h2><p>TSPTW是从传统的TSP问题中得到的，只是给所有的salesman增加一个时间窗的概念，最终规定需要在规定时间内沿着所有的节点走一圈。</p><p><a href="https://developers.google.com/optimization/routing/vrptw">Vehicle Routing Problem with Time Windows | OR-Tools | Google Developers</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-14a9a0f21f12c2ebcd1cee514206bb06_1440w.png" alt="img"></p><p>image-20221218155120405</p><p>最终证明了结果</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-95a8a3b90498c1e40aa99b21c2687753_1440w.png" alt="img"></p><p>image-20221218155134078</p><h2 id="4-Discussion-and-Limitation"><a href="#4-Discussion-and-Limitation" class="headerlink" title="4 Discussion and Limitation"></a>4 Discussion and Limitation</h2><p>首先本文并不是第一个尝试使用ML来引导优化求解器的分支，但是他们的方法专注于监督学习并且仅限于线性问题，这里本文的共享</p><ol><li>关注与将COP问题建模成为DP</li><li>完全利用RL进行训练</li></ol><p>同时结合CP问题的完整性，可以更好的求解一系列问题包括非线性约束或者非线性约束函数</p><p>同时我们也不需要一些历史数据或者仿真生成器，同时是一种端到端的求解器，并且可以证明求解结果的最优性</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>这些结果表明，该框架可能是解决具有挑战性的组合优化问题的一种有前途的新途径。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9f0c18210ff7beb90ab28b15a9c882b5_1440w.png" alt="img"></p><p>image-20221218155147889</p><p><a href="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/202212181553133.pdf">总结的ppt</a></p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>运筹优化</tag>
      
      <tag>paper 阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>流程图库 Mermaid 总结</title>
    <link href="/posts/354c5ef8.html"/>
    <url>/posts/354c5ef8.html</url>
    
    <content type="html"><![CDATA[<p>关于流程图的绘制中有很多工具，比如微软的visio 或者 PowerPoint、贝尔的Graphviz、Lucidchart、OmniGraffle、Draw.io、processOn、Latex的tikZ；但是这种基于图形界面的操作往往会受限于人本身的不精确导致不好看。所以希望借助代码生成流程图来尽可能保持协调一致的美观。</p><h3 id="0x01-使用教程"><a href="#0x01-使用教程" class="headerlink" title="0x01 使用教程"></a>0x01 使用教程</h3><p>可以直接在Typora中使用，只需要输入即可</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">``` mermaid<br></code></pre></td></tr></table></figure><p>什么是<a href="https://mermaid.js.org/intro/">Mermaid</a>，按照官网的定义是<strong>基于JavaScript</strong>的图表工具，可以渲染受markdown启发的文本定义，用于动态创建和修改图表。其基本的语法结构包括：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs crmsh"><span class="hljs-comment"># Diagrams definitions begin with a declaration of the diagram type</span><br>    <span class="hljs-comment"># Define the blocks</span><br>    <span class="hljs-comment"># Link the blocks</span><br>erDiagram<br>          CUSTOMER &#125;|..|&#123; DELIVERY-ADDRESS : has<br>          CUSTOMER ||--o&#123; <span class="hljs-keyword">ORDER</span> <span class="hljs-title">: places</span><br>          CUSTOMER ||--o&#123; INVOICE : <span class="hljs-string">&quot;liable for&quot;</span><br>          DELIVERY-ADDRESS ||--o&#123; <span class="hljs-keyword">ORDER</span> <span class="hljs-title">: receives</span><br>          INVOICE ||--|&#123; <span class="hljs-keyword">ORDER</span> <span class="hljs-title">: covers</span><br>          <span class="hljs-keyword">ORDER</span> <span class="hljs-title">||--|&#123; ORDER-ITEM</span> : includes<br>          PRODUCT-CATEGORY ||--|&#123; PRODUCT : contains<br>          PRODUCT ||--o&#123; ORDER-ITEM : <span class="hljs-string">&quot;ordered in&quot;</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7730b9eb5ccc51debe6687378fe9fc80_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>常见的表格可以分为：流程图Flowchart、顺序图Sequence Diagram、类图Class Diagram、状态图State Diagram、实体关系图Entity Relationship Diagram、甘特图Gantt、饼图Pie chart、需求图Requirement Diagram、GitGraph、C4C图、思维导图Mindmaps、时间线Timelines</p><h3 id="0x02-流程图-Flowchart"><a href="#0x02-流程图-Flowchart" class="headerlink" title="0x02 流程图 Flowchart"></a>0x02 流程图 Flowchart</h3><p>流程图由节点 (Nodes)、形状 (Edges)、箭头 （Arrows）和线条（Lines）组成，应用示例</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs coq">flowchart <br>    id1(This is the text <span class="hljs-built_in">in</span> the node1)<br>    id2[This is the text <span class="hljs-built_in">in</span> the node2]<br>    db1[(database)]<br>    id1 --&gt; id2<br>    db1 --&gt; id1<br>%% TB : <span class="hljs-built_in">top</span> to <span class="hljs-built_in">bottom</span>,TD<br>%% BT: <span class="hljs-built_in">bottom</span> to <span class="hljs-built_in">top</span><br>%% RL: <span class="hljs-built_in">right</span> to <span class="hljs-built_in">left</span><br>%% LR: <span class="hljs-built_in">left</span> to <span class="hljs-built_in">right</span><br><br>flowchart LR<br>  subgraph BT[Define of the nodes]<br>      id1(圆角)<br>      id2([更大的圆角])<br>      id3[长方形]<br>      id4[(数据库)]<br>      id5((圆形))<br>      id6&gt;what]<br>      id7&#123;方形&#125;<br>      id8&#123;&#123;角&#125;&#125;<br>      id10[\梯形 Trapezoid alt/]<br>    <span class="hljs-keyword">end</span><br><br>    subgraph LR[define of the linkes]<br>       id11[node1]<br>       id12[node2]<br>       id11 --&gt; id12<br>       id11 --- id12<br>       id11 -- Text ---id12<br>       id11 ---|<span class="hljs-type">Text</span>|<span class="hljs-type">id12</span><br>       id11--&gt;|<span class="hljs-type">Text</span>|<span class="hljs-type">id12</span><br>       id11 -.-&gt; id12<br>       id11 -. text .-&gt; id12<br>       A == text ==&gt; id12<br>       a --&gt;b &amp; c--&gt;d<br>    <span class="hljs-keyword">end</span><br><br>    subgraph LR<br>        B[start]<br>        C&#123;Is it?&#125;<br>        B --&gt;|<span class="hljs-type">yes</span>|<span class="hljs-type">C</span><br>        C--&gt;D[rethink]<br>        D--&gt;B<br>        B--&gt;|<span class="hljs-type">No</span>|<span class="hljs-type">E</span>[<span class="hljs-keyword">End</span>]<br>        E[perhaps?]<br><br><br>    <span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f56c727e422977fa5217e6d76bd49aed_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="0x03-时序图-Sequence-diagrams"><a href="#0x03-时序图-Sequence-diagrams" class="headerlink" title="0x03 时序图 Sequence diagrams"></a>0x03 时序图 Sequence diagrams</h3><p>时序图表示用户之间相互交流的过程的图表，其中的语法主要包括：参与者（Participant）、演员（Actor）、别名（Alias）、分组（Group and box）、信息（Arrows and lines）、激活（Activate）、备注（Notes）、循环（Loop）</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">sequenceDiagram</span> <br>    participant <span class="hljs-type">A</span> <span class="hljs-keyword">as</span> <span class="hljs-type">Alice</span><br>    participant <span class="hljs-type">B</span> <span class="hljs-keyword">as</span> <span class="hljs-type">Bob</span><br>    participant <span class="hljs-type">J</span> <span class="hljs-keyword">as</span> <span class="hljs-type">John</span><br>    <span class="hljs-type">A</span>-&gt;&gt;<span class="hljs-type">B</span>:hello <span class="hljs-type">John</span><br>    <span class="hljs-type">B</span>-&gt;&gt;<span class="hljs-type">J</span>:<span class="hljs-type">Great</span>!<br>[<span class="hljs-type">Actor</span>][<span class="hljs-type">Arrow</span>][<span class="hljs-type">Actor</span>]:<span class="hljs-type">Message</span> <span class="hljs-type">Text</span><br>-&gt; 实线<br><span class="hljs-comment">--&gt; 没有箭头的点画线</span><br>-&gt;&gt; 箭头线<br><span class="hljs-comment">--&gt;&gt; 箭头点画线</span><br>-<span class="hljs-type">X</span> corss<br><span class="hljs-comment">--X 点线带cross</span><br>-）<br><span class="hljs-comment">--）</span><br><span class="hljs-title">sequenceDiagram</span><br>    <span class="hljs-type">Alice</span>-&gt;<span class="hljs-type">John</span>: <span class="hljs-type">Hello</span> <span class="hljs-type">John</span>, how are you?<br>    activate <span class="hljs-type">John</span><br>    loop <span class="hljs-type">Every</span> minute<br>        <span class="hljs-type">John</span><span class="hljs-comment">--&gt;Alice: Great!</span><br>    end<br>    deactivate <span class="hljs-type">John</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-bfe3a5640cc2927500e07003c8f5c244_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>初次之外顺序图还需要表示一些基本的逻辑概念，比如激活（Activate）、循环（Loop）、替代（Alt）、平行（Par）、关键区域（Critical）、Break、高亮显示（Rect）</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs vim">sequenceDiagram<br>    participant Alice<br>    participant John<br><br>    rect rgb(<span class="hljs-number">191</span>, <span class="hljs-number">223</span>, <span class="hljs-number">255</span>)<br>    note <span class="hljs-keyword">right</span> of Alice: Alice calls John.<br>    Alice-&gt;&gt;+John: Hello John, how are you?<br>    rect rgb(<span class="hljs-number">200</span>, <span class="hljs-number">150</span>, <span class="hljs-number">255</span>)<br>    Alice-&gt;&gt;+John: John, can you hear <span class="hljs-keyword">me</span>?<br>    John--&gt;&gt;-Alice: Hi Alice, I can hear you!<br>    end<br>    John--&gt;&gt;-Alice: I feel great!<br>    end<br>    Alice -&gt;&gt;+ John: Did you want <span class="hljs-keyword">to</span> <span class="hljs-keyword">go</span> <span class="hljs-keyword">to</span> the game tonight?<br>    John --&gt;&gt;- Alice: Yeah! See you there.<br>sequenceDiagram<br>    participant C <span class="hljs-keyword">as</span> Client<br>    participant S <span class="hljs-keyword">as</span> Server<br>    autonumber<br>    note <span class="hljs-keyword">left</span> of C: 选择初始seq <span class="hljs-keyword">number</span>=<span class="hljs-keyword">x</span>&lt;<span class="hljs-keyword">br</span>/&gt;发送客户端的TCP SYN段<br>    C-&gt;&gt;S:SYNbit=<span class="hljs-number">1</span>,Seq=<span class="hljs-keyword">X</span><br>    note <span class="hljs-keyword">right</span> of S:选择初始seq <span class="hljs-keyword">number</span>=<span class="hljs-keyword">y</span>&lt;<span class="hljs-keyword">br</span>/&gt;发送服务端TCP SYN信息&lt;<span class="hljs-keyword">br</span>/&gt;利用SYN+<span class="hljs-number">1</span>作为回复<br>    S-&gt;&gt;C:SYNbit=<span class="hljs-number">1</span>,Seq=Y&lt;<span class="hljs-keyword">br</span>/&gt;ACKbit=<span class="hljs-number">1</span>,ACKnum=<span class="hljs-keyword">x</span>+<span class="hljs-number">1</span><br>    note <span class="hljs-keyword">left</span> of C:收到SYNACK(<span class="hljs-keyword">x</span>),确认服务器存在&lt;<span class="hljs-keyword">br</span>/&gt;发送ACK回复SYNACK&lt;<span class="hljs-keyword">br</span>/&gt;不包含client-<span class="hljs-keyword">to</span>-server数据<br>    C-&gt;&gt;S:ACKbit=<span class="hljs-number">1</span>,ACKnum=<span class="hljs-keyword">y</span>+<span class="hljs-number">1</span><br>    note <span class="hljs-keyword">right</span> of S:收到 ACK(<span class="hljs-keyword">y</span>)&lt;<span class="hljs-keyword">br</span>/&gt;确认用户存在<br></code></pre></td></tr></table></figure><p><a href="https://zhuanlan.zhihu.com/p/53374516">https://zhuanlan.zhihu.com/p/53374516</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-60dbdeb2a314a6bba52ffa4aede9fd77_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="0x04-GitGraph"><a href="#0x04-GitGraph" class="headerlink" title="0x04 GitGraph"></a>0x04 GitGraph</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs sql">gitGraph<br>    <span class="hljs-keyword">commit</span> id: &quot;Set up folder&quot;<br>    branch <span class="hljs-keyword">release</span><br>    branch development<br>    checkout development<br>    <span class="hljs-keyword">commit</span><br>    branch feature01<br>    <span class="hljs-keyword">commit</span> id:&quot;feature1 add&quot;<br>    <span class="hljs-keyword">commit</span> id:&quot;feature1 add2&quot;<br>    checkout development<br>    branch feature02<br>    <span class="hljs-keyword">commit</span> id:&quot;fixed feature2&quot;<br>    checkout development<br>    <span class="hljs-keyword">merge</span> feature02<br>    <span class="hljs-keyword">commit</span> id:&quot;feature1 add3&quot;<br>    checkout development<br>    <span class="hljs-keyword">merge</span> feature01<br>    <span class="hljs-keyword">commit</span><br>    checkout main<br>    <span class="hljs-keyword">merge</span> development tag:&quot;Beta1.0&quot;<br>    checkout <span class="hljs-keyword">release</span><br>    <span class="hljs-keyword">merge</span> main tag:&quot;V1.0&quot;<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4852e348f84afc85fd575632266e3d10_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JS</tag>
      
      <tag>流程图库</tag>
      
      <tag>绘图</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GIT 管理的常见操作总结</title>
    <link href="/posts/964edeeb.html"/>
    <url>/posts/964edeeb.html</url>
    
    <content type="html"><![CDATA[<p>从<a href="https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell">Git - Branches in a Nutshell</a>中看Git的一些常见操作，主要是设计理念和本地的操作。更复杂的远程协作和项目管理后续给出。</p><p>Git PRO</p><h3 id="What-is-git"><a href="#What-is-git" class="headerlink" title="What is git"></a>What is git</h3><p>分布式版本控制系统（具有很多优点巴拉巴拉）</p><p>几个显著的技术上的特点：</p><ol><li>snapshot，not differences，no delta- based system</li><li>Git thinks of tis data more like a series of a snapshots of a miniature filesystem</li><li>every time you commit, or save the state of your project, Git basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot. If the data is not changed , Git doesn’t store the file again.</li><li>Nearly every operation is local</li><li>you have the entire history of the project right there on your local disk, most operations seem almost instantaneous</li><li>commit to local copy util you get to a network connection to upload</li><li>Git has integrity</li><li>you will see these hash values all over the place in Git because it uses them so much. In fact, Git stores everything in its database not by file name but 40 cahrs</li><li>Git generally only adds data</li><li>When you do actions in Git, nearly all over the place in Git because it uses them so much.  It is hard to get the system to do anything that is not undoable or to make it erase in any way</li><li>three state</li><li>modified</li><li>staged</li><li>committed</li></ol><p>基本的工作流程可以看作：</p><ol><li>修改working directory的文件</li><li>当时觉得做的够好之后，add changed to the staging area</li><li>将所有暂存区的文件提交，stores that snapshot permanently to your Git directory</li></ol><h3 id="Getting-start-—-First-time-git-setup"><a href="#Getting-start-—-First-time-git-setup" class="headerlink" title="Getting start —- First time git setup"></a>Getting start —- First time git setup</h3><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua"># git <span class="hljs-built_in">config</span> 存储的三个地方<br>[<span class="hljs-built_in">path</span>]/etc/gitconfig：包含应用于系统上每个用户及其所有存储库的值，如果你讲选项 system传递给<br>它会从专门的文件中读取和写入<br><br>    ~/.gitconfig 或者 ～/.<span class="hljs-built_in">config</span>/git/<span class="hljs-built_in">config</span> 特定用户个人的库<br><br>        <span class="hljs-built_in">config</span> 当前存储目录的呃设置<br></code></pre></td></tr></table></figure><h3 id="Basic-topic"><a href="#Basic-topic" class="headerlink" title="Basic topic"></a>Basic topic</h3><h3 id="3-1-获取Git仓库"><a href="#3-1-获取Git仓库" class="headerlink" title="3.1 获取Git仓库"></a>3.1 获取Git仓库</h3><p>一种方式是将当前不受版本控制的本地目录转换为Git存储库</p><p>第二种方式是从其他地方clone现有的Git仓库</p><h3 id="3-2-记录更改"><a href="#3-2-记录更改" class="headerlink" title="3.2 记录更改"></a>3.2 记录更改</h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs fortran">git <span class="hljs-keyword">status</span><br>git <span class="hljs-keyword">status</span> -s # for short<br></code></pre></td></tr></table></figure><p>工作目录中每个文件都存在 tracked or  untracked</p><p>从生命周期来看，文件可以分为untracked、unmodified、modified、staged</p><p>   Github将默认转换为main、但是git默认依旧是master  </p><p>同时可以设置gitignore来不希望Git自动添加，甚至限制您未追踪</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># 常见的gitignore规则</span><br><span class="hljs-bullet">1.</span> 忽略空白行或者 #<br><span class="hljs-bullet">2.</span> 在整个工作目录中递归<br><span class="hljs-bullet">3.</span> 可以使用正斜杠 / 开始模式<br><span class="hljs-bullet">4.</span> 可以使用正斜杠 / 结束模式来指定目录<br><span class="hljs-bullet">5.</span> 可以通过感叹号 ！ 来否定模式<br></code></pre></td></tr></table></figure><p>GitHub中维护了一个非常好的示例<a href="https://github.com/github/gitignore">https://github.com/github/gitignore</a></p><p>仅仅查看更改记录不够，你希望检查更改了哪些东西</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">git <span class="hljs-keyword">diff </span><span class="hljs-comment"># 查看更改但是未暂存</span><br>git <span class="hljs-keyword">diff </span>--staged <span class="hljs-comment"># 查看已经提交下一个阶段的内容，或者-- cached</span><br></code></pre></td></tr></table></figure><p>提交记录</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">git commit</span><br></code></pre></td></tr></table></figure><p>从Git中删除文件必须将其从tracked文件中删除，也就是需要从暂存区中删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">rm</span><br>git <span class="hljs-built_in">rm</span> - cached FILENAME <span class="hljs-comment"># 将文件保存在目录中，依旧保留在硬盘。但是从暂存区中删除。</span><br>git <span class="hljs-built_in">mv</span> <span class="hljs-comment"># 重命名文件</span><br></code></pre></td></tr></table></figure><h3 id="3-3-查看提交的历史记录"><a href="#3-3-查看提交的历史记录" class="headerlink" title="3.3 查看提交的历史记录"></a>3.3 查看提交的历史记录</h3><p>上述操作可以完成基本的工作流。在这个基础上，创建很多次提交之后，我们可以使用现有提交历史记录的存储库，回顾一下发生了什么，我们可以使用git log</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gauss">git <span class="hljs-built_in">log</span> <span class="hljs-meta"># 按照反向时间顺序列出提交的更改。同时也具有大量选项</span><br>git <span class="hljs-built_in">log</span> -p <span class="hljs-meta"># 显示每个提交中引入的差异</span><br>git <span class="hljs-built_in">log</span> --stat <span class="hljs-meta"># 缩写的统计信息</span><br>git <span class="hljs-built_in">log</span> --pretty=oneline <span class="hljs-meta"># 将日志输出更改为默认格式以为的格式</span><br>git <span class="hljs-built_in">log</span> --pretty=<span class="hljs-keyword">format</span> --<span class="hljs-keyword">graph</span><br></code></pre></td></tr></table></figure><h3 id="3-4-撤销操作"><a href="#3-4-撤销操作" class="headerlink" title="3.4 撤销操作"></a>3.4 撤销操作</h3><p>在任何staged，你都困难想撤销一些事情，通过回顾一些基本工具来保存撤销的更改</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">git</span> commit -amend <span class="hljs-comment"># 如果你想重做该提交，请你忘记额外的更改，暂存他们</span><br></code></pre></td></tr></table></figure><p>加入在修改两个文件之后，不小心add * 到暂存区</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$ git <span class="hljs-built_in">add</span> *<br>$ git status<br>On branch master<br>Changes <span class="hljs-keyword">to</span> be committed:<br>  (use <span class="hljs-string">&quot;git restore --staged &lt;file&gt;...&quot;</span> <span class="hljs-keyword">to</span> unstage)<br>    modified:   CONTRIBUTING.md<br>    renamed:    README.md -&gt; README<br></code></pre></td></tr></table></figure><p>但是只想commit其中的一个，那么需要</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">git <span class="hljs-keyword">reset</span> HEAD [filename you <span class="hljs-keyword">not</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">commit</span>]<br></code></pre></td></tr></table></figure><p>注意reset是非常危险的命令，checkout也可以实现</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">git checkout --&lt;<span class="hljs-keyword">file</span>&gt; <span class="hljs-comment"># 对文件所做的任何本地更改都已经消失，替换成为最后一个分阶段或者提交的版本。</span><br></code></pre></td></tr></table></figure><p>在之后常用git restore而不是git reset</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">git restore --staged &lt;<span class="hljs-keyword">file</span>&gt; <span class="hljs-comment"># 取消暂存</span><br></code></pre></td></tr></table></figure><h3 id="3-5-远程操作"><a href="#3-5-远程操作" class="headerlink" title="3.5 远程操作"></a>3.5 远程操作</h3><p>origin是Git赋予您克隆的服务器的默认名称</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">git <span class="hljs-keyword">clone</span> <span class="hljs-title">[your-git-url</span>]<br>git remote <span class="hljs-comment"># show your remote git name</span><br>git remote -v <span class="hljs-comment"># 显示存储的URL</span><br>git remote add shortname url <span class="hljs-comment"># git remote add pb &lt;https://github.com/chenxia31/blog&gt;</span><br><span class="hljs-comment">#  common line中可以使用short name来代替url</span><br></code></pre></td></tr></table></figure><p>从远程仓库连接</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs maxima">git fetch &lt;remote&gt; # 从远程仓库获取数据<br>git <span class="hljs-built_in">push</span> <span class="hljs-built_in">origin</span> master # 推送本地的master分支<br>git remote <span class="hljs-built_in">show</span> <span class="hljs-built_in">origin</span> # 显示一些常见的信息<br>git remote <span class="hljs-built_in">rename</span> [oldname] [newname]<br>git remote <span class="hljs-built_in">remove</span> [url]<br></code></pre></td></tr></table></figure><h3 id="3-6-标签"><a href="#3-6-标签" class="headerlink" title="3.6 标签"></a>3.6 标签</h3><p>和branch跟踪每次最新的commit 相反，tag是跟踪某一个特定commit的位置，如果不需要改动而仅查看的时候可以直接使用tag。推送到远程服务器需要显式的表达出tagname</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">git <span class="hljs-keyword">tag</span><br><span class="hljs-title">git</span> <span class="hljs-keyword">tag</span> <span class="hljs-title">-l</span> &#x27;v1.<span class="hljs-number">8.5</span>.*&#x27;<br><br><span class="hljs-comment"># 标签分为轻量级和注释</span><br>git <span class="hljs-keyword">tag</span> <span class="hljs-title">-a</span> v1.<span class="hljs-number">4</span> -m &#x27;my <span class="hljs-keyword">version</span> <span class="hljs-number">1.4</span>&#x27;<br><br>git <span class="hljs-keyword">tag</span> <span class="hljs-title">v1</span>.<span class="hljs-number">4</span><br><br>git push origin [tagname] <span class="hljs-comment"># git push origin v1.5</span><br>git push origin --tags<br><br><span class="hljs-comment"># 删除标签</span><br>git <span class="hljs-keyword">tag</span> <span class="hljs-title">-d</span> v1.<span class="hljs-number">4</span> <span class="hljs-comment"># 但是这样不会从任何远程服务器删除标签</span><br>git push origin --delete <span class="hljs-tag">&lt;tagname&gt;</span><br><br><span class="hljs-comment"># 查看tag</span><br>git checkout v2.<span class="hljs-number">0</span> <span class="hljs-comment">#这种会导致仓库处于“分离的HEAD状态“，如果进行更改、创建提交、标签将保持不变</span><br></code></pre></td></tr></table></figure><h3 id="3-7-别名-alias"><a href="#3-7-别名-alias" class="headerlink" title="3.7 别名 alias"></a>3.7 别名 alias</h3><p>创建自己snippets</p><h3 id="3-8-1-（非常重要）分支模式"><a href="#3-8-1-（非常重要）分支模式" class="headerlink" title="3.8.1 （非常重要）分支模式"></a>3.8.1 （非常重要）分支模式</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">git <span class="hljs-built_in">add</span> [filename]<br>git commit -m <span class="hljs-string">&#x27;message&#x27;</span><br><br><span class="hljs-comment">#git的默认分支名称是master，只不过很多人懒得改</span><br>git branch testing<br>git <span class="hljs-built_in">log</span> <span class="hljs-comment">--oneline --decorate</span><br>git checkout testing <span class="hljs-comment"># 更推荐git switch</span><br>git commit -<span class="hljs-keyword">a</span> -m <span class="hljs-string">&#x27;new commit&#x27;</span><br>git checkout master<br>git commit -<span class="hljs-keyword">a</span> -m <span class="hljs-string">&#x27;new commit2&#x27;</span><br>git <span class="hljs-built_in">log</span> <span class="hljs-comment">--oneline -decorate --graph --asll</span><br></code></pre></td></tr></table></figure><p>后序查看一些基本分支和合并的具体操作。想象一个工作流程，在网站上做一些工作，为一个新的故事创建一个分支，之后在另外分支做一些工作。因此可以参考一下的过程</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">git checkout -b [NewBranchName] <span class="hljs-comment"># git branch and git checkout 缩写</span><br><span class="hljs-comment"># 注意在签出的分支冲突未提交更改，但是可以允许 stashing and cleaning中了解</span><br>git checkout <span class="hljs-keyword">master</span> <span class="hljs-title"># 专注你的主线任务</span><br><span class="hljs-title">git</span> checkout -b &#x27;newfeature2&#x27;<br><br><span class="hljs-comment"># ~~ newfeature2结束</span><br>git checkout <span class="hljs-keyword">master</span><br><span class="hljs-title">git</span> merge newfeature2<br><span class="hljs-comment"># 删除分支</span><br>git branch -d hotfix<br><span class="hljs-comment"># 处理</span><br>git checkout NewBranchName<br><span class="hljs-comment"># 再次合并</span><br>git checkout <span class="hljs-keyword">master</span><br><span class="hljs-title">git</span> merge <span class="hljs-keyword">master</span><br><br><span class="hljs-title"># 两个合并出现冲突</span><br><span class="hljs-title">git</span> status <span class="hljs-comment">#查看</span><br><span class="hljs-comment"># 手动打开文件，处理冲突</span><br>git mergetool<br></code></pre></td></tr></table></figure><p>上述导致已经会创建、合并、删除一些分支，这里还有一些分支管理工具</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">git <span class="hljs-keyword">branch </span>-V <span class="hljs-comment"># 查看每个分支的最后一次提交</span><br>git <span class="hljs-keyword">branch </span>--merged <span class="hljs-keyword">or </span>--no-merged<span class="hljs-comment"># 过滤您已经或尚未合并到您当前使用的分支中的分支</span><br>git <span class="hljs-keyword">brach </span>--<span class="hljs-keyword">move </span><span class="hljs-comment"># 本地重命名分支</span><br>git push --set-upstream <span class="hljs-keyword">origin </span>main<br></code></pre></td></tr></table></figure><h3 id="3-8-2-分支工作流程"><a href="#3-8-2-分支工作流程" class="headerlink" title="3.8.2 分支工作流程"></a>3.8.2 分支工作流程</h3><p>已经有branch和merge的基础知识，应该用此做些什么？这里将轻量级分支的常见的工作流程展示，判断是否需要将其纳入自己的开发周期</p><p><a href="https://git-scm.com/book/en/v2/Git-Branching-Branching-Workflows">Git - Branching Workflows</a></p><p>常见的分类包括</p><ul><li>master 分支中完全稳定的代码</li><li>develop或next 工作中用于测试稳定性，不一定总是稳定的。当它稳定之后可以合并到master</li><li>proposed 协议更新</li><li>topic 短生命周期的分支，用于处理一些简单的feature add</li></ul><h3 id="3-8-3-远程分支"><a href="#3-8-3-远程分支" class="headerlink" title="3.8.3 远程分支"></a>3.8.3 远程分支</h3><ul><li><input disabled="" type="checkbox"> 待定</li></ul><p><a href="https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches">Git - Remote Branches</a></p><h3 id="3-8-4-分支管理"><a href="#3-8-4-分支管理" class="headerlink" title="3.8.4 分支管理"></a>3.8.4 分支管理</h3><h3 id="3-9-Git-服务器"><a href="#3-9-Git-服务器" class="headerlink" title="3.9 Git 服务器"></a>3.9 Git 服务器</h3><h3 id="3-10-分布式工作流"><a href="#3-10-分布式工作流" class="headerlink" title="3.10 分布式工作流"></a>3.10 分布式工作流</h3>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>版本控制系统</tag>
      
      <tag>GIT</tag>
      
      <tag>阶段总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>官方文档 pandas 的阅读和感悟</title>
    <link href="/posts/fdd91ae2.html"/>
    <url>/posts/fdd91ae2.html</url>
    
    <content type="html"><![CDATA[<p>官方文档是开发者对自己代码的解释。对于成熟的框架，官方文档可以最准确、权威的资料。如何阅读英文的官方文档一直是作为Coder weaker和English weaker的心魔，这里以Pandas文档为例子，尝试给出自己对于阅读官方文档、官方文档的查询工具、代码注释和Pandas文档的总结。</p><h3 id="0x01-Why-阅读官方文档"><a href="#0x01-Why-阅读官方文档" class="headerlink" title="0x01 Why 阅读官方文档"></a>0x01 Why 阅读官方文档</h3><p>对于质量比较高的项目，其官方文档往往能反映开发者的最直接的思想，而互联网上经过许多人的编码和解码，导致最后的意思可能和本意相差巨大。虽然可能存在其他更好的教程，但是官方文档给出的思想一定是最准确、权威的。</p><blockquote><p> 之前一直不知道GitHub在国内为什么访问这么慢。 在网上找到很多答案，比如修改DNS的、修改Hosts的、修改镜像源或者增加代理的，但其实本质的问题是GitHub.com的域名解析在国内往往需要多层中转进而因为污染等原因造成速度较慢，因此上述方法对应的是修改DNS、本机解析、设置镜像、走代理等多种方法，这些方法都没有错。但是只是给出这些方法会让人不能一览背后的思想而困惑。</p></blockquote><h3 id="0x02-What-官方文档"><a href="#0x02-What-官方文档" class="headerlink" title="0x02 What 官方文档"></a>0x02 What 官方文档</h3><p>以Pandas为例子，文档通常可以分为</p><ul><li>Getting start：新手教程，一般会教你如何搭建一个简单的应用示例。</li><li>User guide：使用教程，介绍技术的关键知识、思想和概念。</li><li>API reference：API文档，包括具体的API使用细节以及机制</li><li>Developer guide：开发文档</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-88d95021f0405610776fcf6d42d6afc4_1440w.png" alt="img"></p><p>image-20230507140000120</p><p>推荐使用  <a href="https://kapeli.com/dash">Dash gives your Mac instant offline access to 200+ API documentation sets.</a>  来进行官方API文档的管理。作为一个API文档浏览器(API documentation brower)和代码片段管理（Code snippet manager），可以提供的帮助有</p><ol><li>迅速</li><li>直接</li><li>本地</li><li>Alfred协作</li></ol><p>当然常见的函数用法可以直接通过命令来获取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> module_name <span class="hljs-comment">#such as import math</span><br><span class="hljs-built_in">help</span>(module_name) <span class="hljs-comment">#模块帮助查询</span><br><span class="hljs-built_in">dir</span>(module_name) <span class="hljs-comment"># 查询模块下所有的函数</span><br><span class="hljs-built_in">help</span>(module_name.func_name) <span class="hljs-comment">#查询具体函数的用法</span><br><span class="hljs-built_in">print</span>(func_name.__doc__) <span class="hljs-comment"># 打印函数的用法</span><br></code></pre></td></tr></table></figure><h3 id="0x03-Pandas-新手教程"><a href="#0x03-Pandas-新手教程" class="headerlink" title="0x03 Pandas 新手教程"></a>0x03 Pandas 新手教程</h3><p>首先介绍pandas是什么：</p><blockquote><p> Pandas is a <a href="https://www.python.org/">Python</a> package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, <strong>real-world</strong> data analysis in Python.</p></blockquote><p>其中基本的数据类型可以分为Series和DataFrame两种，基于NumPy构建。新手教程给出常见的Pandas使用的Topic。包括</p><h3 id="3-1-创建数据、查看数据、索引数据"><a href="#3-1-创建数据、查看数据、索引数据" class="headerlink" title="3.1 创建数据、查看数据、索引数据"></a>3.1 创建数据、查看数据、索引数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Series 一维标记数组，可以容纳各种数据类型(int、float、string、chat、python object)</span><br><span class="hljs-comment"># Pandas 二维标记数据框架，可以看作是一张数据表格，或者是SQL表格</span><br><span class="hljs-comment"># Insight01：Pandas中的数据结构可以看作是ndarray方式的标量（通过index索引），也可以看作是dict方式的字典（可以通过key索引）</span><br><span class="hljs-comment"># Attention01:Pandas数据之间的运算通常是按照元素的，并且是broadcasting的。对于缺失值通常使用numpy.nan填补</span><br><br><span class="hljs-comment"># 创建数据，可以来自于 1. ndarray、2. dict、3. scalar、4. tuple of dict</span><br>d = &#123;<br>    <span class="hljs-string">&quot;one&quot;</span>: pd.Series([1.0, 2.0, 3.0], index=[<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>]),<br>    <span class="hljs-string">&quot;two&quot;</span>: pd.Series([1.0, 2.0, 3.0, 4.0], index=[<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>]),<br>&#125;<br><br>d = &#123;<span class="hljs-string">&quot;one&quot;</span>: [1.0, 2.0, 3.0, 4.0], <span class="hljs-string">&quot;two&quot;</span>: [4.0, 3.0, 2.0, 1.0]&#125;<br>pd.DataFrame(d, index=[<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>])<br><br>data2 = [&#123;<span class="hljs-string">&quot;a&quot;</span>: 1, <span class="hljs-string">&quot;b&quot;</span>: 2&#125;, &#123;<span class="hljs-string">&quot;a&quot;</span>: 5, <span class="hljs-string">&quot;b&quot;</span>: 10, <span class="hljs-string">&quot;c&quot;</span>: 20&#125;]<br>pd.DataFrame(data2, index=[<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;second&quot;</span>])<br><br>data3= &#123;<br>        (<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): 1, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): 2&#125;,<br>        (<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): 3, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): 4&#125;,<br>        (<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): 5, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): 6&#125;,<br>        (<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>): 7, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): 8&#125;,<br>        (<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>): &#123;(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>): 9, (<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>): 10&#125;,<br>    &#125;<br>pd.DataFrame(data3)<br><br><span class="hljs-comment"># 查看数据</span><br>.<span class="hljs-built_in">head</span>() <span class="hljs-comment">#查看dataframe的顶部</span><br>.<span class="hljs-built_in">tail</span>() <span class="hljs-comment">#查看dataframe的底部</span><br>.columns()<br>.index()<br>.to_numpy() <span class="hljs-comment">#注意这个可能是一个昂贵的操作</span><br>.describe() <span class="hljs-comment"># 显示数据的快速摘要</span><br>.T <span class="hljs-comment"># 转置</span><br><br>df.sort_index(axis=1, ascending=False) <span class="hljs-comment">#按照轴进行排序</span><br>df.sort_values(by=<span class="hljs-string">&quot;B&quot;</span>)<br><br><span class="hljs-comment"># 更改控制台显示</span><br>info() <span class="hljs-comment"># 用于快速显示dataframe信息</span><br>baseball.info()<br><br>to_string() <span class="hljs-comment"># 以表格的形式返回dataframe的字符串表示</span><br><span class="hljs-built_in">print</span>(baseball.iloc[-20:, :12].to_string())<br>display.width <span class="hljs-comment"># 更改单行的打印量</span><br>pd.set_option(<span class="hljs-string">&quot;display.width&quot;</span>, 40)  <span class="hljs-comment"># default is 80</span><br>pd.set_option(<span class="hljs-string">&quot;display.max_colwidth&quot;</span>, 30)<br>pd.DataFrame(np.random.randn(3, 12))<br><br><span class="hljs-comment"># 索引数据</span><br><span class="hljs-comment"># 继承series的索引</span><br><span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;one&quot;</span>]<br><span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;three&quot;</span>] = <span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;one&quot;</span>] * <span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;two&quot;</span>]<br><span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;flag&quot;</span>] = <span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;one&quot;</span>] &gt; 2<br>del <span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;two&quot;</span>]<br>three = df.pop(<span class="hljs-string">&quot;three&quot;</span>)<br><span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;foo&quot;</span>] = <span class="hljs-string">&quot;bar&quot;</span> <span class="hljs-comment">#自动broadcasting</span><br><span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;one_trunc&quot;</span>] = <span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;one&quot;</span>][:2] <span class="hljs-comment"># 自动符合index</span><br>df.insert(1, <span class="hljs-string">&quot;bar&quot;</span>, <span class="hljs-built_in">df</span>[<span class="hljs-string">&quot;one&quot;</span>])<br><br><span class="hljs-comment"># assign() 始终返回数据的副本，但是原始的dataframe并不会修改</span><br>iris.assign(sepal_ratio=lambda x: (x[<span class="hljs-string">&quot;SepalWidth&quot;</span>] / x[<span class="hljs-string">&quot;SepalLength&quot;</span>])).<span class="hljs-built_in">head</span>()<br>df.loc <span class="hljs-comment">#根据标签选择</span><br>df.iloc <span class="hljs-comment">#根据下标选择</span><br><span class="hljs-built_in">df</span>[5:10] <span class="hljs-comment"># 切片行</span><br><span class="hljs-built_in">df</span>[bool] <span class="hljs-comment"># 切片行</span><br></code></pre></td></tr></table></figure><h3 id="3-2-数据缺失处理、运算函数"><a href="#3-2-数据缺失处理、运算函数" class="headerlink" title="3.2 数据缺失处理、运算函数"></a>3.2 数据缺失处理、运算函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">np.nan <span class="hljs-comment">#表示丢失的数据类型</span><br><br>df.reindex() <span class="hljs-comment"># 允许更改、添加和删除指定轴上的索引，同时返回数据副本</span><br>df1 = df.reindex(index=dates[<span class="hljs-number">0</span>:<span class="hljs-number">4</span>], columns=<span class="hljs-built_in">list</span>(df.columns) + [<span class="hljs-string">&quot;E&quot;</span>])<br>df1.loc[dates[<span class="hljs-number">0</span>] : dates[<span class="hljs-number">1</span>], <span class="hljs-string">&quot;E&quot;</span>] = <span class="hljs-number">1</span><br><br>df.dropna() <span class="hljs-comment"># 删除任何缺少数据的行</span><br>df.fillna() <span class="hljs-comment"># 填充确实的数据</span><br>isna() <span class="hljs-comment">#获取值是否为nan</span><br><br>默认按照broadcasting进行计算<br>df.mean() <span class="hljs-comment"># 计算平均值</span><br>df.mean(<span class="hljs-number">1</span>) <span class="hljs-comment"># 另外一个方向</span><br>df.apply() <span class="hljs-comment"># 将用户自定义的函数应用于数据</span><br>df.apply(np.cumsum)<br>df.apply(<span class="hljs-keyword">lambda</span> x: x.<span class="hljs-built_in">max</span>() - x.<span class="hljs-built_in">min</span>())<br><br>values_count()<br>df.<span class="hljs-built_in">str</span>.ufunc() <span class="hljs-comment"># 在str属性下，可以调用需要其他的字符串</span><br></code></pre></td></tr></table></figure><h3 id="3-3-集合之间的处理：合并、分组、重塑和多索引"><a href="#3-3-集合之间的处理：合并、分组、重塑和多索引" class="headerlink" title="3.3 集合之间的处理：合并、分组、重塑和多索引"></a>3.3 集合之间的处理：合并、分组、重塑和多索引</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">pd.concat([df1,df2,df3]) # 按照axis进行合并，<span class="hljs-keyword">join</span>参数为<span class="hljs-keyword">outer</span>或者<span class="hljs-keyword">inner</span>，ignore_inde重建索引<br>pd.merge(df1,df2,key) # 类似<span class="hljs-keyword">SQL</span>中的连接，可以根据一个或者多个键将不同的dataframe连接起来<br>pd.<span class="hljs-keyword">join</span>() # 用于key的合并<br><br>df.groupby(&quot;A&quot;)[[&quot;C&quot;, &quot;D&quot;]].sum() # 类似<span class="hljs-keyword">SQL</span>中的<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span><br><br># 重塑和多索引看不懂～<br></code></pre></td></tr></table></figure><h3 id="3-4时间序列处理函数专题"><a href="#3-4时间序列处理函数专题" class="headerlink" title="3.4时间序列处理函数专题"></a>3.4时间序列处理函数专题</h3><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">resample<span class="hljs-params">()</span> <span class="hljs-comment">#将第二列数据转换成5min，有点意思</span><br>series.tz_localize<span class="hljs-params">()</span> <span class="hljs-comment"># 将时间序列转换成为本地时区</span><br>series.tz_convert<span class="hljs-params">()</span> <span class="hljs-comment"># 将时区感知时间序列转换成为另一个时区</span><br><span class="hljs-string">.to_period</span><span class="hljs-params">()</span> <span class="hljs-comment"># 转换成为时间跨度</span><br><span class="hljs-string">.to_timestamp</span><span class="hljs-params">()</span> <span class="hljs-comment"># 转换成为时间戳</span><br></code></pre></td></tr></table></figure><h3 id="0x04-Pandas-用户教程"><a href="#0x04-Pandas-用户教程" class="headerlink" title="0x04 Pandas 用户教程"></a>0x04 Pandas 用户教程</h3><p>在用户教程中，Pandas提供了一些更加细致的专题。这里只给出有趣的话题</p><h3 id="4-1-数据结构简介-intro-to-data-structure"><a href="#4-1-数据结构简介-intro-to-data-structure" class="headerlink" title="4.1 数据结构简介 intro to data structure"></a>4.1 数据结构简介 intro to data structure</h3><p>和第三章的类似</p><h3 id="4-2-关键基础函数-essential-basic-functionality"><a href="#4-2-关键基础函数-essential-basic-functionality" class="headerlink" title="4.2 关键基础函数 essential basic functionality"></a>4.2 关键基础函数 essential basic functionality</h3><p>Attention02:对于dataframe的操作可以三个维度：元素的维度、行或者列的维度、整个DataFrame的维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看</span><br>.head()<br>.tail()<br><br>.shape<br>.index<br>.columns<br><br>.to_numpy()<br>.array()<br><span class="hljs-comment"># Attention03:更加推荐使用这两个，而不是使用values来索引。因为dataframe中常见的操作包括扩展类型，或者不同类型，使用values可能会出现性能上的问题</span><br><br><span class="hljs-comment"># 运算（elementLevel）</span><br>df1 &gt; df2<br>df1*df2<br>df1+df2<br><br><span class="hljs-comment"># 二维运算，这里需要重视 1.高维和低维之间存在broadcasting 2.计算中数据丢失的情况</span><br>.add() <span class="hljs-comment"># 相加，可以设置axis函数</span><br>.sub() <span class="hljs-comment"># 相减，可以设置axis</span><br>.mul() <span class="hljs-comment"># 相乘，可以设置axis</span><br>.div() <span class="hljs-comment"># 相除，可以设置axis</span><br><br>.radd()<br>.rsub()<br>.<span class="hljs-built_in">divmod</span>() <span class="hljs-comment"># 商，和余</span><br><br><span class="hljs-comment"># 缺失值操作</span><br>fill_value=<span class="hljs-number">0</span> <span class="hljs-comment">#可选参数</span><br>.fill_na() <span class="hljs-comment">#利用函数填补</span><br><br><span class="hljs-comment"># 二进制比较方法 eq ne lt qt le ge</span><br><span class="hljs-comment"># 布尔运算符号</span><br>.empty() <span class="hljs-comment"># 测试对象是否为空</span><br>.<span class="hljs-built_in">all</span>() <span class="hljs-comment"># 测试并</span><br>.<span class="hljs-built_in">any</span>() <span class="hljs-comment"># 测试或</span><br>.<span class="hljs-built_in">bool</span>() <span class="hljs-comment"># 测试单个元素</span><br><br><span class="hljs-comment">#Attention 04：不可以使用 if df:...，这样不符合布尔逻辑</span><br><br><span class="hljs-comment"># 描述性分析</span><br><span class="hljs-built_in">sum</span>()<br>mean()<br>quantile()<br>cumsum()<br>cumprod()<br><br>describe()<br>idxmin() <span class="hljs-comment"># 寻找最小值下标 argmin</span><br>idxmax() <span class="hljs-comment"># 寻找最大值下标 argmax</span><br>res=df.value_counts()<br>mode()<br><br><span class="hljs-comment"># discretization and quantiling</span><br>cut() <span class="hljs-comment">#基于值的bin</span><br>qcut() <span class="hljs-comment">#基于分位数的bin</span><br><br><span class="hljs-comment"># 函数应用：pipeline(),apply(),applymap(); 聚合函数agg() and transform()</span><br><br><span class="hljs-comment"># 尝试想象一下如何新增两个列</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extractCityName</span>(<span class="hljs-params">df</span>):<br>    df[<span class="hljs-string">&#x27;cityName&#x27;</span>]=df[<span class="hljs-string">&#x27;cityAndCode&#x27;</span>].<span class="hljs-built_in">str</span>.aplit(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>.get(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> df<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">addCountryName</span>(<span class="hljs-params">df,countryName=<span class="hljs-literal">None</span></span>):<br>    col=<span class="hljs-string">&#x27;cityName&#x27;</span><br>    df[<span class="hljs-string">&#x27;cityAndCounty&#x27;</span>]=df[col]+countryName<br>df_p=pd.Dataframe(&#123;<span class="hljs-string">&#x27;cityAndCode&#x27;</span>:[<span class="hljs-string">&#x27;chicago&#x27;</span>,<span class="hljs-string">&#x27;IL&#x27;</span>]&#125;)<br><span class="hljs-comment"># 现在可以有两种方式来新增心得countryName</span><br>addCounryName(extractCityName(df_p),country_name=<span class="hljs-string">&#x27;US&#x27;</span>)<span class="hljs-comment">#法1</span><br>df_p.pipeline(extractCityName).pipe(addCountryName,countryName=<span class="hljs-string">&#x27;US&#x27;</span>)<span class="hljs-comment">#法2:或者使用pipeline</span><br><br><span class="hljs-comment"># 重索引 reindex</span><br><br><span class="hljs-comment"># 迭代 iteration</span><br><span class="hljs-keyword">for</span> label,ser <span class="hljs-keyword">in</span> df.items():<br>  <span class="hljs-built_in">print</span>(label)<br>  <span class="hljs-built_in">print</span>(ser)<br><span class="hljs-keyword">for</span> row_index,row <span class="hljs-keyword">in</span> df.iterrows():<br>    <span class="hljs-built_in">print</span>(row)<br><br><span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> df.itertuples():<br>    <span class="hljs-built_in">print</span>(row)<br><br><span class="hljs-comment"># 排序访问</span><br>.sort_index()<br>.sort_values()<br></code></pre></td></tr></table></figure><h3 id="4-5-索引和选择数据-Indexing-and-selecting-data"><a href="#4-5-索引和选择数据-Indexing-and-selecting-data" class="headerlink" title="4.5 索引和选择数据 Indexing and selecting data"></a>4.5 索引和选择数据 Indexing and selecting data</h3><p>和新手教程差不多</p><h3 id="4-6-多重下标和高级索引-MultiIndex-and-advanced-indexing"><a href="#4-6-多重下标和高级索引-MultiIndex-and-advanced-indexing" class="headerlink" title="4.6 多重下标和高级索引 MultiIndex and advanced indexing"></a>4.6 多重下标和高级索引 MultiIndex and advanced indexing</h3><p>和新手教程差不多</p><h3 id="4-6-5-Merge、join、concatenate、compare"><a href="#4-6-5-Merge、join、concatenate、compare" class="headerlink" title="4.6.5 Merge、join、concatenate、compare"></a>4.6.5 Merge、join、concatenate、compare</h3><p>和SQL教程差不多</p><h3 id="4-7-重塑和数据透视表"><a href="#4-7-重塑和数据透视表" class="headerlink" title="4.7 重塑和数据透视表"></a>4.7 重塑和数据透视表</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b4979b776f518f16933c32856474853c_1440w.png" alt="img"></p><p>..&#x2F;_images&#x2F;reshaping_pivot.png</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># pivot() 来按照元素进行堆叠,可以选择stack或者unstack</span><br>         <span class="hljs-attribute">date</span> variable     value<br><span class="hljs-attribute">0</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">03</span>        A  <span class="hljs-number">0</span>.<span class="hljs-number">469112</span><br><span class="hljs-attribute">1</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>        A -<span class="hljs-number">0</span>.<span class="hljs-number">282863</span><br><span class="hljs-attribute">2</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>        A -<span class="hljs-number">1</span>.<span class="hljs-number">509059</span><br><span class="hljs-attribute">3</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">03</span>        B -<span class="hljs-number">1</span>.<span class="hljs-number">135632</span><br><span class="hljs-attribute">4</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>        B  <span class="hljs-number">1</span>.<span class="hljs-number">212112</span><br><span class="hljs-attribute">5</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>        B -<span class="hljs-number">0</span>.<span class="hljs-number">173215</span><br><span class="hljs-attribute">6</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">03</span>        C  <span class="hljs-number">0</span>.<span class="hljs-number">119209</span><br><span class="hljs-attribute">7</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>        C -<span class="hljs-number">1</span>.<span class="hljs-number">044236</span><br><span class="hljs-attribute">8</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>        C -<span class="hljs-number">0</span>.<span class="hljs-number">861849</span><br><span class="hljs-attribute">9</span>  <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">03</span>        D -<span class="hljs-number">2</span>.<span class="hljs-number">104569</span><br><span class="hljs-attribute">10</span> <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>        D -<span class="hljs-number">0</span>.<span class="hljs-number">494929</span><br><span class="hljs-attribute">11</span> <span class="hljs-number">2000</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>        D  <span class="hljs-number">1</span>.<span class="hljs-number">071804</span><br><br><span class="hljs-attribute">pivoted</span> = df.pivot(index=<span class="hljs-string">&quot;date&quot;</span>, columns=<span class="hljs-string">&quot;variable&quot;</span>, values=<span class="hljs-string">&quot;value&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6bd477e1e00ff31ad153591bbe7b46b4_1440w.png" alt="img"></p><p>..&#x2F;_images&#x2F;reshaping_stack.png</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1329d236b79b3157190ef93dcde0881d_1440w.png" alt="img"></p><p>..&#x2F;_images&#x2F;reshaping_unstack.png</p><p>剩余的太理论了～</p><h3 id="0x05-心得"><a href="#0x05-心得" class="headerlink" title="0x05 心得"></a>0x05 心得</h3><p>文档很多，但是其中给出开发者的default的选项，可以提高对其的理解。</p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>文档阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Everything is Connected Graph Neural Networks</title>
    <link href="/posts/67694a92.html"/>
    <url>/posts/67694a92.html</url>
    
    <content type="html"><![CDATA[<p>在许多地方，Graph是获取信息的主要形式。因为无论在自然界或者是人造系统中通常需要用图结构表示。常见的重要例子包括分子结构、社交网络、交通路网。这被一些关键的科研和公司所了解，并在交通流预测、药物发芽、社交媒体分析和推荐系统中影响。更进一步，之前机器学习在：图片、文本、语音分析都可以看作是一种特殊的Graph  representation learning。因此这些领域之间会有可以相互借鉴的地方。 本片总结的目的是为了让读者可以了解这些领域中共同的概念，并认清楚Graph representation learning与其他领域的关系</p><span id="more"></span><p>## 引言：为什么需要Graphs的数据结构</p><p>本次报告回给出一个充满生机（variant）和令人激动的深度学习研究：Graph representation learning；或者说通过Graph（这个包含nodes和edges）来构建机器学习模型，这些模型被普遍认识为GNN</p><p>这里有一个非常好的学习GNN的原因。从分子到大脑，图结构是描述结构的一个普遍的数据结构，相似的，从交通网络到社交网络也是相似的图的结构</p><p>这些潜在已经被学术界和工业界所熟知，并具有一下作用：发现抗生素、评估出行时间、内容推荐、商品推荐、设计新型机器学习硬件。更进一步，基于GNN的系统包括潜在的数学形式的发现、表示理论领域新的顶级猜想。说数十亿人每天都在接触GNN的预测，这并不轻描淡写。这并非轻描淡写。 因此，研究GNN可能是一个有价值的追求，即使不旨在直接促进其发展。</p><p>除此之外。这也是一种驱动我们提问和做决策的认知过程可能也是基于图结构的。当我们阅读一段对话，我们可能在脑海中得到各种概念，并将他们串联起来，并表示到实际的系统当中。如果实际是这样的，说明我们也可以依靠Graph representation learning来构建一个类似的智能系统。注意这样的GNN研究并不和现有的研究相冲突，我们后续会发现Transformers时一类特殊的GNN</p><p>## 基础：排列等变性与不变性</p><p>除了上述需要Graph的原因，后面推导主要来自于[Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges](<a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</a>)；对于图主要来自于$G&#x3D;(V,E)$,也就是通过一系列边和节点组成。</p><p>因为我们关心基于点的特征，因此对于每个点$u\in V$ 作为一个特征向量，$x_u \in R^k$,因此在常见的机器学习数据集通常可以表示为点特征矩阵（node feature matrix），也就是$X \in R^{|v|*k}$,可以表示为</p><p>$$X&#x3D;[x_1,x_2,…,x_{|v|}]^T$$</p><p>也有很多方式表达边，1⃣️基于线性代数的邻接矩阵（adjacency matrix），可以表示为</p><p>$$ f(x)&#x3D;\left{</p><p>\begin{aligned}</p><p> 1&amp;&amp;(u,v)\in E\</p><p> 0&amp;&amp;(u,v) \notin E</p><p>\end{aligned}</p><p>\right.</p><p>$$</p><p>注意初次之外，我们通常希望一些额外的信息可以添加到边上（比如距离标量、甚至是完整的特征向量），本文中不考虑这样的事情来保证文章的内容纯洁性。</p><p>然而，使用上述表示的行为本身强加了节点排序，因此是一种任意选择，与无序的节点和边不一致！，因此我们需要确保点和边之间是无序并不会导致输出的变化$PAP^T$，我们发现使用下述方式表示一个GNN可能是有用的</p><p>$$f(PX,PAP^T)&#x3D;f(X,A) \ \ (无序性）$$</p><p>$$F(PX,PAP^T)&#x3D;PF(X,A) \ \ (相等性)$$</p><p>这里我们假设函数并不会对Graph的链接性造成影响，我们只是假设根据图返回性质。</p><p>此外，Graph的边天然的导致在一些函数中会有局部性限制（locality constraint），比如CNN在图像中的局部进行卷机运算，GCN也只能在一些节点的邻居节点（neighbourhood of a node），邻居节点的定义为：</p><p>$$N_u&#x3D;{v|(u,v)\in E}$$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/stylename.jpeg" alt="16771285716973"></p><p>在对应节点和节点对应的邻居节点中可以定义相关的函数，用于考虑所有的相近的关系，通过简单的线性组合我们会发现</p><p>$$h_u&#x3D;\phi(X_u,X_{N_u)}$$</p><p>$$F(X)&#x3D;[h_1,h_2,…,h_{|v|}]^T$$</p><p>如果函数$\phi$具有排列不变（permutation invariant），那么函数F具有排列等变性permutation equivariant</p><p>所以接下来的重点是如何定义函数$\phi$</p><p>## GNN：Graph neural network</p><p>不必说，定义函数$\phi$是机器学习领域最活跃的研究之一，按照之前的文献综述，这里可能会有diffusion、propagation、message passing等方法，在2021年的这篇文章中，从空域的角度可以划分为下面三种方式</p><p>- convolutional</p><p>- attentional</p><p>- message- passing</p><p>这篇文章的综述也不仅仅包含在特定的GNN层的理解，也就是说</p><p>- 表示卷积网络 representative convolutional ChebyNet</p><p>- 图卷积网路 GNN</p><p>- 图卷积表示注意力网络混合模型 MoNet</p><p>- 图注意力网络 GAT和GATv2</p><p>- 图网络 GN</p><p>对于上述给定的GNN曾中，我们可以学习一些有趣的任务，表示不同近似联合 approximate combining的变量，这里通常会有三种任务</p><p>- 节点分类 Node classification</p><p>- 图分类 Graph classification</p><p>- 连接预测 Link prediction </p><p>在此基础上，一些更加自然的问题被提出，通过message- passing给出的图神经网络是否可以表示所有有效的permutation</p><p>-equivariant function ,这里具有不同的观点。it is the author’s opinion that the formalism in this section is likely all we need to build powerful GNNs—although, of course, different perspec- tives may benefit different problems, and existence of a powerful GNN does not mean it is easy to find using stochastic gradient descent.</p><p>虽然GNN可能会强大，但是也许找不到一个合适的SDG来寻找到收敛的情况</p><p>## 没有图的GNN：Deep sets and Transformers</p><p>经过之前的章节的讨论，我们的假设都是我们可以得到一个图的形式</p><p>但是往往在实际问题中，我们并不能得到一个明显的A，甚至不清楚图存在的地方，甚至一个如果一个没有噪音的图A，也通常不会是最优的计算图，因为它导致的message passing可能是有问题的，因此需要考虑实际问题中输入图方式的建模</p><p>&gt; As such, it is generally a useful pursuit to study GNNs that are capable of modulating the input graph structure.</p><p>我们可以先假设我们只有一对特征节点X，没有邻接矩阵。最开始的想法是“悲观的”：<strong>假设我们根本没有邻接矩阵，A+I</strong>，在这个假设下，根据三种任务的分类我们可以得到Deep sets models，但是没有体现价值</p><p>另外一种完全相反的想法是，<strong>假设我们具有全链接的图，fully- connected graph</strong>，这种对于数量较少的节点可能是一种比较适合的方式，也常常被捡到卷积图神经网络。但是也有可能会退化成为一种Deepsets</p><p>下面一种是基于注意力模块的GNN，推导如下所示。是Transformer的一种关键应用，对于为什么需要Transformer，这里可以逆向的考虑一下NLP中的东西</p><p>- 原因之一 类似句子中的词元之间关系的远近，在图中</p><p>- 原因之二，attention可以做并行处理。由于不同任务中联系可能是不想死的，我们可以通过attention来学习不同节点的连接关系，这样可以保证对于不同的任务中都可以有相同的最优解的形式。</p><p>除此之外，还有第三种的没有输入图的GNN方式，用来GNN的连接推断，这是一种隐图推断 latent graph inference</p><p>## 排列等变性的GNN：几何图</p><p>为了总结，我们需要回顾另外一个假设：**我们假设图通常是离散的、无序的、具有点和边、同时易受排列对称性的影响。但是这并不能代表完全的图，事实上，通常具有空间几何特征</p><p>更普遍的，我们假设输入时geometric graphs：<strong>节点的属性来自于自身特征和坐标点</strong>，我们希望不仅建立一个基于排列不变性的点，还不包括3D旋转不变性的点。</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper 阅读</tag>
      
      <tag>gnn</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229 机器学习 vol11 ｜ 课后作业 2</title>
    <link href="/posts/ea4b3ffa.html"/>
    <url>/posts/ea4b3ffa.html</url>
    
    <content type="html"><![CDATA[<p>Problem set00 是关于线性代数和多元微积分学的基本知识，Problem set 01主要是监督学习。作业要求最好使用LaTex进行编写，同时需要将library保存到environment.yml文件中，并保证run.py脚本可以正常运行。</p><h3 id="0x01-线性分类（逻辑回归和广义线性模型）"><a href="#0x01-线性分类（逻辑回归和广义线性模型）" class="headerlink" title="0x01 线性分类（逻辑回归和广义线性模型）"></a>0x01 线性分类（逻辑回归和广义线性模型）</h3><h3 id="1-1-问题回顾"><a href="#1-1-问题回顾" class="headerlink" title="1.1 问题回顾"></a>1.1 问题回顾</h3><p>Linear classifiers ( logistic regression and GDA)</p><p>在本次作业中将回顾之前的概率线性分类器，</p><ol><li>判别线性分类 (discriminative linear classifier) ：逻辑回归</li><li>生成线性分类 （generative linear classifier）：高斯判别模型</li></ol><p>两者均可以将一个数据集分成两类，但是基于不同的假设，本次问题的目的是找到两者的相同点和差异。</p><h3 id="1-2-实际问题"><a href="#1-2-实际问题" class="headerlink" title="1.2 实际问题"></a>1.2 实际问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-953073d4bbe0fb585e3671449d2ca2e5_1440w.png" alt="img"></p><p>问题1-(a)</p><p>注意Hessian矩阵为PSD，则说明损失函数是凸（convex）的，也就是存在极值点</p><p>𝑝(𝑦|𝑥,𝑡ℎ𝑒𝑡𝑎)&#x3D;𝑝(𝑦,𝑥,𝑡ℎ𝑒𝑡𝑎)&#x2F;𝑝(𝑥,𝑡ℎ𝑒𝑡𝑎)&#x3D;𝑝(𝑥,𝑡ℎ𝑒𝑡𝑎|𝑦)𝑝(𝑦)&#x2F;𝑝(𝑥,𝑡ℎ𝑒𝑡𝑎) </p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5381f9c5574ce4899eb18cb477728684_1440w.png" alt="img"></p><p>1-b</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8641037bcb189b702521d34b929d6462_1440w.png" alt="img"></p><p>1-c</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8f46410ae30d25bbb247b2d4d0c318b5_1440w.png" alt="img"></p><p>1-c(2)</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c033c6076e08731a98accc169e3667a5_1440w.png" alt="img"></p><p>1-d</p><h3 id="1-3-问题求解"><a href="#1-3-问题求解" class="headerlink" title="1.3 问题求解"></a>1.3 问题求解</h3><p>由于公式较多，只能给出思路</p><p>（1） 展开求导就能发现是大于0</p><p>（2）coding就完事了，numpy的MATLAB的矩阵计算方式</p><p>（3）GDA按照贝叶斯展开就行了</p><p>（4） 发现是相同的</p><p>（5）（6）（7）（8）是一些可视化的东西</p><h3 id="0x02-不完整、只有正标签：Incomplete，Positive-only-labels"><a href="#0x02-不完整、只有正标签：Incomplete，Positive-only-labels" class="headerlink" title="0x02 不完整、只有正标签：Incomplete，Positive- only labels"></a>0x02 不完整、只有正标签：Incomplete，Positive- only labels</h3><h3 id="2-1-问题回顾"><a href="#2-1-问题回顾" class="headerlink" title="2.1 问题回顾"></a>2.1 问题回顾</h3><p>假设在我们没有得到完整的标签的情况下的，只能确定性得到部分的正样本，而不能得到其他样本的标签。也就是所有负样本和剩余的正样本是没有标签的。</p><p>这道题的问题设置更像是一种引导，如何理解这种情况下如何构建模型</p><h3 id="2-2-问题"><a href="#2-2-问题" class="headerlink" title="2.2 问题"></a>2.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f993231b4f134eaddbd837448024a4e8_1440w.png" alt="img"></p><p>2-a</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-fb15d7cc0e440384e9fceff01b58d93c_1440w.png" alt="img"></p><p>2-b</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-25d53c46cb3eea386529b4a69527fd9e_1440w.png" alt="img"></p><p>2-c</p><p>后续是那个coding problem</p><h3 id="2-3-问题解答"><a href="#2-3-问题解答" class="headerlink" title="2.3 问题解答"></a>2.3 问题解答</h3><p>（1） 简单随机样本抽样</p><h3 id="0x03-泊松分布的拟合"><a href="#0x03-泊松分布的拟合" class="headerlink" title="0x03 泊松分布的拟合"></a>0x03 泊松分布的拟合</h3><p>重新回归GLM中的三个基本假设：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7acdf9f7f1f5fafdf9e4637417f51f52_1440w.png" alt="img"></p><p>GLM-assumption</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d4f2e1ec6e32a9029cca092802c7dd3e_1440w.png" alt="img"></p><p>GLM-assumption02</p><p>本重新认识到：</p><ol><li>第一个assumption给出了y的分布！这可以用来计算似然函数</li><li>第二个assumption给出如何确定hypothesis，也就是模型</li><li>第三个assumption似乎是人们设计好的，这样我们才能对一些线性模型做假设</li></ol><h3 id="0x04-广义线性模型的convex相关的研究"><a href="#0x04-广义线性模型的convex相关的研究" class="headerlink" title="0x04 广义线性模型的convex相关的研究"></a>0x04 广义线性模型的convex相关的研究</h3><p>太理论了看不下去～</p><h3 id="0x05-加权线性回归"><a href="#0x05-加权线性回归" class="headerlink" title="0x05 加权线性回归"></a>0x05 加权线性回归</h3><p>除了 利用gradient descent求解，也可以使用normal equation来进行参数的求解；非常的奇妙！</p><p>注意这部分和后面的attention有关系</p><p>后面的公式太多了不想打，</p><h3 id="0x06-感悟"><a href="#0x06-感悟" class="headerlink" title="0x06 感悟"></a>0x06 感悟</h3><p>动手才能发现学习的问题所在，动手才能知道代码应该如何实践！</p><p>对于现实世界的抽象可以帮助我们进一步学习和研究，虽然矩阵的定义很简单，但是只有在这个抽象定义的基础上才能发展出线性代数这门学科，才能用此解决问题。</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
      <tag>课后作业</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229 机器学习 Vol10 ｜ 课后作业 1</title>
    <link href="/posts/e8f3ab7c.html"/>
    <url>/posts/e8f3ab7c.html</url>
    
    <content type="html"><![CDATA[<p>CS229的homework之前一直没有写，趁这个寒假结束掉它！如有错误欢迎指正！</p><h3 id="0x01-Gradients-and-Hessians：求导和海森矩阵"><a href="#0x01-Gradients-and-Hessians：求导和海森矩阵" class="headerlink" title="0x01 Gradients and Hessians：求导和海森矩阵"></a>0x01 Gradients and Hessians：<a href="https://en.wikipedia.org/wiki/Gradient">求导</a>和<a href="https://zh.m.wikipedia.org/zh-hans/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%99%A3">海森矩阵</a></h3><h3 id="1-1-定义回顾"><a href="#1-1-定义回顾" class="headerlink" title="1.1 定义回顾"></a>1.1 定义回顾</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-64759f09f774e3dd1968cca29fe8cd47_1440w.png" alt="img"></p><p>多元函数一阶导</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-bccc21469e5e0b39382edd649a5e15d2_1440w.png" alt="img"></p><p>多元函数二阶导</p><p><a href="https://zh.m.wikipedia.org/zh-hans/%E5%B0%8D%E7%A8%B1%E7%9F%A9%E9%99%A3">Symmetric</a>: 对称矩阵</p><h3 id="1-2-问题"><a href="#1-2-问题" class="headerlink" title="1.2 问题"></a>1.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-fb697873412592cd3f9d55826766567b_1440w.png" alt="img"></p><p>问题</p><h3 id="1-3-解答"><a href="#1-3-解答" class="headerlink" title="1.3 解答"></a>1.3 解答</h3><p>(1)∇𝑓(𝑥)&#x3D;𝐴𝑥+𝑏 </p><p>(2)∇𝑓(𝑥)&#x3D;𝑔′(ℎ(𝑥))∇ℎ(𝑥) </p><p>(3)∇2𝑓(𝑥)&#x3D;𝐴𝑇&#x3D;𝐴 </p><p>(4)∇2𝑓(𝑥)&#x3D;𝑔″(𝑎𝑇𝑥)𝑎𝑇𝑎 </p><h3 id="0x02-Positive-definite-matrices-正定矩阵"><a href="#0x02-Positive-definite-matrices-正定矩阵" class="headerlink" title="0x02 Positive definite matrices : 正定矩阵"></a>0x02 Positive definite matrices : 正定矩阵</h3><h3 id="2-1-定义回顾"><a href="#2-1-定义回顾" class="headerlink" title="2.1 定义回顾"></a>2.1 定义回顾</h3><p>positive semi-definite(PSD): 半正定矩阵</p><p><a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5">positive definite: 正定矩阵，eg：单位阵</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4e92aa0c8070f184aa0bd352744b17b3_1440w.png" alt="img"></p><p>正定矩阵定义</p><p><a href="https://zh.wikipedia.org/wiki/%E9%9B%B6%E7%A9%BA%E9%97%B4">Null-space:</a> 核，表示一个算子的零空间是方程$AV&#x3D;0$的所有解$v$的集合</p><p><a href="https://zh.wikipedia.org/wiki/%E7%A7%A9_(%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0)">Rank</a>: 矩阵A的列秩是A线性无关的纵列的极大数目；可以用于计算线性方程组解的树木、也可以用来确定线性系统是否为可控制的、可观察的</p><h3 id="2-2-问题"><a href="#2-2-问题" class="headerlink" title="2.2 问题"></a>2.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c10d403bbba2c9ee4fcb3f725e56ff22_1440w.png" alt="img"></p><p>2.2 问题</p><h3 id="2-3-解答"><a href="#2-3-解答" class="headerlink" title="2.3 解答"></a>2.3 解答</h3><p>两边同时乘$x^T$</p><p>可以化简为</p><p>因为$Rank(z) \leq 1$,但是z非零;结合$Null(A)&#x3D;Null(z)$</p><p>结合A的PSD特点可以证明，略</p><h3 id="0x03-Eigenvectors，eigenvalues，spectral-theorem：特征向量、特征值、谱定理"><a href="#0x03-Eigenvectors，eigenvalues，spectral-theorem：特征向量、特征值、谱定理" class="headerlink" title="0x03 Eigenvectors，eigenvalues，spectral theorem：特征向量、特征值、谱定理"></a>0x03 Eigenvectors，eigenvalues，spectral theorem：特征向量、特征值、谱定理</h3><h3 id="3-1-定义回顾"><a href="#3-1-定义回顾" class="headerlink" title="3.1 定义回顾"></a>3.1 定义回顾</h3><p>Eigenvectors &amp; Eigenvalues：特征值和特征向量</p><p>求解$p_A(\lambda)&#x3D;det(\lambda I-A)$或者$Ax&#x3D;\lambda x$之间的关系</p><p>Diagonal matrix：对角矩阵，可以用$det(d1,d2,…)$表示</p><p>Orthogonal：正交矩阵</p><p><a href="https://zh.m.wikipedia.org/zh-hans/%E8%B0%B1%E5%AE%9A%E7%90%86">Spectral theorem：谱定理</a></p><h3 id="3-2-问题"><a href="#3-2-问题" class="headerlink" title="3.2 问题"></a>3.2 问题</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6ba21e2f1cb15ab646908acd7271f18c_1440w.png" alt="img"></p><p>image-20230109110250716</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-39f509b8bc55a6b48e3fd0a2c42d7bb5_1440w.png" alt="img"></p><p>image-20230109110258767</p><h3 id="3-3-解读"><a href="#3-3-解读" class="headerlink" title="3.3 解读"></a>3.3 解读</h3><p>两边同乘逆</p><p>又$\Lambda&#x3D;diag(\lambda_1,…,\lambda_n)$</p><p>所以特征值对应的向量为$(\lambda_i,t^{(i)})$</p><p>因为A是对称矩阵，U为正交矩阵，同时</p><p>所以同上  </p><p> 根据谱定理可以得到</p><p>因此大于等于0</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
      <tag>课后作业</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rainbow_Combining_Improvements_in_Deep_Reinforcement_Learning categories</title>
    <link href="/posts/c5b2fc52.html"/>
    <url>/posts/c5b2fc52.html</url>
    
    <content type="html"><![CDATA[<p>作者：Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., … &amp; Silver, D. </p><p>实验室：Google DeepMind</p><p>论文地址：<a href="https://arxiv.org/pdf/1710.02298">https://arxiv.org/pdf/1710.02298</a></p><p>发表： In <em>Thirty-second AAAI conference on artificial intelligence</em>.</p><h3 id="0x01-摘要"><a href="#0x01-摘要" class="headerlink" title="0x01 摘要"></a>0x01 摘要</h3><h3 id="1-1-摘要–背景及问题"><a href="#1-1-摘要–背景及问题" class="headerlink" title="1.1 摘要–背景及问题"></a>1.1 摘要–背景及问题</h3><p>从DQN<a href="https://zhuanlan.zhihu.com/write#ref1">1</a>推导过程中发现依旧存在很多问题，常见的改进措施Double DQN<a href="https://zhuanlan.zhihu.com/write#ref1">2</a>、Dueling DQN<a href="https://zhuanlan.zhihu.com/write#ref1">3</a>、Prioritized replay<a href="https://zhuanlan.zhihu.com/write#ref1">4</a>、Multi-step<a href="https://zhuanlan.zhihu.com/write#ref1">5</a>、Distributional RL<a href="https://zhuanlan.zhihu.com/write#ref1">6</a>、Noisy Net<a href="https://zhuanlan.zhihu.com/write#ref1">7</a>等方法，这些方法并不是完全独立，比如在Dueling中其实已经将Double DQN和Prioritized replay结合起来。</p><h3 id="1-2-摘要–方法"><a href="#1-2-摘要–方法" class="headerlink" title="1.2 摘要–方法"></a>1.2 摘要–方法</h3><p>本文希望将上述六种DQN方法结合经验融合在一起，来得到一个更好的网络。</p><h3 id="1-3-摘要–贡献"><a href="#1-3-摘要–贡献" class="headerlink" title="1.3 摘要–贡献"></a>1.3 摘要–贡献</h3><ol><li>成为Atari 2600中SOTA</li><li>我们还提供详细的消融研究的结果，该研究结果显示了每个组件对整体性能的贡献。</li></ol><h3 id="0x02-问题背景"><a href="#0x02-问题背景" class="headerlink" title="0x02 问题背景"></a>0x02 问题背景</h3><h3 id="2-1-RL-problem-记号"><a href="#2-1-RL-problem-记号" class="headerlink" title="2.1 RL problem &amp; 记号"></a>2.1 RL problem &amp; 记号</h3><p>强化学习希望一个具有行为（action)的智能体(agent) 在与环境(environment)交互的过程可以最大化奖励(reward),在这个过程中并不会直接监督式的学习。这里分享另外一种定义:</p><blockquote><p> 一、Mathematical formalism for learning- based decision making 二、Approach for learning decision making and control form experience</p></blockquote><p><strong>MDP (Markov Decision Process)</strong> 𝑆,𝐴,𝑇,𝑟,𝛾 </p><p>在不同的时间步下$t&#x3D;0,1,2,..$，环境状态$S_t$提供给智能体一个观测信息 $𝑂𝑡$ ,通常我们会认为是完全观测(即 $𝑆𝑡&#x3D;𝑂𝑡$ )，同时智能体根据观测信息做出动作$A_t$, 之后环境给出下一个奖励  ,奖励的折扣  以及更新状态为  </p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-cc2471c3fd3224c57fcb4309ebde94b9_1440w.jpg" alt="img"></p><p>MDP</p><p>在这个过程通常$S,A$是有限的情况,对于环境来说状态转移（stochastic transition function)；奖励方程包括</p><p>对于智能体来说，根据状态$S_t$（或者完全观测下的观测  )得到得到动作  来自于策略  （policy),在序列决策中我们的目标是最大化某个状态采取某个动作的折扣奖励之和</p><p>我们在利用算法进行梯度提升通常会经过三个步骤</p><ol><li>生成样本</li><li>评估模型或者是计算return</li><li>提升策略</li></ol><h3 id="2-2-Policy-Gradient：直接提升policy"><a href="#2-2-Policy-Gradient：直接提升policy" class="headerlink" title="2.2 Policy Gradient：直接提升policy"></a>2.2 Policy Gradient：直接提升policy</h3><p>为了最大化策略的回报，我们可以直接对  最大化（REINFRORCEMENT），推导过程略</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a78b13a4cdc16a979f34e2684e017049_1440w.png" alt="img"></p><p>REINFORCE algorithm </p><p>我们可以利用baseline、n-steps、discount、import sampling的方法对他进行改进。</p><h3 id="2-3-Actor-Crtic方法：Return与Policy分开"><a href="#2-3-Actor-Crtic方法：Return与Policy分开" class="headerlink" title="2.3 Actor-Crtic方法：Return与Policy分开"></a>2.3 <strong>Actor-Crtic方法</strong>：Return与Policy分开</h3><p>也可以引入新的状态价值函数  来结合拟合的方式计算$G_t$之后最大化(A3C),也可以直接利用  和动作状态价值函数  来进行基于价值函数的学习方法。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-02f72a087a1cf9714bbbf71f18e17aca_1440w.png" alt="img"></p><p>batch actor-crtic algorithm</p><p>我们可以利用replay buffer、神经网络来学习降低policy gradient中的方差。</p><h3 id="2-4-Value-based-method-抛弃policy"><a href="#2-4-Value-based-method-抛弃policy" class="headerlink" title="2.4 Value- based method  抛弃policy"></a>2.4 Value- based method  抛弃policy</h3><p>Policy iteration-&gt;Value iteration-&gt;Q learning</p><p>首先从policy iteration与value iteration说起，<a href="https://hrl.boyuai.com/chapter/1/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95">参考链接</a>可以看作是利用动态规划的方式反应强化学习的过程，两者的区别在于反应的是贝尔曼期望还是贝尔曼最优。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3ac2312ee68584e699cf5ddb4029dd76_1440w.png" alt="img"></p><p>贝尔曼期望方程</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-958d0797498237377ada28c5f040fe89_1440w.png" alt="img"></p><p>贝尔曼最优方程</p><p>在基于价值学习算法的过程中，优点是我们只需要一个经验回放池，只需要$(s,a,s’,r)$而不是需要完整的决策序列。我们通常会引入随机探索（exploration）的概念，常见的包括$\epsilon-Greedy$的方法，在一定概率下选择非策略产生的动作。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4e0ed0915b45a8e2808e03a92259174a_1440w.png" alt="img"></p><p>epsilon-greedy</p><p>在 value iteration的基础上，我们可以抛弃对$V(s)$的学习，而只是记录$Q(s,a)$;Q- iteration algorithm(或者$Q- learning$）看过程如下图所示</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-33626839aa82073a3251a3742bbb2bed_1440w.png" alt="img"></p><p>image-20221106114721046</p><h3 id="2-2-DQN推导"><a href="#2-2-DQN推导" class="headerlink" title="2.2 DQN推导"></a>2.2 DQN推导</h3><p>在上述我们认识对于MDP目标，从显式表达Policy，到结合Value function再到之后的完全使用Value function来使用Q- learning的方法，我们没有解决的问题包括</p><ol><li>状态-动作空间的连续性</li><li>在状态空间和动作空间纬度大的时候无法准确刻画 状态-动作价值</li></ol><p>随着神经网络的发展，我们希望利用网络拟合的方式来解决上述问题，同时每一步利用  的方式来探索回放池中的经验，并利用梯度下降等方法最小化价值函数表达的回报</p><ul><li>1 初始化大小为$N$的经验回放池(PS：注意有大小限制)</li><li>2 用相同随机的网络参数初始化  与目标网络  </li><li>3 for 回合episode&#x3D;1，N do：</li><li>4 获取环境初始状态  </li><li>5 for 时间步numstep&#x3D;1，T do：</li><li>6 根据当前网络  结合  方法来得到  （PS：注意这一步动作的确定隐含之后DQN回报偏大的特点）</li><li>7 执行动作  ,获取  ,环境状态变为  </li><li>8 存储上述采样信息到经验回放池</li><li>9 if 经验回放池数目足够：</li><li>10 采样batchsize样本  </li><li>11 计算目标值  </li><li>12 最小化损失函数  </li><li>13 更新网络参数</li><li>14 END FOR</li><li>15 更新目标网络参数</li><li>16 END FOR</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-faee16d293d12c702263d6ecae03b3f1_1440w.png" alt="img"></p><p>Deep Q- learning with Experience Replay</p><p>其中非常有趣的技巧包括：</p><ol><li>经验回放(Experience Replay )与随机探索;这一部分主要是为了提高样本采样效率，同时降低后续梯度下降中样本的相关性。</li><li>目标网络(Target Network)，由于TD误差在策略改变过程中也会改变，因此造成神经网络拟合过程的不稳定性，因此构建新的目标网络，在每次迭代过程中暂时固定，在回合结合后更新参数，这样需要两层Q网络</li></ol><p><a href="https://paperswithcode.com/paper/playing-atari-with-deep-reinforcement">相关代码实现</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-24e9e9b1c1c452283697b2853225d8b4_1440w.png" alt="img"></p><p>A more general view of DQN</p><h3 id="0x03-DQN改进"><a href="#0x03-DQN改进" class="headerlink" title="0x03 DQN改进"></a>0x03 DQN改进</h3><p>虽然DQN成功让强化学习在某些方面超过人类，但是依旧有这许多限制。</p><h3 id="3-1-改进1–Double-Q-Learning"><a href="#3-1-改进1–Double-Q-Learning" class="headerlink" title="3.1 改进1–Double Q- Learning"></a>3.1 改进1–Double Q- Learning</h3><p>在DQN中会有估计值过高的情况，证明如下：</p><p>根据期望公式</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-babe227bf657508c9892776ae274f451_1440w.png" alt="img"></p><p>image-20221106161746893</p><p>我们通过证明发现估计值较大的原因是因为我在模型选择行为和计算Q值使用同一个网络，如果降低行为选择和Q值计算的相关性就可以降低高估，因此直觉的我们可以设计两个网络</p><p>我们的确可以新加一个网络，但是会增加学习难度，需要重新设计架构。所以为什么不直接使用$Q_{\theta}(s,a)$作为行为的估计？</p><h3 id="3-2-改进2：Prioritized-replay"><a href="#3-2-改进2：Prioritized-replay" class="headerlink" title="3.2 改进2：Prioritized replay"></a>3.2 改进2：Prioritized replay</h3><p>在DQN学习中为高效利用（s，a，r，s）样本，我们会使用经验回放的方式来存储一定规模的样本，在梯度下降的时候通常是从经验回放中均匀采样（uniformly sampling）来进行学习，但是我们依旧会存在两个问题：</p><ol><li>依旧没有完全解决数据之间独立同分布的假设</li><li>容易忘记一些罕见的、重要的经验数据</li></ol><p>在该论文中作者首先制定指标“TD-error”作为衡量$(s_t^i,a_t^i,r_t^i,s^i_{t+1})$的信息量大小，作为采样的优先级，同时利用随机优先级采样、偏置和重要性采样等方式来避免贪心的问题。优先级的获取有3.2.1和3.2.2两种方式</p><h3 id="3-2-1-比例优先级（Proportional-prioritization）"><a href="#3-2-1-比例优先级（Proportional-prioritization）" class="headerlink" title="3.2.1 比例优先级（Proportional prioritization）"></a>3.2.1 比例优先级（Proportional prioritization）</h3><h3 id="3-2-2-基于排名的优先级-Rank-based-prioritization"><a href="#3-2-2-基于排名的优先级-Rank-based-prioritization" class="headerlink" title="3.2.2 基于排名的优先级(Rank-based prioritization)"></a>3.2.2 基于排名的优先级(Rank-based prioritization)</h3><p>；优点可以保证线性性质，对异常值不敏感。</p><p>上述两种是不同得到重要性的方式；在实现时候采用sum-tree的数据结构降低算法复杂程度。在采样中考虑重要性采样（importance sampling），并由此来进行热偏置（Annealing the bias）来修正误差</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-db7adb0c329e9262af6a85f74bcc1ecf_1440w.png" alt="img"></p><p>Double DQN with proportional prioritization </p><h3 id="3-3-改进3-Dueling-networks"><a href="#3-3-改进3-Dueling-networks" class="headerlink" title="3.3 改进3: Dueling networks"></a>3.3 改进3: Dueling networks</h3><p>Dueling DQN是一种针对基于价值函数的强化学习的网络结构设计，其并不直接输出$Q(s,a)$，而是输出$V(s)$与$A(s,a)$,通常会共用前几层的卷积参数，在后面则是状态价值函数与优势函数各自的参数。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b3f3418fb6be4620a05c0cd6b3a4711a_1440w.png" alt="img"></p><p>image-20221106165135693</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-cab3f6edb16e72bafac1d719d04c58cd_1440w.png" alt="img"></p><p>Dueling DQN</p><h3 id="3-4-改进4-Multi-step-learning"><a href="#3-4-改进4-Multi-step-learning" class="headerlink" title="3.4 改进4:Multi-step learning"></a>3.4 改进4:Multi-step learning</h3><p>在对状态动作函数的优势估计时候，通常我们会分为蒙特卡洛方法与Bootstrap(或者是Actor- critic内的C)的方法</p><p>前者方法偏差低但是方差较大；后者方差低但是有偏。因此结合两者我们通常会有Multi-step target的方法。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-384e86c45e4e05170247ed8de5768876_1440w.png" alt="img"></p><p>N-step更进一步，广义优势估计</p><p>同样的也可以用在DQN中对于状态动作价值函数的估计：</p><p>更新之后的损失函数为</p><h3 id="3-5-改进5-Distributional-RL"><a href="#3-5-改进5-Distributional-RL" class="headerlink" title="3.5 改进5:Distributional RL"></a>3.5 改进5:Distributional RL</h3><p>在基于价值函数的学习中我们通常是返回一个期望或者最大值而丢失很多其他信息，因此Distributional RL尝试利用其分布而不是单个值来进行强化学习。首先本文尝试将价值函数范围$[V_{min},V_{max}]$划分为N个各自来估计价值函数，利用Boltzmann分布表示价值函数的分布，同时利用投影的操作</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b31a71e49725736c62b5ccc61443ae65_1440w.png" alt="img"></p><p>估计Z(s)</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-86cf9dbbfbdd340d28d05c8fcd11d722_1440w.png" alt="img"></p><p>不同option</p><p>由此对于分布拟合可以划分为交叉熵的形式，算法流程</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5eb13a004a7b0c1e8be3c99dc490c454_1440w.png" alt="img"></p><p>Distribution RL</p><h3 id="3-6-改进6：Noisy-Nets"><a href="#3-6-改进6：Noisy-Nets" class="headerlink" title="3.6 改进6：Noisy Nets"></a>3.6 改进6：Noisy Nets</h3><p>在Q- learning或者是DQN中，我们的轨迹并不是完全采样的，而是与我们的探索策略相关，最原本的是$\epsilon-Greedy$策略，这里提出一种NoisyNet来对参数增加噪声来增加模型的探索能力</p><p>噪声的生成可以分为Independent Gaussian noise；Factorised Gaussian noise两种方式。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-21600e262570a348eeeb34b769eae3c3_1440w.png" alt="img"></p><p>Nosy net的效果</p><h3 id="3-7-融合上述策略"><a href="#3-7-融合上述策略" class="headerlink" title="3.7 融合上述策略"></a>3.7 融合上述策略</h3><p>首先将（改进5:Distributional RL）中的损失函数更换称为（改进4:Multi-step learning），并利用（改进1–Double Q- Learning）计算新的目标值</p><p>损失函数为</p><p>同时在采样过程中我们通常会减少TD-error，而在本文中我们的损失函数为KL损失，因此我们的（改进2：Prioritized replay）中的优先级定义为</p><p>同时改变（改进3: Dueling networks）由接受期望转向接受价值函数分布，最后更改所有的线性层更换为（改进6：Noisy Nets）</p><h3 id="0x04-实验"><a href="#0x04-实验" class="headerlink" title="0x04 实验"></a>0x04 实验</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7f47620ec1f75b3c027213f2466e4c0c_1440w.png" alt="img"></p><p>Median human- normalized performance</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0051ac2a60cefae48683c4cccff9c982_1440w.png" alt="img"></p><p>结果</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-52e28513cf5535cd82701933f348e200_1440w.png" alt="img"></p><p>image-20221106175357633</p><h3 id="0x05-总结"><a href="#0x05-总结" class="headerlink" title="0x05 总结"></a>0x05 总结</h3><h3 id="5-1-结论"><a href="#5-1-结论" class="headerlink" title="5.1 结论"></a>5.1 结论</h3><ol><li>rainbow想比较其他现有的算法要更好，速度也会更快</li><li>在消融实现中；我们会发现（改进2：Prioritized replay）与（改进4:Multi-step learning）会造成结果中位数大幅度下降;(改进5:Distributional RL)在最开始表现良好，但是最终结果表现较差；同时（改进6：Noisy Nets）通常会有更好的中位数表现，同时由于本次状态中通常是underestimate的，所以（改进1–Double Q- Learning）效果并不显著，（改进3: Dueling networks）提升幅度不大。</li></ol><h3 id="5-2-讨论"><a href="#5-2-讨论" class="headerlink" title="5.2 讨论"></a>5.2 讨论</h3><p>作者在最后总结他们的工作，主要是从value- based的Q-learning方法集合中寻找，而没有考虑purely policy- based的算法（比如TRPO),本文从网络探索、网络初始化、数据使用、损失或函数等方面进行集合，与之相对应的同样有很多工作，未来还可以用很多其他的方法。但是我们相信</p><blockquote><p> In general, we believe that exposing the real game to the agent is a promising direction for future research.</p></blockquote><h3 id="5-3-个人感悟"><a href="#5-3-个人感悟" class="headerlink" title="5.3 个人感悟"></a>5.3 个人感悟</h3><p>这篇论文看上去很水，但其实作者做了很多dirty work并最终有效，工作量非常大！</p><p>【1】<a href="https://arxiv.org/abs/1312.5602">Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstr<em>a, D., &amp; Riedmiller, M. (2</em>013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.</a></p><p>【2】<a href="http://arxiv.org/abs/1509.06461">Van Hasselt, H., Guez, A., &amp;am<em>p; Silver, D. (2016, March). Deep reinforcement learning with</em> double q-learning. In Proceedings of the AAAI conference on artificial intelligence (Vol. 30, No. 1).</a></p><p>【3】  <a href="https://arxiv.org/abs/1511.05952">Schaul, *T., Quan, J., Antonoglou, I., &amp;*amp; Silver, D. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952.</a></p><p>【4】<a href="https://www.cs.mcgill.ca/~dprecup/courses/RL/Lectures/8-multistep-2019.pdf">Multi Step Learning</a></p><p>【5】<a href="https://arxiv.org/abs/1511.06581">Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., &amp; Freita<em>s, N. (2016, June). Dueling network architec</em>tures for deep reinforcement learning. In International conference on machine learning (pp. 1995-2003). PMLR.</a></p><p>【6】<a href="https://arxiv.org/pdf/1707.06887.pdf">Bellemare, M. G., Dabney, <em>W., &amp; Munos, R. (2017, July). A distrib</em>utional perspective on reinforcement learning. In International Conference on Machine Learning (pp. 449-458). PMLR.</a></p><p>【7】<a href="https://arxiv.org/abs/1706.10295">Fortunato, M., Azar, M. G., Piot, B., Menick, J*., Osband, I., Graves, A., …* &amp; Legg, S. (2017). Noisy networks for exploration. arXiv preprint arXiv:1706.10295.</a></p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>Paper 阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol8 ｜ Q 函数</title>
    <link href="/posts/7e704bf4.html"/>
    <url>/posts/7e704bf4.html</url>
    
    <content type="html"><![CDATA[<p>从Policy gradient，到Actor- critic，我们尝试丢弃policy，从AC到Q- iteration或者Q- learning我们彻底丢弃Policy，但是遇到不能收敛的问题。现在我们尝试改进Q- learning来实现想要的功能</p><h3 id="0x01-Q-learning中的问题"><a href="#0x01-Q-learning中的问题" class="headerlink" title="0x01 Q- learning中的问题"></a>0x01 Q- learning中的问题</h3><p>上次从理论的角度看出online Q iteration很难收敛，更直观的看待问题我们会发现有以下几点的原因：</p><ol><li>Q- learning中通常使用的是单个样本，这样对于更新迭代并不友好</li><li>在第三步中的有一个类似gradient descent的公式但是其实在同一个轨迹中两者之间具有强烈相关性</li><li>不满足iid假设样本中的效果并不好</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c1d91e32f5ff28d1f146cca885d9baf8_1440w.png" alt="img"></p><p>image-20221102223452106</p><p>解决思路</p><ol><li>修正样本，同样借鉴AC中的方法可以使用synchronized parallel Q- learning或者asynchronous parallel Q- learning来实现。</li><li>使用replay buffers，因为Q- learning只需要（state，action，next- state，reward）就行</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ac9ee308c712738a359daef7914d72f8_1440w.png" alt="img"></p><p>image-20221102223917061</p><h3 id="0x02-Q-learning-with-replay-buffer的问题"><a href="#0x02-Q-learning-with-replay-buffer的问题" class="headerlink" title="0x02 Q-learning with replay- buffer的问题"></a>0x02 Q-learning with replay- buffer的问题</h3><p>按照厂里后面的一部份应该是没有梯度的，但是因为两者具有相关性我们的效果会很差，起不到梯度下降的效果。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4cd2d97b204e83c6a5cc26e1fcf41c12_1440w.png" alt="img"></p><p>image-20221102224124136</p><p>因此我们会设置一个target network，在每次更新的时候，target也不会改变，同时这个过程中两者是某种意义上的不想关的。背后的方式是启发式的，但是实际上是一种经验很好的实践方法。通常更新的时候可以用指数平均的方法</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-858ae38d9cd26e82f0db62d602dcb3f0_1440w.png" alt="img"></p><p>image-20221102225023533</p><p>更高的视角</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-fbd12e57b426cf7f20ea8cafac4b6256_1440w.png" alt="img"></p><p>image-20221102225216215</p><h3 id="0x03-提高Q-learning"><a href="#0x03-提高Q-learning" class="headerlink" title="0x03 提高Q-learning"></a>0x03 提高Q-learning</h3><p>后面结合一篇论文来介绍，也就是Rainbow～</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-85311453c1891e8367356451b6b0741b_1440w.png" alt="img"></p><p>image-20221102225350693</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS285</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>动作状态价值</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol7 ｜ 值函数</title>
    <link href="/posts/f2288390.html"/>
    <url>/posts/f2288390.html</url>
    
    <content type="html"><![CDATA[<p>在经过Lecture04介绍RL的基本概念之后，Lecture05介绍基于policy的方法，我们直接利用return的梯度进行策略学习，之后我们尝试利用Q或者V来改进policy evaluation步骤，那么我们是否可以直接抛弃参数化的梯度（parameterized policies），转向仅仅利用Q或者V进行RL</p><h3 id="0x01-抛弃Actor-critic中的Actor"><a href="#0x01-抛弃Actor-critic中的Actor" class="headerlink" title="0x01 抛弃Actor- critic中的Actor"></a>0x01 抛弃Actor- critic中的Actor</h3><h3 id="1-1-回顾Actor-critic"><a href="#1-1-回顾Actor-critic" class="headerlink" title="1.1 回顾Actor- critic"></a>1.1 回顾Actor- critic</h3><p>第一步：我们利用现有的policy得到多个采样轨迹$\tau_i$  第二步：利用数据拟合$\hat V_\phi^\pi(s)$（常见用蒙特卡洛方法、bootstrapped、n-step等方法） 第三步：评估优势策略A 第四步：根据优势策略得到Actor的梯度 第五步：更新Actor的梯度</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3033b37cb9c3a8a28f6091af830e22e9_1440w.png" alt="img"></p><p>image-20221102214616955</p><p>但是这样存在的潜在问题是方差太大，因为我们拟合出来的state- value- function方差太大，通常我们会做经验重放等方法，但是并不能完全解决这个问题。</p><p>所以为什么我们不直接丢到有参的policy，转向生成一个$\pi*$ 𝜋∗&#x3D;𝑎𝑟𝑔𝑚𝑎𝑥𝑎𝑡𝐴𝜋(𝑠𝑡,𝑎𝑡) </p><h3 id="1-2-Policy-Evaluation"><a href="#1-2-Policy-Evaluation" class="headerlink" title="1.2 Policy Evaluation"></a>1.2 Policy Evaluation</h3><p>在探讨上述可能性我们需要先了解一下policy evaluation的概念；也就是从Dynamic programming的来看我们会如何求解问题；同时从policy evaluation我们会逐渐走向value evaluation。</p><p>第一步：首先状态转移概率T是均匀的，那么我们可以在Policy evaluation()中得到每个状态下的Q-state-action-list；求平均来得到V-state-list。当然一次计算不够，我们会设置一个epsilon，当V- state变化的最大值都小于这个epsilon的时候停止更新。说明我们得到所有状态下的V- state 第二步：在得到V- state下我们便能判断什么action是好的，那么我们就利用Q- state- action与V- state之间的关系来选择不同state下的最优action，重新分配概率，也就是policy evaluation 第三步：由于policy evaluation会重新改变空间。V- state也会改变，所以需要重新计算Policy evaluation，直到收敛。</p><p>Policy evaluation与Value evaluation之间的区别在于Policy evaluation中根据Q- state- value-list得到V- state-list是求期望，还是求最大值的方式。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-63aeb1bf1dce8628c0391bdaa8659fab_1440w.png" alt="img"></p><p>image-20221102221107185</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-78b90f5955c86ad937e3d2caa2647ccc_1440w.png" alt="img"></p><p>image-20221102221124987</p><p>这里参考，可以很清晰的看出policy evaluation，以及value evaluation对其做了什么简化</p><p><a href="https://hrl.boyuai.com/chapter/1/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95">动手学强化学习- Dynamic Programming</a></p><h3 id="0x02-Fitted-value-iteration-and-Q-iteration"><a href="#0x02-Fitted-value-iteration-and-Q-iteration" class="headerlink" title="0x02 Fitted value  iteration and Q- iteration"></a>0x02 Fitted value  iteration and Q- iteration</h3><h3 id="2-1-我们如何表示state-value-function-V-s"><a href="#2-1-我们如何表示state-value-function-V-s" class="headerlink" title="2.1 我们如何表示state- value- function V(s)"></a>2.1 我们如何表示state- value- function V(s)</h3><p>在数量较少的离散中我们可以使用表格或者什么来记录，但是对于连续空间我们可能无能为力。所以我们可以使用<strong>神经网络来实现两者的近似</strong>，也就是转换为</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-264ab90af8cbfaa5152a1cb8372aa29a_1440w.png" alt="img"></p><p>image-20221102221230276</p><h3 id="2-2-仍然有问题"><a href="#2-2-仍然有问题" class="headerlink" title="2.2 仍然有问题"></a>2.2 仍然有问题</h3><p>因为在离散状态下，我们需要知道状态概率转移方式。我们会假设一个平均概率再重新分配的方式来更新转移概率。但是在连续情况下虽然我们知道拟合拟合出state- value- function，但是我们还不知道状态转移概率，所以我们求解不出state- value- function</p><p>但是为什么我们需要state- function？直接利用state- action- function来取最大值就行</p><p>但是这样我们并不能保证收敛性</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0cf7ec47a81ba7ba8287e03a1735a530_1440w.png" alt="img"></p><p>image-20221102221836862</p><h3 id="0x03-Q-learning-method"><a href="#0x03-Q-learning-method" class="headerlink" title="0x03 Q- learning method"></a>0x03 Q- learning method</h3><h3 id="3-1-Q-iteration-的一些性质"><a href="#3-1-Q-iteration-的一些性质" class="headerlink" title="3.1 Q-iteration 的一些性质"></a>3.1 Q-iteration 的一些性质</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-78738f638c702cb23ee96b6efd10b0ab_1440w.png" alt="img"></p><p>image-20221102222232800</p><ol><li>它是off- policy的</li><li>等价优化bellman error</li></ol><h3 id="3-2-Online-Q-learning"><a href="#3-2-Online-Q-learning" class="headerlink" title="3.2 Online Q- learning"></a>3.2 Online Q- learning</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5c9870b1f223e0735e7df3adb2314ecc_1440w.png" alt="img"></p><p>image-20221102222533482</p><p>通常会需要一些exploration</p><ol><li>$\epsilon-greedy$</li><li>Boltzman exploration</li></ol><h3 id="0x04-Value-Functions-in-Theory"><a href="#0x04-Value-Functions-in-Theory" class="headerlink" title="0x04 Value Functions in Theory"></a>0x04 Value Functions in Theory</h3><p>我们需要尝试解释之前取最大值的方式是否可以收敛（converge）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6270cc134f6c0bac989188dcf7e86787_1440w.png" alt="img"></p><p>image-20221102222722182</p><ol><li>MDP中的bellman一定是收敛的</li><li>在利用近似网络（function approximation）的方式下并不收敛</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-322370ad4b907e3ec3dbb7163ab1b4f8_1440w.png" alt="img"></p><p>image-20221102223053608</p><p>我们得到一个非常sad corollary，但是在下一张的DQN中可以让它变的更好</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS285</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>Q函数</tag>
      
      <tag>价值学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol5 ｜ Policy gradient</title>
    <link href="/posts/89c781fe.html"/>
    <url>/posts/89c781fe.html</url>
    
    <content type="html"><![CDATA[<p>本节课的目标是搞明白Policy gradient一类的REINFORCEMENT方法，并且理解背后的局限性，然后知道为什么textbook很容易讲清楚，但是在实践中不行的原因。之后从Causality和Baseline两种方法来降低On- policy PG的方差。并给出结合IS（important sampling）的Off policy的梯度方法。最后利用代码实现RL里面的hello world—CartPole-v0来实现。</p><h3 id="01-什么是Policy-gradients"><a href="#01-什么是Policy-gradients" class="headerlink" title="01 什么是Policy gradients"></a>01 什么是Policy gradients</h3><h3 id="1-1-PG的梯度函数的下载"><a href="#1-1-PG的梯度函数的下载" class="headerlink" title="1.1 PG的梯度函数的下载"></a>1.1 PG的梯度函数的下载</h3><p>回顾DRL的目标，我们需要的是增加累积奖励的期望：</p><p>（目标函数）（目标函数）𝑚𝑎𝑥 𝐸𝜏−𝑝𝜃(𝜏)[Σ𝑡𝑟(𝑠𝑡,𝑎𝑡)] </p><p>根据$(s_t,a_t)$ 对的马尔可夫性质，(在有限的情况下finite horizon case）下简化成为</p><p>𝑚𝑎𝑥 Σ𝑡&#x3D;1𝑇𝐸(𝑠𝑡,𝑎𝑡)−𝑝𝜃(𝑠𝑡,𝑎𝑡)[𝑟(𝑠𝑡,𝑎𝑡)] </p><p>因此我们可以根据$\pi_\theta$采样N个$\tau$,然后求得期望为</p><p>（采样后的目标函数）（采样后的目标函数）𝐽(𝜃)&#x3D;1𝑁Σ𝑖Σ𝑡𝑟(𝑠𝑖,𝑡,𝑎𝑖,𝑡) </p><p>这个式子的重要意义在于它直接给我们反应当前policy的优劣，也指明了我们需要前进的方向，就是最大化这个期望，虽然有的时候这个式子并不是容易计算。但是请把哪些问题先放放，直观的最大化这个问题，也就是用梯度提升的方法：</p><p>（上式简写）𝐽(𝜃)&#x3D;∫𝑝𝜃(𝜏)𝑟(𝜏)𝑑𝜏（上式简写） </p><p>那么对参数求导的结果就是；同时由于我们无法直接取值函数，我们通常需要转换为期望的方式来利用sampling—expectation的方式来求期望</p><p>（目标函数梯度）（目标函数梯度）∇𝜃𝐽(𝜃)&#x3D;∫∇𝜃𝑝𝜃(𝜏)∗𝑟(𝜏)𝑑𝜏)&#x3D;∫𝑝𝜃(𝜏)∗∇𝜃𝑙𝑜𝑔𝑝𝜃(𝜏)∗𝑟(𝜏)𝑑𝜏&#x3D;𝐸𝜏−𝑝𝜃(𝜏)[∇𝜃𝑙𝑜𝑔𝑝𝜃(𝜏)∗𝑟(𝜏)] </p><p>我们对上面的目标函数更进一步的探讨；首先疑问就是什么是$p_\theta(\tau)$,在MDP的state- observation-action- state转移中我们可以认识到</p><p>𝑝𝜃(𝜏)&#x3D;𝑝(𝑠1)∏𝜋𝜃(𝑎𝑡|𝑠𝑡)𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡) </p><p>所以可以很轻松的替换掉上述</p><p>𝑙𝑜𝑔𝑝𝜃(𝜏)&#x3D;𝑙𝑜𝑔𝑝(𝑠1)+Σ𝑙𝑜𝑔𝜋𝜃(𝑎𝑡|𝑠𝑡)+𝑙𝑜𝑔𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡) </p><p>假设我们求导的化，那么目标函数梯度就会变成</p><p>Stop to think : just like before we <strong>evaluated our reinforcement learning objective</strong> by <strong>generating samples</strong> by actually running our policy in the world to <strong>get an estimate of an exception</strong></p><p>这样情况下我们就可以得到采样的后的梯度计算：</p><p>虽然我们成功得到了，我们如何从采样结果中得到如何计算梯度；但是我们需要对分析它具体的含义。</p><h3 id="1-2-理解PG含义"><a href="#1-2-理解PG含义" class="headerlink" title="1.2 理解PG含义"></a>1.2 理解PG含义</h3><p>我们可以看到损失函数内部主要有两部分组成，第一部分是有梯度的东西，第二部份是奖励的求和；那么对于两项分别分析，可以发现第一部分梯度与简单的MLE类似，也就是直接的梯度下降，但是由于有奖励的加权，导致我们的结果是：</p><p>good stuff is made more likely;bad stuff is made less likely（增加好的可能性，降低坏的可能性），也标记为“trial and error”</p><p>对于中间的一项，  ,假设我们用一个神经网络来进行拟合，假如采用Gaussian policies；也就是我们的模型假设是</p><p>那么我们的损失函数，根据MLE自然而然就可以得到(参考linear regression）</p><p>这样我们就得到我们的REINFORCEMENT algorithm；这里缩写是针对：REward Increments&#x3D;nonnegative Factor<em>Offset Reinforcement</em> Characteristic+Eligibility</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b232b70f6250e776dde17d59773edf3d_1440w.jpg" alt="img"></p><p>image-20221026121451902</p><p>但是PG的问题在于：</p><ol><li>我们是否可以在partial observation，也就是o不等于s情况下使用？</li><li>具有很大的方差，对于不同的reward取值结果大不相同。</li><li>example：假设N&#x3D;3；其中总的奖励分为与（一负，两正），那么更新后的策略会偏向于右边，假如是加上constant（三正），那么更新的策略会比偏向中间。这种不确定性导致最终结果的方差偏大</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-e3299aa006dac93d215a2022656004f7_1440w.jpg" alt="img"></p><p>image-20221026121503952</p><h3 id="0x02-如何降低方差一：Causality"><a href="#0x02-如何降低方差一：Causality" class="headerlink" title="0x02 如何降低方差一：Causality"></a>0x02 如何降低方差一：Causality</h3><p>在未来发生的事情并不会对现在的事情造成影响（policy at time b cannot affect reward at time a when a&lt;b）,因此在t之前的梯度不会对后面造成影响</p><p>所以我们的梯度需要：</p><p>变成；注意后面计算奖励的下标！有效的证明过程可能需要在paper中才能看到</p><p>通常的我们会将后面的记为：reward to go：</p><h3 id="0x03-如何降低发差二：Baselines"><a href="#0x03-如何降低发差二：Baselines" class="headerlink" title="0x03 如何降低发差二：Baselines"></a>0x03 如何降低发差二：Baselines</h3><p>首先直觉的感受不同reward的影响，还是之前的例子，如果reward&#x3D;(-1,2,2)和（1，4，4）或者是（10000，10005，10005）这样计算的结果通常是不一样的。我们需要在其中减去一个baseline，相当于unbiased in expectation，很直观的我们是减去奖励的平均值，注意！！平均值不一定是最好的</p><p>可以简单的证明减去一个baseline不会对梯度造成影响，但是会对梯度变化程度造成影响</p><p>除去平均值，我们会采取什么baseline来计算？我们的目标是降低模型的方差，那么最好的baseline就是让模型方差最低的，根据方差计算</p><p>那么</p><p>直接对方差Var的bias求导</p><p>这样就可以得到最好的b值，但是通常在实践中很难计算，所以还君之好用</p><h3 id="0x04-Off-policy-Policy-gradient"><a href="#0x04-Off-policy-Policy-gradient" class="headerlink" title="0x04 Off-policy Policy gradient"></a>0x04 Off-policy Policy gradient</h3><p>我们在之前的讨论中得到最终的梯度理论公式和采样公式，但是我们仔细思考这个过程，我们需要第i次采样得到完整  之后才能利用梯度提升更新policy，这样对于每次policy只是简单的更新一次梯度；这样的结果就是采样效率非常低。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-cf39e4ecb013d356eaddc033700c4b6f_1440w.jpg" alt="img"></p><p>image-20221026121521658</p><p>可以简单的引入 important sampling；来尝试解决这个问题，其理论过程是这样的</p><p>这样的好处是我们可以使用智能体旧策略  来更新策略梯度的值,得到  ,也就是目标函数会变成</p><p>之间的比值可以展开得到</p><p>因此可以得到 Important sampling下的目标函数的期望</p><p>继续我们之前的causality的说明，所以采样情况下上面展开得到</p><p>通常reward to go里的分布会删掉，因此不影响结果（paper中有）</p><p>同时可以改变策略的比值，这个在之后课程中会讲到,这里采用first- order approximation，</p><p>因此得到Off-Policy Policy gradient with important sampling的目标函数梯度：</p><p>作为对比之前的on- policy gradient是：</p><h3 id="0x05-代码实现"><a href="#0x05-代码实现" class="headerlink" title="0x05 代码实现"></a>0x05 代码实现</h3><p>参考<a href="https://hrl.boyuai.com/chapter/2/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95">《动手学深度学习》</a></p><p>首先几个有趣的发现</p><ol><li>在CartPole-v0成功的不一定能在MountainCar-v0中实现，这个与奖励函数的设置关系很大（小车没有上山之前都是-1，被打击到了）</li><li>不同的超参数对最终结果影响很大</li><li>推荐使用GYM 0.21版本；最新版本总会出问题</li></ol><h3 id="5-1-环境搭建"><a href="#5-1-环境搭建" class="headerlink" title="5.1 环境搭建"></a>5.1 环境搭建</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta">## 环境说明：Cartpole-v0 </span><br>我们采用的测试环境是 <span class="hljs-type">CartPole</span>-v0，其状态空间相对简单，只有 <span class="hljs-number">4</span> 个变量，因此网络结构<br>的设计也相对简单：采用一层 <span class="hljs-number">128</span> 个神经元的全连接并以 <span class="hljs-type">ReLU</span> 作为激活函数。当遇到更复<br>杂的诸如以图像作为输入的环境时，我们可以考虑采用深度卷积神经网络。<br><span class="hljs-meta"># 引入一些包</span><br><span class="hljs-title">from</span> pyvirtualdisplay <span class="hljs-keyword">import</span> Display<br><span class="hljs-title">display</span> = <span class="hljs-type">Display</span>(visible=<span class="hljs-number">0</span>, size=(<span class="hljs-number">1400</span>, <span class="hljs-number">900</span>))<br><span class="hljs-title">display</span>.start()<br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-title">from</span> <span class="hljs-type">IPython</span> <span class="hljs-keyword">import</span> display<br><span class="hljs-keyword">import</span> time<br><span class="hljs-title">from</span> <span class="hljs-type">IPython</span> <span class="hljs-keyword">import</span> display<br><span class="hljs-title">from</span> <span class="hljs-type">PIL</span> <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> gym<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-title">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br></code></pre></td></tr></table></figure><h3 id="5-2-定义Policy-net"><a href="#5-2-定义Policy-net" class="headerlink" title="5.2 定义Policy net"></a>5.2 定义Policy net</h3><p>输入是state，输出是action。随便一个浅层MLP就够了</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">PolicyNet</span>(<span class="hljs-title">torch</span>.<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>,<span class="hljs-title">state_dim</span>,<span class="hljs-title">hidden_dim</span>,<span class="hljs-title">action_dim</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">PolicyNet</span>,<span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.fc1=torch.nn.<span class="hljs-type">Linear</span>(<span class="hljs-title">state_dim</span>,<span class="hljs-title">hidden_dim</span>)</span><br><span class="hljs-class">        self.fc2=torch.nn.<span class="hljs-type">Linear</span>(<span class="hljs-title">hidden_dim</span>,<span class="hljs-title">action_dim</span>)</span><br><span class="hljs-class">        for m in self.modules():</span><br><span class="hljs-class">            if isinstance(<span class="hljs-title">m</span>, <span class="hljs-title">torch</span>.<span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>):</span><br><span class="hljs-class">                torch.nn.init.normal_(<span class="hljs-title">m</span>.<span class="hljs-title">weight</span>, <span class="hljs-title">std</span>=0.01)</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">        x=<span class="hljs-type">F</span>.relu(<span class="hljs-title">self</span>.<span class="hljs-title">fc1</span>(<span class="hljs-title">x</span>))</span><br><span class="hljs-class">        return <span class="hljs-type">F</span>.softmax(<span class="hljs-title">self</span>.<span class="hljs-title">fc2</span>(<span class="hljs-title">x</span>),dim=1)</span><br></code></pre></td></tr></table></figure><h3 id="5-3-定义算法"><a href="#5-3-定义算法" class="headerlink" title="5.3 定义算法"></a>5.3 定义算法</h3><p>三步，首先sample generation、然后policy evaluate，之后是improve policy</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> traitlets import observe<br><br>class REINFORCE:<br>    def __init__(self,state_dim,hidden_dim,action_dim,learning_rate,gamma,device):<br>        self.<span class="hljs-attribute">policy_net</span>=PolicyNet(state_dim,hidden_dim,action_dim).to(device)<br>        self.<span class="hljs-attribute">optimizer</span>=torch.optim.Adam(self.policy_net.parameters(),lr=learning_rate)<br>        self.<span class="hljs-attribute">gamma</span>=gamma<br>        self.<span class="hljs-attribute">device</span>=device<br><br>    def take_action(self,state):<br>        # 根据动作概率分布随机采样<br>        <span class="hljs-attribute">state</span>=torch.tensor([state],<span class="hljs-attribute">dtype</span>=torch.float).to(self.device)<br>        <span class="hljs-attribute">probs</span>=self.policy_net(state)<br>        <span class="hljs-attribute">action_dist</span>=torch.distributions.Categorical(probs)<br>        <span class="hljs-attribute">action</span>=action_dist.sample()<br>        return action.item()<br><br>    def update(self,transition_dict):<br>        <span class="hljs-attribute">reward_list</span>=transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>]<br>        <span class="hljs-attribute">state_list</span>=transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>]<br>        <span class="hljs-attribute">action_list</span>=transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]<br><br>        <span class="hljs-attribute">G</span>=0<br>        self.optimizer.zero_grad()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> reversed(range(len(reward_list))):<br>            # 从最后一步开始<br>            <span class="hljs-attribute">state</span>=torch.tensor([state_list[i]],<span class="hljs-attribute">dtype</span>=torch.float).to(self.device)<br>            <span class="hljs-attribute">reward</span>=reward_list[i]<br>            <span class="hljs-attribute">action</span>=torch.tensor([action_list[i]]).view(-1,1).<span class="hljs-keyword">to</span>(self.device)<br>            <span class="hljs-attribute">log_prob</span>=torch.log(self.policy_net(state).gather(1,action))<br>            <span class="hljs-attribute">G</span>=self.gamma*G+reward<br>            <span class="hljs-attribute">loss</span>=-log_prob*G<br>            loss.backward()<br>        self.optimizer.<span class="hljs-keyword">step</span>()<br></code></pre></td></tr></table></figure><h3 id="5-3-开始实验"><a href="#5-3-开始实验" class="headerlink" title="5.3 开始实验"></a>5.3 开始实验</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">learning_rate</span>=0.001<br><span class="hljs-attribute">num_episodes</span>=1000<br><span class="hljs-attribute">hidden_dim</span>=128<br><span class="hljs-attribute">gamma</span>=0.98<br><span class="hljs-attribute">device</span>=torch.device(&#x27;cuda&#x27; <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><span class="hljs-attribute">env_name</span>=<span class="hljs-string">&#x27;CartPole-v0&#x27;</span><br><span class="hljs-comment"># env_name=&#x27;MountainCar-v0&#x27;</span><br><span class="hljs-comment"># env_name=&#x27;LunarLander-v2&#x27;</span><br><span class="hljs-comment">## 观察环境</span><br><br><span class="hljs-attribute">env</span>=gym.make(env_name)<br>env.seed(0)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state:&#123;&#125;&#x27;</span>.format(env.observation_space.dtype))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;action:&#123;&#125;&#x27;</span>.format(env.action_space))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;state shape:&#123;&#125;~&#123;&#125;&#x27;</span>.format(env.observation_space.low,env.observation_space.high))<br>torch.manual_seed(0)<br><span class="hljs-attribute">state_dim</span>=env.observation_space.shape[0]<br><span class="hljs-attribute">action_dim</span>=env.action_space.n<br><span class="hljs-attribute">agent</span>=REINFORCE(state_dim,hidden_dim,action_dim,learning_rate,gamma,device)<br>return_list = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(10):<br>    with tqdm(<span class="hljs-attribute">total</span>=int(num_episodes / 10), <span class="hljs-attribute">desc</span>=<span class="hljs-string">&#x27;Iteration %d&#x27;</span> % i) as pbar:<br>        <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> range(int(num_episodes / 10)):<br>            episode_return = 0<br>            transition_dict = &#123;<br>                <span class="hljs-string">&#x27;states&#x27;</span>: [],<br>                <span class="hljs-string">&#x27;actions&#x27;</span>: [],<br>                <span class="hljs-string">&#x27;next_states&#x27;</span>: [],<br>                <span class="hljs-string">&#x27;rewards&#x27;</span>: [],<br>                <span class="hljs-string">&#x27;dones&#x27;</span>: []<br>            &#125;<br><br>            state = env.reset()<br>            # 清除当前 Cell 的输出<br>            done = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:<br>                action = agent.take_action(state)<br>                next_state, reward, done, _ = env.<span class="hljs-keyword">step</span>(action)<br>                transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>].append(state)<br>                transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>].append(action)<br>                transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>].append(next_state)<br>                # special <span class="hljs-keyword">for</span> Mountain<br>                # reward=(next_state[0]+1)*<span class="hljs-number">*5</span><br>                # <span class="hljs-keyword">if</span> abs(next_state[1])&gt;0.04:<br>                #     reward+=1<br>                transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>].append(reward)<br>                transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>].append(done)<br>                state = next_state<br>                episode_return += reward<br>            return_list.append(episode_return)<br>            agent.update(transition_dict)<br>            <span class="hljs-keyword">if</span> (i_episode + 1) % 10 == 0:<br>                pbar.set_postfix(&#123;<br>                    <span class="hljs-string">&#x27;episode&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%d&#x27;</span> % (num_episodes / 10 * i + i_episode + 1),<br>                    <span class="hljs-string">&#x27;return&#x27;</span>:<br>                    <span class="hljs-string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-10:])<br>                &#125;)<br>            pbar.update(1)<br></code></pre></td></tr></table></figure><p>假如设置三层网络结果有点奇怪，但是policy是好的</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-33f57c325f5522b1dbbce7d466ad7ec6_1440w.jpg" alt="img"></p><p>两层结果</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-edcc9ffe4ccab0bd2aee53ee3848c595_1440w.jpg" alt="img"></p><p>三层神经网络，但是policy好的</p><h3 id="5-4-观察"><a href="#5-4-观察" class="headerlink" title="5.4 观察"></a>5.4 观察</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs routeros">done = <span class="hljs-literal">False</span><br>state = env.reset()<br><span class="hljs-attribute">flag</span>=1<br>    # 防止刷新过快<br><span class="hljs-keyword">while</span> flag&lt;100:<br>    action = agent.take_action(state)<br>    next_state, reward, done, _ = env.<span class="hljs-keyword">step</span>(action)<br>    <span class="hljs-attribute">state</span>=next_state<br>    # 清除当前 Cell 的输出<br>    display.clear_output(<span class="hljs-attribute">wait</span>=<span class="hljs-literal">True</span>)<br>    # 渲染画面，得到画面的像素数组<br>    rgb_array = env.render(<span class="hljs-attribute">mode</span>=<span class="hljs-string">&#x27;rgb_array&#x27;</span>)<br>    # 使用像素数组生成图片<br>    img = Image.fromarray(rgb_array)<br>    # 当前 Cell 中展示图片<br>    display.display(img)<br>    time.sleep(1/24)<br>    flag+=1<br></code></pre></td></tr></table></figure><p>第一个是2层，第二是3层</p><p>视频可以看</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-efdade153c15120cad04afa9df41f47a.jpg" alt="视频封面"></p><p>上传视频封面</p><p>两层网络情况</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-015579d9f5f28bc794f3c4e0c728dace.jpg" alt="视频封面"></p><p>上传视频封面</p><p>三层网络情况</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>梯度提升</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol6 ｜ Actor Critic</title>
    <link href="/posts/17be7a15.html"/>
    <url>/posts/17be7a15.html</url>
    
    <content type="html"><![CDATA[<p>主要介绍演员评论家过程,从最基础的Policy Gradient中剖析如何拟合Policy Evaluation的部分，并推导得到Actor- Critic算法。可以看出从之前直观的Policy，到利用state- value或者action- state- value进行近似。这个过程中采样结果与理论推导的权衡贯穿理论推导过程。在这个基础上也会引来下一节Lecture07关于value function方法。同时突然有个新的想法，并不是所有的代码都需要手撕，baseline is always there！</p><h3 id="0x01-从策略梯度（Policy-Gradient）到评判家（Critic）"><a href="#0x01-从策略梯度（Policy-Gradient）到评判家（Critic）" class="headerlink" title="0x01 从策略梯度（Policy Gradient）到评判家（Critic）"></a>0x01 从策略梯度（Policy Gradient）到评判家（Critic）</h3><h3 id="1-1-回顾REINFORCE算法"><a href="#1-1-回顾REINFORCE算法" class="headerlink" title="1.1 回顾REINFORCE算法"></a>1.1 回顾REINFORCE算法</h3><p>从强化学习的三步范式出发：</p><p>第一步：通过运行策略$\Pi_\theta(a_t|s_t)$得到多个样本${\tau_i}$</p><p>第二步：根据采样样本（Sample）得到策略的梯度</p><p>∇𝜃𝐽(𝜃)&#x3D;1𝑁Σ𝑖&#x3D;1𝑁Σ𝑡&#x3D;1𝑇[∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑎𝑖,𝑡|𝑠𝑖,𝑡)][Σ𝑡′&#x3D;𝑡𝑇𝑟(𝑠𝑡′𝑖,𝑎𝑡′𝑖)] </p><p>&#x3D;∇𝜃𝐽(𝜃)&#x3D;1𝑁Σ𝑖&#x3D;1𝑁Σ𝑡&#x3D;1𝑇[∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑎𝑖,𝑡|𝑠𝑖,𝑡)][𝑄^𝑖,𝑡] </p><p>第三步：梯度上升</p><h3 id="1-2-回顾梯度公式"><a href="#1-2-回顾梯度公式" class="headerlink" title="1.2 回顾梯度公式"></a>1.2 回顾梯度公式</h3><p>梯度公式中的log项目在REINFORCEMENT中已经详细推导过，相当于是LIkelihood的一种加权；而后面的 $𝑄^𝑖,𝑡$ 代表的是我们在轨迹 $𝜏𝑖$ 中时间t中状态 $𝑠𝑖,𝑡$ 采取 $𝑎𝑖,𝑡$ 所得到的后续state- action期望奖励（excepted reward）；下面是真实的reward to go，注意和采样时候的区别<a href="https://blog.tjdata.site/posts/d30aca79.html">CS285_Lecture06_Actor_Critic_Algorithms</a>梯度公式中的log项目在REINFORCEMENT中已经详细推导过，相当于是LIkelihood的一种加权；而后面的 $𝑄^𝑖,𝑡$ 代表的是我们在轨迹$\tau_i$中时间t中状态 $𝑠𝑖,𝑡$ 采取 $𝑎𝑖,𝑡$ 所得到的后续state- action期望奖励（excepted reward）；下面是真实的reward to go，注意和采样时候的区别</p><p>（理论）（理论𝑠𝑡𝑎𝑡𝑒−𝑎𝑐𝑡𝑖𝑜𝑛−𝑣𝑎𝑙𝑢𝑒）𝑄(𝑠𝑡,𝑎𝑡)&#x3D;Σ𝑡′&#x3D;𝑡𝑇𝐸𝜋𝜃[𝑟(𝑠𝑡′,𝑎𝑡′)|𝑠𝑡,𝑎𝑡] </p><p>不要忘记我们的baseline，它可以起到降低方差的作用。我们可以将baseline加到REINFORCEMENT算法中。同时可以设置成为一个与状态有关的值，详细见上一个Lecture</p><p>（理论）（理论𝑠𝑡𝑎𝑡𝑒−𝑣𝑎𝑙𝑢𝑒）𝑉(𝑠𝑡)&#x3D;𝐸𝑎𝑡−𝜋𝜃(𝑎𝑡|𝑠𝑡)𝑄(𝑠𝑡,𝑎𝑡) </p><p>以上是理论的推导</p><p>我们在实际采样中往往只能从有限的样本中得到，所以对于某个采样轨迹得到的梯度可以计算为：</p><p>注意这里的N代表的是总共有i个轨迹，采样和理论之间的区别，更近一步我们可以将：【某个状态下的动作总奖励】减去【某个状态下的所有动作期望】可以定义为这个【特定动作在该状态下的优势】</p><p>同样的和我们不知道Q和V一样，我们通常也不知道A的实际价值，但是我们使用近似（approximate）的方法来近似它，也就是我们的estimate return。通常如果我们的采样方式只是单纯的蒙特卡洛采样，我们得到的结果往往是：无偏差但是高方差的简单采样估计。</p><h3 id="0x02-梯度评估（Policy-Evaluation）"><a href="#0x02-梯度评估（Policy-Evaluation）" class="headerlink" title="0x02 梯度评估（Policy Evaluation）"></a>0x02 梯度评估（Policy Evaluation）</h3><p>在上述我们可以看到在REINFORCEMENT里面对于policy gradient我们通常是使用【样本中特定的奖励求和】，这样带来的结果是因为我们采样的误差性导致最终结果具有high variance，这种情况下我们可能希望：</p><p>为什么不把这个函数拟合出来？——&gt; value function fitting</p><p>如果我们希望拟合函数，上述的$Q(s,a),V(s),A(a,s)$中会选择哪个函数，可以看到</p><p>但是由于当前的状态我们知道，所以可以写成</p><p>因此我们可以得到在理论情况下原来的policy gradient为</p><p>在采样情况下</p><h3 id="2-1-Policy-evaluation-based-on-Monte-Carlo-sampling"><a href="#2-1-Policy-evaluation-based-on-Monte-Carlo-sampling" class="headerlink" title="2.1 Policy evaluation based on Monte Carlo sampling"></a>2.1 Policy evaluation based on Monte Carlo sampling</h3><p>我们知道</p><p>所以我们的policy评价就是</p><p>如果我们只是从采样的方式来得到我们的估计，那么</p><p>虽然不想理论推导那样的优秀，但是同样是有效的。注意这里的过程是我们从REINFORCEMENT里面单纯的使用采样结果，转向构建一个state- based- function，我们需要做的是根据样本拟合这样的函数。但是注意这里！！我们有了dataset（input，output），或许我们可以使用监督学习（supervised learning）来使用一种神经网络的方法来学习一种函数关系。所以接下来问题是【我们如何从少量的样本中得到足够好的数据】</p><h3 id="0x03-从Evaluation到Actor-Critic"><a href="#0x03-从Evaluation到Actor-Critic" class="headerlink" title="0x03 从Evaluation到Actor- Critic"></a>0x03 从Evaluation到Actor- Critic</h3><h3 id="3-1-批量AC算法（actor-critic-algorithm）"><a href="#3-1-批量AC算法（actor-critic-algorithm）" class="headerlink" title="3.1 批量AC算法（actor- critic algorithm）"></a>3.1 批量AC算法（actor- critic algorithm）</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-209a6e8999170497ed50b7b1c7cd206c_1440w.png" alt="img"></p><p>image-20221031195007227</p><p>如果我们使用这样的算法，我们会发现其实在轨迹之间都可以对state- value进行评估，但是这样求解得到的y是不一样的，在不同轨迹中因为状态变换不一样，如果最后都达到station状态是可以的，但是对于不平稳是不OK的，因为我们的label不准确，所以我们可以增加一个discount factor</p><p>在这种情况下，如果我们采样Monte Carlo采样，计算梯度，哪一个是对的？</p><p>如果只是从option1采样，我们得到的结果是从每一步重新开始做discount，这个是符合直觉的</p><p>如果是从option2采样也不是没有结果的，如果我们针对未来恐惧所以会有很多的discount。</p><p>example：这里Sergey Levine尝试使用dead来解释option1和option2的区别，两者都是对的。</p><p>所以总结我们的算法，我们可以分为batcc AC与online AC两种方法</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-2cda1544d77e3696538b4c8fe0bb07b3_1440w.png" alt="img"></p><p>image-20221031200710794</p><h3 id="0x04-演员评论算法-Actor-Critics-Algorithm细节"><a href="#0x04-演员评论算法-Actor-Critics-Algorithm细节" class="headerlink" title="0x04 演员评论算法 Actor- Critics Algorithm细节"></a>0x04 演员评论算法 Actor- Critics Algorithm细节</h3><h3 id="4-1-拟合函数训练"><a href="#4-1-拟合函数训练" class="headerlink" title="4.1 拟合函数训练"></a>4.1 拟合函数训练</h3><p>我们可以选择用两个network来分别拟合state- value function与policy- state- action网络；但是在比较复杂的输入，比如游戏图片中我们往往需要使用convolution layer共享参数来减少网络的参数</p><h3 id="4-2-在线AC训练"><a href="#4-2-在线AC训练" class="headerlink" title="4.2 在线AC训练"></a>4.2 在线AC训练</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c37d557a80278d775e0c117b99078ee8_1440w.png" alt="img"></p><p>image-20221031201549764</p><p>如果只是这样训练，相当于我们每次更新梯度使用批量batchsize&#x3D;1，所以非常差。我们从采样的角度出发，我们可以使用同步并行(synchronized parallel actor-critic) 或者异步平行（asynchronous parallel actor- critic）来多次采样，但是这样的损耗太大了，我们可以使用replay buffer来存储之前采样的数据。</p><p>但是如果我们只是简单的从replay buffer中得到train-data，但是这样是没有效果的，因为不是同一个轨迹构成的policy gradient是没有效果的，所以这样的算法是不对的。所以我们会尝试将之间的抽样得到的state- value转换成为包括Q(s,a)的值</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7dbe4021a21bc7b9ede2bbb084673a2c_1440w.png" alt="img"></p><p>image-20221031202523007</p><p>所以在这里我们不再使用state- value，而是使用action- state- value</p><h3 id="4-3-Baseline设置"><a href="#4-3-Baseline设置" class="headerlink" title="4.3 Baseline设置"></a>4.3 Baseline设置</h3><p>我们在Policy gradient引入一个常数，得到无偏差、但是高方差的梯度；在AC中我们使用 r+V（t+1）- V（t）来得到评估，这样会有一个低方差但是有偏的方程，所以我们如何在拟合价值函数过程中得到我们的梯度，也就是用Q-V来评估（称为蒙特卡洛方法），但是这样对采样数量要求高。</p><p>我们可以考虑使用某种方式来结合两者，也就是采样N-step returns</p><p>之后包括generalized advantage estimation推导可以发现$\gamma$可以有效的降低方差</p><h3 id="0x05-review"><a href="#0x05-review" class="headerlink" title="0x05 review"></a>0x05 review</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9824bd3e57bf1e7dd26d6f74b886af0a_1440w.png" alt="img"></p><p>image-20221031203538556</p><h3 id="0x06-code"><a href="#0x06-code" class="headerlink" title="0x06 code"></a>0x06 code</h3><p>参考<a href="https://hrl.boyuai.com/chapter/intro/">动手学强化学习</a></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs css">import gym<br>import torch<br>import torch<span class="hljs-selector-class">.nn</span><span class="hljs-selector-class">.functional</span> as F<br>import numpy as np<br>import matplotlib<span class="hljs-selector-class">.pyplot</span> as plt<br>import rl_utils<br>class ActorCritic:<br>    def <span class="hljs-built_in">__init__</span>(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,<br>                 gamma, device):<br>        # 策略网络<br>        self.actor = <span class="hljs-built_in">PolicyNet</span>(state_dim, hidden_dim, action_dim).<span class="hljs-built_in">to</span>(device)<br>        self.critic = <span class="hljs-built_in">ValueNet</span>(state_dim, hidden_dim).<span class="hljs-built_in">to</span>(device)  # 价值网络<br>        # 策略网络优化器<br>        self.actor_optimizer = torch.optim.<span class="hljs-built_in">Adam</span>(self.actor.<span class="hljs-built_in">parameters</span>(),<br>                                                lr=actor_lr)<br>        self.critic_optimizer = torch.optim.<span class="hljs-built_in">Adam</span>(self.critic.<span class="hljs-built_in">parameters</span>(),<br>                                                 lr=critic_lr)  # 价值网络优化器<br>        self.gamma = gamma<br>        self.device = device<br><br>    def <span class="hljs-built_in">take_action</span>(self, state):<br>        state = torch.<span class="hljs-built_in">tensor</span>([state], dtype=torch.float).<span class="hljs-built_in">to</span>(self.device)<br>        probs = self.<span class="hljs-built_in">actor</span>(state)<br>        action_dist = torch.distributions.<span class="hljs-built_in">Categorical</span>(probs)<br>        action = action_dist.<span class="hljs-built_in">sample</span>()<br>        return action.<span class="hljs-built_in">item</span>()<br><br>    def <span class="hljs-built_in">update</span>(self, transition_dict):<br>        states = torch.<span class="hljs-built_in">tensor</span>(transition_dict[<span class="hljs-string">&#x27;states&#x27;</span>],<br>                              dtype=torch.float).<span class="hljs-built_in">to</span>(self.device)<br>        actions = torch.<span class="hljs-built_in">tensor</span>(transition_dict[<span class="hljs-string">&#x27;actions&#x27;</span>]).<span class="hljs-built_in">view</span>(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).<span class="hljs-built_in">to</span>(<br>            self.device)<br>        rewards = torch.<span class="hljs-built_in">tensor</span>(transition_dict[<span class="hljs-string">&#x27;rewards&#x27;</span>],<br>                               dtype=torch.float).<span class="hljs-built_in">view</span>(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).<span class="hljs-built_in">to</span>(self.device)<br>        next_states = torch.<span class="hljs-built_in">tensor</span>(transition_dict[<span class="hljs-string">&#x27;next_states&#x27;</span>],<br>                                   dtype=torch.float).<span class="hljs-built_in">to</span>(self.device)<br>        dones = torch.<span class="hljs-built_in">tensor</span>(transition_dict[<span class="hljs-string">&#x27;dones&#x27;</span>],<br>                             dtype=torch.float).<span class="hljs-built_in">view</span>(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).<span class="hljs-built_in">to</span>(self.device)<br><br>        # 时序差分目标<br>        td_target = rewards + self.gamma * self.<span class="hljs-built_in">critic</span>(next_states) * (<span class="hljs-number">1</span> -<br>                                                                       dones)<br>        td_delta = td_target - self.<span class="hljs-built_in">critic</span>(states)  # 时序差分误差<br>        log_probs = torch.<span class="hljs-built_in">log</span>(self.<span class="hljs-built_in">actor</span>(states).<span class="hljs-built_in">gather</span>(<span class="hljs-number">1</span>, actions))<br>        actor_loss = torch.<span class="hljs-built_in">mean</span>(-log_probs * td_delta.<span class="hljs-built_in">detach</span>())<br>        # 均方误差损失函数<br>        critic_loss = torch.<span class="hljs-built_in">mean</span>(<br>            F.<span class="hljs-built_in">mse_loss</span>(self.<span class="hljs-built_in">critic</span>(states), td_target.<span class="hljs-built_in">detach</span>()))<br>        self.actor_optimizer.<span class="hljs-built_in">zero_grad</span>()<br>        self.critic_optimizer.<span class="hljs-built_in">zero_grad</span>()<br>        actor_loss.<span class="hljs-built_in">backward</span>()  # 计算策略网络的梯度<br>        critic_loss.<span class="hljs-built_in">backward</span>()  # 计算价值网络的梯度<br>        self.actor_optimizer.<span class="hljs-built_in">step</span>()  # 更新策略网络的参数<br>        self.critic_optimizer.<span class="hljs-built_in">step</span>()  # 更新价值网络的参数<br>  actor_lr = <span class="hljs-number">1</span>e-<span class="hljs-number">3</span><br>critic_lr = <span class="hljs-number">1</span>e-<span class="hljs-number">2</span><br>num_episodes = <span class="hljs-number">1000</span><br>hidden_dim = <span class="hljs-number">128</span><br>gamma = <span class="hljs-number">0.98</span><br>device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cuda&quot;</span>) if torch.cuda.<span class="hljs-built_in">is_available</span>() else torch.<span class="hljs-built_in">device</span>(<br>    <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>env_name = <span class="hljs-string">&#x27;CartPole-v0&#x27;</span><br>env_name=<span class="hljs-string">&#x27;MountainCar-v0&#x27;</span><br>env = gym.<span class="hljs-built_in">make</span>(env_name)<br>env.<span class="hljs-built_in">seed</span>(<span class="hljs-number">0</span>)<br>torch.<span class="hljs-built_in">manual_seed</span>(<span class="hljs-number">0</span>)<br>state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br>action_dim = env.action_space.n<br>agent = <span class="hljs-built_in">ActorCritic</span>(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,<br>                    gamma, device)<br><br>return_list = rl_utils.<span class="hljs-built_in">train_on_policy_agent</span>(env, agent, num_episodes)<br>episodes_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(return_list)))<br>plt.<span class="hljs-built_in">plot</span>(episodes_list, return_list)<br>plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.<span class="hljs-built_in">title</span>(<span class="hljs-string">&#x27;Actor-Critic on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br>plt.<span class="hljs-built_in">show</span>()<br><br>mv_return = rl_utils.<span class="hljs-built_in">moving_average</span>(return_list, <span class="hljs-number">9</span>)<br>plt.<span class="hljs-built_in">plot</span>(episodes_list, mv_return)<br>plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">&#x27;Episodes&#x27;</span>)<br>plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">&#x27;Returns&#x27;</span>)<br>plt.<span class="hljs-built_in">title</span>(<span class="hljs-string">&#x27;Actor-Critic on &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(env_name))<br>plt.<span class="hljs-built_in">show</span>()<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b0a10e263038b5313485c3235f2750b9_1440w.png" alt="img"></p><p>image-20221031233251437</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS285</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>Actor critic 算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol4 ｜ 算法分类</title>
    <link href="/posts/409fbee8.html"/>
    <url>/posts/409fbee8.html</url>
    
    <content type="html"><![CDATA[<p>Sergev Levine的课讲强化学习我唯一听得懂不同强化学习究竟有哪些分类，同时是如何进行分类的。本次Lecture的目标是完善定义Definition与记号notation，并认清楚RL objective，并给出具体强化学习算法的总结。</p><h3 id="0x01-Definition-of-a-MDP"><a href="#0x01-Definition-of-a-MDP" class="headerlink" title="0x01 Definition of a MDP"></a>0x01 Definition of a MDP</h3><h3 id="1-1-Recap-RL-with-imitation-learning"><a href="#1-1-Recap-RL-with-imitation-learning" class="headerlink" title="1.1 Recap RL with imitation learning"></a>1.1 Recap RL with imitation learning</h3><p>我们在之前的<a href="https://zhuanlan.zhihu.com/p/576779527/https://blog.tjdata.site/posts/7d84bcc6.html">Lecture02-<em>Supervised_Learning_of_Behavior</em></a>中认识了Notation，以及imitation learning的局限性，其中最重要的是需要一个专家智能体或者人工来提供数据。但是显然这是不可行的。所以首先我们需要认识到我们的任务应该是什么？或许我们不知道任务具体样子，但我们知道它和我们状态转变序列中的每一步的reward有关。</p><p>举个例子，在我们observation&#x3D;看到红灯的情况下，action&#x3D;停车往往代表更高的reward，如果action&#x3D;开车闯过交叉口往往代表负的reward。</p><h3 id="0x02-Definition-of-Reinforcement-learning-problem"><a href="#0x02-Definition-of-Reinforcement-learning-problem" class="headerlink" title="0x02 Definition of Reinforcement learning problem"></a>0x02 Definition of Reinforcement learning problem</h3><p>在之前的<a href="https://blog.tjdata.site/posts/3e3a5c92.html">强化学习-基本概念</a>中已经给出MP、MDP、MRP的定义，这里给出CS285官方对于MDP和POMDP（Partial observation MDP，暂时不讨论）的记号</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8ecbf8f0ad9d5bebcfb620c8f024becf_1440w.jpg" alt="img"></p><p>image-20221024165643496</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-2229125fd791c85648d0f7b1e4d3a7cb_1440w.jpg" alt="img"></p><p>image-20221024165705565</p><p>在强化学习过程中 state-observation- action- state转变中，我们可以将 （）（𝑠1,𝑎1,…,𝑠𝑇,𝑎𝑇） 标记为一个trajectory，不同轨迹的概率可以由下图表示；由于状态s1是随机变量，策略也会输出不同的值，因此这里通常是具有指数的trajectory，因此通常只能用采样的方法来近似的描述总体。</p><p>𝑝𝜃(𝜏&#x3D;𝜏)&#x3D;𝑝(𝑠1)∏𝑖&#x3D;0𝑇𝜋𝜃(𝑎𝑡|𝑠𝑡)𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡) 如果我们知道所有可能的轨迹的概率，那么我们就能求解出我们策略对应的期望;很</p><p>很不幸的是，在  的概率分布中，我们不知道$s_1$的分布，也不知道状态转移  ;但是我们想确定的是  ；所以我们需要一些知识来做演化，比如我们知道$(s_t,a_t)$是具有MP性质的，也就是</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4f96e6878bf9e806d354c51ab2504101_1440w.jpg" alt="img"></p><p>image-20221024165732870</p><p>那么我们的奖励期望就可以等价</p><p>其中的  表示为状态动作的边缘概率分布。</p><p>我们需要做的就是找到它其中的最大值，同样的对于场景中我们会简单的区分为有限场景与无限场景</p><ol><li>infinite horizon case，我们需要最终达到稳定分布（stationary distribution），也就是状态状态不发生改变，  ,这时候定义平均回报公式（无折扣）的目标函数为 1.</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-eba8d554c179cb775de44d59f2b3499d_1440w.jpg" alt="img"></p><p>image-20221024165749396</p><ol><li>finite horizon case ,我们依旧需要 state- action marginal来求解</li></ol><p>Question：为什么我们需要期望？</p><p>课程中给出的解答是在现实世界中的期望不连续的情况下，通过随机性和期望我们可以将其转变称为连续（smooth）的问题</p><p>PS：个人看法除了上述之外，还有一个原因是RL中我们通常希望使用采样Sampling的方法来进行求解，因为我们不可能罗列出所有的可能性，而采样方法得到的往往是期望而不是积分式。因此在RL好的sampling方法也是很重要的。常见的有基于马尔可夫蒙特卡洛MCMC方法，在此基础上蒙地卡罗、重要性采样、基于能量采样、Gibbs采样等等奇奇怪怪的采样方式。</p><h3 id="0x03-Anatomy-of-a-RL-algorithm"><a href="#0x03-Anatomy-of-a-RL-algorithm" class="headerlink" title="0x03 Anatomy of a RL algorithm"></a>0x03 Anatomy of a RL algorithm</h3><p>我们可以回顾一下监督学习的范式，对于dataset，我们会有一个假设hypothesis，之后我们可以在dataset中利用learning model来通过loss来optim假设中的param，然后得到我们的model，并可以evaluate。</p><p>强化学习中的方式包括</p><ol><li>generate sample，与环境交互或者收集样本来得到基础数据</li><li>fit a model或者estimate the return，我们可以直接拟合一个策略（用神经网络）；或者计算轨迹的奖励值（reward？Q？value？）来间接反应好的策略</li><li>improve the policy，无论第二步我们采用的是什么方法，我们的最终目的是需要提升我们的policy</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-e0a0dbba567b571d018db0210a7df159_1440w.jpg" alt="img"></p><p>image-20221024165819656</p><h3 id="0x04-Brief-overview-of-RL-algorithm-types"><a href="#0x04-Brief-overview-of-RL-algorithm-types" class="headerlink" title="0x04 Brief overview of RL algorithm types"></a>0x04 Brief overview of RL algorithm types</h3><p>在step2中，我们天然的可以将RL算法分为两类，一类是是直接拟合policy的，一类是根据reward来间接反映policy的好坏。但是reward并不是容易计算的，所以我们会有reward、Q- function或者value - function</p><h3 id="4-1-关于评价Policy的好坏"><a href="#4-1-关于评价Policy的好坏" class="headerlink" title="4.1 关于评价Policy的好坏"></a>4.1 关于评价Policy的好坏</h3><p>最直接的我们在0x02中已经定义好了，我们来展开计算它：</p><p>如果继续写下去，我们会发现其中的“待定“是可以递归的写下去的，直到轨迹的最后，假设我们定义一种函数，名称为Q- function</p><p>那么我们的期望奖励就成为</p><p>更详细的定义Q- function：</p><p>在MRP问题中我们会有一个状态价值的概念，同样我们这里可以由state-value- function，用来反应所有行为下的期望</p><p>但是这样得到的Q有用吗？</p><ol><li>（Q- learning）这样我们可以通过改变$\pi_\theta$来改变总期望吗？答案是可以的，我们可以选择对应更大Q值的行为，虽然这不是数学上的描述，但是可以给我们直觉上的可能性。</li><li>（Actor- critic）尽可能选择Q&gt;V的行为，因为V反应这个状态下所有action的平均值</li></ol><p>PS：这一段真是太棒了！！！</p><h3 id="4-2-Types-of-RL-algorithm"><a href="#4-2-Types-of-RL-algorithm" class="headerlink" title="4.2 Types of RL algorithm"></a>4.2 Types of RL algorithm</h3><p>根据第二步fit model我们可以认为是model- based RL，通常包括</p><ol><li>planing；trajectory optimization、optimal control、MCTS</li><li>improve a policy，back propagate gradients into the policy with tricks</li><li>something else，dynamic programming，simulated experience for model-free leaner</li><li>后面再说</li></ol><p>如果是evaluate return，我们可以认为是model- free RL，通常包括</p><ol><li>policy gradients，直接计算Objective和gradient</li><li>value- based，通过平局value- function和Q- function来寻找比较好的方法</li><li>actor- critic，通过评估现在的value function和Q- function，结合PG的想法来提升策略</li></ol><h3 id="4-3-why-so-many-algorithms？"><a href="#4-3-why-so-many-algorithms？" class="headerlink" title="4.3 why so many algorithms？"></a>4.3 why so many algorithms？</h3><p>模型本身有efficiency- stability；不同的assumption；以及不同问题的使用场景；正如ML中的variance-bias tradeoff，RL中通常是efficiency- stability trade-off</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0a3e980c1f1e1bef9f0cd2a40c852778_1440w.jpg" alt="img"></p><p>image-20221024165833359</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8575301577bd433235ee89136e81a70f_1440w.jpg" alt="img"></p><p>image-20221024165841658</p><p>同时不同模型也会有不同的assumption，比如是否全观测，是否连续等</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5424b86051eb599c62b0edf6482720b2_1440w.jpg" alt="img"></p><p>image-20221024165849690</p><h3 id="4-4-example-of-algorithm"><a href="#4-4-example-of-algorithm" class="headerlink" title="4.4 example of algorithm"></a>4.4 example of algorithm</h3><p>具体的分类；后面在具体实现他们吧！</p><ol><li>model- based</li><li>Dyna</li><li>Guided policy search</li><li>model - free</li><li>Policy gradient</li><li>REINFORCE（REward Increments&#x3D;nonnegative Factor<em>Offset Reinforcement</em>Characteristic+Eligibility）</li><li>Natural Policy gradient</li><li>Trust region policy optimization</li><li>Value function fitting methods</li><li>Q- learning，DQN</li><li>Temporal difference learning</li><li>Fitted value iteration</li><li>Actor- critic algorithms</li><li>Asynchronous advantage actor- critic</li><li>Soft actor- critic（SAC）</li></ol>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS285</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>导论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习Vol3 ｜ 整体框架</title>
    <link href="/posts/4e932249.html"/>
    <url>/posts/4e932249.html</url>
    
    <content type="html"><![CDATA[<p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d456cec8fea97724563ea98fa952f194_1440w.png" alt="img"></p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS285</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 动手学强化学习 Vol2 ｜ 行为克隆</title>
    <link href="/posts/f91d2a02.html"/>
    <url>/posts/f91d2a02.html</url>
    
    <content type="html"><![CDATA[<p>目标是明白RL中的一些定义（definition）和记号（notation），主要是从MDP的定义，然后模仿学习（Imitation learning）和$\epsilon$-greedy 学习的角度介绍了强化学习中的case</p><h3 id="0x01-Definition-of-Sequential-decision-problem"><a href="#0x01-Definition-of-Sequential-decision-problem" class="headerlink" title="0x01 Definition of Sequential decision problem"></a><strong>0x01 Definition of Sequential decision problem</strong></h3><h3 id="1-1-概率论"><a href="#1-1-概率论" class="headerlink" title="1.1 概率论"></a>1.1 概率论</h3><p>关于概率论的记号，与研一课程《应用统计》不一样。我猜想多是具有另外的规定，这里引用从《Deep learning》（2015，Lan Goodfellow）中的说明：</p><p>随机变量（random variable）是可以随机取不同值的变量，我们通常使用无格式字体（plain typeface）中的小写字母表示随机变量本身，用手写体中的小写字母表示随机变量能够取的值。$x_1,x_2$表示随机变量x可能的取值，对于向量（vector）的变量，我们通常用x，可能的取值为$x$。所以$p($x$&#x3D;x）$可以简写为$p(x)$</p><h3 id="1-2-强化学习"><a href="#1-2-强化学习" class="headerlink" title="1.2 强化学习"></a>1.2 强化学习</h3><p>与强化学习典中典的：环境（Environment）与智能体（Agent）和动作（Action）三者交互类似，CS285课程同样以这样的例子作为开端，并利用概率图的方式来分析这个过程。</p><ul><li>（第一层）在草原这个环境中看到了某个东西，然后我们判别出来它是老虎，这个时候我们通常做出某个动作，但是我们的动作也会影响草原环境，进而影响老虎的动作</li><li>（第二层），假设草原环境作为Environment，某个时间点的状态为State，我们作为Agent通过看认识这个状态得到Observation（老虎的概率为0.99，猫的概率为0.01），之后我们会根据Observation来采取Policy( $\pi_\theta(a_t|o_t)$)（如果是老虎，则逃跑概率&#x3D;0.9，当宠物&#x3D;0.1；如果是猫，则逃跑概率&#x3D;0.1，当宠物&#x3D;0.9）；之后我们的动作action结合现有的state会影响下一步的next-state</li><li>(第三层）$s_t$得到$o_t$，之后根据策略得到$a_t&#x3D;\Pi_\theta(a_t|o_t)$,再影响下一步$s_{t+1}&#x3D;P(s_{t+1}|s_t,a_t)$；state — observation — action — next_state…</li></ul><p>在这个过程中我们会有几个疑问：</p><ol><li>智能体对环境是否是完全观测（fully observe），这直接会影响我们学习的策略，通常分为$\pi_\theta(a_t|s_t)或者\pi_\theta(o_t|s_t)$ ，经常会出错</li><li>由图可以看出状态转移（state transit）是具有马尔可夫性质（Markov property），但是observation不一定、action也不一定、policy也不一定</li><li>我们的objective还不确定</li></ol><p>在不明白目标情况下，不如去看一种简单的方式是如何学习的</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a0d841a06baab792fc426f4fc43be778_1440w.jpg" alt="img"></p><p>image-20221024152834540</p><p>PS：强化学习的记号通常认为是研究运筹或动态规划（Dynamic programming）的Richard Bellman和研究自动控制的Lev Pontryagin不同的发展。</p><h3 id="0x02-Imitation-Learning"><a href="#0x02-Imitation-Learning" class="headerlink" title="0x02 Imitation Learning"></a><strong>0x02 Imitation Learning</strong></h3><p>在Lecture01中虽然对比了强化学习（reinforcement learning）与监督学习（supervised learning）之间的差别，但是我们依旧会发现两者之间相似的地方</p><p><strong>给出一个state或者observation，来输出一个action</strong></p><p>因此我们可以假设存在一个专家智能体，其策略可以看成最优策略，我们可以直接模仿这个专家在环境中交互状态动作（state，action）数据来训练一个policy，而不需要环境提供的reward，这样便是imitation learning研究的问题，不需要reward信号变可以达到一个专家策略，常用的方法分为</p><ol><li>Behaviour Cloning</li><li>Inverse RL</li><li>Generative Adversarial Imitation Learning，GAIL</li></ol><h3 id="2-1-Behaviour-cloning"><a href="#2-1-Behaviour-cloning" class="headerlink" title="2.1 Behaviour cloning"></a>2.1 Behaviour cloning</h3><p>输入$（s_t，a_t）$中的state作为样本输入，action作为标签，目标为</p><p><strong>Does it ok? — No</strong></p><p>在数据量较小的情况下，我们只能得到少量的专家数据作为训练，因此只有在数据集的分布下预测准确。但是因为我们不断经历state- observation- action- state的循环，并不是监督学习的一次终止，因此我们的每次偏差都会累积，产生复合物差（compounding error）的问题，导致差距来越来大而失败。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-38d4d39342882e3ac031339af4c04a6d_1440w.png" alt="img"></p><p>img</p><p><strong>Does it ok？yes，but more data</strong></p><p>但是在具有很多数据的情况下是可以实现的，比如*<strong>*</strong><a href="https://arxiv.org/abs/1604.07316"><strong>End to End Learning for Self-Driving Cars</strong></a><strong>文章中收集了足够多的数据来实现运行的稳定性</strong></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-76fb851f4b6e2f3ab63a30888b9dd6dc_1440w.jpg" alt="img"></p><p>image-20221024152923468</p><p><strong>How we make it more offen？</strong></p><p>我们分析之间之前不同 trajectory产生偏差的原因是由于$p_{data}(o_t)!&#x3D;p_{\pi_{\theta}}(o_t)$,因此我们希望用某种方式来保证，因此可以使用 DAgger（Dataset Aggregation）：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-36f4115a54d2f615fcf2c58774f449ab_1440w.jpg" alt="img"></p><p>image-20221024152938796</p><p>但是第一点：人工标记本身就不可靠，因为我们在相同的场景下也会出现不同的行为，同时我们的动作会和一定的记忆有关；第二点我们在复杂场景中很难准确给出新的state的action，所以本质上我们依旧需要学习机器，我们需要序列学习的概念</p><p>当然我们可以使用CNN+RNN的特点，他们依旧只是复杂的模仿学习，并没有解决分布不匹配的问题。总是我们用DAgger来解决这样的问题，依旧只是在特定场景下具有可能性。</p><h3 id="0x03-A-little-bit-of-theory-for-imitation-learning"><a href="#0x03-A-little-bit-of-theory-for-imitation-learning" class="headerlink" title="0x03 A little bit of theory for imitation learning"></a><strong>0x03 A little bit of theory</strong> for imitation learning</h3><p><strong>可以不看，因为看不懂！</strong></p><p>结合之前基本概念里的MRP（Markov reward process）我们可以理解reward，因此在一段轨迹中我们希望,首先我们定义cost</p><p>同时cost反面等于奖励，也就是</p><p>我们将真实的策略表示为$\pi^*_{\theta}(s)$，那么我们的奖励函数应该是</p><p>$r(s,a)&#x3D;log p(a&#x3D;\pi^*_{\theta}(s))$y</p><p>因此在采样错误率低于一定程度之下，我们可以得到</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7f16838a6bc80679b3114c0d64890f8a_1440w.jpg" alt="img"></p><p>image-20221024152956985</p><p>改进之后我们有需要其他的方式来学习，这种主要是生成模型了～</p><h3 id="3-1-生成对抗模型学习（generative-adverbial-imitation-learning）"><a href="#3-1-生成对抗模型学习（generative-adverbial-imitation-learning）" class="headerlink" title="3.1 生成对抗模型学习（generative adverbial imitation learning）"></a>3.1 生成对抗模型学习（generative adverbial imitation learning）</h3><p>摘自<a href="https://hrl.boyuai.com/chapter/3/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0">动手学强化学习</a></p><p>（generative adversarial imitation learning，GAIL）是 2016 年由斯坦福大学研究团队提出的基于生成式对抗网络的模仿学习，它诠释了生成式对抗网络的本质其实就是模仿学习。GAIL 实质上是模仿了专家策略的占用度量即尽量使得策略在环境中的所有状态动作对的占用度量和专家策略的占用度量一致。为了达成这个目标，策略需要和环境进行交互，收集下一个状态的信息并进一步做出动作。这一点和 BC 不同，BC 完全不需要和环境交互。GAIL 算法中有一个判别器和一个策略，策略就相当于是生成式对抗网络中的<strong>生成器</strong>（generator），给定一个状态，策略会输出这个状态下应该采取的动作，而<strong>判别器</strong>（discriminator）将状态动作对作为输入，输出一个 0 到 1 之间的实数，表示判别器认为该状态动作对是来自智能体策略而非专家的概率。判别器的目标是尽量将专家数据的输出靠近 0，将模仿者策略的输出靠近 1，这样就可以将两组数据分辨开来。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-19ae0ebf6d7173799524279d4076f973_1440w.jpg" alt="img"></p><p>image-20221024153011985</p><h3 id="0x04-总结"><a href="#0x04-总结" class="headerlink" title="0x04 总结"></a><strong>0x04 总结</strong></h3><p>Lecture02主要还是利用imitation learning介绍了</p><ol><li>监督学习用于强化学习的局限性</li><li>强化学习的notation</li></ol><p>下一章是及其Fancy的强化学习方法的总结</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>行为克隆</tag>
      
      <tag>CS285</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习 Vol5｜循环神经网络RNN</title>
    <link href="/posts/fca5cf9.html"/>
    <url>/posts/fca5cf9.html</url>
    
    <content type="html"><![CDATA[<p>承接上一会对Attention中seq2seq的模型的困惑，这里从RNN到介绍到更传统的Seq2Seq模型。同时结合最近接触的花书来利用更有利的工具解释RNN中的梯度。并给出序列学习中对于gradient或者hidden state转变的过程，在针对hidden state中的转变中引申出GRU、LSTM之类的变种，并结合D@L给出代码的实现。</p><h3 id="0x01-序列学习与RNN"><a href="#0x01-序列学习与RNN" class="headerlink" title="0x01 序列学习与RNN"></a>0x01 序列学习与RNN</h3><h3 id="1-1-序列学习Seq2Seq"><a href="#1-1-序列学习Seq2Seq" class="headerlink" title="1.1 序列学习Seq2Seq"></a>1.1 序列学习Seq2Seq</h3><p>输入与输出均为序列数据的模型叫做Seq2Seq，注意这里不在按照传统的监督或者非监督的方式来限定模型的类型。常见序列模型的应用包括：语音识别Speech recognition、音乐生成Music generation、情感分析 Sentiment classification、DNA序列分析DNA sequence analysis、机器翻译Machine translation、视频行为识别Video activity recognition、命名实体识别Name entity recognition。</p><p>在Seq2Seq过程中，我们很难像MLP或者CNN得到一个明确的 feature，label；并且序列数据大多是按照一定的顺序来进行的，比如文本序列、视频信号、网站浏览行为等等，并不满足之前的独立同分布的假设。</p><p>为什么RNN类似的网络在Seq中会取得好的效果？主要是解决20世纪80年代机器学习和统计模型的思想的优点：在模型的不同部分共享参数。<strong>参数共享</strong>使得模型能够拓展到不同形式的样本并进行泛化。specially：假如在seq的每一个时间点都具有一个单独的参数，但是我们不能泛化我们没有见过的类型，也不能在时间上共享不同序列长度和不同位置的统计强度。同时我们也不能理解不同顺序seq的含义（比如In 2009，I went to Nepal和I went to Nepal in 2009之间的区别）</p><p>Shortly：RNN的优势主要是</p><ol><li>跨时间权重共享</li><li>时间平移不对称</li><li>同一架构适应不同长度</li></ol><h3 id="1-2-RNN结构前向传播-Forward-Propagation"><a href="#1-2-RNN结构前向传播-Forward-Propagation" class="headerlink" title="1.2 RNN结构前向传播 Forward Propagation"></a>1.2 RNN结构前向传播 Forward Propagation</h3><p>这里先不管RNN内部的具体结构的原因，只要知道RNN包括时间步的input和output以及hidden state就可以，原因在后面会说明。计算图是形式化一组计算结构的方式，如果设计将输入和参数映射到输出和损失的计算。</p><p>先假设数据集为$X&#x3D;(x^1,x^2,..,x^t,…)\mathbb{R}^{n \times d}$</p><p>通常的训练过程是将$H^t\in \mathbb{R^{1 \times h}}$作为hidden state来进行分析，它是根据下式子进行更新</p><p>𝐻𝑡&#x3D;𝑓(𝐻𝑡−1,𝑥𝑡;𝜃) </p><p>如果采用神经网络，格式通常是</p><p>其中各项的分别为$W_{xh}\in \mathbb{R}^{d\times h},W_{hh}\in\mathbb{R}^{h\times h},b_h\in\mathbb{R}^{1\times h}$</p><p>在得到hidden state我们可以进行下一步状态转换，也可以输出当前的值</p><p>其中各项的形状为$W_{oh}\in\mathbb{R}^{h\times o},b_o\in\mathbb{R}^{1\times o}$</p><p>得到模型的输入和输出之后就可以计算模型的损失</p><p>得到</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c7b0f36d46c644fe0c999b2839ee82eb_1440w.jpg" alt="img"></p><p>image-20221019135246524</p><h3 id="1-3-反向传播的梯度下降-Back-Propagation"><a href="#1-3-反向传播的梯度下降-Back-Propagation" class="headerlink" title="1.3 反向传播的梯度下降 Back Propagation"></a>1.3 反向传播的梯度下降 Back Propagation</h3><p>在之前中我们包括至少五个参数，$[W_{xh},W_{hh},b_h,W_{ho},b_o]$,通过上处更新的式子我们计算不同的梯度,比如针对$W_{hh}$</p><p>注意这里的第一项我们是知道的，第二项我们也是知道的，但是第三项它是会一直Recurrent计算下去的</p><p>这样梯度其实是会随着时间t不断拉长，这里通常称为<strong>长期依赖</strong>的问题这样我们可能出现梯度消失、或者梯度爆炸的情况。常见的操作有三种</p><ol><li>完全计算直到收敛</li><li>采用N步马尔可夫模型，来截断时间步，到达时间步$\tau$</li><li>采用随机截断的思想</li></ol><p>还有针对梯度的一些操作，常见的有<strong>截断梯度</strong>和<strong>引导信息流的正则化</strong>，</p><ul><li>针对梯度爆炸，截断梯度也就是在参数更新之前截断梯度的范数，在时间的过程中被证明是有用的。</li><li>针对梯度消失，引导信息流的正则化，一种是加入门控单元（如LSTM、GRU），也可以解决梯度爆炸的问题；一种是正则化项，后者被证明不有效</li></ul><h3 id="1-4-进一步。高级一点的RNN"><a href="#1-4-进一步。高级一点的RNN" class="headerlink" title="1.4 进一步。高级一点的RNN"></a>1.4 进一步。高级一点的RNN</h3><p>在单向RNN的基础上，可以自然而然的联想到</p><ol><li>双向RNN</li><li>深度RNN</li><li>递归神经网络，一个潜在的idea是研究推论</li><li>增加门控单元的不同变种GRU、LSTM</li><li>基于encoder- decoder的seq2seq框架</li><li>回声状态网络（echo state network），流体状态机（liquid state machines）</li></ol><h3 id="1-5-回过头解释RNN"><a href="#1-5-回过头解释RNN" class="headerlink" title="1.5 回过头解释RNN"></a>1.5 回过头解释RNN</h3><p>首先我们可以回顾我们在时间序列预测中常用的一些方法：自回归模型、隐变量自回归模型、马尔可夫模型、因果关系模型等等，常用的是</p><ul><li></li></ul><p>因此我们也可以希望得到</p><p>也就是假如一个hidden state来作为对之前序列总结，将其送入状态转变中。</p><h3 id="0x02-变种与思考GRU、LSTM"><a href="#0x02-变种与思考GRU、LSTM" class="headerlink" title="0x02 变种与思考GRU、LSTM"></a>0x02 变种与思考GRU、LSTM</h3><p>我们之前分析了长期依赖对于梯度消失或者梯度爆炸的影响，从实际效果来看它的影响是：</p><ol><li>早期的预测值对预测所有未来预测值具有非常重要的意义</li><li>一些词元没有相关的观测值，比如对网页内容进行情感分析可能有一些辅助HTML代码与网页传达的情绪无关</li><li>序列之间的不同部分之间存在逻辑中断</li></ol><p>在过去采取过很多方法来进行改进，目前主流的是使用门控的方式来确定什么时候更新hidden state、什么时候重置reset。</p><p><a href="https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjmi4L93uv6AhVHed4KHVDfBIEQFnoECAwQAQ&url=http://www.bioinf.jku.at/publications/older/2604.pdf&usg=AOvVaw3mXlGPo2DVtYp8Vue3sl4y">LSTM</a></p><h3 id="2-1-Gate"><a href="#2-1-Gate" class="headerlink" title="2.1 Gate"></a>2.1 Gate</h3><p>设计为（0，1）区间内的向量，这样我们可以进行凸组合：reset-gate允许我们控制可能还想记住的过去状态的数量，更新们能讲允许我们控制新状态中有多少是旧状态的副本</p><p>具体的计算过程如图：</p><ul><li></li><li></li></ul><p>注意在求和过程会触发 广播机制,使用sigmoid函数将其转换到0，1区间，在这个基础上更新 候选隐藏状态</p><ul><li></li></ul><p>使用tanh非线性激活函数来保证hidden state中的值是保持在区间-1，1之间的；Hadamar元素表示按照元素进行乘积我们结合更新门来对候选隐藏状态进行进一步的修正，得到最终hidden state的取值</p><ul><li></li></ul><p>由此得到门控循环单元具有两个显著的特征</p><ul><li>reset有助于捕获序列中的短期依赖关系      </li><li>updater有助于捕获序列中的长期依赖关系</li></ul><h3 id="2-2-GRU"><a href="#2-2-GRU" class="headerlink" title="2.2 GRU"></a>2.2 GRU</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-51dd38ee5c89f01a86397bf7ea65ebde_1440w.jpg" alt="img"></p><p>image-20221019152134663</p><h3 id="2-3-LSTM"><a href="#2-3-LSTM" class="headerlink" title="2.3 LSTM"></a>2.3 LSTM</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f6d2611b6db6d67827408baaf003f595_1440w.jpg" alt="img"></p><p>image-20221019152055705</p><h3 id="2-4-DRNN"><a href="#2-4-DRNN" class="headerlink" title="2.4 DRNN"></a>2.4 DRNN</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9ed8793177103d5dcb43981ab5d51aa9_1440w.jpg" alt="img"></p><p>image-20221019152209098</p><h3 id="2-5-BRNN"><a href="#2-5-BRNN" class="headerlink" title="2.5 BRNN"></a>2.5 BRNN</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4240a2e049f7565b7444b061d7750ab3_1440w.jpg" alt="img"></p><p>image-20221019152224395</p><h3 id="0x03-逻辑过程分析"><a href="#0x03-逻辑过程分析" class="headerlink" title="0x03 逻辑过程分析"></a>0x03 逻辑过程分析</h3><h3 id="3-1-序列数据读取（example：文本数据）-token-vocab"><a href="#3-1-序列数据读取（example：文本数据）-token-vocab" class="headerlink" title="3.1 序列数据读取（example：文本数据）-token-vocab"></a>3.1 序列数据读取（example：文本数据）-token-vocab</h3><p>首先文本数据集从文本读取出来是这样子的</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c97369ad8a99f54a2bdf8bc3b7619a8e_1440w.jpg" alt="img"></p><p>文本数据集time machine</p><p>我们总的idea是希望将上述不规则的文本，转换为可以用向量表示的数字，且为序列数据。Obviously，我们可以用一个词典vocab来将这些词语转换称为数字。那么这个词典的顺序怎么确定？通常是根据词的出现频率来实现的。同时对于出现频率过少的词可以用或者其他来表示。加入我们使用每个单词来表示，得到的vocab就是</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-bd9217544fa54e2715a1f9c1290675eb_1440w.jpg" alt="img"></p><p>vocab-word</p><p>除了单词我们也可以使用单个字符char来分割数据集</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5770cec9107de708ca09e52f07ea9e74_1440w.jpg" alt="img"></p><p>vocab-word</p><h3 id="3-2-数据预处理-input：-batchsize，num-steps–dataloader"><a href="#3-2-数据预处理-input：-batchsize，num-steps–dataloader" class="headerlink" title="3.2 数据预处理-input： batchsize，num_steps–dataloader"></a>3.2 数据预处理-input： batchsize，num_steps–dataloader</h3><p>在这个基础上我们就可以生成我们的dataloader，因为seq序列天然就有顺序性，而为了提高泛化性我们希望他们可以学习到顺序性，但是学习的又不太多，因此会有两种采样方式来做：</p><ol><li>随机采样</li><li>顺序分区</li></ol><p>批量的大小由batchsize来确定，同时单次训练的顺序包括num_steps个单词，因此经过这样我们采取送入网络训练的单个batch的形状为</p><p>$batch&#x3D;{X_i,y_i},i&#x3D;{X^1,X^2,..,X^{batchsize}};X_i\in\mathbb{R}^{num_steps,len(vocab)}$</p><p>总体的shape就是$(batchsize,num_steps,len(vocab))$</p><p>这里有个操作是为了后面小批量矩阵运算，会做一个reshape，变成$(num_steps,batchsize,len(vocab))$;可以自行计算梯度变换的方式体会</p><h3 id="3-3-模型主干-model-loss-optim"><a href="#3-3-模型主干-model-loss-optim" class="headerlink" title="3.3 模型主干-model-loss-optim"></a>3.3 模型主干-model-loss-optim</h3><p>初始化参数</p><p>选择适合的loss</p><p>定义RNN model</p><h3 id="3-4-训练与可视化-train-predict-ppl"><a href="#3-4-训练与可视化-train-predict-ppl" class="headerlink" title="3.4 训练与可视化- train- predict-ppl"></a>3.4 训练与可视化- train- predict-ppl</h3><p>train也就是设置learning rate和epoch训练就好了</p><h3 id="0x04-代码实现1，2，3"><a href="#0x04-代码实现1，2，3" class="headerlink" title="0x04 代码实现1，2，3"></a>0x04 代码实现1，2，3</h3><p>这里完全参考李沐大大的代码，然后将其中的d2l的部分完全剔除得到如下的代码</p><h3 id="4-1-数据集读取设置"><a href="#4-1-数据集读取设置" class="headerlink" title="4.1 数据集读取设置"></a>4.1 数据集读取设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-comment">## 李沐中d2l服务器中存储的数据</span><br>DATA_HUB = <span class="hljs-built_in">dict</span>()<br>DATA_URL = <span class="hljs-string">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span><br>dataset=<span class="hljs-string">&#x27;time_machine&#x27;</span><br>dataset_sha256=<span class="hljs-string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span><br>DATA_HUB[dataset]=(DATA_URL+<span class="hljs-string">&#x27;timemachine.txt&#x27;</span>,dataset_sha256)<br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> hashlib<br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download</span>(<span class="hljs-params">name, cache_dir=os.path.join(<span class="hljs-params"><span class="hljs-string">&#x27;..&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span></span>)</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;下载一个DATA_HUB中的文件 返回本地文件名&quot;&quot;&quot;</span><br>    url, sha1_hash = DATA_HUB[name]<br>    os.makedirs(cache_dir, exist_ok=<span class="hljs-literal">True</span>)<br>    fname = os.path.join(cache_dir, url.split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">if</span> os.path.exists(fname):<br>        sha1 = hashlib.sha1()<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fname, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>                data = f.read(<span class="hljs-number">1048576</span>)<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> data:<br>                    <span class="hljs-keyword">break</span><br>                sha1.update(data)<br>        <span class="hljs-keyword">if</span> sha1.hexdigest() == sha1_hash:<br>            <span class="hljs-keyword">return</span> fname  <span class="hljs-comment"># 命中缓存</span><br>    r = requests.get(url, stream=<span class="hljs-literal">True</span>, verify=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fname, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        f.write(r.content)<br>    <span class="hljs-keyword">return</span> fname<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_time_machine</span>():  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(download(<span class="hljs-string">&#x27;time_machine&#x27;</span>), <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    <span class="hljs-keyword">return</span> [re.sub(<span class="hljs-string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, line).strip().lower() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">lines, token=<span class="hljs-string">&#x27;char&#x27;</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将文本行拆分为单词或字符&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> token == <span class="hljs-string">&#x27;word&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [line.split(<span class="hljs-string">&#x27; &#x27;</span>) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;char&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [<span class="hljs-built_in">list</span>(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;错误：未知令牌类型：&#x27;</span>+token)<br><br><span class="hljs-keyword">import</span> collections  <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vocab</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokens=<span class="hljs-literal">None</span>, min_freq=<span class="hljs-number">0</span>, reserved_tokens=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            tokens = []<br>        <span class="hljs-keyword">if</span> reserved_tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            reserved_tokens = []<br>        <span class="hljs-comment"># 按出现频率排序</span><br>        counter = count_corpus(tokens)<br>        <span class="hljs-variable language_">self</span>._token_freqs = <span class="hljs-built_in">sorted</span>(counter.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>],<br>                                   reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-variable language_">self</span>.idx_to_token = [<span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens<br>        <span class="hljs-variable language_">self</span>.token_to_idx = &#123;token: idx<br>                             <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.idx_to_token)&#125;<br>        <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._token_freqs:<br>            <span class="hljs-keyword">if</span> freq &lt; min_freq:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.token_to_idx:<br>                <span class="hljs-variable language_">self</span>.idx_to_token.append(token)<br>                <span class="hljs-variable language_">self</span>.token_to_idx[token] = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token) - <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(tokens, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.token_to_idx.get(tokens, <span class="hljs-variable language_">self</span>.unk)<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.__getitem__(token) <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">to_tokens</span>(<span class="hljs-params">self, indices</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(indices, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.idx_to_token[indices]<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.idx_to_token[index] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> indices]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unk</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">token_freqs</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._token_freqs<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_corpus</span>(<span class="hljs-params">tokens</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 这里的tokens是1D列表或2D列表</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">isinstance</span>(tokens[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>):<br>        <span class="hljs-comment"># 将词元列表展平成一个列表</span><br>        tokens = [token <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">return</span> collections.Counter(tokens)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_corpus_time_machine</span>(<span class="hljs-params">max_tokens=-<span class="hljs-number">1</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span><br>    lines = read_time_machine()<br>    tokens = tokenize(lines, <span class="hljs-string">&#x27;char&#x27;</span>)<br>    vocab = Vocab(tokens)<br>    <span class="hljs-built_in">print</span>(vocab.idx_to_token)<br>    <span class="hljs-comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span><br>    <span class="hljs-comment"># 所以将所有文本行展平到一个列表中</span><br>    corpus = [vocab[token] <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">if</span> max_tokens &gt; <span class="hljs-number">0</span>:<br>        corpus = corpus[:max_tokens]<br>    <span class="hljs-keyword">return</span> corpus, vocab<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq_data_iter_random</span>(<span class="hljs-params">corpus, batch_size, num_steps</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span><br>    corpus = corpus[random.randint(<span class="hljs-number">0</span>, num_steps - <span class="hljs-number">1</span>):]<br>    <span class="hljs-comment"># 减去1，是因为我们需要考虑标签</span><br>    num_subseqs = (<span class="hljs-built_in">len</span>(corpus) - <span class="hljs-number">1</span>) // num_steps<br>    <span class="hljs-comment"># 长度为num_steps的子序列的起始索引</span><br>    initial_indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_subseqs * num_steps, num_steps))<br>    <span class="hljs-comment"># 在随机抽样的迭代过程中，</span><br>    <span class="hljs-comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span><br>    random.shuffle(initial_indices)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">data</span>(<span class="hljs-params">pos</span>):<br>        <span class="hljs-comment"># 返回从pos位置开始的长度为num_steps的序列</span><br>        <span class="hljs-keyword">return</span> corpus[pos: pos + num_steps]<br><br>    num_batches = num_subseqs // batch_size<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, batch_size * num_batches, batch_size):<br>        <span class="hljs-comment"># 在这里，initial_indices包含子序列的随机起始索引</span><br>        initial_indices_per_batch = initial_indices[i: i + batch_size]<br>        X = [data(j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> initial_indices_per_batch]<br>        Y = [data(j + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> initial_indices_per_batch]<br>        <span class="hljs-keyword">yield</span> torch.tensor(X), torch.tensor(Y)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq_data_iter_sequential</span>(<span class="hljs-params">corpus, batch_size, num_steps</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 从随机偏移量开始划分序列</span><br>    offset = random.randint(<span class="hljs-number">0</span>, num_steps)<br>    num_tokens = ((<span class="hljs-built_in">len</span>(corpus) - offset - <span class="hljs-number">1</span>) // batch_size) * batch_size<br>    Xs = torch.tensor(corpus[offset: offset + num_tokens])<br>    Ys = torch.tensor(corpus[offset + <span class="hljs-number">1</span>: offset + <span class="hljs-number">1</span> + num_tokens])<br>    Xs, Ys = Xs.reshape(batch_size, -<span class="hljs-number">1</span>), Ys.reshape(batch_size, -<span class="hljs-number">1</span>)<br>    num_batches = Xs.shape[<span class="hljs-number">1</span>] // num_steps<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_steps * num_batches, num_steps):<br>        X = Xs[:, i: i + num_steps]<br>        Y = Ys[:, i: i + num_steps]<br>        <span class="hljs-keyword">yield</span> X, Y<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SeqDataLoader</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):<br>        <span class="hljs-keyword">if</span> use_random_iter:<br>            <span class="hljs-variable language_">self</span>.data_iter_fn = seq_data_iter_random<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.data_iter_fn = seq_data_iter_sequential<br>        <span class="hljs-variable language_">self</span>.corpus, <span class="hljs-variable language_">self</span>.vocab = load_corpus_time_machine(max_tokens)<br>        <span class="hljs-variable language_">self</span>.batch_size, <span class="hljs-variable language_">self</span>.num_steps = batch_size, num_steps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.data_iter_fn(<span class="hljs-variable language_">self</span>.corpus, <span class="hljs-variable language_">self</span>.batch_size, <span class="hljs-variable language_">self</span>.num_steps)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data_time_machine</span>(<span class="hljs-params">batch_size, num_steps,  <span class="hljs-comment">#@save</span></span><br><span class="hljs-params">                           use_random_iter=<span class="hljs-literal">False</span>, max_tokens=<span class="hljs-number">10000</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span><br>    data_iter = SeqDataLoader(<br>        batch_size, num_steps, use_random_iter, max_tokens)<br>    <span class="hljs-keyword">return</span> data_iter, data_iter.vocab<br></code></pre></td></tr></table></figure><h3 id="4-2-模型搭建"><a href="#4-2-模型搭建" class="headerlink" title="4.2 模型搭建"></a>4.2 模型搭建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## define the RNN layer</span><br>num_hiddens=<span class="hljs-number">256</span><br>rnn_layer=nn.RNN(<span class="hljs-built_in">len</span>(vocab),num_hiddens)<br><br><span class="hljs-comment">## define the hidden state</span><br>state=torch.zeros((<span class="hljs-number">1</span>,batch_size,num_hiddens))<br><br><span class="hljs-comment">## define the RNN model</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNmodel</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;Recurrent nerual netowrk model&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,rnn_layer,vocab_size,**kwargs</span>):<br>        <span class="hljs-built_in">super</span>(RNNmodel,<span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.rnn=rnn_layer<br>        <span class="hljs-variable language_">self</span>.vocab_size=vocab_size<br>        <span class="hljs-variable language_">self</span>.num_hiddens=<span class="hljs-variable language_">self</span>.rnn.hidden_size<br><br>        <span class="hljs-comment"># prepare for the GRU</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.rnn.bidirectional:<br>            <span class="hljs-variable language_">self</span>.num_directions=<span class="hljs-number">1</span><br>            <span class="hljs-variable language_">self</span>.linear=nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens,<span class="hljs-variable language_">self</span>.vocab_size)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.num_directions=<span class="hljs-number">2</span><br>            <span class="hljs-variable language_">self</span>.linear=nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens*<span class="hljs-number">2</span>,<span class="hljs-variable language_">self</span>.vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,inputs,state</span>):<br>        X=F.one_hot(inputs.T.long(),<span class="hljs-variable language_">self</span>.vocab_size)<br>        X=X.to(torch.float32)<br>        Y,state=<span class="hljs-variable language_">self</span>.rnn(X,state)<br><br>        <span class="hljs-comment"># 转换y为num_steps,batch_size,num_hiddens</span><br>        output=<span class="hljs-variable language_">self</span>.linear(Y.reshape((-<span class="hljs-number">1</span>,Y.shape[-<span class="hljs-number">1</span>])))<br>        <span class="hljs-keyword">return</span> output,state<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self,device,batch_size=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.rnn,nn.LSTM):<br>            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-variable language_">self</span>.num_directions*<span class="hljs-variable language_">self</span>.rnn.num_layers,batch_size,<span class="hljs-variable language_">self</span>.num_hiddens,device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># nn.LSTM is hidden state as dict</span><br>            <span class="hljs-keyword">return</span> (torch.zeros((<br>                <span class="hljs-variable language_">self</span>.num_directions*<span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                batch_size,<br>                <span class="hljs-variable language_">self</span>.num_hiddens),device=device),<br>                torch.zeros((<span class="hljs-variable language_">self</span>.num_directions*<span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                batch_size,<span class="hljs-variable language_">self</span>.num_hiddens),<br>                device=device<br>            ))<br></code></pre></td></tr></table></figure><h3 id="4-3-预测与训练"><a href="#4-3-预测与训练" class="headerlink" title="4.3 预测与训练"></a>4.3 预测与训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">try_gpu</span>(<span class="hljs-params">i=<span class="hljs-number">0</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> torch.device(<span class="hljs-string">f&#x27;cuda:<span class="hljs-subst">&#123;i&#125;</span>&#x27;</span>)<br>    <span class="hljs-keyword">return</span> torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br>device=try_gpu()<br>net=RNNmodel(rnn_layer,vocab_size=<span class="hljs-built_in">len</span>(vocab))<br>net=net.to(device)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_ch8</span>(<span class="hljs-params">prefix, num_preds, net, vocab, device</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span><br>    state = net.begin_state(batch_size=<span class="hljs-number">1</span>, device=device)<br>    outputs = [vocab[prefix[<span class="hljs-number">0</span>]]]<br><br>    get_input = <span class="hljs-keyword">lambda</span>: torch.tensor([outputs[-<span class="hljs-number">1</span>]], device=device).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> prefix[<span class="hljs-number">1</span>:]:  <span class="hljs-comment"># 预热期</span><br>        _, state = net(get_input(), state)<br>        outputs.append(vocab[y])<br><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_preds):  <span class="hljs-comment"># 预测num_preds步</span><br>        y, state = net(get_input(), state)<br>        outputs.append(<span class="hljs-built_in">int</span>(y.argmax(dim=<span class="hljs-number">1</span>).reshape(<span class="hljs-number">1</span>)))<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outputs])<br><br>predict_ch8(<span class="hljs-string">&#x27;time traveller&#x27;</span>,<span class="hljs-number">10</span>,net,vocab,device)<br></code></pre></td></tr></table></figure><p>下面选择一个模型就ok</p><ol><li># 实际训练过程    # params setting    from matplotlib.pyplot import isinteractive    # model setting    net&#x3D;RNNmodel(rnn_layer,vocab_size&#x3D;len(vocab))    net&#x3D;net.to(device) </li><li># 第二种GRU    num_inputs&#x3D;len(vocab)    gru_layer&#x3D;nn.GRU(num_inputs,num_hiddens)    net&#x3D;RNNmodel(gru_layer,len(vocab))    net&#x3D;net.to(device) </li><li># 第三种LSTM    num_inputs&#x3D;len(vocab)    num_layers&#x3D;2    lstm_layer&#x3D;nn.LSTM(num_inputs,num_hiddens,num_layers)    net&#x3D;RNNmodel(lstm_layer,len(vocab))    net&#x3D;net.to(device) </li><li># 第四种 Deep-LSTM    num_inputs&#x3D;len(vocab)    num_layers&#x3D;32    lstm_layer&#x3D;nn.LSTM(num_inputs,num_hiddens,num_layers)    net&#x3D;RNNmodel(lstm_layer,len(vocab))    net&#x3D;net.to(device)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs, lr = <span class="hljs-number">5000</span>, <span class="hljs-number">0.5</span><br><span class="hljs-comment"># loss function</span><br>loss=nn.CrossEntropyLoss()<br><br><span class="hljs-comment"># optimer or updater</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd</span>(<span class="hljs-params">params, lr, batch_size</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Minibatch stochastic gradient descent.</span><br><span class="hljs-string">    Defined in :numref:`sec_utils`&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param -= lr * param.grad / batch_size<br>            param.grad.zero_()<br><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net,nn.Module):<br>    <span class="hljs-comment"># what is for??</span><br>    updater=torch.optim.SGD(net.parameters(),lr)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;net is not as nnModule&#x27;</span>)<br>    updater=<span class="hljs-keyword">lambda</span> batch_size:sgd(net.params,lr,batch_size)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_clipping</span>(<span class="hljs-params">net, theta</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        params = [p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> net.parameters() <span class="hljs-keyword">if</span> p.requires_grad]<br>    <span class="hljs-keyword">else</span>:<br>        params = net.params<br>    norm = torch.sqrt(<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">sum</span>((p.grad ** <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> params))<br>    <span class="hljs-keyword">if</span> norm &gt; theta:<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param.grad[:] *= theta / norm<br><br>predict=<span class="hljs-keyword">lambda</span> prefix:predict_ch8(prefix,<span class="hljs-number">50</span>,net,vocab,device)<br><br>use_random_iter=<span class="hljs-literal">False</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epoches):<br>    state=<span class="hljs-literal">None</span><br>    res=[]<br>    <span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> train_iter:<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> use_random_iter:<br>            state=net.begin_state(batch_size=X.shape[<span class="hljs-number">0</span>],device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net,nn.Module) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(state,<span class="hljs-built_in">tuple</span>):<br>                state.detach_()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> state:<br>                    s.detach_()<br><br>        y=Y.T.reshape(-<span class="hljs-number">1</span>)<br>        X,y=X.to(device),y.to(device)<br>        y_hat,state=net(X,state)<br>        l=loss(y_hat,y.long()).mean()<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(updater,torch.optim.Optimizer):<br>            <span class="hljs-comment"># judge is handSGD</span><br>            updater.zero_grad()<br>            l.backward()<br>            grad_clipping(net,<span class="hljs-number">1</span>)<br>            updater.step()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># define in d2l.sgd</span><br>            l.backward()<br>            grad_clipping(net,<span class="hljs-number">1</span>)<br>            updater(batch_size=<span class="hljs-number">1</span>)<br>        res.append(l)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;困惑度&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">str</span>(res[-<span class="hljs-number">1</span>])))<br></code></pre></td></tr></table></figure><h3 id="4-5-结果"><a href="#4-5-结果" class="headerlink" title="4.5 结果"></a>4.5 结果</h3><ol><li>肯定都能训练出来</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4288ec2e45151b3e22c38e92752e827c_1440w.jpg" alt="img"></p><p>GRU结果</p><ol><li>使用word的训练时间会大于char很多</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8b40b7fcd7aeccdc306d70f29567523a_1440w.jpg" alt="img"></p><p>word的训练过程</p><p>给他一个time traveller，可以得到这个：</p><p>time traveller stooping to light a spill at the fire then he turned lighting his pipe to look at the psychologist s face the psychologist to show that he was not unhinged helped himself to a cigar and tried to light it uncut what is more i have a big machine nearly</p><p>-—-</p><p>时间旅行者弯腰在火上点燃一个溢出物，然后他转动点亮他的烟斗，看着心理学家的脸，心理学家表明他不是不合时宜地帮助自己抽雪茄，并试图点燃它未切割，更重要的是我有一个大机器几乎</p><ol><li>深层RNN不一定有效</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d88257394c9eb89b85df093af2b4b065_1440w.jpg" alt="img"></p><p>32层LSTM的结果</p><h3 id="0x05-Discussion（个人见识）"><a href="#0x05-Discussion（个人见识）" class="headerlink" title="0x05 Discussion（个人见识）"></a>0x05 Discussion（个人见识）</h3><ol><li>MLP或者CNN有点像顺序结构，RNN有点像循环结构，Attention有点像判断结构 </li><li>不同结构带来的是不同的感受野，MLP的感受野是个点，CNN的感受野可能是长方形，RNN的感受野可能是很长的序列，Attention则是选择感受野 </li><li>RNN很土</li></ol><p><a href="https://cloud.tencent.com/developer/article/1664708">序列模型——吴恩达深度学习课程笔记（五）</a></p><p><a href="https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert.html">动手学深度学习</a></p><p><a href="https://www.deeplearningbook.org/">Deep learning，lan Goodfellow.etal</a></p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习 Vol4 ｜ Attention机制</title>
    <link href="/posts/acc5a9e3.html"/>
    <url>/posts/acc5a9e3.html</url>
    
    <content type="html"><![CDATA[<p>本来就以Attention为基础的Transformer结构很感兴趣，想直接从CNN跳到Attention来学习，但是通过了解一些基本的背景发现并不现实，transformer的提出本来就是在机器翻译的基础上，所以先天的在一些问题场景的解释中并不是以回归数据、或者说二维图像数据，而是基于序列（seq）类型的数据进行解释的，所以需要在了解认清楚seq2seq学习之后才能对attention有一些理解。这次只是对attention作为简单的了解，并对深度学习中一些做出review，来修正自己之前错误的看法以及重建新的世界观。</p><h3 id="0x01-什么是Attention（文字版）"><a href="#0x01-什么是Attention（文字版）" class="headerlink" title="0x01 什么是Attention（文字版）"></a>0x01 什么是Attention（文字版）</h3><h3 id="1-1-从生物学玄学的说起注意力-attention"><a href="#1-1-从生物学玄学的说起注意力-attention" class="headerlink" title="1.1 从生物学玄学的说起注意力 attention"></a>1.1 从生物学玄学的说起注意力 attention</h3><blockquote><p>  （很有意思的一段） 注意力是稀缺的，而环境中干扰注意力的信息并不少。比如我们的视觉神经系统大约收到10e8位的信息，远远超过了大脑能够完全处理的水平，但是幸运的是我们的祖先已经通过经验认识到“并非感官的所有输入都是一样的”，在整个人类历史中，这种将注意力引向感兴趣的一部分信息的能力使得我们的大脑能够更加明智地分配资源来生存、成长和社交，例如发现天敌、找寻食物或者伴侣</p></blockquote><p>生物学中常见的利用双组件（tow- component）来解释注意力，将注意力划分为两类：</p><ol><li>非自发：单纯的根据观察到的”数据“来确定你所注意的的东西，比如桌子上有试卷、可乐、游戏机、一个钥匙和一个活动的公鸡；你可能最先关注的鸡</li><li>自发性：在自我意识的控制下来选择关注合适的元素，比如上述如果你是为了寻找一个钥匙而来，那么你就会将注意力专注到对应的元素中。</li></ol><h3 id="1-2-害死人的Q（query）、K（key）、V（value）"><a href="#1-2-害死人的Q（query）、K（key）、V（value）" class="headerlink" title="1.2 害死人的Q（query）、K（key）、V（value）"></a>1.2 害死人的Q（query）、K（key）、V（value）</h3><p>这里的起源利用另外一种Notation来表述，定义如下：</p><p>query：自主性提示成为query</p><p>key：自主性提示</p><p>attention pooling：注意力汇聚，将 选择（output）引导感官输入</p><p>感官输入称为value，每一个value都与一个key匹配</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0584266446dd160a340a456669a27f1c_1440w.jpg" alt="img"></p><p>image-20221013203214511</p><h3 id="1-4-利用找书小故事来解释QKV模型（个人猜想！）"><a href="#1-4-利用找书小故事来解释QKV模型（个人猜想！）" class="headerlink" title="1.4 利用找书小故事来解释QKV模型（个人猜想！）"></a>1.4 利用找书小故事来解释QKV模型（个人猜想！）</h3><p>我们将图书馆里面所有的书称为Key，每个书可能会对应不同的value（看或者不看）；当我们有自己的query（想看交通方面的书），就会将query对不同的key进行attention pooling，这样output就是与交通相关的书才会看，与交通不相关的书不看。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0c1f0a4d5d32fd5632e4dbf8b0f7669c_1440w.jpg" alt="img"></p><p>image-20221013203802267</p><h3 id="1-5-从机器学习里的-来看（个人猜想！）"><a href="#1-5-从机器学习里的-来看（个人猜想！）" class="headerlink" title="1.5 从机器学习里的  来看（个人猜想！）"></a>1.5 从机器学习里的  来看（个人猜想！）</h3><p>其中的  相当于Key，  相当于Value</p><p>当我们对一个新的$x$预测的时候相当于是对这个发出query，这个时候我们会怎么样？</p><ol><li>首先根据$x$与$X&#x3D;{x^{(i)}}i&#x3D;1…n$进行attention score的计算，然后再归一化（比如softmax）</li><li>之后再利用归一化的结果作为 权重 来拟合得到output</li></ol><p>这里与线性回归的区别在于权重的确定和使用上，线性回归的权重是严格推导并使用在X的不同特征上，而attention的权重是通过学习并使用在X（key）对应的y（value）</p><p>同时这里的attention value的权重可以是通过学习，也可以是通过人为定义来得到的，因为我们采用的vector形式的学习，因此我们可以使用kernel function或者线性空间一些距离度量来作为计算方式，在后续进化版的升级中，我们可能会会有其他score的度量，比如书与书之间是好度量的，但是说的话和书之间如何度量？</p><h3 id="0x02-可视化Attention"><a href="#0x02-可视化Attention" class="headerlink" title="0x02 可视化Attention"></a>0x02 可视化Attention</h3><p>接着上述的1.5我们可以看出《D2L》书中介绍Nadaraya- Watson核回归，只不过其中的attention score 的计算方式是用<a href="https://en.wikipedia.org/wiki/Kernel_method">核机器</a>；幸运的是我们在<a href="https://blog.tjdata.site/2022/02/05/CS229-02--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/">SVM</a>里面已经初步了解过，它只是一种将低纬度的向量距离投射到高纬度，比如我们可以采用Gaussian Kernel</p><p>这样取评价之后就可以得到我们对于新的query的x的预测为</p><p>这里取的就是按照一种高斯距离，离的越近越好，如果按照这个方式得到的attention score可以看出近大远小的特点。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8e1d36a6678f9a5f916d6c710e0ea958_1440w.jpg" alt="img"></p><p>image-20221013205856315</p><p>但是另外一种我们希望可以对于不同query具有不同的高斯函数的宽度，也就是我们希望方差不一样</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-16bed8e9ae100321f5168baea8068c5d_1440w.jpg" alt="img"></p><p>image-20221013210023487</p><p>这样对于不同的value就会有不同的权重（注意这里的权重值得不是w），指的是利用w计算出来的attention score</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3ba5c2121bd9a203690295db25a4fb66_1440w.jpg" alt="img"></p><p>image-20221013210112922</p><h3 id="0x03-对于attention的初识"><a href="#0x03-对于attention的初识" class="headerlink" title="0x03 对于attention的初识"></a>0x03 对于attention的初识</h3><p>所以attention是啥，如果利用QKV的角度来看比较复杂，如果从简单的回归问题角度来看它只是另外一种计算权重方式的函数。</p><p>但是！attention的作用不仅局限于回归问题，它从seq问题中而来，解决的就是CNN或者RNN对于距离的敏感性。</p><p>就像CNN的感受野，对于单层感受野可能是固定好的（与kernel size、padding、step有关），但是多层累积中感受野是可以逐渐扩大的。但是CNN适用于图像的可能解释是一，图像的维度是固定size的（像素无论多大都是有限的）</p><p>而RNN的感受野，借鉴被人的说法是取决于词元模型和hidden layer，但是由于hidden layer也是具有顺序的特征，它的感受野也不会传递的太远。</p><p>attention利用attention socre对不同的key进行筛选，来得到一个足够远但是量不大的感受野。选择重要的数据。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3272ee7e92d6af904382cd88ac30baa3_1440w.jpg" alt="img"></p><p>image-20221013210849152</p><p>同时在Attention中关于attention score的计算过程中，由于没有接触过seq2seq模型，在后面的计算中可能需要了解之后才能有更近一步的认识。目前的理解是利用一种可学习的方式来得到合理的attention score的计算方式。</p><p>flag：</p><p>看懂RNN、GRU、LSTM</p><p>明白seq2seq框架，encoder- decoder的框架</p><h3 id="0x04-Discussion"><a href="#0x04-Discussion" class="headerlink" title="0x04 Discussion"></a>0x04 Discussion</h3><h3 id="4-1-一致记号的重要性"><a href="#4-1-一致记号的重要性" class="headerlink" title="4.1 一致记号的重要性"></a>4.1 一致记号的重要性</h3><p>还是很喜欢CS229的讲义中Notation一致性的思考，这样前后的公式推导会很清晰，在解释问题中会有一致性</p><p>D2L中沐神讲的真的很好，但是在写作中似乎还是缺少这种基本的素养。这样的素养对于大佬是锦上添花，但是对于初学者是保证不出错的围栏。</p><h3 id="4-2-模拟（print）的重要性"><a href="#4-2-模拟（print）的重要性" class="headerlink" title="4.2 模拟（print）的重要性"></a>4.2 模拟（print）的重要性</h3><p>对于不同的抽象问题，利用具体的东西打印出来一步一步就好了。尤其是向量的每一步的size</p><h3 id="4-3-ML-DL的一些思考"><a href="#4-3-ML-DL的一些思考" class="headerlink" title="4.3 ML&amp;DL的一些思考"></a>4.3 ML&amp;DL的一些思考</h3><p>可能是沿着D2L书的缘故，对深度学习的认识过程是从MLP到CNN&#x2F;GCN到RNN到Transformer，可能还有其他的，这样网络结果一样但又似乎不太一样，主要对特征提取方式上。</p><p>下面这张图可能内容不对或者不全，但是分类的思路从学习的角度还是有益的</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0a258029f4ebf64c83f054fcd4b96d52_1440w.jpg" alt="img"></p><p>image-20221013212732442</p><p>PS:</p><p>不幸运的网站的主题崩掉了</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>李沐</tag>
      
      <tag>attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS285 深度强化学习 Vol1 ｜ 基本概念</title>
    <link href="/posts/b0c6d2c.html"/>
    <url>/posts/b0c6d2c.html</url>
    
    <content type="html"><![CDATA[<p>初始强化学习最难的地方在于晦涩的专业词汇以及复杂的数学推导；同时深度强化学习并不是强化学习的终点，在这个过程中依旧需要human在其中扮演的地方。这里浅薄看<a href="https://datawhalechina.github.io/easy-rl/#/">蘑菇书来了解</a>。主要作为了解强化学习的基本概念，希望从随机过程出发，来给出强化学习中智能体agent与环境 env之间的基本概念</p><h3 id="0x01-随机过程的基础"><a href="#0x01-随机过程的基础" class="headerlink" title="0x01 随机过程的基础"></a>0x01 随机过程的基础</h3><p>概率论和应用统计是研究概率论中多个随机事件所构成的具有一般性规律的学科；而随机过程则是对这个过程进行描述。</p><h3 id="1-1-马尔可夫过程-MP-Markov-process"><a href="#1-1-马尔可夫过程-MP-Markov-process" class="headerlink" title="1.1 马尔可夫过程 MP- Markov process"></a>1.1 马尔可夫过程 MP- Markov process</h3><p>马尔可夫性质指的是一个随机过程在给定现在状态和所有过去状态的情况下，未来分布概率仅依赖于当前的状态；也就是在知道当前的状态，将来与未来是独立的</p><p>𝑃(𝑋𝑇+1&#x3D;𝑥𝑡+1|𝑋0&#x3D;𝑥0,𝑥1&#x3D;𝑥1,…,𝑋𝑇&#x3D;𝑥𝑡)&#x3D;𝑃(𝑋𝑇+1&#x3D;𝑥𝑡+1|𝑋𝑇&#x3D;𝑥𝑡) </p><p>如果一个随机过程是离散的，可以被称为马尔可夫链（Markov chain）</p><p>在这个过程中我们看到不同状态之间的概率转换</p><h3 id="1-2-马尔可夫奖励过程-MRP-Markov-reward-process"><a href="#1-2-马尔可夫奖励过程-MRP-Markov-reward-process" class="headerlink" title="1.2 马尔可夫奖励过程-MRP- Markov reward- process"></a>1.2 马尔可夫奖励过程-MRP- Markov reward- process</h3><p>在上述马尔可夫链的基础上，我们引入一个新的概念：</p><p>奖励- reward function：是一个期望，表示我们到达某个过程能够获得的最大的奖励，通常用horizon表示有个回合的长度，那么在奖励的基础上我们可以得到随机过程序列的回报</p><p>回报- return $G_t&#x3D;r_{t+1}+\gamma<em>r_{t+2}+\gamma^2</em>r_{t+3}+…$</p><p>这里的$\gamma$代表折扣系数；这个位置是表示这个一系列的随机过程所带来的回报，但是通常为收敛会规定 回报- return的长度来；在回报- return的基础上我们可以得到每个状态- state的价值- value；也就是回报- return的期望就是价值</p><p>𝑉(𝑆𝑇)&#x3D;𝐸(𝐺𝑇|𝑆𝑇&#x3D;𝑠𝑡) </p><p>这里有待讨论，感觉和高中物理中的万有引力与势能很相似</p><p>关于价值- value的求解会有很多方法</p><ol><li>利用定义，遍历多种状态得到value</li><li>利用蒙特卡洛MC随机采样的方法</li><li>利用Bellman equation得到解析解</li><li>其他的方法，比如TD learning等</li></ol><h3 id="1-25关于MRP贝尔曼方程求解"><a href="#1-25关于MRP贝尔曼方程求解" class="headerlink" title="1.25关于MRP贝尔曼方程求解"></a>1.25关于MRP贝尔曼方程求解</h3><p>太菜了，所以用手写，这样我们看出MRP重点是在知道转移概率P和各状态奖励reward以及折扣因子$\gamma$的基础上，求解出状态的价值（用不同的方法）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a8e283e4093bbf799726b3d53a8e3986_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-2a7bb6b705e17efd979e214a92d425f7_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ff43b46c8d01f592a86c35f166ac2493_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="1-3-马尔可夫决策过程-MDP-Markov-decision-process"><a href="#1-3-马尔可夫决策过程-MDP-Markov-decision-process" class="headerlink" title="1.3 马尔可夫决策过程-MDP-Markov- decision- process"></a>1.3 马尔可夫决策过程-MDP-Markov- decision- process</h3><p>MRP在MP中增加来 “奖励reward、回报return和价值value”，MDP则是继续在MRP的基础中增加动作项目action</p><p>这里的$\pi$称为是策略，在这个基础上，下一个状态应该取决于当前的状态和动作</p><p>同样 奖励reward 的定义也会更加复杂</p><p>2022-09-30 这部分后面就不太懂啦～～未完待续</p><h3 id="1-4-举个例子"><a href="#1-4-举个例子" class="headerlink" title="1.4 举个例子"></a>1.4 举个例子</h3><p>计算状态</p><p>比如状态空间包括{wechat；class1;class2;class3;sleep;pass;pub}</p><p>之间的状态概率为</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b3c3404e0dd9231f2584b6ebedcd1eaf_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>不同状态的reward是</p><p><img src="https://pica.zhimg.com/80/v2-5a00345aa837a5d7641513eff8fae762_1440w.png?source=d16d100b" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>利用贝尔曼方程就可以得到最终的value结果</p><h3 id="1-5-MDP的核心问题"><a href="#1-5-MDP的核心问题" class="headerlink" title="1.5 MDP的核心问题"></a>1.5 MDP的核心问题</h3><p>虽然2022-09-30还没有搞懂MDP的数学过程，但是可以看出起核心的问题包括</p><p>【预测】与【控制】</p><p>预测就是给出 state、action、P、R与policy来得到每个状态的value</p><p>控制就是给出state、action、P、R来得到value与policy</p><p>在求解中常分为 策略迭代policy- iteration与价值迭代 value- iteration</p><p>细节后面在了解</p><h3 id="0x02-强化学习概述"><a href="#0x02-强化学习概述" class="headerlink" title="0x02 强化学习概述"></a>0x02 强化学习概述</h3><p>最经典就是这样的图，强化学习的过程可以看作是实际化的MDP过程；我们可以知道我们将问题分为 智能体agent与环境env两部分；状态state与奖励reward由env给出；agent会根据状态与奖励来实现不同的动作action（动作序列也就是需要学习的策略policy），再得到下一个状态以及当前状态得到的奖励</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-fc6cd1cf1a397b72cb6a791f328b352f_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="0x03-强化学习与监督学习"><a href="#0x03-强化学习与监督学习" class="headerlink" title="0x03 强化学习与监督学习"></a>0x03 强化学习与监督学习</h3><h3 id="3-1-监督学习"><a href="#3-1-监督学习" class="headerlink" title="3.1 监督学习"></a>3.1 监督学习</h3><ul><li>我们首先假设我们有大量被标注的数据，假设之间满足同分布，之间是没有关联的。在这个基础上我们训练我们的模型（比如一个分类器），我们可能通过将标签信息传递给神经网络，当神经网络做出错误的预测的时候我们会给他一个错误的反馈，我们利用这个错误写出一个 损失函数 loss- function，通过 反向传播 back propagate来训练</li><li>假设1:输出的数据应该是没有关联的</li><li>假设2:我们需要告诉学习正确的标签是什么，这样它可以通过正确的标签来修正自己的预测</li></ul><h3 id="3-2-强化学习"><a href="#3-2-强化学习" class="headerlink" title="3.2 强化学习"></a>3.2 强化学习</h3><ul><li>在强化学习中，监督学习的上述的两个假设都不满足</li><li>每个frame之间具有非常强的连续性，同时没有iid</li><li>强化学习中得到的训练数据是一个玩游戏的序列，我们将这个序列放进网络、希望网络输出一些动作，在这个问题中我们并没有标签来说明现在这个动作是正确的还是i错误的，必须等到游戏结束才能知道，因此这里我们面临 [[延迟奖励delayed- reward]] 的问题</li></ul><h3 id="3-3-区别总结"><a href="#3-3-区别总结" class="headerlink" title="3.3 区别总结"></a>3.3 区别总结</h3><ul><li>第一点，强化学习输入的样本是序列数据，不是iid</li><li>第二点，学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现那细微动作可以带来最多的奖励</li><li>第三点，智能体获得自己能力的过程就是 [[不断试错探索trial-and-error- exploration]]的过程， 探索exploration 和 利用exploitation</li><li>第四点，在强化学习过程中并没有非常强的 监督者supervisor ，只有 [[奖励信号reward- signal]]，并且具有 [[延迟奖励delayed- reward]] 的情况</li></ul><h3 id="0x04-序列决策"><a href="#0x04-序列决策" class="headerlink" title="0x04 序列决策"></a>0x04 序列决策</h3><h3 id="4-1-奖励-reward的定义"><a href="#4-1-奖励-reward的定义" class="headerlink" title="4.1 奖励 reward的定义"></a>4.1 奖励 reward的定义</h3><p>是环境给的一种 [[标量反馈信号-scalar-feedback-signal]]，这种信号显示智能体在某一步才去某个策略的表现如何</p><h3 id="4-2-序列过程描述"><a href="#4-2-序列过程描述" class="headerlink" title="4.2 序列过程描述"></a>4.2 序列过程描述</h3><p>强化学习中一个重要的课题就是近期奖励和远期奖励的权衡，也就是如何让智能体可以获得更多的远期奖励</p><p>在与环境的交互过程中， [[智能体agent]] 会获得很多 [[观测observation]] ，针对每个 [[观测observation]] ， [[智能体agent]]会采取一个 [[动作action]]，然后获得 [[奖励reward]]，所以这样的序列是</p><p>[[智能体agent]]在采取当前动作的时候会依赖之前得到的历史，因此可以将整个学习的[[状态state]]看作是这个历史的函数</p><h3 id="4-3-观测-observation-与-状态state区别"><a href="#4-3-观测-observation-与-状态state区别" class="headerlink" title="4.3 观测 observation 与 状态state区别"></a>4.3 观测 observation 与 状态state区别</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8e7ac501e53cfff71e32f8caef5da84a_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="4-4-动作action"><a href="#4-4-动作action" class="headerlink" title="4.4 动作action"></a>4.4 动作action</h3><p>分为离散与连续</p><h3 id="4-5-智能体分类"><a href="#4-5-智能体分类" class="headerlink" title="4.5 智能体分类"></a>4.5 智能体分类</h3><p>第一种分类：</p><ol><li>基于policy划分；随机与确定</li><li>基于value- function；value或Q函数</li><li>模型model；模型决定下一步的状态，状态取决于当前的状态以及当前采取的动作，由 [[状态转移概率]]和 [[奖励函数]]组成</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1ce7e41f2f475a4f754d5ac1496e917a_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>第二种分类：</p><ol><li>基于价值的智能体</li><li>基于策略的智能体</li><li>演员评论员智能体</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-29b80bae0bdeb4686b984a55a27b73f4_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS285</tag>
      
      <tag>强化学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习Vol3 ｜ 经典卷积神经网络实现</title>
    <link href="/posts/a3b583b7.html"/>
    <url>/posts/a3b583b7.html</url>
    
    <content type="html"><![CDATA[<p>在初等卷积神经网络中，我们可以看出从平移不变和缩放不变性质而得到的卷积性质的基础上，在上个世纪诞生的LeNet成功实现与之前不同的路径，在GPU诞生之后，新的网络AlexNet、VGG、NiN、GoogLeNet、Residual Net、DenseNet逐渐发展。卷积神经网络从变深变多，逐渐到变成块以及一些trick的增加。</p><h3 id="0x01-introduction"><a href="#0x01-introduction" class="headerlink" title="0x01 introduction"></a>0x01 introduction</h3><h3 id="1-1-PyTorch"><a href="#1-1-PyTorch" class="headerlink" title="1.1 PyTorch"></a>1.1 PyTorch</h3><p>之前在讲座中听到深度学习框架是深度学习时代的操作系统，怎么说这个比喻有点粗旷但是又不失正确性。个人感觉在计算机时代人们协作的能力达到空前的地步，我们可以上千人共同开发app，这种和建造房屋似的过程充分体现了软件工程的魅力。与土建中规划、设计、建造、 验收等流程相似，个人验证计算机成就的结果是建立在一层一层的抽象的基础上的。</p><p>从硬件的角度，现在并不是所有人都需要关心芯片上的三极管，也不需要关心加法器的构成。从之前手工焊机，到芯片的模块化设计，到更高级的抽象工艺和建造工艺的诞生，人们设计更高水平的芯片产生了可能。正是有这些抽象我们才能往前走。</p><p>从深度学习框架的角度也是如此，从零实现某个网络在学习过程是有意思的，但是从汇编语言包装成高级编程语言，再包装成一个一个package而形成框架，在前人抽象好的基础上，在深度学习框架搭建的基础上，在使用者的角度更应该重视如何高效的使用。</p><p>基于torch构建的PyTorch便是抽象好深度学习的框架。</p><p>这里并没有参照 <a href="https://pytorch.org/">PyTorch官网</a></p><p>而是找到一个开源的 <a href="https://handbook.pytorch.wiki/index.html">PyTorch中文手册</a> 来了解这个深度学习框架</p><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vhdl">Torch <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> going anywhere. PyTorch <span class="hljs-keyword">and</span> Torch <span class="hljs-keyword">use</span> the same C libraries that contain <span class="hljs-keyword">all</span> the performance: TH, THC, THNN, THCUNN <span class="hljs-keyword">and</span> they will continue <span class="hljs-keyword">to</span> be <span class="hljs-keyword">shared</span>.<br>We still <span class="hljs-keyword">and</span> will have continued engineering <span class="hljs-keyword">on</span> Torch itself, <span class="hljs-keyword">and</span> we have no immediate plan <span class="hljs-keyword">to</span> remove that.<br>PyTorch <span class="hljs-keyword">is</span> an <span class="hljs-keyword">open</span> source machine learning <span class="hljs-keyword">library</span> <span class="hljs-keyword">for</span> Python, based <span class="hljs-keyword">on</span> Torch, used <span class="hljs-keyword">for</span> applications such as <span class="hljs-built_in">natural</span> language processing. It <span class="hljs-keyword">is</span> primarily developed by Facebook<span class="hljs-symbol">&#x27;s</span> artificial-intelligence research <span class="hljs-keyword">group</span>, <span class="hljs-keyword">and</span> Uber<span class="hljs-symbol">&#x27;s</span> <span class="hljs-string">&quot;Pyro&quot;</span> software <span class="hljs-keyword">for</span> probabilistic programming <span class="hljs-keyword">is</span> built <span class="hljs-keyword">on</span> it.<br>PyTorch <span class="hljs-keyword">is</span> a Python <span class="hljs-keyword">package</span> that provides two high-level features:<br><br>    Tensor computation (like NumPy) <span class="hljs-keyword">with</span> <span class="hljs-keyword">strong</span> GPU acceleration<br><br>    Deep neural networks built <span class="hljs-keyword">on</span> a tape-based autograd system<br>You can reuse your favorite Python packages such as NumPy, SciPy <span class="hljs-keyword">and</span> Cython <span class="hljs-keyword">to</span> extend PyTorch <span class="hljs-keyword">when</span> needed.<br></code></pre></td></tr></table></figure><h3 id="1-2-GPU"><a href="#1-2-GPU" class="headerlink" title="1.2 GPU"></a>1.2 GPU</h3><p>这里参照wiki所给的解释：<a href="https://zh.wikipedia.org/zh-sg/%E5%9C%96%E5%BD%A2%E8%99%95%E7%90%86%E5%99%A8">图形处理器GPU解释</a></p><p><strong>图形处理器</strong>（英语：<strong>G</strong>raphics <strong>P</strong>rocessing <strong>U</strong>nit，缩写：<strong>GPU</strong>；又称<strong>显示核心</strong>、<strong>显卡</strong>、<strong>视觉处理器</strong>、<strong>显示芯片</strong>或<strong>绘图芯片</strong>）是一种专门在<a href="https://zh.wikipedia.org/wiki/%E5%80%8B%E4%BA%BA%E9%9B%BB%E8%85%A6">个人电脑</a>、<a href="https://zh.wikipedia.org/wiki/%E5%B7%A5%E4%BD%9C%E7%AB%99">工作站</a>、<a href="https://zh.wikipedia.org/wiki/%E9%81%8A%E6%88%B2%E6%A9%9F">游戏机</a>和一些<a href="https://zh.wikipedia.org/wiki/%E8%A1%8C%E5%8B%95%E8%A3%9D%E7%BD%AE">移动设备</a>（如<a href="https://zh.wikipedia.org/wiki/%E5%B9%B3%E6%9D%BF%E9%9B%BB%E8%85%A6">平板电脑</a>、<a href="https://zh.wikipedia.org/wiki/%E6%99%BA%E6%85%A7%E5%9E%8B%E6%89%8B%E6%A9%9F">智能手机</a>等）上执行绘图运算工作的<a href="https://zh.wikipedia.org/wiki/%E5%BE%AE%E8%99%95%E7%90%86%E5%99%A8">微处理器</a>。</p><p>GPU不同于传统的CPU，如<a href="https://zh.wikipedia.org/wiki/Intel">Intel</a> <a href="https://zh.wikipedia.org/wiki/Intel_Core_i5">i5</a>或<a href="https://zh.wikipedia.org/wiki/Intel_Core_i7">i7</a>处理器，其<a href="https://zh.wikipedia.org/wiki/%E5%86%85%E6%A0%B8">内核</a>数量较少，专为通用计算而设计。相反，GPU是一种特殊类型的处理器，具有数百或数千个内核，经过优化，可并行运行大量计算。虽然GPU在游戏中以<a href="https://zh.wikipedia.org/wiki/3D%E6%B8%B2%E6%9F%93">3D渲染</a>而闻名，但它们对运行分析、<a href="https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>和<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>算法尤其有用。GPU允许某些计算比传统CPU上运行相同的计算速度快10倍至100倍。</p><h3 id="1-3-数据集输入和结果计算"><a href="#1-3-数据集输入和结果计算" class="headerlink" title="1.3 数据集输入和结果计算"></a>1.3 数据集输入和结果计算</h3><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><code class="hljs scss"># 读取fashion数据集<br>def <span class="hljs-built_in">load_fashion_mnist</span>(batchsize,resize=None):<br>    # resize相当于基本的填充模块<br>    import torch<br>    from torch.utils import data<br>    import torchvision<br>    from torchvision import transforms<br><br>    if resize:<br>        trans=[transforms.<span class="hljs-built_in">ToTensor</span>()]<br>        trans.<span class="hljs-built_in">insert</span>(<span class="hljs-number">0</span>, transforms.<span class="hljs-built_in">Resize</span>(resize))<br>        trans = transforms.<span class="hljs-built_in">Compose</span>(trans)<br>    else:<br>        trans=transforms.<span class="hljs-built_in">ToTensor</span>()<br>    train_mnist=torchvision.datasets.<span class="hljs-built_in">FashionMNIST</span>(root=<span class="hljs-string">&#x27;../data/FashionMNIST&#x27;</span>,train=True,transform=trans,download=True)<br>    test_mnist=torchvision.datasets.<span class="hljs-built_in">FashionMNIST</span>(root=<span class="hljs-string">&#x27;../data/FashionMNIST&#x27;</span>,train=False,transform=trans,download=True)<br>    train_iter=data.<span class="hljs-built_in">DataLoader</span>(train_mnist,batch_size=batchsize,shuffle=True,num_workers=<span class="hljs-number">4</span>)<br>    test_iter=data.<span class="hljs-built_in">DataLoader</span>(test_mnist,batch_size=batchsize,shuffle=True,num_workers=<span class="hljs-number">4</span>)<br>    return train_iter,test_iter<br><br>batchsize=<span class="hljs-number">256</span><br>train_iter,test_iter=<span class="hljs-built_in">load_fashion_mnist</span>(batchsize,<span class="hljs-number">224</span>)<br>import torch<br>def <span class="hljs-built_in">accuracy</span>(y_hat,y):<br>    # 数正确的数量<br>    if <span class="hljs-built_in">len</span>(y_hat.shape)&gt;<span class="hljs-number">1</span> and y_hat.shape[<span class="hljs-number">1</span>]&gt;<span class="hljs-number">1</span>:<br>        y_hat=y_hat.<span class="hljs-built_in">argmax</span>(axis=<span class="hljs-number">1</span>)<br>    cmp=y_hat.<span class="hljs-built_in">type</span>(y.dtype)==y<br>    return <span class="hljs-built_in">float</span>(cmp.<span class="hljs-built_in">type</span>(y.dtype).<span class="hljs-built_in">sum</span>())<br><br>def <span class="hljs-built_in">gpu_acc</span>(model,data_iter,device=None):<br>    acc=<span class="hljs-number">0</span><br>    num=<span class="hljs-number">0</span><br>    if <span class="hljs-built_in">isinstance</span>(net,nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()<br>        if not device:<br>            device=<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.<span class="hljs-built_in">parameters</span>())).device<br>    with torch.<span class="hljs-built_in">no_grad</span>():<br>        for X,y in data_iter:<br>            if <span class="hljs-built_in">isinstance</span>(X,list):<br>                # 这里的if仅仅是为之后bert微调所需的<br>                X=[x.<span class="hljs-built_in">to</span>(device) for x in X]<br>            else:<br>                X=X.<span class="hljs-built_in">to</span>(device)<br>            y=y.<span class="hljs-built_in">to</span>(device)<br>            acc+=<span class="hljs-built_in">accuracy</span>(<span class="hljs-built_in">net</span>(X),y)<br>            num+=y.<span class="hljs-built_in">numel</span>()<br>    return acc/num<br><br><br><br>def <span class="hljs-built_in">trainer</span>(net,train_iter,test_iter,num_epoches,lr,device):<br>    <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;用GPU训练模型&#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br>    #初始化参数<br>    def <span class="hljs-built_in">init_weights</span>(m):<br>        if <span class="hljs-built_in">type</span>(m)==nn.Linear or <span class="hljs-built_in">type</span>(m)==nn.Conv2d:<br>            nn.init.<span class="hljs-built_in">xavier_uniform_</span>(m.weight)<br>            #nn.init.<span class="hljs-built_in">normal_</span>(m.weight,std=<span class="hljs-number">0.01</span>)<br>    net.<span class="hljs-built_in">apply</span>(init_weights)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;training on&#x27;</span>,device)<br>    # <span class="hljs-built_in">print</span>(torch.cuda.<span class="hljs-built_in">get_device_name</span>(<span class="hljs-number">0</span>))<br>    net.<span class="hljs-built_in">to</span>(device)<br>    optimizer=torch.optim.<span class="hljs-built_in">SGD</span>(net.<span class="hljs-built_in">parameters</span>(),lr=lr)<br>    loss=nn.<span class="hljs-built_in">CrossEntropyLoss</span>()<br><br>    train_acc=[]<br>    test_acc=[]<br>    loss_epoches=[]<br>    for epoch in <span class="hljs-built_in">range</span>(num_epoches):<br>        temp_train_acc=<span class="hljs-number">0</span><br>        loss_epoch=<span class="hljs-number">0</span><br>        num=<span class="hljs-number">0</span><br>        net.<span class="hljs-built_in">train</span>()<br>        for i,(X,y) in <span class="hljs-built_in">enumerate</span>(train_iter):<br>            optimizer.<span class="hljs-built_in">zero_grad</span>()<br>            X,y=X.<span class="hljs-built_in">to</span>(device),y.<span class="hljs-built_in">to</span>(device)<br>            with torch.<span class="hljs-built_in">no_grad</span>():<br>                temp_train_acc+=<span class="hljs-built_in">accuracy</span>(<span class="hljs-built_in">net</span>(X),y)<br>            y_hat=<span class="hljs-built_in">net</span>(X)<br>            l=<span class="hljs-built_in">loss</span>(y_hat,y)<br>            l.<span class="hljs-built_in">backward</span>()<br>            optimizer.<span class="hljs-built_in">step</span>()<br>            loss_epoch+=l*X.shape[<span class="hljs-number">0</span>]<br>            num+=y.<span class="hljs-built_in">numel</span>()<br>        train_acc.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">float</span>(temp_train_acc/num))<br>        test_acc.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">gpu_acc</span>(net,test_iter))<br>        loss_epoches.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">float</span>(loss_epoch)/num)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;本轮&#123;&#125;训练的结果，test—acc:&#123;&#125;,train-acc:&#123;&#125;,train-loss:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">str</span>(epoch+<span class="hljs-number">1</span>),<span class="hljs-built_in">str</span>(test_acc[-<span class="hljs-number">1</span>]),<span class="hljs-built_in">str</span>(train_acc[-<span class="hljs-number">1</span>]),<span class="hljs-built_in">str</span>(loss_epoches[-<span class="hljs-number">1</span>])))<br>    import matplotlib.pyplot as plt<br>    plt.<span class="hljs-built_in">plot</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>+num_epoches)),test_acc,label=<span class="hljs-string">&#x27;test_acc&#x27;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>+num_epoches)),train_acc,label=<span class="hljs-string">&#x27;train_acc&#x27;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>+num_epoches)),loss_epoches,label=<span class="hljs-string">&#x27;loss&#x27;</span>)<br>    plt.<span class="hljs-built_in">legend</span>()<br>    plt.<span class="hljs-built_in">show</span>()<br><br><br>lr,num_epoches=<span class="hljs-number">0.2</span>,<span class="hljs-number">20</span><br><br><br>device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cuda:0&quot;</span> if torch.cuda.<span class="hljs-built_in">is_available</span>() else <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-built_in">trainer</span>(net,train_iter,test_iter,num_epoches,lr,device)<br></code></pre></td></tr></table></figure><h3 id="0x02-经典的CNN"><a href="#0x02-经典的CNN" class="headerlink" title="0x02 经典的CNN"></a>0x02 经典的CNN</h3><h3 id="2-1-AlexNet"><a href="#2-1-AlexNet" class="headerlink" title="2.1 AlexNet"></a>2.1 AlexNet</h3><p>之前的阅读笔记<a href="https://blog.tjdata.site/2022/08/27/paper02-2012-imagenet-classification-with-deep-convolutional-neural-networks/">paper02-2012-imagenet-classification-with-deep-convolutional-neural-networks</a></p><p>主要的贡献，以及后面没有用的地方</p><ol><li>引入了ReLu激活函数，让训练的更快</li><li>利用双GPU训练，注意这里对网络结构进行划分（对网络不同层进行切割而不是切割成两种小的部分），工程能力太强了</li><li>（没有用）一种local response normalization，但是确实需要在这个方面做trick，在后面的resNet中可以看到</li><li>（没有用）新的overlapping pooling，一种新的池化方法，这个与双GPU有关</li><li>（有用，但是解释不对）将dropout引入训练过程，尝试解释为ensemble learning，但现在更偏向于regularization</li><li>数据增强，才有随机抽样数据的方式，将256提取得到224来扩充数据集</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-82c974a4743e8a72fe6736f3e8cc3903_1440w.jpg" alt="img"></p><p>image-20220923164858838</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch <br><span class="hljs-attribute">from</span> torch import nn<br><span class="hljs-comment">#AlexNet</span><br><span class="hljs-attribute">net</span> = nn.Sequential(<br>    <span class="hljs-comment"># 这里，我们使用一个11*11的更大窗口来捕捉对象。</span><br>    <span class="hljs-comment"># 同时，步幅为4，以减少输出的高度和宽度。</span><br>    <span class="hljs-comment"># 另外，输出通道的数目远大于LeNet</span><br>    <span class="hljs-attribute">nn</span>.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    <span class="hljs-attribute">nn</span>.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br>    <span class="hljs-attribute">nn</span>.Conv2d(<span class="hljs-number">96</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.ReLU(),<br>    <span class="hljs-attribute">nn</span>.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 使用三个连续的卷积层和较小的卷积窗口。</span><br>    <span class="hljs-comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span><br>    <span class="hljs-comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span><br>    <span class="hljs-attribute">nn</span>.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    <span class="hljs-attribute">nn</span>.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    <span class="hljs-attribute">nn</span>.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    <span class="hljs-attribute">nn</span>.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-attribute">nn</span>.Flatten(),<br>    <span class="hljs-comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span><br>    <span class="hljs-attribute">nn</span>.Linear(<span class="hljs-number">6400</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    <span class="hljs-attribute">nn</span>.Dropout(p=<span class="hljs-number">0</span>.<span class="hljs-number">5</span>),<br>    <span class="hljs-attribute">nn</span>.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    <span class="hljs-attribute">nn</span>.Dropout(p=<span class="hljs-number">0</span>.<span class="hljs-number">5</span>),<br>    <span class="hljs-comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span><br>    <span class="hljs-attribute">nn</span>.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><h3 id="2-2-VGG"><a href="#2-2-VGG" class="headerlink" title="2.2 VGG"></a>2.2 VGG</h3><p><a href="https://arxiv.org/pdf/1409.1556.pdf">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</a></p><p>AlexNet证明 #卷积神经网络 CNN convolutional- neural-network 的深度是有效的，但并没有给出模版来进行创建网络。这里一个直觉的想法</p><p>与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构设计也逐渐变成更加抽象，研究人员开始从单个神经元的角度思考问题，发展到整个层，转向可以复用层的块的模式 #memo</p><p>使用 #循环loop 和 子函数 来实现 #VGG</p><p>主要做出的贡献</p><ol><li>利用复用的思路构建卷积块</li><li>作者尝试各种架构，最终发现深且窄的网络效果是较好的</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b0e04e336aa7a60b78eced9b95966d58_1440w.jpg" alt="img"></p><p>image-20220923165253079</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs scss">import torch <br>from torch import nn<br>def <span class="hljs-built_in">vgg_block</span>(num_convs, in_channels, out_channels):<br>    layers = []<br>    for _ in <span class="hljs-built_in">range</span>(num_convs):<br>        layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">Conv2d</span>(in_channels, out_channels,<br>                                kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">ReLU</span>())<br>        in_channels = out_channels<br>    layers.<span class="hljs-built_in">append</span>(nn.<span class="hljs-built_in">MaxPool2d</span>(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>    return nn.<span class="hljs-built_in">Sequential</span>(*layers)<br><br>def <span class="hljs-built_in">vgg</span>(conv_arch):<br>    conv_blks = []<br>    in_channels = <span class="hljs-number">1</span><br>    # 卷积层部分<br>    for (num_convs, out_channels) in conv_arch:<br>        conv_blks.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">vgg_block</span>(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br><br>    return nn.<span class="hljs-built_in">Sequential</span>(<br>        *conv_blks, nn.<span class="hljs-built_in">Flatten</span>(),<br>        # 全连接层部分<br>        nn.<span class="hljs-built_in">Linear</span>(out_channels * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">4096</span>), nn.<span class="hljs-built_in">ReLU</span>(), nn.<span class="hljs-built_in">Dropout</span>(<span class="hljs-number">0.5</span>),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.<span class="hljs-built_in">ReLU</span>(), nn.<span class="hljs-built_in">Dropout</span>(<span class="hljs-number">0.5</span>),<br>        nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br><br>conv_arch = ((<span class="hljs-number">1</span>, <span class="hljs-number">64</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>))<br>ratio = <span class="hljs-number">4</span><br>small_conv_arch = [(pair[<span class="hljs-number">0</span>], pair[<span class="hljs-number">1</span>] // ratio) for pair in conv_arch]<br>net = <span class="hljs-built_in">vgg</span>(small_conv_arch)<br></code></pre></td></tr></table></figure><h3 id="2-3-NiN"><a href="#2-3-NiN" class="headerlink" title="2.3 NiN"></a>2.3 NiN</h3><p><a href="https://arxiv.org/pdf/1312.4400.pdf">Network in network</a></p><p>AlexNet提出使用深度卷积神经网络，VGG给出如何复用的构建卷积神经网络，VGG则是在这个基础上利用MLP对多通道输入和多通道输出进行操作，取代之前简单的加权方式，NiN利用这来对网络的扩大和深度。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b09e6855cc43e71d568df37059a8725a_1440w.jpg" alt="img"></p><p>image-20220923165759261</p><h3 id="2-4-GoogLeNet"><a href="#2-4-GoogLeNet" class="headerlink" title="2.4 GoogLeNet"></a>2.4 GoogLeNet</h3><p><a href="https://arxiv.org/pdf/1409.4842.pdf">Going deeper with convolutions</a></p><p>GoogLeNet借用NiN的思想，在它的基础上设置来inception块，用来实现多个卷积核对图形特征的提取</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8e5eccd09aeec660fae9c5c468070765_1440w.jpg" alt="img"></p><p>image-20220923165859815</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 网络3.5 GoogLeNet</span><br><br><span class="hljs-attribute">import</span> torch<br><span class="hljs-attribute">from</span> torch import nn<br><span class="hljs-attribute">from</span> torch.nn import functional as F<br><br><br><br><span class="hljs-attribute">class</span> Inception(nn.Module):<br>    <span class="hljs-comment"># c1--c4是每条路径的输出通道数</span><br>    <span class="hljs-attribute">def</span> __init__(self, in_channels, c1, c2, c3, c4, **kwargs):<br>        <span class="hljs-attribute">super</span>(Inception, self).__init__(**kwargs)<br>        <span class="hljs-comment"># 线路1，单1x1卷积层</span><br>        <span class="hljs-attribute">self</span>.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路2，1x1卷积层后接3x3卷积层</span><br>        <span class="hljs-attribute">self</span>.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-attribute">self</span>.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路3，1x1卷积层后接5x5卷积层</span><br>        <span class="hljs-attribute">self</span>.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-attribute">self</span>.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span><br>        <span class="hljs-attribute">self</span>.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-attribute">self</span>.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-attribute">def</span> forward(self, x):<br>        <span class="hljs-attribute">p1</span> = F.relu(self.p1_1(x))<br>        <span class="hljs-attribute">p2</span> = F.relu(self.p2_2(F.relu(self.p2_1(x))))<br>        <span class="hljs-attribute">p3</span> = F.relu(self.p3_2(F.relu(self.p3_1(x))))<br>        <span class="hljs-attribute">p4</span> = F.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 在通道维度上连结输出</span><br>        <span class="hljs-attribute">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-attribute">b1</span> = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   <span class="hljs-attribute">nn</span>.ReLU(),<br>                   <span class="hljs-attribute">nn</span>.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br><span class="hljs-attribute">b2</span> = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>),<br>                   <span class="hljs-attribute">nn</span>.ReLU(),<br>                   <span class="hljs-attribute">nn</span>.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   <span class="hljs-attribute">nn</span>.ReLU(),<br>                   <span class="hljs-attribute">nn</span>.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-attribute">b3</span> = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   <span class="hljs-attribute">Inception</span>(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   <span class="hljs-attribute">nn</span>.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-attribute">b4</span> = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   <span class="hljs-attribute">Inception</span>(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   <span class="hljs-attribute">Inception</span>(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   <span class="hljs-attribute">Inception</span>(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   <span class="hljs-attribute">Inception</span>(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   <span class="hljs-attribute">nn</span>.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-attribute">b5</span> = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   <span class="hljs-attribute">Inception</span>(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   <span class="hljs-attribute">nn</span>.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                   <span class="hljs-attribute">nn</span>.Flatten())<br><br><span class="hljs-attribute">net</span> = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><h3 id="2-5-ResNet"><a href="#2-5-ResNet" class="headerlink" title="2.5 ResNet"></a>2.5 ResNet</h3><p><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p><p>在之前介绍更深、更广的基础上，resnet更多的是解释如何提高神经网络的性能</p><p>部分的数学基础，从学习理论的角度出发，如果模型的复杂程度逐渐增加，这样寻找之前的函数集合的范围是比较大，这样寻找到之后的结果是非常困难的，因此可以尝试将恒等映射转换成为包含数量本身和残差之间的关系</p><p>借鉴GBDT中梯度提升的思想，残差问题在现实中往往更加容易优化，同时在模型为理想参数更容易捕捉恒等映射的细微波动，；在实际训练中，残差块中输入可以通过跨层数据线路更快的向前传播</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-title">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-title">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><span class="hljs-class"></span><br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Residual</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):  #@save</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>, <span class="hljs-title">input_channels</span>, <span class="hljs-title">num_channels</span>,</span><br><span class="hljs-class">                 <span class="hljs-title">use_1x1conv</span>=<span class="hljs-type">False</span>, <span class="hljs-title">strides</span>=1):</span><br><span class="hljs-class">        super().__init__()</span><br><span class="hljs-class">        self.conv1 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">input_channels</span>, <span class="hljs-title">num_channels</span>,</span><br><span class="hljs-class">                               <span class="hljs-title">kernel_size</span>=3, <span class="hljs-title">padding</span>=1, <span class="hljs-title">stride</span>=<span class="hljs-title">strides</span>)</span><br><span class="hljs-class">        self.conv2 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">num_channels</span>, <span class="hljs-title">num_channels</span>,</span><br><span class="hljs-class">                               <span class="hljs-title">kernel_size</span>=3, <span class="hljs-title">padding</span>=1)</span><br><span class="hljs-class">        if use_1x1conv:</span><br><span class="hljs-class">            self.conv3 = nn.<span class="hljs-type">Conv2d</span>(<span class="hljs-title">input_channels</span>, <span class="hljs-title">num_channels</span>,</span><br><span class="hljs-class">                                   <span class="hljs-title">kernel_size</span>=1, <span class="hljs-title">stride</span>=<span class="hljs-title">strides</span>)</span><br><span class="hljs-class">        else:</span><br><span class="hljs-class">            self.conv3 = <span class="hljs-type">None</span></span><br><span class="hljs-class">        self.bn1 = nn.<span class="hljs-type">BatchNorm2d</span>(<span class="hljs-title">num_channels</span>)</span><br><span class="hljs-class">        self.bn2 = nn.<span class="hljs-type">BatchNorm2d</span>(<span class="hljs-title">num_channels</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-type">X</span>):</span><br><span class="hljs-class">        <span class="hljs-type">Y</span> = <span class="hljs-type">F</span>.relu(<span class="hljs-title">self</span>.<span class="hljs-title">bn1</span>(<span class="hljs-title">self</span>.<span class="hljs-title">conv1</span>(<span class="hljs-type">X</span>)))</span><br><span class="hljs-class">        <span class="hljs-type">Y</span> = self.bn2(<span class="hljs-title">self</span>.<span class="hljs-title">conv2</span>(<span class="hljs-type">Y</span>))</span><br><span class="hljs-class">        if self.conv3:</span><br><span class="hljs-class">            <span class="hljs-type">X</span> = self.conv3(<span class="hljs-type">X</span>)</span><br><span class="hljs-class">        <span class="hljs-type">Y</span> += <span class="hljs-type">X</span></span><br><span class="hljs-class">        return <span class="hljs-type">F</span>.relu(<span class="hljs-type">Y</span>)</span><br><span class="hljs-class">b1 = nn.<span class="hljs-type">Sequential</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(1, 64, <span class="hljs-title">kernel_size</span>=7, <span class="hljs-title">stride</span>=2, <span class="hljs-title">padding</span>=3),</span><br><span class="hljs-class">                   nn.<span class="hljs-type">BatchNorm2d</span>(64), nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">                   nn.<span class="hljs-type">MaxPool2d</span>(<span class="hljs-title">kernel_size</span>=3, <span class="hljs-title">stride</span>=2, <span class="hljs-title">padding</span>=1))</span><br><span class="hljs-class"></span><br><span class="hljs-class">def resnet_block(<span class="hljs-title">input_channels</span>, <span class="hljs-title">num_channels</span>, <span class="hljs-title">num_residuals</span>,</span><br><span class="hljs-class">                 <span class="hljs-title">first_block</span>=<span class="hljs-type">False</span>):</span><br><span class="hljs-class">    blk = []</span><br><span class="hljs-class">    for i in range(<span class="hljs-title">num_residuals</span>):</span><br><span class="hljs-class">        if i == 0 and not first_block:</span><br><span class="hljs-class">            blk.append(<span class="hljs-type">Residual</span>(<span class="hljs-title">input_channels</span>, <span class="hljs-title">num_channels</span>,</span><br><span class="hljs-class">                                <span class="hljs-title">use_1x1conv</span>=<span class="hljs-type">True</span>, <span class="hljs-title">strides</span>=2))</span><br><span class="hljs-class">        else:</span><br><span class="hljs-class">            blk.append(<span class="hljs-type">Residual</span>(<span class="hljs-title">num_channels</span>, <span class="hljs-title">num_channels</span>))</span><br><span class="hljs-class">    return blk</span><br><span class="hljs-class">b2 = nn.<span class="hljs-type">Sequential</span>(*<span class="hljs-title">resnet_block</span>(64, 64, 2, <span class="hljs-title">first_block</span>=<span class="hljs-type">True</span>))</span><br><span class="hljs-class">b3 = nn.<span class="hljs-type">Sequential</span>(*<span class="hljs-title">resnet_block</span>(64, 128, 2))</span><br><span class="hljs-class">b4 = nn.<span class="hljs-type">Sequential</span>(*<span class="hljs-title">resnet_block</span>(128, 256, 2))</span><br><span class="hljs-class">b5 = nn.<span class="hljs-type">Sequential</span>(*<span class="hljs-title">resnet_block</span>(256, 512, 2))</span><br><span class="hljs-class">net = nn.<span class="hljs-type">Sequential</span>(<span class="hljs-title">b1</span>, <span class="hljs-title">b2</span>, <span class="hljs-title">b3</span>, <span class="hljs-title">b4</span>, <span class="hljs-title">b5</span>,</span><br><span class="hljs-class">                    <span class="hljs-title">nn</span>.<span class="hljs-type">AdaptiveAvgPool2d</span>((1,1)),</span><br><span class="hljs-class">                    nn.<span class="hljs-type">Flatten</span>(), nn.<span class="hljs-type">Linear</span>(512, 10))</span><br></code></pre></td></tr></table></figure><h3 id="2-6-DenseNet"><a href="#2-6-DenseNet" class="headerlink" title="2.6 DenseNet"></a>2.6 DenseNet</h3><p><a href="https://arxiv.org/pdf/1608.06993.pdf">Densely Connected Convolutional Networks</a></p><p>～还没看不重要捏</p><h3 id="0x03-Result"><a href="#0x03-Result" class="headerlink" title="0x03 Result"></a>0x03 Result</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0c2e26a48b683ad9442614123093bcaf_1440w.jpg" alt="img"></p><p>AlexNet</p><h3 id="0x04-Discussion"><a href="#0x04-Discussion" class="headerlink" title="0x04 Discussion"></a>0x04 Discussion</h3><p>卷积神经网络提取特征的基本在于特征的平移不变和缩放不变，在这个基础上利用互相关操作（错误的被称为卷积）在提取特征。同时在这个过程中围绕了如何构建深度卷积神经网络，主流的blakcbone给出自己自己的发展历史：</p><ol><li>lenet最早给出卷积神经网络的雏形</li><li>alexnet从训练硬件、训练网络、训练数据、数据预处理、训练过程多个方面给出了自己的trick，为后面打下基础</li><li>vgg围绕如何构建更深的网络，引入复用</li><li>nin提出一个trick来构建更广的网络</li><li>Googlenet在nin的基础上，受启发采用多个卷积核提取特征</li><li>在前人构建好完整的网络的基础上，resnet给出如何保持网络稳定性的方法</li><li>densenet还没有了解</li></ol><p>这个过程非常的有意思，可以看到别人是如何前进的。这些都归功李沐老师的总结！</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
      <tag>李沐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习 Vol2 ｜ 初等卷积神经网络实现</title>
    <link href="/posts/347a4d5.html"/>
    <url>/posts/347a4d5.html</url>
    
    <content type="html"><![CDATA[<p>上一篇文章中介绍一个基本的机器学习模型线性回归实现的整个过程，将其步骤分为八个部分。之后D2L中介绍了soft Max、MLP等简单的模型。这里迈向深度学习，利用简单的卷积神经网络来介绍深度学习API的使用过程，从零开始搭建模型，将其分为数据集预处理、构建模型和训练及评估三个步骤。</p><h3 id="0x01-Introduction-of-CNN"><a href="#0x01-Introduction-of-CNN" class="headerlink" title="0x01 Introduction of CNN"></a>0x01 Introduction of CNN</h3><h3 id="1-1-卷积的由来"><a href="#1-1-卷积的由来" class="headerlink" title="1.1 卷积的由来"></a>1.1 卷积的由来</h3><p>我们可以将MNIST数据集中的输入变量（28，28）的二维张量给展平（flatten）成为784的向量，然后在使用足够深的MLP来进行训练，但是这样造成最直观的结果就是模型的参数会直线上升，因为我的输入足够多，同时为希望模型结果好的情况需要也需要足够深，这样造成训练过程的计算开销是巨大的。（这是很直观的，但是理论上这样训练出来的网络会非常好。可能是这样？）</p><p>为了弥补算力的不足，需要寻找新的方式？分析图像识别中目标我们会看出具有两个基本特性：</p><ol><li>平移不变性translating- invariance，不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应</li><li>局部性locality，神经网络前几层应该只探索输入图像中的局部区域，而不是过度在意整个图像其他区域的关系</li></ol><p>同时在信号处理中我们也有会滤波器的概念，其基本思想是利用像素之间变化的趋势来进行处理，比如中值滤波可以降噪、利用差分可以识别图像轮廓</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d8f166354472fb0f4575047e44c73459_1440w.jpg" alt="img"></p><p>image-20220916214004258</p><p>我们可以看出卷积的操作是有效的，同时使用（3，3）的filter来对（28，28）的图像处理最终得到（26，26）的图像，执行的操作次数仅为26*26.会让参数数量下降</p><h3 id="1-2-卷积的实现（互相关运算、填充padding、步长stride、多通道）"><a href="#1-2-卷积的实现（互相关运算、填充padding、步长stride、多通道）" class="headerlink" title="1.2 卷积的实现（互相关运算、填充padding、步长stride、多通道）"></a>1.2 卷积的实现（互相关运算、填充padding、步长stride、多通道）</h3><p>具体公式就不再这里展开，dddd。</p><p>经过下列的二维卷积操作会将（n，m）经过（k，k）转换为大小（n+1-k，m+1-k）</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Conv2D</span>(<span class="hljs-title">nn</span>.<span class="hljs-title">module</span>):</span><br><span class="hljs-class">  def __init__(<span class="hljs-title">self</span>,<span class="hljs-title">kernel_size</span>):</span><br><span class="hljs-class">    super().__init__()</span><br><span class="hljs-class">    self.weight=nn.<span class="hljs-type">Parameter</span>(<span class="hljs-title">torch</span>.<span class="hljs-title">rand</span>(<span class="hljs-title">kernel_size</span>))</span><br><span class="hljs-class">    self.bias=nn.<span class="hljs-type">Parameter</span>(<span class="hljs-title">torch</span>.<span class="hljs-title">zeros</span>(1))</span><br><span class="hljs-class"></span><br><span class="hljs-class">  def forward(<span class="hljs-title">self</span>,<span class="hljs-title">x</span>):</span><br><span class="hljs-class">    return corr2d(<span class="hljs-title">x</span>,<span class="hljs-title">self</span>.<span class="hljs-title">weight</span>)+self.bias</span><br><span class="hljs-class"></span><br><span class="hljs-class">  def corr2d(<span class="hljs-type">X</span>,<span class="hljs-title">k</span>):</span><br><span class="hljs-class">    ‘’‘计算二维互相关运算```</span><br><span class="hljs-class">    h,w=k.shape</span><br><span class="hljs-class">    <span class="hljs-type">Y</span>=torch.zeros((<span class="hljs-type">X</span>.<span class="hljs-title">shape</span>[0]-<span class="hljs-title">h</span>+1,<span class="hljs-type">X</span>.<span class="hljs-title">shape</span>[1]-<span class="hljs-title">w</span>+1))</span><br><span class="hljs-class">    for i in range(<span class="hljs-type">Y</span>.<span class="hljs-title">shape</span>[0]):</span><br><span class="hljs-class">      for j in range(<span class="hljs-type">Y</span>.<span class="hljs-title">shape</span>[1]):</span><br><span class="hljs-class">        <span class="hljs-type">Y</span>[i,j]=(<span class="hljs-type">X</span>[<span class="hljs-title">i</span>:<span class="hljs-title">i</span>+<span class="hljs-title">h</span>,<span class="hljs-title">j</span>:<span class="hljs-title">j</span>+<span class="hljs-title">w</span>]*<span class="hljs-type">K</span>).sum()</span><br><span class="hljs-class">    return <span class="hljs-type">Y</span></span><br></code></pre></td></tr></table></figure><p>直接进行卷积操作可以看出向量是会变化的，我们还会有两种操作：填充padding和步长stride。</p><p>针对二维的图像边缘，我们通常需要设置填充padding来保留周边信息，通常是填充0</p><p>（n,m)经过（k，k）后，填充p，得到的结果为（n+p+1-k，m+p+1-k）</p><p>针对清晰度不够的照片，我们可能认为其中部分像素冗余，因此可以利用步幅stride来降低采样</p><p>（n，m）经过（k，k）加上p之后s的输出是（（n+p-k+s）&#x2F;&#x2F;s，（m+p-k+s）&#x2F;&#x2F;s ）（向下取整）</p><ul><li>在二维基础上，理解多通道卷积，首先是多通道输出一个的情况，就是设计三个卷积核然后相加 </li><li>然后多通道输入对应多通道输出，就是设计output*input个卷积核，然后output的卷积核相加即可</li></ul><h3 id="1-3-CNN与深度之间的关系（感受野-receptive-field、汇聚层pooling）"><a href="#1-3-CNN与深度之间的关系（感受野-receptive-field、汇聚层pooling）" class="headerlink" title="1.3 CNN与深度之间的关系（感受野 receptive- field、汇聚层pooling）"></a>1.3 CNN与深度之间的关系（感受野 receptive- field、汇聚层pooling）</h3><p>CNN中感受野的问题，对于一个（3，3）所感受的是一个（3，3）的区域，但是如果再有一个（3，3）他所感受的就是（5，5）的区域。因此随着卷积层数的增加，卷积神经考虑到的值是越来越大的。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-17db7202edcf8dabb0ac45b21b9aa8e3_1440w.jpg" alt="img"></p><p>image-20220916215637261</p><p>这里也会存在一个trade-off，我们希望卷积的结果感受野不会太小，这样我可以得到一些纹理信息，但是我们也不希望感受野太大，不然所训练出来的CNN会对输入敏感，因此在常见的操作中我们会使用汇聚层来对卷积结果进行平滑或者重新采样。</p><h3 id="0x02-Program-structure-analysis-with-PyTorch-API"><a href="#0x02-Program-structure-analysis-with-PyTorch-API" class="headerlink" title="0x02 Program structure analysis with PyTorch API"></a>0x02 Program structure analysis with PyTorch API</h3><h3 id="2-1-代码整体框架分析（尽可能解耦）"><a href="#2-1-代码整体框架分析（尽可能解耦）" class="headerlink" title="2.1 代码整体框架分析（尽可能解耦）"></a>2.1 代码整体框架分析（尽可能解耦）</h3><p>首先神经网络的训练必然是需要多个epoch的，同时在每轮epoch中由于物理空间的限制，我们需要设置batchsize来分批的将数据送入网络，当所有的batch走完说明我们完成一个epoch，这个时候需要记录训练损失（train-loss），训练精度（train-acc）和验证精度（valid- acc）。所以步骤分为：</p><ul><li>第一部分   输入：原始数据集，batchsize   输出：训练数据集（可迭代形式）、验证数据集（可迭代形式）</li><li>第二部分   输入：网络结构、学习率lr   输出：网络结构net、损失函数loss、优化算法trainer</li><li>第三部分   输入：epoch数量num- epoches，训练数据集、验证数据集、net、loss、trainer等   输出：训练好的网络，评估指标</li></ul><h3 id="2-2-PyTorch高效API"><a href="#2-2-PyTorch高效API" class="headerlink" title="2.2 PyTorch高效API"></a>2.2 PyTorch高效API</h3><p>这个看官网，或者看别人的代码多悟就行</p><ul><li><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#">加载数据集</a>   dataset   dataloader </li><li><a href="https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html">定义网络</a>   torch.nn.module   Torch.nn.sequential </li><li><a href="https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html">访问与初始化参数</a>   net.apply()</li></ul><p>def <em>init_weigths</em>(self)</p><h3 id="0x03-Code-of-LeNet"><a href="#0x03-Code-of-LeNet" class="headerlink" title="0x03 Code of LeNet"></a>0x03 Code of LeNet</h3><h3 id="3-0-LeNet简介"><a href="#3-0-LeNet简介" class="headerlink" title="3.0 LeNet简介"></a>3.0 <a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">LeNet简介</a></h3><p>LeNet是最早发布的卷积神经网络，起源于89年，在当时还没有强算力的时代，被广泛应用于ATM机器中，帮助处理支票中的数字</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b0f88594cb7ed87aa5fdc71062417be5_1440w.jpg" alt="img"></p><p>image-20220916222118334</p><p>这里因为Fashion-MNIST是（28，28）的数据集，因此在这个基础上稍微更改一下</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-567dc5a57ae6b80a8591ca0e4ed4559a_1440w.jpg" alt="img"></p><p>image-20220916222329819</p><p>各层参数设置：</p><p>输入：（28，28） 第一层：（5，5）卷积，输出6通道，填充&#x3D;2 -&gt;输出（6，28，28） 第二层：（2，2）池化，步长&#x3D;2 -&gt; 输出（6，14，14） 第三层：（5，5）卷积，输出16通道，-&gt;输出（16，10，10） 第四层：（2，2）池化，步长&#x3D;2 -&gt;输出（16，5，5） 之后展平</p><h3 id="3-1-数据集获取"><a href="#3-1-数据集获取" class="headerlink" title="3.1 数据集获取"></a>3.1 数据集获取</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 读取fashion数据集</span><br><span class="hljs-title">def</span> load_fashion_mnist(batchsize):<br>    <span class="hljs-keyword">import</span> torch<br>    from torch.utils <span class="hljs-keyword">import</span> data<br>    <span class="hljs-keyword">import</span> torchvision<br>    from torchvision <span class="hljs-keyword">import</span> transforms<br><br>    trans=transforms.<span class="hljs-type">ToTensor</span>()<br>    train_mnist=torchvision.datasets.<span class="hljs-type">FashionMNIST</span>(root=&#x27;../<span class="hljs-class"><span class="hljs-keyword">data</span>/<span class="hljs-type">FashionMNIST&#x27;</span>,train=<span class="hljs-type">True</span>,transform=trans,download=<span class="hljs-type">True</span>)</span><br>    test_mnist=torchvision.datasets.<span class="hljs-type">FashionMNIST</span>(root=&#x27;../<span class="hljs-class"><span class="hljs-keyword">data</span>/<span class="hljs-type">FashionMNIST&#x27;</span>,train=<span class="hljs-type">False</span>,transform=trans,download=<span class="hljs-type">True</span>)</span><br>    train_iter=<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">DataLoader</span>(<span class="hljs-title">train_mnist</span>,<span class="hljs-title">batch_size</span>=<span class="hljs-title">batchsize</span>,<span class="hljs-title">shuffle</span>=<span class="hljs-type">True</span>,<span class="hljs-title">num_workers</span>=4)</span><br>    test_iter=<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">DataLoader</span>(<span class="hljs-title">test_mnist</span>,<span class="hljs-title">batch_size</span>=<span class="hljs-title">batchsize</span>,<span class="hljs-title">shuffle</span>=<span class="hljs-type">True</span>,<span class="hljs-title">num_workers</span>=4)</span><br>    return train_iter,test_iter<br><br><span class="hljs-title">batchsize</span>=<span class="hljs-number">256</span><br><span class="hljs-title">train_iter</span>,test_iter=load_fashion_mnist(batchsize)<br></code></pre></td></tr></table></figure><h3 id="3-2-模型定义"><a href="#3-2-模型定义" class="headerlink" title="3.2 模型定义"></a>3.2 模型定义</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch <br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-attribute">net</span>=nn.Sequential(<br>    nn.Conv2d(1,6,<span class="hljs-attribute">kernel_size</span>=5,padding=2),nn.Sigmoid(),<br>    nn.AvgPool2d(<span class="hljs-attribute">kernel_size</span>=2,stride=2),<br>    nn.Conv2d(6,16,<span class="hljs-attribute">kernel_size</span>=5),nn.Sigmoid(),<br>    nn.AvgPool2d(<span class="hljs-attribute">kernel_size</span>=2,stride=2),<br>    nn.Flatten(),<br>    nn.Linear(16<span class="hljs-number">*5</span><span class="hljs-number">*5</span>,120),nn.ReLU(),<br>    nn.Linear(120,84),nn.ReLU(),<br>    nn.Linear(84,10)<br>)<br></code></pre></td></tr></table></figure><h3 id="3-3-训练过程"><a href="#3-3-训练过程" class="headerlink" title="3.3 训练过程"></a>3.3 训练过程</h3><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">accuracy</span>(y_hat,y):<br>    # 数正确的数量<br>    if <span class="hljs-built_in">len</span>(y_hat.shape)&gt;<span class="hljs-number">1</span> and y_hat.shape[<span class="hljs-number">1</span>]&gt;<span class="hljs-number">1</span>:<br>        y_hat=y_hat.<span class="hljs-built_in">argmax</span>(axis=<span class="hljs-number">1</span>)<br>    cmp=y_hat.<span class="hljs-built_in">type</span>(y.dtype)==y<br>    return <span class="hljs-built_in">float</span>(cmp.<span class="hljs-built_in">type</span>(y.dtype).<span class="hljs-built_in">sum</span>())<br><br>def <span class="hljs-built_in">gpu_acc</span>(model,data_iter,device=None):<br>    acc=<span class="hljs-number">0</span><br>    num=<span class="hljs-number">0</span><br>    if <span class="hljs-built_in">isinstance</span>(net,nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()<br>        if not device:<br>            device=<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.<span class="hljs-built_in">parameters</span>())).device<br>    with torch.<span class="hljs-built_in">no_grad</span>():<br>        for X,y in data_iter:<br>            if <span class="hljs-built_in">isinstance</span>(X,list):<br>                # 这里的if仅仅是为之后bert微调所需的<br>                X=[x.<span class="hljs-built_in">to</span>(device) for x in X]<br>            else:<br>                X=X.<span class="hljs-built_in">to</span>(device)<br>            y=y.<span class="hljs-built_in">to</span>(device)<br>            acc+=<span class="hljs-built_in">accuracy</span>(<span class="hljs-built_in">net</span>(X),y)<br>            num+=y.<span class="hljs-built_in">numel</span>()<br>    return acc/num<br><br><br><br>def <span class="hljs-built_in">trainer</span>(net,train_iter,test_iter,num_epoches,lr,device):<br>    <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;用GPU训练模型&#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br>    #初始化参数<br>    def <span class="hljs-built_in">init_weights</span>(m):<br>        if <span class="hljs-built_in">type</span>(m)==nn.Linear or <span class="hljs-built_in">type</span>(m)==nn.Conv2d:<br>            nn.init.<span class="hljs-built_in">xavier_uniform_</span>(m.weight)<br>            #nn.init.<span class="hljs-built_in">normal_</span>(m.weight,std=<span class="hljs-number">0.01</span>)<br>    net.<span class="hljs-built_in">apply</span>(init_weights)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;training on&#x27;</span>,device)<br>    <span class="hljs-built_in">print</span>(torch.cuda.<span class="hljs-built_in">get_device_name</span>(<span class="hljs-number">0</span>))<br>    net.<span class="hljs-built_in">to</span>(device)<br>    optimizer=torch.optim.<span class="hljs-built_in">SGD</span>(net.<span class="hljs-built_in">parameters</span>(),lr=lr)<br>    loss=nn.<span class="hljs-built_in">CrossEntropyLoss</span>()<br><br>    train_acc=[]<br>    test_acc=[]<br>    loss_epoches=[]<br>    for epoch in <span class="hljs-built_in">range</span>(num_epoches):<br>        temp_train_acc=<span class="hljs-number">0</span><br>        loss_epoch=<span class="hljs-number">0</span><br>        num=<span class="hljs-number">0</span><br>        net.<span class="hljs-built_in">train</span>()<br>        for i,(X,y) in <span class="hljs-built_in">enumerate</span>(train_iter):<br>            optimizer.<span class="hljs-built_in">zero_grad</span>()<br>            X,y=X.<span class="hljs-built_in">to</span>(device),y.<span class="hljs-built_in">to</span>(device)<br>            with torch.<span class="hljs-built_in">no_grad</span>():<br>                temp_train_acc+=<span class="hljs-built_in">accuracy</span>(<span class="hljs-built_in">net</span>(X),y)<br>            y_hat=<span class="hljs-built_in">net</span>(X)<br>            l=<span class="hljs-built_in">loss</span>(y_hat,y)<br>            l.<span class="hljs-built_in">backward</span>()<br>            optimizer.<span class="hljs-built_in">step</span>()<br>            loss_epoch+=l*X.shape[<span class="hljs-number">0</span>]<br>            num+=y.<span class="hljs-built_in">numel</span>()<br>        train_acc.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">float</span>(temp_train_acc/num))<br>        test_acc.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">gpu_acc</span>(net,test_iter))<br>        loss_epoches.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">float</span>(loss_epoch)/num)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;本轮&#123;&#125;训练的结果，test—acc:&#123;&#125;,train-acc:&#123;&#125;,train-loss:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">str</span>(epoch+<span class="hljs-number">1</span>),<span class="hljs-built_in">str</span>(test_acc[-<span class="hljs-number">1</span>]),<span class="hljs-built_in">str</span>(train_acc[-<span class="hljs-number">1</span>]),<span class="hljs-built_in">str</span>(loss_epoches[-<span class="hljs-number">1</span>])))<br>    import matplotlib.pyplot as plt<br>    plt.<span class="hljs-built_in">plot</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>+num_epoches)),test_acc,label=<span class="hljs-string">&#x27;test_acc&#x27;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>+num_epoches)),train_acc,label=<span class="hljs-string">&#x27;train_acc&#x27;</span>)<br>    plt.<span class="hljs-built_in">plot</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>+num_epoches)),loss_epoches,label=<span class="hljs-string">&#x27;loss&#x27;</span>)<br>    plt.<span class="hljs-built_in">legend</span>()<br>    plt.<span class="hljs-built_in">show</span>()<br><br><br>lr,num_epoches=<span class="hljs-number">0.2</span>,<span class="hljs-number">20</span><br><br><br>device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cuda:0&quot;</span> if torch.cuda.<span class="hljs-built_in">is_available</span>() else <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-built_in">trainer</span>(net,train_iter,test_iter,num_epoches,lr,device)<br></code></pre></td></tr></table></figure><h3 id="3-4-训练结果"><a href="#3-4-训练结果" class="headerlink" title="3.4 训练结果"></a>3.4 训练结果</h3><p>（秀一下3090～）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4cd34878a29824c19cd3aac85848a352_1440w.jpg" alt="img"></p><p>image-20220916222831979</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-58a495a63d7aefe991d208f30478b310_1440w.jpg" alt="img"></p><p>image-20220916222841744</p><p>CS229中关于模型选择中的bias- variance trade off在这里依旧适用</p><h3 id="0x04-Discussion-and-conclusion"><a href="#0x04-Discussion-and-conclusion" class="headerlink" title="0x04 Discussion and conclusion"></a>0x04 Discussion and conclusion</h3><h3 id="4-1-module和sequential之间的区别？"><a href="#4-1-module和sequential之间的区别？" class="headerlink" title="4.1 module和sequential之间的区别？"></a>4.1 module和sequential之间的区别？</h3><p><a href="https://stackoverflow.com/questions/68606661/what-is-difference-between-nn-module-and-nn-sequential">What is difference between nn.Module and nn.Sequential</a></p><p>使用module定义模型的时候，我们需要额外def forward，对于卷积神经网络这种按照顺序的结构并不需要关心，所以前向传播是按照sequential进行的，但是在后面的学习中更现代化的CNN或者RNN等会进行输出的迁移，这个时候使用module会更好</p><h3 id="4-2-归一化-normalization和标准化standardization之间的区别？"><a href="#4-2-归一化-normalization和标准化standardization之间的区别？" class="headerlink" title="4.2 归一化 normalization和标准化standardization之间的区别？"></a>4.2 归一化 normalization和标准化standardization之间的区别？</h3><p>不太懂，先留着</p><p><a href="https://blog.csdn.net/qq_35290785/article/details/89322289">可以参考这个</a></p><p>还有<a href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/">这个</a></p><h3 id="4-3-如何写出规范的代码？"><a href="#4-3-如何写出规范的代码？" class="headerlink" title="4.3 如何写出规范的代码？"></a>4.3 如何写出规范的代码？</h3><p>多看别人的代码</p><p>看完自己敲出来</p><h3 id="4-4-为什么loss会大于1"><a href="#4-4-为什么loss会大于1" class="headerlink" title="4.4 为什么loss会大于1"></a>4.4 为什么loss会大于1</h3><p>虽然说看loss只需要下降就好了，但是还是不太明白为什么在mean之后还是大于1</p><p>在手敲出来上面的代码能够清晰的感受到，工程对于软件构建的重要性，可能需要先做规划？了解好API才能用别人的抽象好的工具实现更多的功能。虽然这是一个简单的wheel，但是在初始解的基础上可以再次优化</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
      <tag>李沐</tag>
      
      <tag>LeNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习 Vol1 ｜ 线性回归实现</title>
    <link href="/posts/815c2675.html"/>
    <url>/posts/815c2675.html</url>
    
    <content type="html"><![CDATA[<p>在CS229中主要关注的是一些传统机器学习的模型，包括广义线性模型（GLM）包含的Logistic Regression、Softmax Regression，生成模型的高斯判别式、Decision Tree、Support Vector Machine、浅层神经网络；侧重于对于理论的介绍。李沐课程中对于实际操作的代码实现过程给出了详细的介绍，是非常有意思的。其实整个学习过程的框架无论是简单的逻辑回归还是复杂的YOLO之类的代码框架其实都已经给好了，如何高效的利用框架所提供的抽象好的API来实现复杂的功能是值得思考的。</p><h3 id="0x00-Environment-build（with-conda）"><a href="#0x00-Environment-build（with-conda）" class="headerlink" title="0x00 Environment build（with conda）"></a>0x00 Environment build（with conda）</h3><p><a href="https://www.bilibili.com/video/BV1LT411F77M?spm_id_from=333.337.search-card.all.click&vd_source=a43f0eddfeb89c08a00be8485e811ab0">环境安装，性能测试与横向对比</a>中给出Pytorch安装的三种方式：1. 直接安装、2. conda环境、3. Docker安装。后面有时间可以借鉴一下思路来使用Docker管理环境而不是Conda。本次还是利用conda来管理环境</p><blockquote><p> ps：关于conda与docker管理环境之间的差异 – 个人看法 如果有更好的回答希望可以发送到 <a href="mailto:&#99;&#104;&#x65;&#110;&#x78;&#105;&#97;&#51;&#49;&#64;&#111;&#x75;&#116;&#x6c;&#111;&#111;&#x6b;&#x2e;&#x63;&#x6f;&#x6d;">&#99;&#104;&#x65;&#110;&#x78;&#105;&#97;&#51;&#49;&#64;&#111;&#x75;&#116;&#x6c;&#111;&#111;&#x6b;&#x2e;&#x63;&#x6f;&#x6d;</a> 交流！ 作为一个计算机小白，在最初接触Python的时候知道一个电脑中会存在不同版本的python，这个需要我们来进行不同环境的隔离，常见的有python的envs、或者conda管理工具。这样不同的python环境可以使用不同的package。 但是docker可以说是一个更高level的工具，实际上他是一个弱化的虚拟机，并不是从硬件层面来虚拟一整套冯诺依曼体系，而是利用部分硬件和软件调度的方式来降低虚拟机所需要的硬件资源而实现虚拟机的操作系统和主机操作系统之间隔离的目的。在这个过程中可以实现某种意义上的硬件方式的环境隔离。 两者可以说是风马牛，但是在管理python环境中却又有着异曲同工之妙，都能起到环境隔离的作用，但是实现的过程是不一样的： </p></blockquote><ol><li>conda是python的package的管理者，它为不同的python提供不同的package的路径。这也就是conda install的package值需要装一份就可以为所有环境使用（如果版本一致的话）</li><li>docker是操作系统层面的硬件管理者，它为不同的虚拟机提供硬件和软件资源</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 新建conda环境</span><br>conda create -n <span class="hljs-string">&#x27;datastudy&#x27;</span> python=<span class="hljs-number">3.9</span><br><br><span class="hljs-comment"># 设置一下pip镜像</span><br><span class="hljs-comment"># 清华镜像源https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</span><br>python -m pip install --upgrade pip<br>pip config <span class="hljs-built_in">set</span> <span class="hljs-keyword">global</span>.index-url https://pypi.tuna.tsinghua.edu.cn/simple<br><br><span class="hljs-comment"># 安装pytorch，https://pytorch.org/get-started/locally/</span><br><span class="hljs-comment"># 默认配置好CUDA</span><br><span class="hljs-comment"># ps：中文教程很多安装pytorch好复杂，其实很多东西官网都给好了</span><br>conda install pytorch torchvision torchaudio cudatoolkit=<span class="hljs-number">11.6</span> -c pytorch -c conda-forge<br><br><span class="hljs-comment"># 检查是否成功，</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-built_in">print</span>(torch.__version__)<br><span class="hljs-built_in">print</span>(torch.cuda.get_device_name(<span class="hljs-number">0</span>))<br><br><span class="hljs-comment"># 检查代码是否能跑，参考链接如下</span><br><span class="hljs-comment"># Summary: 使用PyTorch玩转MNIST </span><br><span class="hljs-comment"># Author:  Amusi</span><br><span class="hljs-comment"># Date:    2018-12-20 </span><br><span class="hljs-comment"># github:  https://github.com/amusi/PyTorch-From-Zero-To-One</span><br><span class="hljs-comment"># Reference: https://blog.csdn.net/victoriaw/article/details/72354307</span><br><br><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function<br><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><br><span class="hljs-comment"># Training settings</span><br>parser = argparse.ArgumentParser(description=<span class="hljs-string">&#x27;PyTorch MNIST Example&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--batch-size&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">64</span>, metavar=<span class="hljs-string">&#x27;N&#x27;</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;input batch size for training (default: 64)&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--test-batch-size&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">1000</span>, metavar=<span class="hljs-string">&#x27;N&#x27;</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;input batch size for testing (default: 1000)&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--epochs&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">10</span>, metavar=<span class="hljs-string">&#x27;N&#x27;</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;number of epochs to train (default: 10)&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--lr&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.01</span>, metavar=<span class="hljs-string">&#x27;LR&#x27;</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;learning rate (default: 0.01)&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--momentum&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, default=<span class="hljs-number">0.5</span>, metavar=<span class="hljs-string">&#x27;M&#x27;</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;SGD momentum (default: 0.5)&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--no-cuda&#x27;</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>, default=<span class="hljs-literal">False</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;disables CUDA training&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--seed&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">1</span>, metavar=<span class="hljs-string">&#x27;S&#x27;</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;random seed (default: 1)&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--log-interval&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">10</span>, metavar=<span class="hljs-string">&#x27;N&#x27;</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;how many batches to wait before logging training status&#x27;</span>)<br>args = parser.parse_args()<br>args.cuda = <span class="hljs-keyword">not</span> args.no_cuda <span class="hljs-keyword">and</span> torch.cuda.is_available()<br><br>torch.manual_seed(args.seed) <span class="hljs-comment">#为CPU设置种子用于生成随机数，以使得结果是确定的</span><br><span class="hljs-keyword">if</span> args.cuda:<br>    torch.cuda.manual_seed(args.seed)<span class="hljs-comment">#为当前GPU设置随机种子；如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。</span><br><br><br>kwargs = &#123;<span class="hljs-string">&#x27;num_workers&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;pin_memory&#x27;</span>: <span class="hljs-literal">True</span>&#125; <span class="hljs-keyword">if</span> args.cuda <span class="hljs-keyword">else</span> &#123;&#125;<br><span class="hljs-string">&quot;&quot;&quot;加载数据。组合数据集和采样器，提供数据上的单或多进程迭代器</span><br><span class="hljs-string">参数：</span><br><span class="hljs-string">dataset：Dataset类型，从其中加载数据</span><br><span class="hljs-string">batch_size：int，可选。每个batch加载多少样本</span><br><span class="hljs-string">shuffle：bool，可选。为True时表示每个epoch都对数据进行洗牌</span><br><span class="hljs-string">sampler：Sampler，可选。从数据集中采样样本的方法。</span><br><span class="hljs-string">num_workers：int，可选。加载数据时使用多少子进程。默认值为0，表示在主进程中加载数据。</span><br><span class="hljs-string">collate_fn：callable，可选。</span><br><span class="hljs-string">pin_memory：bool，可选</span><br><span class="hljs-string">drop_last：bool，可选。True表示如果最后剩下不完全的batch,丢弃。False表示不丢弃。</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>train_loader = torch.utils.data.DataLoader(<br>    datasets.MNIST(<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>,<br>                   transform=transforms.Compose([<br>                       transforms.ToTensor(),<br>                       transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))<br>                   ])),<br>    batch_size=args.batch_size, shuffle=<span class="hljs-literal">True</span>, **kwargs)<br>test_loader = torch.utils.data.DataLoader(<br>    datasets.MNIST(<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=transforms.Compose([<br>                       transforms.ToTensor(),<br>                       transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))<br>                   ])),<br>    batch_size=args.batch_size, shuffle=<span class="hljs-literal">True</span>, **kwargs)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, kernel_size=<span class="hljs-number">5</span>)<span class="hljs-comment">#输入和输出通道数分别为1和10</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, kernel_size=<span class="hljs-number">5</span>)<span class="hljs-comment">#输入和输出通道数分别为10和20</span><br>        <span class="hljs-variable language_">self</span>.conv2_drop = nn.Dropout2d()<span class="hljs-comment">#随机选择输入的信道，将其设为0</span><br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">320</span>, <span class="hljs-number">50</span>)<span class="hljs-comment">#输入的向量大小和输出的大小分别为320和50</span><br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(F.max_pool2d(<span class="hljs-variable language_">self</span>.conv1(x), <span class="hljs-number">2</span>))<span class="hljs-comment">#conv-&gt;max_pool-&gt;relu</span><br>        x = F.relu(F.max_pool2d(<span class="hljs-variable language_">self</span>.conv2_drop(<span class="hljs-variable language_">self</span>.conv2(x)), <span class="hljs-number">2</span>))<span class="hljs-comment">#conv-&gt;dropout-&gt;max_pool-&gt;relu</span><br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">320</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<span class="hljs-comment">#fc-&gt;relu</span><br>        x = F.dropout(x, training=<span class="hljs-variable language_">self</span>.training)<span class="hljs-comment">#dropout</span><br>        x = <span class="hljs-variable language_">self</span>.fc2(x)<br>        <span class="hljs-keyword">return</span> F.log_softmax(x)<br><br>model = Net()<br><span class="hljs-keyword">if</span> args.cuda:<br>    model.cuda()<span class="hljs-comment">#将所有的模型参数移动到GPU上</span><br><br>optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch</span>):<br>    model.train()<span class="hljs-comment">#把module设成training模式，对Dropout和BatchNorm有影响</span><br>    <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        <span class="hljs-keyword">if</span> args.cuda:<br>            data, target = data.cuda(), target.cuda()<br>        data, target = Variable(data), Variable(target)<span class="hljs-comment">#Variable类对Tensor对象进行封装，会保存该张量对应的梯度，以及对生成该张量的函数grad_fn的一个引用。如果该张量是用户创建的，grad_fn是None，称这样的Variable为叶子Variable。</span><br>        optimizer.zero_grad()<br>        output = model(data)<br>        loss = F.nll_loss(output, target)<span class="hljs-comment">#负log似然损失</span><br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-keyword">if</span> batch_idx % args.log_interval == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                <span class="hljs-number">100.</span> * batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item()))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">epoch</span>):<br>    model.<span class="hljs-built_in">eval</span>()<span class="hljs-comment">#把module设置为评估模式，只对Dropout和BatchNorm模块有影响</span><br>    test_loss = <span class="hljs-number">0</span><br>    correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:<br>        <span class="hljs-keyword">if</span> args.cuda:<br>            data, target = data.cuda(), target.cuda()<br>        data, target = Variable(data, volatile=<span class="hljs-literal">True</span>), Variable(target)<br>        output = model(data)<br>        test_loss += F.nll_loss(output, target).item()<span class="hljs-comment">#Variable.data</span><br>        pred = output.data.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] <span class="hljs-comment"># get the index of the max log-probability</span><br>        correct += pred.eq(target.data).cpu().<span class="hljs-built_in">sum</span>()<br><br>    test_loss = test_loss<br>    test_loss /= <span class="hljs-built_in">len</span>(test_loader) <span class="hljs-comment"># loss function already averages over batch size</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="hljs-built_in">format</span>(<br>        test_loss, correct, <span class="hljs-built_in">len</span>(test_loader.dataset),<br>        <span class="hljs-number">100.</span> * correct / <span class="hljs-built_in">len</span>(test_loader.dataset)))<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, args.epochs + <span class="hljs-number">1</span>):<br>        train(epoch)<br>        test(epoch)<br></code></pre></td></tr></table></figure><p>别人总结的 <a href="https://handbook.pytorch.wiki/index.html">Pytorch handbook</a>，看上去还不错</p><h3 id="0x01-Review-of-linear-regression"><a href="#0x01-Review-of-linear-regression" class="headerlink" title="0x01 Review of linear regression"></a>0x01 Review of linear regression</h3><p>线性回归是非常经典的模型，其求解方法并不是只有梯度下降一种，<a href="https://blog.tjdata.site/2022/01/29/CS229-01--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">CS229-01–线性模型</a>之前的blog中给出了相关的介绍，这里再重新复述一下和关于自己在其中新的思考</p><h3 id="1-1-记号和引言"><a href="#1-1-记号和引言" class="headerlink" title="1.1 记号和引言"></a>1.1 记号和引言</h3><p>假设每个<strong>sample</strong>包含<strong>feature（n*1）</strong>和<strong>label（1*1）</strong>，也就是$[(x^{(i)},y^{(i)}),i&#x3D;0:…:m]$,在这种情况下我们希望构建一个<strong>model</strong>（或者说一种<strong>function</strong>）来作为feature和label之间的<strong>映射</strong>关系，这样在一个新的feature到来的时候，可以做出了预测。这个<strong>model</strong>其实就是实例化的<strong>hypothesis</strong>，实例化的过程便是<strong>optimization</strong>确定模型<strong>parameters</strong>的过程，其中包括<strong>loss function</strong>与<strong>optim method</strong></p><h3 id="1-2-模型实现的过程"><a href="#1-2-模型实现的过程" class="headerlink" title="1.2 模型实现的过程"></a>1.2 模型实现的过程</h3><p>对于线性回归，所做出的<strong>hypothesis</strong>自然是线性的，这也是它属于广义线性模型的原因</p><p>对于优化过程，第一步是确定损失函数，第二步是利用损失函数来找到对应的$w,b$参数值使得最小</p><p>损失函数是用来描述预测值和实际值之间的差别，通常模型越好损失函数需要最小，这里采用的是欧氏距离作为损失函数，具体原因在1.3中解释</p><p>之后便是求解这个优化问题的方式，对于线性回归有两种，一种是利用矩阵的normal equation方法得到<strong>解析解</strong>；二是利用深度学习中常用的<strong>梯度下降（gradient descent）</strong>的<strong>启发式</strong>方法来得到近似解（ps：和做时刻表一样）</p><ul><li>normal equation，利用导数为0的极值点得到取值，推导过程略</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b600fae2bf4b07fecf9d4aa62de5f337_1440w.jpg" alt="img"></p><p>image-20220911092806762</p><ul><li>gradient descent，利用导数下降的方式来探索的接近极值点</li></ul><p>线性模型的梯度比较好求解</p><h3 id="1-3-模型解释"><a href="#1-3-模型解释" class="headerlink" title="1.3 模型解释"></a>1.3 模型解释</h3><p>机器学习最方便的就是可解释性。首先我们的假设中认为features和labels之间是关于线性分布，但是通常情况下总会有各种各样的因素也就是高斯分布的假设，所以如果有<strong>Ground- truth</strong>的话，那必然是，主义嗷对于不同</p><p>我们可以使用<strong>最大似然法（maximum likelihood）</strong> 的方式来推导我们想要什么？抛弃传统距离的概率，我们用概率的方式来表示.（我们的目的都是预测值和真实值相接近，可以用距离、也可以用分布概率）</p><p>由上我们可以看出预测值的概率值（不是概率分布！！）</p><p>我们希望所有的数据集中的概率都尽可能的大，所以</p><p>这个时候就可以求导得到等价于1.2中损失函数的式子!</p><h3 id="1-4-回顾过程"><a href="#1-4-回顾过程" class="headerlink" title="1.4 回顾过程"></a>1.4 回顾过程</h3><p>1.1 给出为什么要这么做、1.2 给出理论推导过程、1.3给出模型的解释。这里一个困惑的点在于loss function是先有还是先可以被解释的。我的个人理解是它先有欧氏距离的方式，后面随着发展才逐渐利用概率方式解释，同时这种解释的范式我们也可以推广给其他模型。</p><p>比如在逻辑回归中，并不是给出假设就给出损失函数，而是利用概率方式来<strong>推导（！！）</strong>得到损失函数，这里常用的方式是MLP和MAE或者其他的方式，在选择合适的优化算法来寻找参数</p><p>不同于机器学习目的是得到一个有效的模型，传统的统计分析会给出参数估计的优劣以及检验是否合理，并就得到的参数给出自变量和因变量之间的关系。</p><h3 id="0x02-Manual-implementation-process（eight-step）"><a href="#0x02-Manual-implementation-process（eight-step）" class="headerlink" title="0x02  Manual implementation process（eight step）"></a>0x02  Manual implementation process（eight step）</h3><p>接下来就是利用代码实现上述功能，这里参考<a href="https://zh-v2.d2l.ai/chapter_linear-networks/index.html">李沐老师的d2l</a>，在最开始软件模块的设计中就已经给出来代码的框架。</p><h3 id="2-1-生成数据集-读取数据集"><a href="#2-1-生成数据集-读取数据集" class="headerlink" title="2.1 生成数据集&#x2F;读取数据集"></a>2.1 生成数据集&#x2F;读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第一步，生成包含随机噪音的数据</span><br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">synthetic_data</span>(<span class="hljs-params">w,b,num_examples</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;生成包含随机噪音的线性回归数据&#x27;&#x27;&#x27;</span><br>    X=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,(num_examples,<span class="hljs-built_in">len</span>(w)))<br>    y=torch.matmul(X,w)+b<br>    y+=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.01</span>,y.shape)<br>    <span class="hljs-keyword">return</span> X,y.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br><br>true_w=torch.tensor([<span class="hljs-number">2</span>,-<span class="hljs-number">3.4</span>])<br>true_b=<span class="hljs-number">4.2</span><br>features,labels=synthetic_data(true_w,true_b,<span class="hljs-number">1000</span>)<br></code></pre></td></tr></table></figure><h3 id="2-2-数据观察与预处理（暂无）"><a href="#2-2-数据观察与预处理（暂无）" class="headerlink" title="2.2 数据观察与预处理（暂无）"></a>2.2 数据观察与预处理（暂无）</h3><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># 第二步，简单的观察<span class="hljs-built_in">feature</span>与<span class="hljs-built_in">labels</span>之间的关系<br># 本文中有<span class="hljs-built_in">feature</span>为（<span class="hljs-number">1000</span>，<span class="hljs-number">2</span>），因此可以看第二个特征和<span class="hljs-built_in">labels</span>之间的关系<br>import matplotlib.pyplot as plt<br>plt.scatter(<span class="hljs-built_in">features</span>[:,<span class="hljs-number">1</span>],<span class="hljs-built_in">labels</span>)<br></code></pre></td></tr></table></figure><h3 id="2-3-数据迭代器（Dataloader）"><a href="#2-3-数据迭代器（Dataloader）" class="headerlink" title="2.3 数据迭代器（Dataloader）"></a>2.3 数据迭代器（Dataloader）</h3><p>这里采用的小批量（batch）的梯度下降；在梯度下降的过程中最直接的是计算所有数据集的梯度进行更新，这样对于memory的压力比较大，因此可以一个epoch进行划分，划分不同的batch_size，再利用batch进行参数更新。</p><p>但是这样的疑问在于batch能代表整个训练集进行更新吗？所以这样子就需要训练很多次，在某种特殊情况下会震荡多次，所有也会有更多的优化算法，来添加噪音或者动量，提升优化算法的健壮性。</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># 第三部，对数据集进行迭代处理，这样可以保证在随机梯度下降过程中会慢慢转换进去<br>def data_iter(batch_size,<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>):<br>    num_examples=len(<span class="hljs-built_in">features</span>)<br>    <span class="hljs-built_in">indices</span>=list(<span class="hljs-built_in">range</span>(num_examples))<br>    <span class="hljs-built_in">random</span>.shuffle(<span class="hljs-built_in">indices</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,num_examples,batch_size):<br>        batch_indices=torch.tensor(<span class="hljs-built_in">indices</span>[i:<span class="hljs-built_in">min</span>(i+batch_size,num_examples)])<br>        yield  <span class="hljs-built_in">features</span>[batch_indices],<span class="hljs-built_in">labels</span>[batch_indices]<br># yield的作用这里的迭代的结果是逐步发出来的<br>batch_size=<span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> data_iter(batch_size,<span class="hljs-built_in">features</span>,<span class="hljs-built_in">labels</span>):<br>    <span class="hljs-built_in">print</span>(X,&#x27;\n&#x27;,y)<br>    <span class="hljs-built_in">break</span><br></code></pre></td></tr></table></figure><h3 id="2-4-初始化参数"><a href="#2-4-初始化参数" class="headerlink" title="2.4 初始化参数"></a>2.4 初始化参数</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 第四步，初始化参数</span><br><span class="hljs-attribute">w</span>=torch.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>.<span class="hljs-number">01</span>,size=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),requires_grad=True)<br><span class="hljs-attribute">b</span>=torch.zeros(<span class="hljs-number">1</span>,requires_grad=True)<br></code></pre></td></tr></table></figure><h3 id="2-5-定义模型（net）"><a href="#2-5-定义模型（net）" class="headerlink" title="2.5 定义模型（net）"></a>2.5 定义模型（net）</h3><p>其实这里net的输入参数应该是X和param，因为在后续的模型中肯定会有参数和训练样本的输入，但是计算方式和linreg并不是相似，如果输入参数规定好为w和b，在后续更改会比较麻烦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第五步，定义模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linreg</span>(<span class="hljs-params">X,w,b</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;linear regression&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># X M*n</span><br>    <span class="hljs-comment"># w n*1</span><br>    <span class="hljs-keyword">return</span> torch.matmul(X,w)+b<br></code></pre></td></tr></table></figure><h3 id="2-6-定义损失函数（loss）"><a href="#2-6-定义损失函数（loss）" class="headerlink" title="2.6 定义损失函数（loss）"></a>2.6 定义损失函数（loss）</h3><p>损失函数表征的是预测值和真实值之间的差距，因此函数的输入应该为预测值与真值，这样就可以被复用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第六步，定义损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">squared_loss</span>(<span class="hljs-params">y_hat,y</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;均方误差&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">return</span> (y_hat-y.reshape(y_hat.shape))**<span class="hljs-number">2</span>/<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><h3 id="2-7-定义优化算法（optim）"><a href="#2-7-定义优化算法（optim）" class="headerlink" title="2.7 定义优化算法（optim）"></a>2.7 定义优化算法（optim）</h3><p>这里还是使用给好的API实现，同时在优化过程中我们需要设置的除了批量梯度下降的batch-size之外，还需要确定学习率。这个在任何情况下都是需要设置的</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># 第七步，定义优化算法</span><br>def sgd(<span class="hljs-built_in">params</span>,lr,batch_size):<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">param</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">params</span>:<br>            <span class="hljs-built_in">param</span>-=lr*<span class="hljs-built_in">param</span>.grad/batch_size<br>            <span class="hljs-built_in">param</span>.grad.zero_()<br></code></pre></td></tr></table></figure><h3 id="2-8-完整的训练过程"><a href="#2-8-完整的训练过程" class="headerlink" title="2.8 完整的训练过程"></a>2.8 完整的训练过程</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 第八步，完整的训练过程</span><br><span class="hljs-attribute">lr</span>=0.03<br><span class="hljs-attribute">num_epoch</span>=3<br><span class="hljs-attribute">net</span>=linreg<br><span class="hljs-attribute">loss</span>=squared_loss<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epoch):<br>    <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> data_iter(batch_size,features,labels):<br>        <span class="hljs-attribute">l</span>=loss(net(X,w,b),y)<br>        l.sum().backward()<br>        sgd([w,b],lr,batch_size)<br>    with torch.no_grad():<br>        <span class="hljs-attribute">train_l</span>=loss(net(features,w,b),labels)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;epoch&#123;&#125;,loss&#123;&#125;&#x27;</span>.format(str(epoch+1),str(float(train_l.mean()))))<br></code></pre></td></tr></table></figure><h3 id="0x03-Automatic-implementation-process-（Pytorch）"><a href="#0x03-Automatic-implementation-process-（Pytorch）" class="headerlink" title="0x03 Automatic implementation process （Pytorch）"></a>0x03 Automatic implementation process （Pytorch）</h3><p>回顾上述的过程，为了2.8最后的训练过程，我们首先需要生成数据集和读取数据集；之后我们写来一个dataloader来将数据集转换为可迭代（iterable）的对象来方便后续的梯度下降，dataloader中我们需要设置数据集、batch-size、是否可以被打乱；后面定义模型、损失函数和优化算法，模型是需要根据param和x来得到预测值、损失函数需要根据预测值和真实值得到结果、优化算法需要根据学习率、batch size来进行参数更新</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>import torch<br><span class="hljs-keyword">from</span> torch.utils import data<br><span class="hljs-comment"># 第一步生成数据集</span><br><span class="hljs-attribute">true_w</span>=torch.tensor([2,-3.4])<br><span class="hljs-attribute">true_b</span>=4.2<br>features,<span class="hljs-attribute">labels</span>=synthetic_data(true_w,true_b,1000)<br><br><span class="hljs-comment"># 第二步读取数据集</span><br>def load_array(data_arrays,batch_size,<span class="hljs-attribute">is_train</span>=<span class="hljs-literal">True</span>):<br>    <span class="hljs-attribute">dataset</span>=data.TensorDataset(*data_arrays)<br>    return data.DataLoader(dataset,batch_size,<span class="hljs-attribute">shuffle</span>=is_train)<br><br><span class="hljs-attribute">batch_size</span>=10<br><span class="hljs-attribute">data_iter</span>=load_array((features,labels),batch_size)<br><span class="hljs-comment"># 这里data_iter的方式与之前的方式相似</span><br><span class="hljs-comment"># 第三步，可视化掠过</span><br><span class="hljs-comment"># 第四步，定义模型</span><br><span class="hljs-keyword">from</span> torch import nn<br><span class="hljs-attribute">net</span>=nn.Sequential(nn.Linear(2,1))<br>net[0].weight.data.normal_(0,0.01)<br>net[0].bias.data.fill_(0)<br><br><span class="hljs-attribute">loss</span>=nn.MSELoss()<br><span class="hljs-attribute">trainer</span>=torch.optim.SGD(net.parameters(),lr=0.03)<br><br><span class="hljs-attribute">num_epoch</span>=3<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epoch):<br>    <span class="hljs-keyword">for</span> X,y <span class="hljs-keyword">in</span> data_iter:<br>        <span class="hljs-attribute">l</span>=loss(net(X),y)<br>        trainer.zero_grad()<br>        l.backward()<br>        trainer.<span class="hljs-keyword">step</span>()<br>    <span class="hljs-attribute">l</span>=loss(net(features),labels)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;epoch&#123;&#125; loss is &#123;&#125;&#x27;</span>.format(str(epoch+1),str(float(l))))<br></code></pre></td></tr></table></figure><h3 id="0x04-Discussion"><a href="#0x04-Discussion" class="headerlink" title="0x04 Discussion"></a>0x04 Discussion</h3><p>在了解整个代码实现过程之后，才能更好的了解到后续的逻辑回归、图像深度学习等代码实现的方式，以及如何学习pytorch的结构</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>李沐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络 Vol5 ｜ 完整的 web 请求过程</title>
    <link href="/posts/22420aa8.html"/>
    <url>/posts/22420aa8.html</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>应用层协议 [计算机网络学习笔记 – 应用层协议](<a href="https://blog.tjdata.site/2022/06/18/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-02-Chapter02">https://blog.tjdata.site/2022/06/18/计算机网络-02-Chapter02</a> 应用层协议&#x2F;) 万维网 HTTP Proxy 电子游戏 SMTP POP3IMAP 局域网IP地址分配 DHCP 域名解析 DNS 网络下载 P2P 流媒体 CDN 运输层协议 [计算机网络学习笔记 – 运输层协议](<a href="https://blog.tjdata.site/2022/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-03-Chapter03">https://blog.tjdata.site/2022/06/24/计算机网络-03-Chapter03</a> 运输层协议&#x2F;) UDP TCP 网络层协议 [计算机网络学习笔记 – 网络层协议](<a href="https://blog.tjdata.site/2022/06/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-04-Chapter04">https://blog.tjdata.site/2022/06/28/计算机网络-04-Chapter04</a> 网络层协议&#x2F;) IP协议 - 数据层面 IP协议 - 控制层面 链路层协议 比较偏低层，不重视 以太网 物理层协议 完全底层，不重视 IEEE802.11</p><h3 id="0x02-完整的使用过程"><a href="#0x02-完整的使用过程" class="headerlink" title="0x02 完整的使用过程"></a>0x02 完整的使用过程</h3><p>问题描述：在学校，学生是如何通过校园网链接到网络，然后下载<a href="http://www.baidu.com的主页面的/">www.baidu.com的主页面的</a> 涉及到的协议：应用层的DHCP、DNS、HTTP；运输层的UDP；网络层的IP；链路层的以太网</p><h3 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h3><ul><li>User启动它的PC，可以使用一根以太网电缆或者WI-FI与学校的以太网交换机连接，学校的交换机也和学校的Router连接。   学校的Router与本地的ISP（可能是中国移动也有可能是其他的什么东西，原文中的历史comcast.net)连接，因此提供DNS服务，也就是说DNS服务器是在ISP的网络中</li><li>User为了连接网络，首先需要通过DHCP来获得自己的IP地址，因此它通过DHCP请求报文来获得自己的IP地址</li><li>请求报文放到目的端口67和请求端口68的UDP报文段，放到广播IP（255.255.255.255）和源IP地址（0.0.0.0）的IP数据报中     包含IP数据报防止道以太网帧中，目的MAC地址（FF.FF.FF.FF.FF.FF），源Mac地址就是User的PC的Mac地址，比如说是3c:a6:f6:05:15:2c</li><li>Router接受到对应的Mac地址后，从以太网Frame中抽取出IP数据报，然后获得UDP报文段，由此得到了DHCP的请求报文     假设Router中处理 DHCP请求报文的方式是使用CIDR，68.85.2.0&#x2F;24分配IP地址，因此分配68.85.2.101给PC、网关地址68.85.2.1和子网掩码68.85.2.0&#x2F;24的DHCPACK报文和DNS的地址，放到UDP报文段、IP数据报、以太网数据帧，发送回发出请求的PC的mac地址</li><li>⚠️返回的时候mac地址并不是广播的，在DHCP的ACK到达PC的时候，开始得到自己的IP地址和DNS的IP地址，可以开始上网了！</li><li>当User键入<a href="http://www.google.com之后,web浏览器生成一个tcp/">www.google.com之后，web浏览器生成一个TCP</a> socket开始发送HTTP request，这里需要得到<a href="http://www.google.com的ip地址/">www.google.com的IP地址</a></li><li>为了得到<a href="http://www.google.com的ip地址,需要经历dns查询报文,这个报文需要有dns服务器的地址(比如68.87.71.226)以及源ip地址(68.85.2.101),以及字符串(‘www.google.com/">www.google.com的IP地址，需要经历DNS查询报文，这个报文需要有DNS服务器的地址（比如68.87.71.226）以及源IP地址（68.85.2.101），以及字符串（‘www.google.com</a>‘)</li><li>为了得到学校网络的网关路由器，需要通过ARP得到网关路由器的Mac地址</li><li>得到网关路由器的地址之后，PC发出的Frame中的IP数据报的目的IP地址为DNS、源IP地址为PC地址、帧的目的地址是网关路由器的地址</li><li>网关路由器接受到Frame病抽取包含DNS查询的IP数据报，解析在重新原则路由器，再将IP数据报放置到链路层Frame中国发送     到达之后，重新返回给PC</li><li>得到<a href="http://www.google.com的ip地址之后,就可以进行交互了,需要tcp连接和http协议/">www.google.com的IP地址之后，就可以进行交互了，需要TCP连接和HTTP协议</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>web 请求</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Imagenet-classification-with-deep-convolutional-neural-networks</title>
    <link href="/posts/7500ff59.html"/>
    <url>/posts/7500ff59.html</url>
    
    <content type="html"><![CDATA[<p>LeNet和AlexNet是从传统手工特征提取SIFI、HOG等向深度卷积神经网络转变的过渡期。</p><p><a href="https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf</a></p><h2 id="0x01-Abstract"><a href="#0x01-Abstract" class="headerlink" title="0x01 Abstract"></a>0x01 Abstract</h2><p>训练一个deep convolutional nerual network来区分ImageNet的LSVRC-2010比赛中的120万张 high-resolution到1000个不同的class   （网络效果）在我们的test中，我们错误率从37.5%到17%的提升，显著的好于现有的SOTA   （网络结构）该neural network包括600万参数和65万参数，包括5个convolutional layers，顺序是1个max-pooling layers、3个fullyconnected layers、以及最终的1000个softmax   （训练过程）我们使用 non-saturating神经元和高效的GPU卷积实现，同时为了减少overfitting，我们使用最近一种新的regularization方法dropout</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>在object recognition中的关键方法是采用一些 <strong>machine learning</strong> 的方法；由此我们收集更多的<strong>dataset</strong>、研究更强的<strong>models</strong>、以及使用更好的训练<strong>techniques</strong>来防止 overfitting。的却在一些大量数据集的加持下，一些简单的识别任务可以非常轻松的解决。比如在MNIST数据集中错误已经和人相当，同时数据集小的情况也被认识到缺点，因此新的更大的数据集LabelMe和ImageNet被开发出来</p><p>为了从数百万的图片中学习数千数据集，我们需要一个新的模型。从一些论文中我们可以看到 deep convolutional nerual network在训练中是有效的，但是我们的数据集是如此之大以至于其中的prior knowledge并不能被人为获得，而是需要从数据集中得到【 owever, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t hav】；同时CNN的复杂度可以由深度和广度决定，同时其卷积天然的具有强有力的图片先验知识、同时因为卷积层存在其比feedforward neural network的参数要小但是性能并没有明显的下降</p><p>尽管CNN的有效，依旧很难在高分辨率的图像中进行训练，但是幸运的是现有的GPU可以高度有效2D的卷积操作来实现大范围的训练。在本文中采用两块GTX580 3GB，训练时间为5～6天</p><p>本文的贡献            The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly1. Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance.</p><h2 id="0x02-DataSet"><a href="#0x02-DataSet" class="headerlink" title="0x02 DataSet"></a>0x02 DataSet</h2><p>ImageNet</p><p>ILSVRC，其中常见的指标为top1和top5          </p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs tap">其中前<span class="hljs-number"> 5 </span>个错误率是测试图像中正确标签不在模型认为最可能的五个标签中的部分<br></code></pre></td></tr></table></figure><p>重采样256*256，对于非长方形的数据scale到相同的像素。我们并没有做其他与处理          </p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">4</span>.<span class="hljs-number">1</span> Data Augmentation<br></code></pre></td></tr></table></figure><h2 id="0x03-Architecture"><a href="#0x03-Architecture" class="headerlink" title="0x03 Architecture"></a>0x03 Architecture</h2><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-71c61019612460c312f7c26fe6916afc_1440w.jpg" alt="img"></p><p>image_1661565390730_0</p><h3 id="3-1-ReLU-Nonlinearity"><a href="#3-1-ReLU-Nonlinearity" class="headerlink" title="3.1 ReLU  Nonlinearity"></a>3.1 ReLU  Nonlinearity</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">常见的激活函数<span class="hljs-built_in">f</span>(<span class="hljs-attribute">x</span>)=<span class="hljs-built_in">tanh</span>(x)或者<span class="hljs-built_in">f</span>(x)=(<span class="hljs-number">1</span>+e^&#123;-x&#125;)^-<span class="hljs-number">1</span>属于saturating非线性神经元<br>而ReLU函数<span class="hljs-built_in">f</span>(<span class="hljs-attribute">x</span>)=<span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>,x)属于 non-saturating非线性神经元<br>后者速度要显著快于前者,这个对于大训练集的训练是非常有效的<br>同样我们并不是第一个去考虑替换激活函数，比如 有人尝试使用<span class="hljs-built_in">f</span>(<span class="hljs-attribute">x</span>)=|<span class="hljs-built_in">tanh</span>(x)|<br></code></pre></td></tr></table></figure><h3 id="3-2-Training-on-Multiple-GPUs"><a href="#3-2-Training-on-Multiple-GPUs" class="headerlink" title="3.2 Training on Multiple GPUs"></a>3.2 Training on Multiple GPUs</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs">如何使用不同的GPU进行训练<br>GPU存在的问题是3GB的显存并不够完整数据的网络在其中训练（只能包含12万参数），因此我们将网络划分在两个GPU中训练，整个过程只有效<br></code></pre></td></tr></table></figure><h3 id="3-3-Local-Response-Normalization-card"><a href="#3-3-Local-Response-Normalization-card" class="headerlink" title="3.3 Local Response Normalization card"></a>3.3 Local Response Normalization card</h3><p>ReLU中并不需要输入 normalization来方式神经元 saturating，只需要发生正样本则会产生训练结果，但是我们仍然发现下面的 normalization对于泛化性能是有效  </p><p>  k、n、、都是超参数，取值分别为2、5、1e-4、0.75，这是由训练集表现来确定的</p><h3 id="3-4-Overlapping-Poolingcard"><a href="#3-4-Overlapping-Poolingcard" class="headerlink" title="3.4 Overlapping Poolingcard"></a>3.4 Overlapping Poolingcard</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">一般来说两个pooling是不重叠的，但是这里采用了一种对传统的pooling改进的方式，效果很好<br></code></pre></td></tr></table></figure><h3 id="3-5-Overall-Architecture"><a href="#3-5-Overall-Architecture" class="headerlink" title="3.5 Overall Architecture"></a>3.5 Overall Architecture</h3><p>完整的网络结构如图所示，借用(动手学深度学习-现代卷积神经网络-AlexNet)[<a href="https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id11]">https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id11]</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-14a302cd39b8be7fe3b5af6eebbdeabb_1440w.jpg" alt="img"></p><p>image_1661567510569_0</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs routeros">```<br>          AlexNet(<br>            (features): Sequential(<br>              (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))<br>              (1): ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>              (2): MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2, <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">ceil_mode</span>=<span class="hljs-literal">False</span>)<br>              (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br>              (4): ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>              (5): MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2, <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">ceil_mode</span>=<span class="hljs-literal">False</span>)<br>              (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>              (7): ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>              (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>              (9): ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>              (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>              (11): ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>              (12): MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2, <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">ceil_mode</span>=<span class="hljs-literal">False</span>)<br>            )<br>            (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))<br>            (classifier): Sequential(<br>              (0): Dropout(<span class="hljs-attribute">p</span>=0.5, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br>              (1): Linear(<span class="hljs-attribute">in_features</span>=9216, <span class="hljs-attribute">out_features</span>=4096, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>              (2): ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>              (3): Dropout(<span class="hljs-attribute">p</span>=0.5, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br>              (4): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=4096, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>              (5): ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br>              (6): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=1000, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>            )<br>          )<br>  ```<br>class AlexNet(nn.Module):<br>    def __init__(self, <span class="hljs-attribute">num_classes</span>=1000, <span class="hljs-attribute">init_weights</span>=<span class="hljs-literal">False</span>):<br>        super(AlexNet, self).__init__()<br>        self.features = nn.Sequential(<br>            nn.Conv2d(3, 64, <span class="hljs-attribute">kernel_size</span>=11, <span class="hljs-attribute">stride</span>=4, <span class="hljs-attribute">padding</span>=2),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>            nn.Conv2d(64, 192, <span class="hljs-attribute">kernel_size</span>=5, <span class="hljs-attribute">padding</span>=2),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>            nn.Conv2d(192, 384, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(384, 256, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(256, 256, <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">stride</span>=2),<br>        )<br>        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))<br>        self.classifier = nn.Sequential(<br>            nn.Dropout(),<br>            nn.Linear(256 * 6 * 6, 4096),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.Dropout(),<br>            nn.Linear(4096, 4096),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.Linear(4096, num_classes),<br>        )<br>        <span class="hljs-keyword">if</span> init_weights:<br>            self._initialize_weights()<br><br>    def forward(self, x):<br>        x = self.features(x)<br>        x = torch.flatten(x, <span class="hljs-attribute">start_dim</span>=1)<br>        x = self.classifier(x)<br>        return x<br><br>    def _initialize_weights(self):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>            <span class="hljs-keyword">if</span> isinstance(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, <span class="hljs-attribute">mode</span>=<span class="hljs-string">&#x27;fan_out&#x27;</span>, <span class="hljs-attribute">nonlinearity</span>=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>                <span class="hljs-keyword">if</span> m.bias is <span class="hljs-keyword">not</span> None:<br>                    nn.init.constant_(m.bias, 0)<br>            elif isinstance(m, nn.Linear):<br>                nn.init.normal_(m.weight, 0, 0.01)<br>                nn.init.constant_(m.bias, 0)<br></code></pre></td></tr></table></figure><p>`&#96;&#96;</p><p>特点          </p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs autoit">因为在两块GPU上运行，因此网络结构被一分为二<br>五层 convolutional layers加上三层 fullyconnected layers<br><span class="hljs-number">2</span>，<span class="hljs-number">4</span>，<span class="hljs-number">5</span>层仅与自己之前的核有关系，也就是仅与自己这个GPU前一层训练的输出有关，第三层与前一层的两个GPU有关，在通道上做了一层融合<br><span class="hljs-keyword">Local</span> Response Normalization运用在第一层和第二层<br>每一层都使用了 ReLu<br></code></pre></td></tr></table></figure><h2 id="0x04-Reducing-overfitting"><a href="#0x04-Reducing-overfitting" class="headerlink" title="0x04 Reducing overfitting"></a>0x04 Reducing overfitting</h2><h3 id="4-1-Data-Augmentation"><a href="#4-1-Data-Augmentation" class="headerlink" title="4.1 Data Augmentation"></a>4.1 Data Augmentation</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">这里的输入是256<span class="hljs-number">*256</span>，但是网络的输入是224<span class="hljs-number">*224</span>，因为为了扩充数据集的大小，在之后的训练过程中可以随机选择，这样可以得到2^<span class="hljs-attribute">11</span>=2048倍的扩充<br>另外一种方法是使用PCA对RGB进行修正，是的数据的数量得到增多，最终的效果也是成功的<br></code></pre></td></tr></table></figure><h3 id="4-2-dropout"><a href="#4-2-dropout" class="headerlink" title="4.2 dropout"></a>4.2 dropout</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">0</span>.<span class="hljs-number">5</span>的概率将一些神经元的输出设置为<span class="hljs-number">0</span>，可以有效的降低过拟合<br></code></pre></td></tr></table></figure><h2 id="0x05-Details-of-learning"><a href="#0x05-Details-of-learning" class="headerlink" title="0x05 Details of learning"></a>0x05 Details of learning</h2><p>SGD，随机梯度下降，增加动量的选项   初始化参数，使用均值为0、方差为0.01的高斯随机变量初始化权重参数   LearningRate设置为0.01</p><h2 id="0x06-Results"><a href="#0x06-Results" class="headerlink" title="0x06 Results"></a>0x06 Results</h2><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f64251eb38a7f703c265e55ee91e60d2_1440w.jpg" alt="img"></p><p>image_1661568191988_0</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5560703391f869113fc5bb7d78069f87_1440w.jpg" alt="img"></p><p>image_1661568198286_0</p><h2 id="0x07-参考别人的复现"><a href="#0x07-参考别人的复现" class="headerlink" title="0x07 参考别人的复现"></a>0x07 参考别人的复现</h2><p>to be continued ~~</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Paper 阅读</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GUI 的历史和 macOS 窗口机制</title>
    <link href="/posts/fc40e199.html"/>
    <url>/posts/fc40e199.html</url>
    
    <content type="html"><![CDATA[<p>本次主要想总结一些目前使用的GUI界面，以及macOS界面中的窗口和应用程序的概念。首先对于一个新的东西了解之前是采用浏览引擎，从百度到Google，或者新生代的duckduckgo等等；但是因为互联网的垃圾太多，自己去寻找的过程也是挑挑拣拣的过程，直到发现wiki 百科。它可以更清晰的说明一个领域</p><h3 id="0x01-引言：从Vscode引起的macOS应用和窗口的疑惑"><a href="#0x01-引言：从Vscode引起的macOS应用和窗口的疑惑" class="headerlink" title="0x01 引言：从Vscode引起的macOS应用和窗口的疑惑"></a>0x01 引言：从Vscode引起的macOS应用和窗口的疑惑</h3><p>macOS的文档（Document）、窗口（window）和应用程序（application）的关系之前一直分不清楚，使用的时候能感受到和window不一样的地方，但是一直不知道区别在哪里。直到使用vs code对于默认打开文件夹的疑惑。</p><p>事情是这样的，我的vs code每次打开的时候都不会显示上次打开的文件夹，在网上搜索设置的时候看到可以在</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">windows.restore</span> = <span class="hljs-string">&#x27;one&#x27;</span> // 可以打开上次关闭的文件夹<br></code></pre></td></tr></table></figure><p>但是如果我使用左上角的红 ’x‘关闭，还是会退出文件夹，但是并没有关闭vs code（因为指示灯还是开着）；如果使用dock内的vs code’退出‘则可以关闭vs code并在下次打开的时候正常出现文件夹</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c36f803ae8c41d1aa3aa36cdd92b5b90_1440w.jpg" alt="img"></p><p>7151660891268_.pic</p><h3 id="0x02-GUI的发展历史"><a href="#0x02-GUI的发展历史" class="headerlink" title="0x02 GUI的发展历史"></a>0x02 GUI的发展历史</h3><p>我们使用鼠标（mouse）进行交互，使用单击（click）图标启动程序，使用图形控件（graphical control）操作屏幕上的各种窗口（window），这些都和图形操作有关，<a href="https://zh.wikipedia.org/wiki/%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2">这里参照wiki百科–GUI</a></p><h3 id="2-1-没有计算机的时代"><a href="#2-1-没有计算机的时代" class="headerlink" title="2.1 没有计算机的时代"></a>2.1 没有计算机的时代</h3><p>1930年，Vannevar Bush首次编写一种称为 Memex的设备，设想像一个桌子，上面有两个触摸屏图形显示器、一个键盘和一个扫描仪，允许用户使用与超链接的工作方式来访问所有的人类知识。注意这个时候并没有数字计算机，所以这个想法并没有得到任何讨论，<a href="https://www.ruanyifeng.com/blog/2007/11/memex.html">《信息机器Memex》–阮一峰</a></p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4e5fff3af67a4d6c0a4f13406d7cc140_1440w.jpg" alt="img"></p><p>bg2007112201</p><p>这种机器内部用微缩胶卷（microfile）存储信息，也就是自动翻拍，可以不断往里面添加新的信息；桌面上有阅读屏，用来放大阅读微缩胶卷；还有许多个按钮，每一个按钮代表一个主题，只要按一下，相应的微缩胶卷就会显示出来。每一个胶卷内部还记录着相关的其他胶卷的编号，可以方便地切换，形成同主题阅读</p><h3 id="2-2-Douglas-Englebart-–-诞生初期"><a href="#2-2-Douglas-Englebart-–-诞生初期" class="headerlink" title="2.2 Douglas Englebart –  诞生初期"></a>2.2 Douglas Englebart –  诞生初期</h3><p>1968年，被称为GUI之父的Douglas Englebart （道格拉斯·恩格尔巴特）在NACA（NASA前身）在某些开车上班的日子EMO的时候突然想到，作为一名工程师，他真正的使命不是从事可能只会让少数人受益的小项目。因此回顾布什的文章来思考如何建造一台可以增强人类智力的机器，在战争时期担任雷达操作员，主要设想围绕阴极射线管构建显示系统；在1962年在一篇<a href="http://www.invisiblerevolution.net/engelbart/full_62_paper_augm_hum_int.html">《AUGMENTING HUMAN INTELLECT: A Conceptual Framework》</a>提出认为数字计算机可以提供最快的方法来提高一个人处理复杂问题情况的能力，获得理解以适应他的特殊需求，并得到问题的解决方案，他也设想<strong>计算机不是人类智力的替代品，而是增强智力的工具</strong>，比如建筑师使用使用CAD来设计建筑物，而不是机器自己设计建筑物</p><p>这个时候时间点还是1962年，人们采用的计算机的体积非常巨大，通常用户会使用打孔纸带和他们进行交互，然后计算机在数小时或者数天之后输出结果。但是人们的想象力并不会拘束与时代！</p><p>因为这是一项全新的技术，Englebart的演示使用电视摄像机对准他的脸、手和正在观看的小显示屏，用来给人们展示一些全新的概念。这个显示系统基于矢量图形技术，可以在同一屏幕上显示文本和实线，受限于内存大小，它只能显示大写字母。Englebart采用的交互工具包括：一个标准打字机键盘、一个五键“和弦键盘”以及一个长方形三个按钮的盒子</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a59e953d8265b7bd499c609d2af44e45_1440w.jpg" alt="img"></p><p>2-NLSdisplay</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-49448f79adc455606d303d9ca196f428_1440w.jpg" alt="img"></p><p>3-NLSmouse</p><blockquote><p> 这个长方形的盒子、用着长电线连接到计算机，就是鼠标（mouse）；没有人知道谁先叫它鼠标的，但是它在当时出现，并一直保留到现在，在机械上，它与现代鼠标略有不同，因为连接到内部电位器的两个圆形轮子直接在桌面上滚动，而不是由单个鼠标球与滚轮摩擦来操纵。然而，对于最终用户来说，它的操作几乎与现代鼠标相同。其他输入设备也曾尝试过（如触摸屏和光笔），但用户测试发现鼠标是操作屏幕光标最自然的方式。今天仍然如此。 鼠标的发明的伴生 – 指针也出现了，在当时演示的系统中指针是一个箭头，大约是单个字符的高度，指向正上方，指针在最初被称为‘Bug’。但是这个称呼并没有传承下来，而是pointer</p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b3e8ffb4beae309e4d278cf003d530d9_1440w.jpg" alt="img"></p><p>4-NLSgui</p><p>在当时的演示中包括：超文本连接（hypertext linking）、全屏文档编辑（full screen document editing）、上下文操作（context- sensitive help）、网络文档协同（networked document collaboration）、电子邮件（email）、即时消息（instant message）、视频会议（video meeting）；但是由于视频系统的显示很难分辨发生了什么。</p><p>可惜没有钱来实现商业化</p><h3 id="2-3-PARC-–-施乐（Xerox）-复印机公司的自救"><a href="#2-3-PARC-–-施乐（Xerox）-复印机公司的自救" class="headerlink" title="2.3 PARC – 施乐（Xerox） 复印机公司的自救"></a>2.3 PARC – 施乐（Xerox） 复印机公司的自救</h3><p>当复印机厂商看到了computer带来的电子文档进行协作创作的时候，会担心自己的公司会在无纸化的进程中不可避免的消亡，这个时候他们需要做的是确保自己控制这项新的技术，因此在1970年成立了PARC（帕洛阿尔托研究中心），来使用五年的时间自由的做感兴趣的任何事情</p><p>第一件发明是激光打印机，是复印机业务的自然补充，因为打印需要一种更图形化的方式来让计算机开始准备文件，因为当时没有这样的计算机，它们方面来自己的计算机称为ALTO，它的显示屏的界面为606*808，每个像素都可以独立打开和关闭，但是只能显示固定的文本字符。但是具有一个键盘和更加现代化的Englebart鼠标，这个鼠标具有三个按钮，本身也变成了位图图像，并首次使用我们熟悉的对角线箭头形状，并可以根据正在执行的任务变成其他形状。Alto上软件开始非常粗糙，包括文件管理器（Norton commander）、图形文字处理器（Bravo，DOS的word前身）、位图图形编辑器（类似paint）；但是这些不同的应用程序需要一个一致的用户界面，因此它们发明了Smalltalk，第一个现代GUI</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d7331b7021ac9787318186b13b2bf9ee_1440w.gif" alt="img"></p><p>6-AltoFM</p><p>第二件，smalltalk是一种易于使用的编程语言和开发环境，也可以算得上是第第一个面向对象的编程语言，其中程序代码和数据可以封装到称为对象的单元中来进行复用而不用了解其中实现细节，在1974年具有雏形并不不断完善，Smalltalk是一个图形开发环境，类似于windows和visual studio之间的感觉。Smalltalk 中的各个窗口都包含在图形边框中，并在其下方背景的灰色图案中显得格外醒目。他们每个人在每个窗口的顶行都有一个标题栏，可用于识别窗口并在屏幕上移动它。与 BeOS 类似，标题栏并没有延伸到窗口的整个长度，而是从左上角开始，仅延伸到标题本身。窗口可以与屏幕上的其他窗口重叠，并且选定的窗口会将自身移动到“堆栈”的顶部。此时还发明了“图标”的概念——程序或文档的小图标表示，可以单击以运行或操作它们。弹出菜单也是同时发明的——用户单击鼠标按钮之一并分层，基于手头任务的图形菜单将出现在鼠标光标的最后位置。首次出现的还有滚动条、单选按钮和对话框。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-2b35792ab765f8e029079a063a042c62_1440w.jpg" alt="img"></p><p>7-AltoST</p><p>第三件便是Xerox star 8010文档，Smalltalk是如此优美，但是施乐公司管理层并没有允许其作为商业产品推销，而是在1981年推出了ALTO精简版Alto，最重要的是取消了重叠窗口的功能，但是对于公众来说太混乱了。但是太晚了，因为Apple Computer.Inc在1976年创立.</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6e8c755e81c3f60a43719ae5ca74399e_1440w.jpg" alt="img"></p><p>8-Star</p><h3 id="2-4-Apple-Steven-Jobs-推动商业化的典范，从Lisa到Macintosh"><a href="#2-4-Apple-Steven-Jobs-推动商业化的典范，从Lisa到Macintosh" class="headerlink" title="2.4 Apple&#x2F;Steven Jobs - - 推动商业化的典范，从Lisa到Macintosh"></a>2.4 Apple&#x2F;Steven Jobs - - 推动商业化的典范，从Lisa到Macintosh</h3><p>1976年，Steve Jobs和Steve Woznlak在车库内创立一家Apple Computer的小型创业公司，在推出饱受欢迎具有传统命令行界面的Apple II，因为其年轻的属性，资金充裕同时愿意冒险，在继承诸多Xerox PARC工程师之后，Apple的下一代计算机Lisa（Local Integrated Software Architecture）也因此改变，从商业用途的传统基于文本的命令行计算机直到转变为第一台采用GUI和鼠标的个人电脑。Lisa 界面的许多不同原型都在 Apple ]II上进行了模拟，包括一个基于任务的界面被称为“二十个问题”，因为它似乎需要很长时间才能让计算机做任何事情，以及一个类似的四列文件浏览器与 Smalltalk 一起出现的，后来在 NeXTstep 和 Mac OSX 中重新出现。Lisa 团队最终确定了一个基于图标的界面，其中每个图标都表示一个文档或一个应用程序，并开发了第一个下拉菜单栏，所有菜单都出现在屏幕的最顶行。</p><p>LISA中创新包括，选定菜单项旁边显示复选标记、键盘快捷键、固定高度的滚动条、垃圾桶、灰显的菜单。但是也将鼠标从三个键简化称为一个键，所以引入双击的概念。同时也提出图标来代替文件系统中的文件，并使用分层目录结构浏览这些文件</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-58d690f24a88d46629dc3d230942c6b7_1440w.gif" alt="img"></p><p>9-Lisamockup1</p><p>1983年发布的Lisa因为高昂的售价并没有成功，在1984年（也是IBM被打倒的一年），推出了Macintosh项目配备9英寸、128KB、没有多任务处理、低成本的图形计算机，并成功实现商业化。在1984年底销量开始下降、以及其他问题乔布斯在1985年9月17日离开苹果，前往NeXT</p><blockquote><p> Macintosh的名称来自于，公司员工喜欢的苹果品种（真的苹果）为mcIntosh，但是有一家音响公司为McIntosh，因此拼为Macintosh</p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ece42bf51b69c7632df6cd7ef907d079_1440w.gif" alt="img"></p><p>11-Mac1</p><p>1988 年发布了 NeXTSTEP，这是史蒂夫·乔布斯的 NeXT 计算机的新 GUI 和操作系统，这是他在 1985 年离开 Apple 后的第一个重大项目。NeXTSTEP 为其所有 GUI 组件引入了锐利的 3D 斜面外观，是第一个使用“X”符号表示关闭窗口小部件，并在左上角引入了垂直菜单条的想法，也可以在任何时候“撕掉”，以便用户可以随时离开特定菜单点在屏幕上。NeXTSTEP 也有一个位于屏幕任意一侧的 Dock（但默认位于右侧）。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-fea6b263d2bef58e96bb632142ae7ca1_1440w.gif" alt="img"></p><p>21-next</p><h3 id="2-5-Windows-更加商业化的GUI推广"><a href="#2-5-Windows-更加商业化的GUI推广" class="headerlink" title="2.5 Windows - - 更加商业化的GUI推广"></a>2.5 Windows - - 更加商业化的GUI推广</h3><p>1985年的windows1.0版本，比尔盖茨来源于VisiOn、Dos的word的交叉，包括彩色界面、具有常见的GUI组件滚动条、窗口控件小部件和菜单；但是不想LIsa或者Macintosh上的单个菜单栏，每个应用程序都有自己的菜单栏；同时采用平铺的窗口而不是可以重叠的窗口，但是后续会改进</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-519cd8c116df3586f765b95dadb02818_1440w.gif" alt="img"></p><p>14-win101</p><p>1987年，Windows2.0，采用现有的传统的重叠方法，增加了最大化和最小化窗口，也由此与Apple发生专利冲突</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d4db3ac14790d30b14201040c8f3ef4f_1440w.gif" alt="img"></p><p>19-win2</p><p>1990的3.0、1992年的3.1版本的发布，一致到windows95的巩固其在GUI操作系统的领先地位</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-50798b5a80098ab9c2d0098656ad1a22_1440w.gif" alt="img"></p><p>25-win95</p><h3 id="2-6-others"><a href="#2-6-others" class="headerlink" title="2.6 others"></a>2.6 others</h3><p>GUI界面也包括其他厂商的努力，比如GEM、Amiga Workbench、GEOS、Acorn等等</p><h3 id="0x03-Mac中的窗口、文档的概念"><a href="#0x03-Mac中的窗口、文档的概念" class="headerlink" title="0x03 Mac中的窗口、文档的概念"></a>0x03 Mac中的窗口、文档的概念</h3><p><a href="https://www.zhihu.com/question/21143701/answer/2521552530">参考</a></p><p>在GUI的设计中面向用户的概念有三个：应用、窗口和文档。在三者之间的关系可以分为MDI、SDI和TDI</p><ul><li>MDI（multiple document interface），多份文档包含在窗口下，多个窗口保存在同一个应用</li><li>SDI（Single Document interface），每个窗口都是独立的应用，由OS自动调度。这也是windows的首创，因此<strong>一个窗口就是一份应用，也就是一个文档</strong></li><li>TDI（Tab document interface）比如chrome，术语两者的混合体</li></ul><p>每个应用可以打开一个或者多个窗口，每个窗口可以打开一份或者多份文档。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-648071d9578b25be319d330fd135dad3_1440w.jpg" alt="img"></p><p>image-20220819161527184</p><p>这个简单的概念在只有128KB的Macintosh上这一层抽象并不容易，这样多个窗口的共存可能需要多个应用的同步，因为当时的内存只能跑一个前台程序，因此需要传达给用户</p><ol><li>一个应用在逻辑上只能打开一个窗口</li><li>用户应当尽可能的停留在单个页面</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0de26ec90ae54d63677d64481eefa32a_1440w.jpg" alt="img"></p><p>img</p><p>所以Mac系统的红x对应的是<strong>关闭现有的窗口，以及其打开的文档，但是并没有退出应用</strong>，退出的话可以用dock里面的退出或者使用“command+Q”</p><p>而windows代表的操作系统让应用可以实现多开，逐渐弱化了窗口的概念，每份文档对应每个app，关闭文档也就是关闭对应的app</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f3c3da9cb3736fb1fbbf794a66f7ca5f_1440w.jpg" alt="img"></p><p>img</p><p>更对应的，在ios或者android、harmony中已经完全失去了文档的概念，只存在应用和窗口。</p><h3 id="0x04-使用注意"><a href="#0x04-使用注意" class="headerlink" title="0x04 使用注意"></a>0x04 使用注意</h3><p>macOS中窗口的概念由内存和算力限制而诞生，也因现有GUI发展而茁壮，在使用中注意区分application和window的区别，关闭window使用红‘x’、退出应用使用‘command+Q’；或许这就是window- server内存为什么这么高的原因吧</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b73199c3848008547680face1ada475a_1440w.jpg" alt="img"></p><p>image-20220819163754181</p><p><a href="https://jamesfriend.com.au/pce-js/">1984年Macintosh模拟</a></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p>PARC：Palo Alto Research Center，帕洛阿尔托研究中心</p><p>Lisa：（Local Integrated Software Architecture），但是显然是Jobs的女儿名字</p><p><a href="https://www.zhihu.com/question/21143701/answer/2521552530">https://www.zhihu.com/question/21143701/answer/2521552530</a></p><p><a href="https://arstechnica.com/features/2005/05/gui/">https://arstechnica.com/features/2005/05/gui/</a></p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GUI</tag>
      
      <tag>Apple</tag>
      
      <tag>桌面管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和程序基础 Vol8</title>
    <link href="/posts/fec84c31.html"/>
    <url>/posts/fec84c31.html</url>
    
    <content type="html"><![CDATA[<p>在前面稍微的完整了解的基础的线性表结构，知道数组、链表、队、栈、哈希表、堆之后；本来按照数据结构要学习高级数据结构字符串和二叉树，但是在学习KMP的时候对一些算法中的基本概念不清晰，同时需要对前一阶段中的排序、二分、双指针、优先队列、单调栈等进行总结，所以借此机会了解一下算法。算法中最基础的就是枚举或者说迭代，之后便是递归为基础的分治和回溯算法。</p><h2 id="0x01-枚举算法（Enumeration-Algorithm）"><a href="#0x01-枚举算法（Enumeration-Algorithm）" class="headerlink" title="0x01 枚举算法（Enumeration Algorithm）"></a>0x01 枚举算法（Enumeration Algorithm）</h2><h3 id="1-1-枚举算法简介"><a href="#1-1-枚举算法简介" class="headerlink" title="1.1  枚举算法简介"></a>1.1  枚举算法简介</h3><p>穷举算法，指的是按照问题的本身的性质，来列举出所有该问题可能存在的解，并在逐一枚举的过程中讲它们逐一和目标状态进行比较来得到满足问题要求的解</p><p>枚举算法的核心是需要列举问题的所有状态，并与目标状态进行比较。它的优点是，容易编程调试、算法的正确性容易证明；它的缺点是效率比较低，不适合求解规模较大的问题</p><h3 id="1-2-枚举算法的思路"><a href="#1-2-枚举算法的思路" class="headerlink" title="1.2 枚举算法的思路"></a>1.2 枚举算法的思路</h3><ol><li>确定枚举对象、 枚举范围和判断条件，并判断条件设立的正确性</li><li>一一枚举可能的情况并验证是否是问题的解</li><li>考虑提高枚举算法的效率</li></ol><h3 id="1-3-枚举算法的应用"><a href="#1-3-枚举算法的应用" class="headerlink" title="1.3  枚举算法的应用"></a>1.3  枚举算法的应用</h3><p><a href="https://leetcode.cn/problems/subsets/">LC78 子集</a></p><p>如果集合A的任意一个元素都是集合S的元素，则集合A事集合S的子集，枚举子集的方法很多，一种简单有效的枚举方法是二进制枚举子集算法</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def subsets(self, nums: List[<span class="hljs-keyword">int</span>]) -&gt; List[List[<span class="hljs-keyword">int</span>]]:<br>        <span class="hljs-keyword">res</span>=[]<br>        path=[]<br>        def backtracking(nums,<span class="hljs-built_in">index</span>):<br>            <span class="hljs-keyword">res</span>.<span class="hljs-keyword">append</span>(path[:])<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">index</span>&gt;=<span class="hljs-built_in">len</span>(nums):<br>                <span class="hljs-keyword">return</span> <br>            <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">index</span>,<span class="hljs-built_in">len</span>(nums)):<br>                path.<span class="hljs-keyword">append</span>(nums[i])<br>                backtracking(nums,i+<span class="hljs-number">1</span>)<br>                path.<span class="hljs-keyword">pop</span>()<br>        backtracking(nums,<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">res</span><br></code></pre></td></tr></table></figure><p>python 的位运算符；</p><p>&amp; ：and运算，参与运算的两个值相应位为1，则结果为1</p><p>｜：or 运算，只要对应的两个二位有一个为1，结果就为1</p><p>^:  XOR运算，两个对应的相异的时候，结果为1</p><p>～：not运算，</p><p>&lt;&lt; 左移运算符，</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">Solution</span>:</span><br><span class="hljs-class">    def subsets(<span class="hljs-title">self</span>, <span class="hljs-type">S</span>: <span class="hljs-type">List</span>[<span class="hljs-title">int</span>]) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[int]]:</span><br><span class="hljs-class">        sub_sets=[]</span><br><span class="hljs-class">        n=len(<span class="hljs-type">S</span>)</span><br><span class="hljs-class">        # 1&lt;&lt;n 相当于2^n次方，range(1&lt;&lt;<span class="hljs-title">n</span>)相当于0～2^n-1</span><br><span class="hljs-class">        for i in range(1&lt;&lt;<span class="hljs-title">n</span>):</span><br><span class="hljs-class">            sub_set=[]</span><br><span class="hljs-class">            for j in range(<span class="hljs-title">n</span>):</span><br><span class="hljs-class">                # &amp;1 相当于取最后一位</span><br><span class="hljs-class">                if i&gt;&gt;j&amp;1:</span><br><span class="hljs-class">                    sub_set.append(<span class="hljs-type">S</span>[<span class="hljs-title">j</span>])</span><br><span class="hljs-class">            sub_sets.append(<span class="hljs-title">sub_set</span>)</span><br><span class="hljs-class">        return sub_sets</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/maximal-square/">LC221 最大正方形</a></p><p>需要用到一些简单的DP</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs inform7">class Solution:<br>    def maximalSquare(self, matrix: List<span class="hljs-comment">[List<span class="hljs-comment">[str]</span>]</span>) -&gt; int:<br>        # 最大面积的正方形<br>        # 积分图的方式<br>        max_len=0          <br>        m,n=len(matrix),len(matrix<span class="hljs-comment">[0]</span>)<br>        dp=<span class="hljs-comment">[]</span><br>        for i in range(m):<br>            sub_dp=<span class="hljs-comment">[]</span><br>            for j in range(n):<br>                matrix<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span>=int(matrix<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span>)<br>                if matrix<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span>==1:<br>                    max_len=1<br>                sub_dp.append(matrix<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span>)<br>            dp.append(sub_dp)<br><br><br>        for i in range(1,m):<br>            for j in range(1,n):<br>                if matrix<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span>==1 and dp<span class="hljs-comment">[i-1]</span><span class="hljs-comment">[j]</span>&gt;=1 and dp<span class="hljs-comment">[i]</span><span class="hljs-comment">[j-1]</span>&gt;=1 and dp<span class="hljs-comment">[i-1]</span><span class="hljs-comment">[j-1]</span>&gt;=1:<br>                    dp<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span>=min(dp<span class="hljs-comment">[i-1]</span><span class="hljs-comment">[j-1]</span>,dp<span class="hljs-comment">[i-1]</span><span class="hljs-comment">[j]</span>,dp<span class="hljs-comment">[i]</span><span class="hljs-comment">[j-1]</span>)+1<br>                if dp<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span>&gt;max_len:<br>                    max_len=dp<span class="hljs-comment">[i]</span><span class="hljs-comment">[j]</span><br>        return max_len*max_len<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/count-primes/">LC204 计算质数</a></p><p>和以前在学校学习的就完全不一样，枚举最重要的不是要确定列举的对象，还需要确定剪枝的条件。所以本题可以有筛子的概念</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">countPrimes</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        is_prime = [<span class="hljs-literal">True</span>]*(n)<br>        ans = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>,n): <br>            <span class="hljs-keyword">if</span> is_prime[num]:<br>                ans+=<span class="hljs-number">1</span><br>                <span class="hljs-comment"># 右边界:因为数字最大是n-1 所以只需要到(n-1)//num 右边是开区间 所以+1</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,(n-<span class="hljs-number">1</span>)//num+<span class="hljs-number">1</span>):<br>                    is_prime[num*k]=<span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><h2 id="0x02-递归算法"><a href="#0x02-递归算法" class="headerlink" title="0x02 递归算法"></a>0x02 递归算法</h2><h3 id="2-1-递归简介"><a href="#2-1-递归简介" class="headerlink" title="2.1 递归简介"></a>2.1 递归简介</h3><p>是一种通过重复讲原问题分解为同类的子问题而解决的方法，在绝大数编程语言中，可以通过在函数中再次调用函数自身的方式来实现递归</p><p>简单的例子就是阶乘的计算</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs excel">def <span class="hljs-built_in">fact</span>(<span class="hljs-built_in">n</span>)<span class="hljs-symbol">:</span><br>    <span class="hljs-built_in">if</span> <span class="hljs-built_in">n</span>==<span class="hljs-symbol">0:</span><br>        return <span class="hljs-number">1</span><br>    return <span class="hljs-built_in">n</span>*<span class="hljs-built_in">fact</span>(<span class="hljs-built_in">n</span>-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>递归可以分为两个部分：</p><ol><li>（递推过程）先逐层向下调用自身，直到达到结束条件；指的是<strong>将原问题一层一层分解为与原问题形式相同、规模更小的字问题，直到达到结束条件时停止，此时返回最底层问题的解</strong></li><li>然后想上逐层返回结果，直到返回原问题的解；指的是<strong>从最底层字问题的解开始，逆向逐一回归，最终达到递推开始的原问题，直到返回原问题的解</strong></li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-be1d25f9c4aa77104cfb51ae7a4caa55_1440w.jpg" alt="img"></p><p>image-20220731230242978</p><h3 id="2-2-递推与数据归纳法"><a href="#2-2-递推与数据归纳法" class="headerlink" title="2.2  递推与数据归纳法"></a>2.2  递推与数据归纳法</h3><p>多米罗骨牌类似的数学归纳法的步骤是：</p><ol><li>证明当n&#x3D;&#x3D;b的时候，基本情况下成立</li><li>证明当n&gt;b的时候，n&#x3D;k成立的情况下，可以推导得出n&#x3D;k+1成立</li></ol><p>这个时候需要解决的递归就需要</p><ol><li>递归的终止条件</li><li>递归的过程</li><li>回归过程</li></ol><h3 id="2-3-递归的模版"><a href="#2-3-递归的模版" class="headerlink" title="2.3 递归的模版"></a>2.3 递归的模版</h3><h3 id="2-3-1-递归公式"><a href="#2-3-1-递归公式" class="headerlink" title="2.3.1 递归公式"></a>2.3.1 递归公式</h3><p>需要找到原问题分解称为子问题的规律，并且根据规律写出递推公式。这里的关键是需要找到原问题分解称为子问题的规律，并抽象称为递推公式。</p><p>注意，在思考递归公式的时候没有必要将整个递推过程和回归过程一层层的想清楚，这样可能还没有递推到栈底就已经绕晕了。重点在于想清楚n&#x3D;k到n&#x3D;k+1这个步骤，而不是n&#x3D;i到n&#x3D;i+1（i&#x3D;1:m）；也就是找到递归过程中的循环不变量（loop variant），先假设子问题解决了，再看如何将原问题分解</p><h3 id="2-3-2-终止条件"><a href="#2-3-2-终止条件" class="headerlink" title="2.3.2 终止条件"></a>2.3.2 终止条件</h3><p>递归的终止条件也叫做递归出口，在写出递推公式之后，就需要考虑递归的终止条件是什么。通常条件下，递归的终止条件是问题的边界值</p><h3 id="2-3-3-翻译成代码"><a href="#2-3-3-翻译成代码" class="headerlink" title="2.3.3 翻译成代码"></a>2.3.3 翻译成代码</h3><p>第一步，定义递归函数，明确函数意义、input和output</p><p>第二步，根据循环不变量来推论得到递归公式</p><p>第三步，明确递归的终止条件</p><p>第四步，伪代码</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">recursion</span></span>(大规模）：<br>    <span class="hljs-keyword">if</span> 终止条件：<br>        终止条件处理<br>    <span class="hljs-keyword">return</span> recursion（小规模）<br></code></pre></td></tr></table></figure><h3 id="2-3-4-递归的注意点"><a href="#2-3-4-递归的注意点" class="headerlink" title="2.3.4 递归的注意点"></a>2.3.4 递归的注意点</h3><p>避免栈溢出</p><p>避免重复运算</p><h3 id="2-4-递归的应用"><a href="#2-4-递归的应用" class="headerlink" title="2.4 递归的应用"></a>2.4 递归的应用</h3><p>递归在使用的过程需要明确你的loop variant是什么，在每次运行过程中循环不变量是什么</p><p><a href="https://leetcode.cn/problems/fibonacci-number/">LC509 斐波那契数列</a></p><p><a href="https://leetcode.cn/problems/climbing-stairs">LC70爬楼梯</a></p><p><a href="https://leetcode.cn/problems/reverse-string/">LC344 反转字符串</a></p><p><a href="https://leetcode.cn/problems/swap-nodes-in-pairs/">LC24 交换节点</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br>class Solution:<br>    def swapPairs(self, head: ListNode) -&gt; ListNode:<br>        <span class="hljs-keyword">if</span> <span class="hljs-attribute">head</span>==None <span class="hljs-keyword">or</span> head.<span class="hljs-attribute">next</span>==None:<br>            return head<br>        <span class="hljs-attribute">dummy</span>=ListNode(0,head)<br>        <span class="hljs-attribute">node1</span>=head<br>        <span class="hljs-attribute">res</span>=ListNode(0,head.next)<br>        <span class="hljs-attribute">node2</span>=head.next<br>        <span class="hljs-keyword">while</span>(node1!=None <span class="hljs-keyword">and</span> node2!=None):<br>            # 交换两者<br>            node1.<span class="hljs-attribute">next</span>=node2.next<br>            node2.<span class="hljs-attribute">next</span>=node1<br>            dummy.<span class="hljs-attribute">next</span>=node2<br><br>            # 更新node<br>            <span class="hljs-attribute">dummy</span>=node1<br><br>            <span class="hljs-keyword">if</span> node1.<span class="hljs-attribute">next</span>==None:<br>                break<br>            <span class="hljs-attribute">node1</span>=node1.next<br>            <span class="hljs-attribute">node2</span>=node1.next<br>        return res.next<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/pascals-triangle-ii/">LC119 杨辉三角</a></p><p><a href="https://leetcode.cn/problems/maximum-depth-of-binary-tree/">LC104 二叉树最大深度</a></p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs swift"># <span class="hljs-type">Definition</span> <span class="hljs-keyword">for</span> a binary tree node.<br># <span class="hljs-keyword">class</span> <span class="hljs-title class_">TreeNode</span>:<br>#     def __init__(<span class="hljs-keyword">self</span>, val=0, left=<span class="hljs-title class_ inherited__">None</span>, right=<span class="hljs-title class_ inherited__">None</span>):<br>#         <span class="hljs-keyword">self</span>.val = val<br>#         <span class="hljs-keyword">self</span>.left = left<br>#         <span class="hljs-keyword">self</span>.right = right<br><span class="hljs-keyword">class</span> <span class="hljs-title class_ inherited__">Solution</span>:<br>    def max<span class="hljs-title class_ inherited__">Depth</span>(<span class="hljs-keyword">self</span>, root: <span class="hljs-title class_ inherited__">Optional</span>[<span class="hljs-title class_ inherited__">TreeNode</span>]) -&gt; int:<br>        <span class="hljs-keyword">if</span> not root:<br>            <span class="hljs-keyword">return</span> 0<br>        <span class="hljs-keyword">return</span> max(<span class="hljs-keyword">self</span>.max<span class="hljs-title class_ inherited__">Depth</span>(root.left),<span class="hljs-keyword">self</span>.max<span class="hljs-title class_ inherited__">Depth</span>(root.right))+1<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/invert-binary-tree/">LC226 翻转二叉树</a></p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-comment"># Definition for a binary tree node.</span><br><span class="hljs-comment"># class TreeNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, left=None, right=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.left = left</span><br><span class="hljs-comment">#         self.right = right</span><br>class Solution:<br>    def invertTree(self, root: TreeNode) -&gt; TreeNode:<br>        if root == <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> root<br>        <span class="hljs-comment"># 等于左边的反转 加上 右边的反转</span><br>        <span class="hljs-literal">left</span>=self.invertTree(root.<span class="hljs-literal">left</span>)<br>        <span class="hljs-literal">right</span>=self.invertTree(root.<span class="hljs-literal">right</span>)<br>        root.<span class="hljs-literal">right</span>=<span class="hljs-literal">left</span><br>        root.<span class="hljs-literal">left</span>=<span class="hljs-literal">right</span><br>        <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/k-th-symbol-in-grammar/">LC779 第K个语法符号</a></p><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs objectivec"><span class="hljs-keyword">class</span> Solution:<br>    def kthGrammar(<span class="hljs-keyword">self</span>, n: <span class="hljs-type">int</span>, k: <span class="hljs-type">int</span>) -&gt; <span class="hljs-type">int</span>:<br>        <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> k % <span class="hljs-number">2</span> == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">self</span>.kthGrammar(n<span class="hljs-number">-1</span>, (k+<span class="hljs-number">1</span>) <span class="hljs-comment">// 2)</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> abs(<span class="hljs-keyword">self</span>.kthGrammar(n<span class="hljs-number">-1</span>, k <span class="hljs-comment">// 2) - 1)</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/unique-binary-search-trees-ii/">LC95 不同的二叉搜索树II</a></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs sql"># Definition <span class="hljs-keyword">for</span> a <span class="hljs-type">binary</span> tree node.<br># class TreeNode:<br>#     def __init__(self, val<span class="hljs-operator">=</span><span class="hljs-number">0</span>, <span class="hljs-keyword">left</span><span class="hljs-operator">=</span><span class="hljs-keyword">None</span>, <span class="hljs-keyword">right</span><span class="hljs-operator">=</span><span class="hljs-keyword">None</span>):<br>#         self.val <span class="hljs-operator">=</span> val<br>#         self.left <span class="hljs-operator">=</span> <span class="hljs-keyword">left</span><br>#         self.right <span class="hljs-operator">=</span> <span class="hljs-keyword">right</span><br>class Solution:<br>    def generateTrees(self, n: <span class="hljs-type">int</span>) <span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span> List[TreeNode]:<br>        if n<span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> []<br><br><br>        def recursionTree(<span class="hljs-keyword">left</span>,<span class="hljs-keyword">right</span>):<br>            # 生成<span class="hljs-keyword">left</span><span class="hljs-operator">-</span><span class="hljs-keyword">right</span>之间的树<br>            if <span class="hljs-keyword">left</span><span class="hljs-operator">&gt;</span><span class="hljs-keyword">right</span>:<br>                <span class="hljs-keyword">return</span> [<span class="hljs-keyword">None</span>]<br>            trees<span class="hljs-operator">=</span>[]<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-keyword">range</span>(<span class="hljs-keyword">left</span>,<span class="hljs-keyword">right</span><span class="hljs-operator">+</span><span class="hljs-number">1</span>):<br>                left_trees<span class="hljs-operator">=</span>recursionTree(<span class="hljs-keyword">left</span>,i<span class="hljs-number">-1</span>)<br>                right_trees<span class="hljs-operator">=</span>recursionTree(i<span class="hljs-operator">+</span><span class="hljs-number">1</span>,<span class="hljs-keyword">right</span>)<br>                <span class="hljs-keyword">for</span> left_tree <span class="hljs-keyword">in</span> left_trees:<br>                    <span class="hljs-keyword">for</span> right_tree <span class="hljs-keyword">in</span> right_trees:<br>                        curr_tree<span class="hljs-operator">=</span>TreeNode(i)<br>                        curr_tree.left<span class="hljs-operator">=</span>left_tree<br>                        curr_tree.right<span class="hljs-operator">=</span>right_tree<br>                        trees.append(curr_tree)<br>            <span class="hljs-keyword">return</span> trees<br>        <span class="hljs-keyword">return</span> recursionTree(<span class="hljs-number">1</span>,n)<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/">offer62 圆圈中最后剩下的数字</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lastRemaining</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span>, m: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ans = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, n + <span class="hljs-number">1</span>):<br>            ans = (m + ans) % i<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><h2 id="0x03-分治法（Divide-and-Conquer"><a href="#0x03-分治法（Divide-and-Conquer" class="headerlink" title="0x03 分治法（Divide and Conquer)"></a>0x03 分治法（Divide and Conquer)</h2><p>这里只是简单的了解，以归并排序作为基础。理解一个算法的好处就是去模拟它</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">def mergeSort(arr):<br>    <span class="hljs-comment"># 归并排序的基本思想：</span><br>    <span class="hljs-comment"># 采用经典的分治策略，先递归将当前序列平均分成两半，然后将有序序列合并，最终合并成一个有序序列</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 【算法步骤】</span><br>    <span class="hljs-comment"># 1. 将数组中的所有数据堪称n有序的子序列</span><br>    <span class="hljs-comment"># 2. 将当前序列组中的有序序列两两归并，完成一遍之后序列组里的排序序列的个数减版，每个子序列的长度加倍</span><br>    <span class="hljs-comment"># 3. 重复上述操作得到一个长度为n的有序序列</span><br>    <span class="hljs-comment"># </span><br>    def <span class="hljs-built_in">merge</span>(left_arr,right_arr):<br>        arr=[]<br>        <span class="hljs-keyword">while</span> left_arr <span class="hljs-keyword">and</span> right_arr:<br>            <span class="hljs-keyword">if</span> left_arr[<span class="hljs-number">0</span>]&lt;=right_arr[<span class="hljs-number">0</span>]:<br>                arr.append(left_arr.pop(<span class="hljs-number">0</span>))<br>            <span class="hljs-keyword">else</span>:<br>                arr.append(right_arr.pop[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-keyword">while</span> left_arr:<br>            arr.append(left_arr.pop(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-keyword">while</span> right_arr:<br>            arr.append(right_arr.pop(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-literal">return</span> arr<br><br>    size =<span class="hljs-built_in">len</span>(arr)<br><br>    <span class="hljs-comment"># 边界情况</span><br>    <span class="hljs-keyword">if</span> size&lt;<span class="hljs-number">2</span>:<br>        <span class="hljs-literal">return</span> arr<br><br>    <span class="hljs-keyword">mid</span> =siz<span class="hljs-comment">e//2</span><br><br>    left_arr,right_arr=arr[<span class="hljs-number">0</span>:<span class="hljs-keyword">mid</span>],arr[<span class="hljs-keyword">mid</span>:]<br>    <span class="hljs-literal">return</span> <span class="hljs-built_in">merge</span>(mergeSort(left_arr),mergeSort(right_arr))<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ec610d0d0eb9cdeb7f4333ae1dd33148_1440w.jpg" alt="img"></p><p>IMG_C3B04D575C51-1</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-007394dc841d4af3d00b6956e35aa6f6_1440w.jpg" alt="img"></p><p>img</p><h2 id="0x04-回溯算法-Backtracking-algorithm"><a href="#0x04-回溯算法-Backtracking-algorithm" class="headerlink" title="0x04 回溯算法(Backtracking algorithm)"></a>0x04 回溯算法(Backtracking algorithm)</h2><h3 id="4-1-回溯算法简介"><a href="#4-1-回溯算法简介" class="headerlink" title="4.1  回溯算法简介"></a>4.1  回溯算法简介</h3><p>一种能够避免不必要搜索的穷举式的搜索算法，采用尝试错误的思想，在搜索尝试过程中寻找问题的解，当探索到某一步的时候，发现原先的选择并不满足求解条件、或者还需要满足更多的求解条件的的时候，就退回一步重新选择。这个过程中走不通就退回的技术称为回溯法、满足回溯条件的某个状态的点称为回溯点</p><p>回溯算法通常用简单的递归的方法来实现，在进行回溯的过程中可能会出现两种情况：</p><ol><li>找到一个可能存在的正确答案</li><li>在尝试所有可能的分布方法之后宣布该问题没有答案</li></ol><h3 id="4-2-回溯算法的例子"><a href="#4-2-回溯算法的例子" class="headerlink" title="4.2  回溯算法的例子"></a>4.2  回溯算法的例子</h3><p>比如求解1，2，3的全排列的问题：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0ceb278bc145c639c5efc3cfe0f54f32_1440w.jpg" alt="img"></p><p>image-20220731231528233</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">premute</span>(<span class="hljs-params">nums</span>):<br>    res=[] <span class="hljs-comment"># 存放所有符合条件结果的集合</span><br>    path=[] <span class="hljs-comment"># 存放当前符合条件的结果</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtracking</span>(<span class="hljs-params">nums</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(path)==<span class="hljs-built_in">len</span>(nums):<br>            res.append(path[:])<br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> nums[i] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> path:<br>                <span class="hljs-comment"># 从当前路径中没有出现的数字中选择</span><br>                path.append(nums[i])<br>                <span class="hljs-comment"># 递归搜索</span><br>                backtracking(nums)<br>                <span class="hljs-comment"># 撤销选择</span><br>                path.pop()<br><br>    backtracking(nums)<br><br>    <span class="hljs-keyword">return</span> res<br><span class="hljs-built_in">print</span>(premute([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br></code></pre></td></tr></table></figure><h3 id="4-3-回溯算法的过程"><a href="#4-3-回溯算法的过程" class="headerlink" title="4.3 回溯算法的过程"></a>4.3 回溯算法的过程</h3><p>回溯算法的解题步骤比较抽象，这里只是做一个简单的介绍</p><p>第一步：结合所给的问题，定义问题的求解空间，包括求解的组织形式和显性约束</p><ol><li>解的组织形式，将解的组织形式都规范称为一个n元祖</li><li>显约束，对解分量的取值范围限定，用来控制解空间的大小</li></ol><p>第二步：确定解空间的组织结构，解空间的组织借口通常用解空间树的方式形象表达，根据解空间树的不同，解空间分为子集树、排列树和m叉树</p><p>第三步：按照深度优先搜索策略，根据隐约束，在解空间中搜索问题的可行解或者最优解，当发现当前节点不满足求解条件时候回溯尝试其他的路径</p><p>上面的是解题的抽象逻辑，后面根据code书写回溯算法的步骤可以分为：</p><ol><li>明确所有的选择，画出搜索过程的决策树，根据决策树来确定搜索路径</li><li>明确终止条件，推敲出递归的终止条件，以及递归终止时要执行的处理方法</li><li>将决策树和终止条件翻译成代码</li></ol><h3 id="4-4-回溯算法的应用"><a href="#4-4-回溯算法的应用" class="headerlink" title="4.4 回溯算法的应用"></a>4.4 回溯算法的应用</h3><p>初步的回溯算法是将现有的可能性分类，然后选择其中的一个递归到下一层，并允许返回后执行下一个可能性。重点在于每一层递归都会有N个可能性，所以更近一步需要考虑剪枝。当然需要先把回溯学会，才能学会剪纸</p><p><a href="https://leetcode.cn/problems/permutations/">LC46 全排列</a></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def permute(self, nums: List[<span class="hljs-keyword">int</span>]) -&gt; List[List[<span class="hljs-keyword">int</span>]]:<br>        <span class="hljs-keyword">res</span>=[]<br>        path=[]<br>        def bt(nums):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(path)==<span class="hljs-built_in">len</span>(nums):<br>                <span class="hljs-keyword">res</span>.<span class="hljs-keyword">append</span>(path[:])<br>                <span class="hljs-keyword">return</span> <br><br>            <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>                <span class="hljs-keyword">if</span> nums[i] not in path:<br>                    path.<span class="hljs-keyword">append</span>(nums[i])<br>                    bt(nums)<br>                    path.<span class="hljs-keyword">pop</span>()<br>        bt(nums)<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">res</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/permutations-ii/">LC 47全排列II</a></p><p><a href="https://leetcode.cn/problems/subarray-sum-equals-k/">LC560 和为k的子数组</a></p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs axapta"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    def subarraySum(self, nums: List[<span class="hljs-built_in">int</span>], k: <span class="hljs-built_in">int</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        pre_dic = &#123;<span class="hljs-number">0</span>: <span class="hljs-number">1</span>&#125;<br>        pre_sum = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">count</span> = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums:<br>            pre_sum += num<br>            <span class="hljs-keyword">if</span> pre_sum - k <span class="hljs-keyword">in</span> pre_dic:<br>                <span class="hljs-keyword">count</span> += pre_dic[pre_sum - k]<br>            <span class="hljs-keyword">if</span> pre_sum <span class="hljs-keyword">in</span> pre_dic:<br>                pre_dic[pre_sum] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                pre_dic[pre_sum] = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">count</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9a38d478e58a1fe5568a9c1d5c161cb1_1440w.jpg" alt="img"></p><p>IMG_CF3CEA8976F6-1</p><p><a href="https://leetcode.cn/problems/generate-parentheses/">LC22 括号生成</a></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def generateParenthesis(self, n: <span class="hljs-keyword">int</span>) -&gt; List[str]:<br>        ans = []<br>        def backtrack(S, <span class="hljs-keyword">left</span>, <span class="hljs-keyword">right</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(S) == <span class="hljs-number">2</span> * n:<br>                ans.<span class="hljs-keyword">append</span>(<span class="hljs-string">&#x27;&#x27;</span>.<span class="hljs-keyword">join</span>(S))<br>                <span class="hljs-keyword">return</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">left</span> &lt; n:<br>                S.<span class="hljs-keyword">append</span>(<span class="hljs-string">&#x27;(&#x27;</span>)<br>                backtrack(S, <span class="hljs-keyword">left</span>+<span class="hljs-number">1</span>, <span class="hljs-keyword">right</span>)<br>                S.<span class="hljs-keyword">pop</span>()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">right</span> &lt; <span class="hljs-keyword">left</span>:<br>                S.<span class="hljs-keyword">append</span>(<span class="hljs-string">&#x27;)&#x27;</span>)<br>                backtrack(S, <span class="hljs-keyword">left</span>, <span class="hljs-keyword">right</span>+<span class="hljs-number">1</span>)<br>                S.<span class="hljs-keyword">pop</span>()<br><br>        backtrack([], <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-28c62ba51b3ecacb061da0f15cb859a0_1440w.jpg" alt="img"></p><p>IMG_D7DDED1F4ECE-1</p><p><a href="https://leetcode.cn/problems/letter-combinations-of-a-phone-number/">LC 17 电话号码的字母组合</a></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def letterCombinations(self, digits: str) -&gt; List[str]:<br>        num2letter=[<span class="hljs-string">&#x27;abc&#x27;</span>,<span class="hljs-string">&#x27;def&#x27;</span>,<span class="hljs-string">&#x27;ghi&#x27;</span>,<span class="hljs-string">&#x27;jkl&#x27;</span>,<span class="hljs-string">&#x27;mno&#x27;</span>,<span class="hljs-string">&#x27;pqrs&#x27;</span>,<span class="hljs-string">&#x27;tuv&#x27;</span>,<span class="hljs-string">&#x27;wxyz&#x27;</span>]<br>        n=<span class="hljs-built_in">len</span>(digits)<br>        <span class="hljs-keyword">res</span>=[]<br>        <span class="hljs-keyword">if</span> not digits:<br>            <span class="hljs-keyword">return</span> []<br>        def backtracking(S):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(S)==n:<br>                <span class="hljs-keyword">res</span>.<span class="hljs-keyword">append</span>(<span class="hljs-string">&#x27;&#x27;</span>.<span class="hljs-keyword">join</span>(S))<br>                <span class="hljs-keyword">return</span><br><br>            <span class="hljs-built_in">index</span>=<span class="hljs-built_in">len</span>(S)<br>            num=<span class="hljs-keyword">int</span>(digits[<span class="hljs-built_in">index</span>])<br>            <span class="hljs-keyword">print</span>(num)<br>            <span class="hljs-keyword">for</span> i in num2letter[num-<span class="hljs-number">2</span>]:<br>                S.<span class="hljs-keyword">append</span>(i)<br>                backtracking(S)<br>                S.<span class="hljs-keyword">pop</span>()<br>        backtracking([])<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">res</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/letter-case-permutation/">LC784 字幕大小写全排列</a></p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs axapta"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    def letterCasePermutation(self, s: <span class="hljs-built_in">str</span>) -&gt; List[<span class="hljs-built_in">str</span>]:<br>        res=[]<br>        path=[]<br><br><br>        def backtracking(path,<span class="hljs-keyword">index</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">index</span>==len(s):<br>                <span class="hljs-meta"># 所有的字母都遍历完毕</span><br>                res.append(<span class="hljs-string">&#x27;&#x27;</span>.<span class="hljs-keyword">join</span>(path[:]))<br>                <span class="hljs-keyword">return</span><br><br>            <span class="hljs-built_in">char</span>=s[<span class="hljs-keyword">index</span>]<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">char</span>.isdigit():<br>                <span class="hljs-meta"># 如果是数字，直接加上去就行</span><br>                path.append(<span class="hljs-built_in">char</span>)<br>                backtracking(path,<span class="hljs-keyword">index</span>+<span class="hljs-number">1</span>)<br>                path.pop()<br>            <span class="hljs-keyword">else</span>:<br>                path.append(<span class="hljs-built_in">char</span>.lower())<br>                backtracking(path,<span class="hljs-keyword">index</span>+<span class="hljs-number">1</span>)<br>                path.pop()<br>                path.append(<span class="hljs-built_in">char</span>.upper())<br>                backtracking(path,<span class="hljs-keyword">index</span>+<span class="hljs-number">1</span>)<br>                path.pop()<br>        backtracking([],<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/word-search/">LC79 单词搜索</a></p><p>暂时还不会hhh</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">exist</span>(<span class="hljs-params">self, board: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]], word: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        directions = [(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)]<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">check</span>(<span class="hljs-params">i: <span class="hljs-built_in">int</span>, j: <span class="hljs-built_in">int</span>, k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>            <span class="hljs-keyword">if</span> board[i][j] != word[k]:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">if</span> k == <span class="hljs-built_in">len</span>(word) - <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br>            visited.add((i, j))<br>            result = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">for</span> di, dj <span class="hljs-keyword">in</span> directions:<br>                newi, newj = i + di, j + dj<br>                <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= newi &lt; <span class="hljs-built_in">len</span>(board) <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= newj &lt; <span class="hljs-built_in">len</span>(board[<span class="hljs-number">0</span>]):<br>                    <span class="hljs-keyword">if</span> (newi, newj) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> visited:<br>                        <span class="hljs-keyword">if</span> check(newi, newj, k + <span class="hljs-number">1</span>):<br>                            result = <span class="hljs-literal">True</span><br>                            <span class="hljs-keyword">break</span><br><br>            visited.remove((i, j))<br>            <span class="hljs-keyword">return</span> result<br><br>        h, w = <span class="hljs-built_in">len</span>(board), <span class="hljs-built_in">len</span>(board[<span class="hljs-number">0</span>])<br>        visited = <span class="hljs-built_in">set</span>()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(h):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(w):<br>                <span class="hljs-keyword">if</span> check(i, j, <span class="hljs-number">0</span>):<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/letter-tile-possibilities/">LC1079 活字印刷</a></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def numTilePossibilities(self, tiles: str) -&gt; <span class="hljs-keyword">int</span>:<br>        <span class="hljs-keyword">res</span>=<span class="hljs-keyword">set</span>()<br>        path=[]<br>        hashmap=&#123;&#125;<br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(tiles)):<br>            <span class="hljs-keyword">if</span> tiles[i] in hashmap:<br>                hashmap[tiles[i]]+=<span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                hashmap[tiles[i]]=<span class="hljs-number">1</span><br><br>        def backtracking(tiles):<br><br>            temp=<span class="hljs-string">&#x27;&#x27;</span>.<span class="hljs-keyword">join</span>(path[:])<br>            <span class="hljs-keyword">if</span> temp!=<span class="hljs-string">&#x27;&#x27;</span> <span class="hljs-built_in">and</span> temp not in <span class="hljs-keyword">res</span>:<br>                <span class="hljs-keyword">res</span>.<span class="hljs-built_in">add</span>(temp)<br><br>            <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(tiles)):<br>                <span class="hljs-keyword">while</span>(hashmap[tiles[i]]==<span class="hljs-number">0</span>):<br>                    i=i+<span class="hljs-number">1</span><br>                    <span class="hljs-keyword">if</span> i ==<span class="hljs-built_in">len</span>(tiles):<br>                        <span class="hljs-keyword">return</span> <br>                hashmap[tiles[i]]-=<span class="hljs-number">1</span><br>                path.<span class="hljs-keyword">append</span>(tiles[i])<br>                backtracking(tiles)<br>                va=path.<span class="hljs-keyword">pop</span>()<br>                hashmap[va]+=<span class="hljs-number">1</span><br>        backtracking(tiles)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-keyword">res</span>)<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/restore-ip-addresses/">LC93 复原IP地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">restoreIpAddresses</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        res=[]<br>        path=[]<br>        flag=<span class="hljs-number">4</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtracking</span>(<span class="hljs-params">s,index</span>):<br>            temp=<span class="hljs-string">&#x27;&#x27;</span>.join(path[:])<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(temp)==<span class="hljs-built_in">len</span>(s)+<span class="hljs-number">4</span>:<br>                res.append(temp[:-<span class="hljs-number">1</span>])<br>                <span class="hljs-keyword">return</span><br><br>            <span class="hljs-keyword">nonlocal</span> flag<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(index+<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(s)+<span class="hljs-number">1</span>):<br>                <span class="hljs-keyword">if</span> flag&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">int</span>(s[index:i])&lt;=<span class="hljs-number">255</span>:<br>                    <span class="hljs-keyword">if</span> i!=index+<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">int</span>(s[index])==<span class="hljs-number">0</span>:<br>                        <span class="hljs-keyword">return</span> <br>                    path.append(s[index:i])<br>                    path.append(<span class="hljs-string">&#x27;.&#x27;</span>)<br>                    flag-=<span class="hljs-number">1</span><br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">return</span><br>                backtracking(s,i)<br>                path.pop()<br>                path.pop()<br>                flag+=<span class="hljs-number">1</span><br>        backtracking(s,<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/n-queens/">LC51-N皇后</a></p><p>感觉N皇后是回溯里面最简单的一种了；确实比较经典</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def solveNQueens(self, n: <span class="hljs-keyword">int</span>) -&gt; List[List[str]]:<br>        <span class="hljs-keyword">res</span>=[]<br>        path=[]<br>        def backtracking(n):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(path)==n:<br>                <span class="hljs-keyword">res</span>.<span class="hljs-keyword">append</span>(path[:])<br>                <span class="hljs-keyword">return</span> <br><br>            <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(n):<br>                # 根据前面的path可以得出目前行能走的路线<br>                forbid=[]<br>                <span class="hljs-keyword">for</span> <span class="hljs-keyword">j</span> in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(path)):<br>                    forbid.<span class="hljs-keyword">append</span>(path[<span class="hljs-keyword">j</span>])<br>                    forbid.<span class="hljs-keyword">append</span>(path[<span class="hljs-keyword">j</span>]+<span class="hljs-built_in">len</span>(path)-<span class="hljs-keyword">j</span>)<br>                    forbid.<span class="hljs-keyword">append</span>(path[<span class="hljs-keyword">j</span>]+<span class="hljs-keyword">j</span>-<span class="hljs-built_in">len</span>(path))<br>                <span class="hljs-keyword">if</span> i not in forbid:<br>                    path.<span class="hljs-keyword">append</span>(i)<br>                    backtracking(n)<br>                    path.<span class="hljs-keyword">pop</span>()<br>        backtracking(n)<br><br>        rest=[]<br>        # 翻译<br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-keyword">res</span>)):<br>            rest.<span class="hljs-keyword">append</span>([])<br>            <span class="hljs-keyword">for</span> <span class="hljs-keyword">j</span> in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-keyword">res</span>[<span class="hljs-number">0</span>])):<br>                temp=[<span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">for</span> _ in <span class="hljs-built_in">range</span>(n)]<br>                temp[<span class="hljs-keyword">res</span>[i][<span class="hljs-keyword">j</span>]]=<span class="hljs-string">&#x27;Q&#x27;</span><br>                #合成<br>                <span class="hljs-keyword">ts</span>=<span class="hljs-string">&#x27;&#x27;</span><br>                <span class="hljs-keyword">for</span> <span class="hljs-keyword">k</span> in temp:<br>                    <span class="hljs-keyword">ts</span>+=<span class="hljs-keyword">k</span><br>                rest[-<span class="hljs-number">1</span>].<span class="hljs-keyword">append</span>(<span class="hljs-keyword">ts</span>)<br>        <span class="hljs-keyword">return</span> rest<br></code></pre></td></tr></table></figure><h2 id="0x05-总结（Conclusion）"><a href="#0x05-总结（Conclusion）" class="headerlink" title="0x05 总结（Conclusion）"></a>0x05 总结（Conclusion）</h2><ol><li>理解一个算法，并不像一个数据结构一样直观。所以模拟运行，一步一步的执行才能理解其中的一些关键点。只是单纯的看概念是不ok的</li><li>无论是算法还是数据结构，学习的过程并不是一个创造性的过程，只是将计算机的单条指令执行的思维强加在人身上，再写出利用合适的数据结构和算法能得到一个解决方案</li><li>分治和回溯的根基还是递归，理解递归需要理解循环不变量</li><li>剪枝后面再看趴</li></ol>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>程序基础</tag>
      
      <tag>算法思想</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法基础 Vol7</title>
    <link href="/posts/21aa2441.html"/>
    <url>/posts/21aa2441.html</url>
    
    <content type="html"><![CDATA[<p>新的数据结构，查找速度贼快的哈希表！！</p><h3 id="s0x01-为什么我们需要哈希表？"><a href="#s0x01-为什么我们需要哈希表？" class="headerlink" title="s0x01 为什么我们需要哈希表？"></a>s0x01 为什么我们需要哈希表？</h3><p>首先我们细数之前学过的线性数据结构，其中最基本的分类为数组（Array）和链表（Linked list），按照我个人的看法，最大的区别在于数组按址取值的方式让其在索引速度很快、链表由于数据之间的隔离让其在插入修改等速度很快。</p><p>然后在这个基础上，我们可以人为的设计规则（FIFO或者LIFO）来更好的描述这个物理世界，也就是在数组和链表基础上所加的一层抽象（就像程序语言在寄存器基础上抽象一样），但是在学习优先队列的时候，发现堆（Heap）的出现是在数组和链表之间的Trade-off，它实现了查找的logN、修改删除的logN时间，发现非常的Amazing</p><p>所以为什么我们需要哈希表呢？因为我们需要实现值的查找</p><p>(Sedgewick and Wayne, 2021)在&lt;算法，第四版&gt;中的第三章（查找）中所描述的：</p><blockquote><p> 现代计算机和网络使我们能够访问海量的信息。高效检索这些信息的能力是处理它们的重要前 提。本章描述的都是数十年来在广泛应用中经过实践检验的经典查找算法。没有这些算法，现代信 息世界的基础计算设施都无从谈起。 我们会使用符号表这个词来描述一张抽象的表格，我们会将信息（值）存储在其中，然后按照 指定的键来搜索并获取这些信息。键和值的具体意义取决于不同的应用。符号表中可能会保存很多 键和很多信息，因此实现一张高效的符号表也是一项很有挑战性的任务。 符号表有时被称为字典，类似于那本将单词的释义按照字母顺序排列起来的历史悠久的参考书。 在英语字典里，键就是单词，值就是单词对应的定义、发音和词源。符号表有时又叫做索引，即书 本最后将术语按照字母顺序列出以方便查找的那部分。在一本书的索引中，键就是术语，而值就是 书中该术语出现的所有页码。 在说明了基本的 API 和两种重要的实现之后，我们会学习用三种经典的数据类型来实现高效的 符号表：二叉查找树、红黑树和散列表。在总结中我们会看到它们的若干扩展和应用，它们的实现 都有赖于我们在本章中将会学到的高效算法。</p></blockquote><p>简单来说，就比如说我们需要删除[1,2,3,4]中3这个值，如果从数组的角度需要知道3对应的下标，就会比较麻烦，所以我们希望借助哈希表的方式来快速找到这个值</p><h3 id="0x02-哈希表和哈希映射"><a href="#0x02-哈希表和哈希映射" class="headerlink" title="0x02 哈希表和哈希映射"></a>0x02 哈希表和哈希映射</h3><p><strong>哈希表（Hash Table），</strong>通过键 key和一个映射函数 Hash(key)计算出对应的值 value ，把关键码值映射到表中一个位置来访问记录，以加快查找的速度。</p><p>举个例子，我们通过value&#x3D;Hash(key)&#x3D;key&#x2F;&#x2F;1000作为哈希函数，由此可以实现插入和查找：</p><ol><li>比如插入0138，我们可以通过哈希函数计算出value&#x3D;0，然后分配到0对应的区块中</li><li>查找2321，通过哈希函数可以得到2，在2对应的区块中寻找就可以了</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6930ab429cd2525cb7425f6113a5d6dd_1440w.jpg" alt="img"></p><p>image-20220720234945173</p><p>在这个过程中最终的点在**哈希函数(Hash Function)***<strong>和**<em><strong>哈希冲突（Hash Collision）</strong>,首先看哈希函数，本质上是一个</em>*多对一的映射，要求：</strong></p><ol><li>容易计算的，并且计算的索引值可以均匀分布</li><li>所得到的哈希值是一个固定长度的输出</li><li>多对一，如果Hash（key1）！&#x3D;Hash（key2），则key1和key2一定不想等</li><li>如果Hash（key1）&#x3D;&#x3D;Hash（key2—），则两者可能相同，也可能不同（冲突）</li></ol><p>上述第4点中的不同key得到相同的hash（key）就是哈希冲突（Hash Collision），这也是好理解的有得必有失，会有针对哈希冲突的优化。这就需要精妙的设计能力了。</p><h3 id="2-1-哈希函数的方法"><a href="#2-1-哈希函数的方法" class="headerlink" title="2.1 哈希函数的方法"></a>2.1 哈希函数的方法</h3><ul><li>直接定址法</li><li>除留余数法</li><li>平方折中法</li><li>基数转换法</li><li>等等</li></ul><p>这里大部分不会用的，需要了解的可以配合搜索引擎使用</p><h3 id="2-2-哈希冲突的解决"><a href="#2-2-哈希冲突的解决" class="headerlink" title="2.2 哈希冲突的解决"></a>2.2 哈希冲突的解决</h3><ul><li>开放地址法（Open Addressing）</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-783dbc8eb070f698dda037290cdce696_1440w.jpg" alt="img"></p><p>image-20220720234955234</p><ul><li>链地址法（Chaining）– 用python会很简单</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7f38deb879d012ebac05ef20d9cd511f_1440w.jpg" alt="img"></p><p>image-20220720235004945</p><h3 id="0x03-哈希表和哈希映射的设计"><a href="#0x03-哈希表和哈希映射的设计" class="headerlink" title="0x03 哈希表和哈希映射的设计"></a>0x03 哈希表和哈希映射的设计</h3><p><a href="https://leetcode.cn/problems/design-hashset/">T705 设计哈希集合</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyHashSet</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>=[[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, key: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        index=key%<span class="hljs-number">1000</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index])):<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index][i]==key:<br>                <span class="hljs-keyword">break</span><br>        <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index].append(key)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">remove</span>(<span class="hljs-params">self, key: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        index = key%<span class="hljs-number">1000</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index])):<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index][i]==key:<br>                <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index][i]=-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">contains</span>(<span class="hljs-params">self, key: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        index=key%<span class="hljs-number">1000</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index])):<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">set</span>[index][i]==key:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/design-hashmap/">T706 设计哈希映射</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyHashMap</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.N=<span class="hljs-number">1000</span><br>        <span class="hljs-variable language_">self</span>.hashMap=[[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.N)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">put</span>(<span class="hljs-params">self, key: <span class="hljs-built_in">int</span>, value: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        index=key%<span class="hljs-variable language_">self</span>.N<br>        <span class="hljs-keyword">for</span> values <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.hashMap[index]:<br>            <span class="hljs-keyword">if</span> values[<span class="hljs-number">0</span>]==key:<br>                <span class="hljs-comment"># 更新值</span><br>                values[<span class="hljs-number">1</span>]=value<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        <span class="hljs-variable language_">self</span>.hashMap[index].append([key,value])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self, key: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        index = key % <span class="hljs-variable language_">self</span>.N<br>        <span class="hljs-keyword">for</span> values <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.hashMap[index]:<br>            <span class="hljs-keyword">if</span> values[<span class="hljs-number">0</span>]==key:<br>                <span class="hljs-keyword">return</span> values[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">remove</span>(<span class="hljs-params">self, key: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        index = key% <span class="hljs-variable language_">self</span>.N<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.hashMap[index])):<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.hashMap[index][i][<span class="hljs-number">0</span>]==key:<br>                <span class="hljs-variable language_">self</span>.hashMap[index].pop(i)<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h3 id="0x04-哈希表的应用"><a href="#0x04-哈希表的应用" class="headerlink" title="0x04 哈希表的应用"></a>0x04 哈希表的应用</h3><p>比较有意思的是原地哈希</p><p><a href="https://leetcode.cn/problems/contains-duplicate/">T217 存在重复元素</a></p><p><a href="https://leetcode.cn/problems/contains-duplicate-ii/">T219 存在重复元素II</a></p><p><a href="https://leetcode.cn/problems/contains-duplicate-iii/">T220 存在重复元素III</a></p><p>这里不能简单的使用之前的哈希函数，因为我不能在一个桶里面比较，我还需要和其他桶对比</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">containsNearbyAlmostDuplicate</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], k: <span class="hljs-built_in">int</span>, t: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        bucket_dict = <span class="hljs-built_in">dict</span>()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-comment"># 将 nums[i] 划分到大小为 t + 1 的不同桶中</span><br>            num = nums[i] // (t + <span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># 桶中已经有元素了</span><br>            <span class="hljs-keyword">if</span> num <span class="hljs-keyword">in</span> bucket_dict:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br>            <span class="hljs-comment"># 把 nums[i] 放入桶中</span><br>            bucket_dict[num] = nums[i]<br><br>            <span class="hljs-comment"># 判断左侧桶是否满足条件</span><br>            <span class="hljs-keyword">if</span> (num - <span class="hljs-number">1</span>) <span class="hljs-keyword">in</span> bucket_dict <span class="hljs-keyword">and</span> <span class="hljs-built_in">abs</span>(bucket_dict[num - <span class="hljs-number">1</span>] - nums[i]) &lt;= t:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>            <span class="hljs-comment"># 判断右侧桶是否满足条件</span><br>            <span class="hljs-keyword">if</span> (num + <span class="hljs-number">1</span>) <span class="hljs-keyword">in</span> bucket_dict <span class="hljs-keyword">and</span> <span class="hljs-built_in">abs</span>(bucket_dict[num + <span class="hljs-number">1</span>] - nums[i]) &lt;= t:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>            <span class="hljs-comment"># 将 i-k 之前的旧桶清除，因为之前的桶已经不满足条件了</span><br>            <span class="hljs-keyword">if</span> i &gt;= k:<br>                bucket_dict.pop(nums[i-k] // (t + <span class="hljs-number">1</span>))<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/single-number/">T136 只出现一次的数字</a></p><p><a href="https://leetcode.cn/problems/two-sum/">T001 两数之和</a></p><p>第一道题，很经典</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        headMap=&#123;&#125;<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> nums[i] <span class="hljs-keyword">in</span> headMap:<br>                <span class="hljs-keyword">return</span> [i,headMap[nums[i]]]<br>            headMap[target-nums[i]]=i<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/3sum/">T015 三数之和</a></p><p>重大打击的一题，也很经典</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def threeSum(self, nums: List[<span class="hljs-keyword">int</span>]) -&gt; List[List[<span class="hljs-keyword">int</span>]]:<br>        <span class="hljs-keyword">res</span>=[]<br>        def twoSum(arr,target):<br>            <span class="hljs-keyword">left</span>=<span class="hljs-number">0</span><br>            <span class="hljs-keyword">right</span>=<span class="hljs-built_in">len</span>(arr)-<span class="hljs-number">1</span><br><br>            <span class="hljs-keyword">while</span>(<span class="hljs-keyword">left</span>&lt;<span class="hljs-keyword">right</span>):<br>                temp_sum=arr[<span class="hljs-keyword">left</span>]+arr[<span class="hljs-keyword">right</span>]<br>                <span class="hljs-keyword">if</span> temp_sum==target:<br>                    <span class="hljs-keyword">if</span> <span class="hljs-keyword">left</span>&gt;<span class="hljs-number">0</span> <span class="hljs-built_in">and</span> arr[<span class="hljs-keyword">left</span>]==arr[<span class="hljs-keyword">left</span>-<span class="hljs-number">1</span>]:<br>                        <span class="hljs-keyword">left</span>+=<span class="hljs-number">1</span><br>                    elif <span class="hljs-keyword">right</span>&lt;<span class="hljs-built_in">len</span>(arr)-<span class="hljs-number">1</span> <span class="hljs-built_in">and</span> arr[<span class="hljs-keyword">right</span>]==arr[<span class="hljs-keyword">right</span>+<span class="hljs-number">1</span>]:<br>                        <span class="hljs-keyword">right</span>-=<span class="hljs-number">1</span><br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">res</span>.<span class="hljs-keyword">append</span>([target*(-<span class="hljs-number">1</span>),arr[<span class="hljs-keyword">left</span>],arr[<span class="hljs-keyword">right</span>]])<br>                        <span class="hljs-keyword">left</span>+=<span class="hljs-number">1</span><br>                        <span class="hljs-keyword">right</span>-=<span class="hljs-number">1</span><br>                elif temp_sum&lt;target:<br>                    <span class="hljs-keyword">left</span>+=<span class="hljs-number">1</span><br>                elif temp_sum&gt;target:<br>                    <span class="hljs-keyword">right</span>-=<span class="hljs-number">1</span><br>        nums.<span class="hljs-keyword">sort</span>()<br><br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> nums[i]&gt;<span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> i&gt;<span class="hljs-number">0</span> <span class="hljs-built_in">and</span> nums[i]==nums[i-<span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">continue</span><br>            twoSum(nums[i+<span class="hljs-number">1</span>:],(-<span class="hljs-number">1</span>)*nums[i])        <br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">res</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/4sum-ii/">T454 四数相加II</a></p><p>大事化小</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">class Solution:</span><br>    <span class="hljs-string">def</span> <span class="hljs-string">fourSumCount(self,</span> <span class="hljs-attr">nums1:</span> <span class="hljs-string">List[int],</span> <span class="hljs-attr">nums2:</span> <span class="hljs-string">List[int],</span> <span class="hljs-attr">nums3:</span> <span class="hljs-string">List[int],</span> <span class="hljs-attr">nums4:</span> <span class="hljs-string">List[int])</span> <span class="hljs-string">-&gt;</span> <span class="hljs-attr">int:</span><br>        <span class="hljs-string">hashMap1=&#123;&#125;</span><br>        <span class="hljs-attr">for num1 in nums1:</span><br>            <span class="hljs-attr">for num2 in nums2:</span><br>                <span class="hljs-string">if</span> <span class="hljs-string">num1+num2</span> <span class="hljs-attr">in hashMap1:</span><br>                    <span class="hljs-string">hashMap1[num1+num2]+=1</span><br>                <span class="hljs-attr">else:</span><br>                    <span class="hljs-string">hashMap1[num1+num2]=1</span><br><br>        <span class="hljs-string">count=0</span><br><br>        <span class="hljs-attr">for num3 in nums3:</span><br>            <span class="hljs-attr">for num4 in nums4:</span><br>                <span class="hljs-string">if</span> <span class="hljs-string">-(num3+num4)</span> <span class="hljs-attr">in hashMap1:</span><br>                    <span class="hljs-string">count+=hashMap1[-(num3+num4)]</span><br><br>        <span class="hljs-string">return</span> <span class="hljs-string">count</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/find-all-duplicates-in-an-array/">T442 数组中重复的数据</a></p><p>很有意思</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">findDuplicates</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-comment"># 依旧使用原地哈希</span><br>        <span class="hljs-comment"># hashtable(key)=key-1</span><br>        <span class="hljs-comment"># 数组中只会存在两种情况，一种是正常映射、另外一种是重复的，每次交换的终止条件是</span><br>        <span class="hljs-comment"># 1. i处的正常映射</span><br>        <span class="hljs-comment"># 2. i处与nums[i]出现重复</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">while</span> nums[i]!=i+<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[i]!=nums[nums[i]-<span class="hljs-number">1</span>]:<br>                nums[nums[i]-<span class="hljs-number">1</span>],nums[i]=nums[i],nums[nums[i]-<span class="hljs-number">1</span>]<br><br>        <span class="hljs-keyword">return</span> [num <span class="hljs-keyword">for</span> i, num <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(nums) <span class="hljs-keyword">if</span> num - <span class="hljs-number">1</span> != i]<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/max-points-on-a-line/">T149 直线上最多的点数</a></p><p>需要注意python运算的时候，小数的问题，比如0.1+0.2&#x3D;0.333334；结合贪心的思想使用更佳</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">class</span> Solution:<br>    <span class="hljs-attribute">def</span> maxPoints(self, points: List[List[int]]) -&gt; int:<br>        <span class="hljs-attribute">n</span> = len(points)<br>        <span class="hljs-attribute">if</span> n &lt; <span class="hljs-number">3</span>:<br>            <span class="hljs-attribute">return</span> n<br>        <span class="hljs-attribute">ans</span> = <span class="hljs-number">0</span><br>        <span class="hljs-attribute">for</span> i in range(n):<br>            <span class="hljs-attribute">line_dict</span> = dict()<br>            <span class="hljs-attribute">line_dict</span>[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br>            <span class="hljs-attribute">same</span> = <span class="hljs-number">1</span><br>            <span class="hljs-attribute">for</span> j in range(i+<span class="hljs-number">1</span>, n):<br>                <span class="hljs-attribute">dx</span> = points[j][<span class="hljs-number">0</span>] - points[i][<span class="hljs-number">0</span>]<br>                <span class="hljs-attribute">dy</span> = points[j][<span class="hljs-number">1</span>] - points[i][<span class="hljs-number">1</span>]<br>                <span class="hljs-attribute">if</span> dx == <span class="hljs-number">0</span> and dy == <span class="hljs-number">0</span>:<br>                    <span class="hljs-attribute">same</span> += <span class="hljs-number">1</span><br>                    <span class="hljs-attribute">continue</span><br>                <span class="hljs-attribute">gcd_dx_dy</span> = math.gcd(abs(dx), abs(dy))<br>                <span class="hljs-attribute">if</span> (dx &gt; <span class="hljs-number">0</span> and dy &gt; <span class="hljs-number">0</span>) or (dx &lt; <span class="hljs-number">0</span> and dy &lt; <span class="hljs-number">0</span>):<br>                    <span class="hljs-attribute">dx</span> = abs(dx) // gcd_dx_dy<br>                    <span class="hljs-attribute">dy</span> = abs(dy) // gcd_dx_dy<br>                <span class="hljs-attribute">elif</span> dx &lt; <span class="hljs-number">0</span> and dy &gt; <span class="hljs-number">0</span>:<br>                    <span class="hljs-attribute">dx</span> = -dx // gcd_dx_dy<br>                    <span class="hljs-attribute">dy</span> = -dy // gcd_dx_dy<br>                <span class="hljs-attribute">elif</span> dx &gt; <span class="hljs-number">0</span> and dy &lt; <span class="hljs-number">0</span>:<br>                    <span class="hljs-attribute">dx</span> = dx // gcd_dx_dy<br>                    <span class="hljs-attribute">dy</span> = dy // gcd_dx_dy<br>                <span class="hljs-attribute">elif</span> dx == <span class="hljs-number">0</span> and dy != <span class="hljs-number">0</span>:<br>                    <span class="hljs-attribute">dy</span> = <span class="hljs-number">1</span><br>                <span class="hljs-attribute">elif</span> dx != <span class="hljs-number">0</span> and dy == <span class="hljs-number">0</span>:<br>                    <span class="hljs-attribute">dx</span> = <span class="hljs-number">1</span><br>                <span class="hljs-attribute">key</span> = (dx, dy)<br>                <span class="hljs-attribute">if</span> key in line_dict:<br>                    <span class="hljs-attribute">line_dict</span>[key] += <span class="hljs-number">1</span><br>                <span class="hljs-attribute">else</span>:<br>                    <span class="hljs-attribute">line_dict</span>[key] = <span class="hljs-number">1</span><br>            <span class="hljs-attribute">ans</span> = max(ans, same + max(line_dict.values()))<br>        <span class="hljs-attribute">return</span> ans<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/subdomain-visit-count/">T811 子域名的访问计数</a></p><p>学会python的字符串操作还是很有用的</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def subdomainVisits(self, cpdomains: List[str]) -&gt; List[str]:<br>        ans=&#123;&#125;<br>        <span class="hljs-keyword">for</span> domain in cpdomains:<br>            <span class="hljs-built_in">count</span>,domains=domain.<span class="hljs-keyword">split</span>()<br>            <span class="hljs-built_in">count</span>=<span class="hljs-keyword">int</span>(<span class="hljs-built_in">count</span>)<br><br>            subdomains=domains.<span class="hljs-keyword">split</span>(<span class="hljs-string">&#x27;.&#x27;</span>)<br>            temp=<span class="hljs-string">&#x27;&#x27;</span><br>            <span class="hljs-keyword">for</span> subdomain in subdomains[::-<span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">if</span> temp:<br>                    temp=subdomain+<span class="hljs-string">&#x27;.&#x27;</span>+temp<br>                <span class="hljs-keyword">else</span>:<br>                    temp=subdomain<br>                <span class="hljs-keyword">if</span> temp in ans:<br>                    ans[temp]+=<span class="hljs-built_in">count</span><br>                <span class="hljs-keyword">else</span>:<br>                    ans[temp]=<span class="hljs-built_in">count</span><br>        <span class="hljs-keyword">res</span>=[]<br>        <span class="hljs-keyword">for</span> key,value in ans.<span class="hljs-built_in">items</span>():<br>            <span class="hljs-keyword">res</span>.<span class="hljs-keyword">append</span>(str(value)+<span class="hljs-string">&#x27; &#x27;</span>+key)<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">res</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法基础</tag>
      
      <tag>哈希表</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法基础 Vol6</title>
    <link href="/posts/56ad14d7.html"/>
    <url>/posts/56ad14d7.html</url>
    
    <content type="html"><![CDATA[<p>数据结构、算法思想，具体问题的设计能力，以及一些黑话也是有必要的，比如双指针、滑动窗口、单调栈、优先队列等</p><h3 id="0x01-堆栈的基本知识"><a href="#0x01-堆栈的基本知识" class="headerlink" title="0x01 堆栈的基本知识"></a>0x01 堆栈的基本知识</h3><p><strong>堆栈（Stack）</strong>简称为栈。一种线性表数据结构，是一种只允许在表的一端进行插入和删除操作的线性表。</p><p>我们把栈中允许插入和删除的一端称为 <strong>「栈顶（top）」；</strong>另一端则称为 <strong>「栈底（bottom）」</strong> 。当表中没有任何数据元素时，称之为 <strong>「空栈」。</strong></p><p>主要操作分为插入和删除操作，分别是入栈操作和出栈操作。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-15a438bf4f94a4ccb7586d82ce694b13_1440w.jpg" alt="img"></p><p>image-20220708221924230</p><p>让我们来设计一个堆栈，对一个非直观的数据结构掌握的最好方法是用基本的数据结构来实现它，参照：<a href="https://leetcode.cn/problems/min-stack/"><strong>T155 最小栈设计</strong></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MinStack</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.stack=[]<br>        <span class="hljs-variable language_">self</span>.topPointer=-<span class="hljs-number">1</span><br>        <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">min</span>=[]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">push</span>(<span class="hljs-params">self, val: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.stack.append(val)<br>        <span class="hljs-variable language_">self</span>.topPointer+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.topPointer==<span class="hljs-number">0</span>:<br>            <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">min</span>.append(val)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> val&lt;<span class="hljs-variable language_">self</span>.<span class="hljs-built_in">min</span>[-<span class="hljs-number">1</span>]:<br>                <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">min</span>.append(val)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">min</span>.append(<span class="hljs-variable language_">self</span>.<span class="hljs-built_in">min</span>[-<span class="hljs-number">1</span>])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">pop</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">min</span>.pop()<br>        <span class="hljs-variable language_">self</span>.stack.pop()<br>        <span class="hljs-variable language_">self</span>.topPointer-=<span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">top</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.stack[-<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><h3 id="0x02-堆栈的基本作用"><a href="#0x02-堆栈的基本作用" class="headerlink" title="0x02 堆栈的基本作用"></a>0x02 堆栈的基本作用</h3><p>与其说堆栈是一种数据结构，它更多的是一种设计算法的技巧（如果单纯从算法的角度来说），当然在实际硬件中也存在由于硬件的限制（比如磁带）导致顺序的问题。</p><p>从算法的角度来说，堆栈具有的作用</p><ul><li>使用堆栈可以很方便的保存和取用信息，因此长被用作算法和程序中的辅助存储结构，临时保存信息，供后面操作中使用。</li><li>例如：操作系统中的函数调用栈，浏览器中的前进、后退功能。</li><li>堆栈的后进先出规则，可以保证特定的存取顺序。</li><li>例如：翻转一组元素的顺序、铁路列车车辆调度。</li></ul><h3 id="0x03-堆栈的题目"><a href="#0x03-堆栈的题目" class="headerlink" title="0x03 堆栈的题目"></a>0x03 堆栈的题目</h3><h3 id="3-1-有效的括号"><a href="#3-1-有效的括号" class="headerlink" title="3.1 有效的括号"></a>3.1 有效的括号</h3><p><a href="https://leetcode.cn/problems/valid-parentheses/">T20 有效的括号</a></p><p>一个非常经典的题目，算是堆栈的先入后出的典型的应用</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs gradle"><span class="hljs-keyword">class</span> Solution:<br>    <span class="hljs-keyword">def</span> isValid(self, s: str) -&gt; bool:<br>        <span class="hljs-keyword">if</span> (len(s)%<span class="hljs-number">2</span>==<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span><br><br>        flagStack=[]<br>        pair=&#123;<span class="hljs-string">&#x27;)&#x27;</span>:<span class="hljs-string">&#x27;(&#x27;</span>,<span class="hljs-string">&#x27;&#125;&#x27;</span>:<span class="hljs-string">&#x27;&#123;&#x27;</span>,<span class="hljs-string">&#x27;]&#x27;</span>:<span class="hljs-string">&#x27;[&#x27;</span>&#125;<br>        <span class="hljs-keyword">for</span> <span class="hljs-keyword">char</span> in s:<br>            <span class="hljs-keyword">if</span> len(flagStack)!=<span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">char</span> in pair and flagStack[-<span class="hljs-number">1</span>]==pair[<span class="hljs-keyword">char</span>]:<br>                    flagStack.<span class="hljs-keyword">pop</span>()<br>                <span class="hljs-keyword">else</span>: <br>                    flagStack.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">char</span>)<br>            <span class="hljs-keyword">else</span>:<br>                flagStack.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">char</span>)<br><br>        <span class="hljs-keyword">if</span> flagStack:<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">True</span><br></code></pre></td></tr></table></figure><h3 id="3-2-基本计算器II"><a href="#3-2-基本计算器II" class="headerlink" title="3.2 基本计算器II"></a>3.2 基本计算器II</h3><p><a href="https://leetcode.cn/problems/basic-calculator-ii/">T227 基本计算器II</a></p><p>重点思想是将每个元素，都看成num在option的后面，但是逐个取出num和option；z这里没有考虑 到python的字符串操作功能</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs routeros">s Solution:<br>    def calculate(self, s: str) -&gt; int:<br>        <span class="hljs-attribute">size</span>=len(s)<br>        stack=[]<br>        <span class="hljs-attribute">op</span>=<span class="hljs-string">&#x27;+&#x27;</span><br>        <span class="hljs-attribute">index</span>=0<br>        <span class="hljs-attribute">num</span>=<span class="hljs-string">&#x27;&#x27;</span><br>        <span class="hljs-keyword">while</span>(index&lt;size):<br><br>            <span class="hljs-keyword">if</span> s[index] <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;+-*/&#x27;</span>:<br>                # 如果是运算符<br>                <span class="hljs-attribute">num</span>=<span class="hljs-string">&#x27;&#x27;</span><br>                <span class="hljs-attribute">op</span>=s[index]<br>                index+=1<br>            <span class="hljs-keyword">else</span>:<br>                # 需要将num取出来<br>                <span class="hljs-keyword">while</span>(index&lt;size <span class="hljs-keyword">and</span> s[index] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;+-*/&#x27;</span>):<br>                    num+=s[index]<br>                    index+=1<br>                <span class="hljs-attribute">num</span>=int(num)<br>                <span class="hljs-keyword">if</span> <span class="hljs-attribute">op</span>==&#x27;+&#x27;:<br>                    stack.append(num)<br>                elif <span class="hljs-attribute">op</span>==&#x27;-&#x27;:<br>                    stack.append(-num)<br>                elif <span class="hljs-attribute">op</span>==&#x27;*&#x27;:<br>                    <span class="hljs-attribute">top</span>=stack.pop()<br>                    stack.append(top*num)<br>                elif <span class="hljs-attribute">op</span>==&#x27;/&#x27;:<br>                    <span class="hljs-attribute">top</span>=stack.pop()<br>                    stack.append(int(top/num))<br></code></pre></td></tr></table></figure><h3 id="3-3-逆波兰表达式求值"><a href="#3-3-逆波兰表达式求值" class="headerlink" title="3.3 逆波兰表达式求值"></a>3.3 逆波兰表达式求值</h3><p><a href="https://leetcode.cn/problems/evaluate-reverse-polish-notation/">T150 逆波兰表达式求值</a></p><p>思路和T227差不多</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs gradle"><span class="hljs-keyword">class</span> Solution:<br>    <span class="hljs-keyword">def</span> evalRPN(self, tokens: List[str]) -&gt; <span class="hljs-keyword">int</span>:<br>        numStack=[]<br>        <span class="hljs-keyword">for</span> <span class="hljs-keyword">char</span> in tokens:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">char</span> in <span class="hljs-string">&#x27;+-*/&#x27;</span>:<br>                # 取出来前两个数字<br>                num1=numStack.<span class="hljs-keyword">pop</span>()<br>                num2=numStack.<span class="hljs-keyword">pop</span>()<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">char</span>==<span class="hljs-string">&#x27;+&#x27;</span>:<br>                    numStack.<span class="hljs-keyword">append</span>(num1+num2)<br>                elif <span class="hljs-keyword">char</span>==<span class="hljs-string">&#x27;-&#x27;</span>:<br>                    numStack.<span class="hljs-keyword">append</span>(num2-num1)<br>                elif <span class="hljs-keyword">char</span>==<span class="hljs-string">&#x27;*&#x27;</span>:<br>                    numStack.<span class="hljs-keyword">append</span>(num1*num2)<br>                <span class="hljs-keyword">else</span>:<br>                    numStack.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">int</span>(num2/num1))<br>            <span class="hljs-keyword">else</span>:<br>                numStack.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">int</span>(<span class="hljs-keyword">char</span>))<br>        <span class="hljs-keyword">return</span> numStack.<span class="hljs-keyword">pop</span>()<br></code></pre></td></tr></table></figure><h3 id="3-4-字符串解码"><a href="#3-4-字符串解码" class="headerlink" title="3.4 字符串解码"></a>3.4 字符串解码</h3><p>用这个栈来记录当前的元素，如果为】就向后一直搜寻到【，得到num和str，来反复</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs gradle"><span class="hljs-keyword">class</span> Solution:<br>    <span class="hljs-keyword">def</span> decodeString(self, s: str) -&gt; str:<br>        elementStack=[]<br>        temp=<span class="hljs-string">&#x27;&#x27;</span><br>        num=<span class="hljs-string">&#x27;&#x27;</span><br>        <span class="hljs-keyword">for</span> <span class="hljs-keyword">char</span> in s:<br>            elementStack.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">char</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">char</span>==<span class="hljs-string">&#x27;]&#x27;</span>:<br>                # 提取字符串<br>                elementStack.<span class="hljs-keyword">pop</span>()<br>                <span class="hljs-keyword">while</span>(elementStack[-<span class="hljs-number">1</span>]!=<span class="hljs-string">&#x27;[&#x27;</span>):<br>                    top=elementStack.<span class="hljs-keyword">pop</span>()<br>                    temp+=top[::-<span class="hljs-number">1</span>]<br>                elementStack.<span class="hljs-keyword">pop</span>()<br>                <span class="hljs-keyword">while</span>(len(elementStack)&gt;<span class="hljs-number">0</span> and elementStack[-<span class="hljs-number">1</span>].isdigit()):<br>                    num+=elementStack.<span class="hljs-keyword">pop</span>()<br>                # 新的字符<br>                <span class="hljs-keyword">for</span> i in range(<span class="hljs-keyword">int</span>(num[::-<span class="hljs-number">1</span>])):<br>                    elementStack.<span class="hljs-keyword">append</span>(temp[::-<span class="hljs-number">1</span>])<br>                num=<span class="hljs-string">&#x27;&#x27;</span><br>                temp=<span class="hljs-string">&#x27;&#x27;</span><br>        res=<span class="hljs-string">&#x27;&#x27;</span><br>        <span class="hljs-keyword">for</span> e in elementStack:<br>            res+=e<br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><h3 id="3-5-验证栈序列"><a href="#3-5-验证栈序列" class="headerlink" title="3.5 验证栈序列"></a>3.5 验证栈序列</h3><p><a href="https://leetcode.cn/problems/validate-stack-sequences/">T946 验证栈序列</a></p><p>感觉这个就非常考验思维能力，因为pushed和popped相当于为逆反操作，因此可以翻归来按照堆栈的方式继续</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">validateStackSequences</span>(<span class="hljs-params">self, pushed: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], popped: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-comment"># 和括号一样的</span><br>        <span class="hljs-comment"># 既然pushed和popped都是按照stack的规则</span><br>        <span class="hljs-comment"># 反过来也可以用stack的规则来进行排序</span><br>        j=<span class="hljs-number">0</span><br>        stack=[]<br>        j=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> pushed:<br>            stack.append(p)<br>            <span class="hljs-keyword">while</span>(stack <span class="hljs-keyword">and</span> stack[-<span class="hljs-number">1</span>]==popped[j]):<br>                stack.pop()<br>                j+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> j==<span class="hljs-built_in">len</span>(pushed)<br></code></pre></td></tr></table></figure><h3 id="0x04-单调栈（Monotone-stack）基础知识"><a href="#0x04-单调栈（Monotone-stack）基础知识" class="headerlink" title="0x04 单调栈（Monotone stack）基础知识"></a>0x04 单调栈（Monotone stack）基础知识</h3><p>单调栈只是具有更强限制的栈，它需要人为设置规则（设置递增或者递减）的单调栈，由此可以实现的目的为：【可以在常数时间内找到一定数组内的最大值或者最小值；同时精简删除一些无用的信息】，所以在算法设计上需要更加巧妙。</p><p>通常可以分为单调递增栈和单调递减栈，这个顺序和什么是栈顶、什么是栈底一样很难让人理清楚顺序，因此只需要考虑在单调栈中栈的大小顺序是单调的，由此可以设计进出栈的顺序，以单调递增栈为例：</p><ul><li>假设当前进栈元素为 x，如果 x 比栈顶元素小，则直接入栈。</li><li>否则从栈顶开始遍历栈中元素，把小于 x 或者等于 x 的元素弹出栈，直到遇到一个大于 x 的元素为止，然后再把 x 压入栈中。</li></ul><h3 id="0x05-单调栈的使用场景"><a href="#0x05-单调栈的使用场景" class="headerlink" title="0x05 单调栈的使用场景"></a>0x05 单调栈的使用场景</h3><p>网上有很多博客，也有很多的教学视频，这里摘选一些总结的，单调栈主要解决的问题为：</p><ol><li>左侧第一个比当前元素大的元素</li><li>左侧第一个比当前元素小的元素</li><li>右侧第一个比当前元素大的元素</li><li>右侧第一个比当前元素小的元素</li></ol><p>在实际的应用中，需要先将实际问题抽象称为上述的四种问题，再利用单调栈求解，有点过于抽象了，结合问题来看会好一点</p><h3 id="5-1-寻找左侧第一个比当前元素大的元素"><a href="#5-1-寻找左侧第一个比当前元素大的元素" class="headerlink" title="5.1 寻找左侧第一个比当前元素大的元素"></a>5.1 寻找左侧第一个比当前元素大的元素</h3><p>从左到右遍历元素，构造单调递增栈（从栈顶到栈底递增）：一个元素左侧第一个比它大的元素就是将其「插入单调递增栈」时的栈顶元素。如果插入时的栈为空，则说明左侧不存在比当前元素大的元素。</p><h3 id="5-2-寻找左侧第一个比当前元素小的元素"><a href="#5-2-寻找左侧第一个比当前元素小的元素" class="headerlink" title="5.2 寻找左侧第一个比当前元素小的元素 #"></a>5.2 寻找左侧第一个比当前元素小的元素 <a href="https://algo.itcharge.cn/03.Stack/02.Monotone-Stack/01.Monotone-Stack/#22-%E5%AF%BB%E6%89%BE%E5%B7%A6%E4%BE%A7%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AF%94%E5%BD%93%E5%89%8D%E5%85%83%E7%B4%A0%E5%B0%8F%E7%9A%84%E5%85%83%E7%B4%A0">#</a></h3><p>从左到右遍历元素，构造单调递减栈（从栈顶到栈底递减）：一个元素左侧第一个比它小的元素就是将其「插入单调递减栈」时的栈顶元素。如果插入时的栈为空，则说明左侧不存在比当前元素小的元素。</p><h3 id="5-3-寻找右侧第一个比当前元素大的元素"><a href="#5-3-寻找右侧第一个比当前元素大的元素" class="headerlink" title="5.3 寻找右侧第一个比当前元素大的元素 #"></a>5.3 寻找右侧第一个比当前元素大的元素 <a href="https://algo.itcharge.cn/03.Stack/02.Monotone-Stack/01.Monotone-Stack/#23-%E5%AF%BB%E6%89%BE%E5%8F%B3%E4%BE%A7%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AF%94%E5%BD%93%E5%89%8D%E5%85%83%E7%B4%A0%E5%A4%A7%E7%9A%84%E5%85%83%E7%B4%A0">#</a></h3><p>从左到右遍历元素，构造单调递增栈（从栈顶到栈底递增）：一个元素右侧第一个比它大的元素就是将其「弹出单调递增栈」时即将插入的元素。如果该元素没有被弹出栈，则说明右侧不存在比当前元素大的元素。</p><p>从右到左遍历元素，构造单调递增栈（从栈顶到栈底递增）：一个元素右侧第一个比它大的元素就是将其「插入单调递增栈」时的栈顶元素。如果插入时的栈为空，则说明右侧不存在比当前元素大的元素。</p><h3 id="5-4-寻找右侧第一个比当前元素小的元素"><a href="#5-4-寻找右侧第一个比当前元素小的元素" class="headerlink" title="5.4 寻找右侧第一个比当前元素小的元素 #"></a>5.4 寻找右侧第一个比当前元素小的元素 <a href="https://algo.itcharge.cn/03.Stack/02.Monotone-Stack/01.Monotone-Stack/#24-%E5%AF%BB%E6%89%BE%E5%8F%B3%E4%BE%A7%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AF%94%E5%BD%93%E5%89%8D%E5%85%83%E7%B4%A0%E5%B0%8F%E7%9A%84%E5%85%83%E7%B4%A0">#</a></h3><ul><li>从左到右遍历元素，构造单调递减栈（从栈顶到栈底递减）：一个元素右侧第一个比它小的元素就是将其「弹出单调递减栈」时即将插入的元素。如果该元素没有被弹出栈，则说明右侧不存在比当前元素小的元素。</li><li>从右到左遍历元素，构造单调递减栈（从栈顶到栈底递减）：一个元素右侧第一个比它小的元素就是将其「插入单调递减栈」时的栈顶元素。如果插入时的栈为空，则说明右侧不存在比当前元素小的元素。</li></ul><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stata">def 递增栈(nums):<br>    <span class="hljs-keyword">stack</span>=[]<br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums:<br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">stack</span> and num&gt;=<span class="hljs-keyword">stack</span>[-1]:<br>            <span class="hljs-keyword">stack</span>.pop()<br>        <span class="hljs-keyword">stack</span>.<span class="hljs-keyword">append</span>(num)<br></code></pre></td></tr></table></figure><h3 id="0x06-单调栈的实际应用"><a href="#0x06-单调栈的实际应用" class="headerlink" title="0x06 单调栈的实际应用"></a>0x06 单调栈的实际应用</h3><h3 id="6-1-下一个更大元素I"><a href="#6-1-下一个更大元素I" class="headerlink" title="6.1 下一个更大元素I"></a>6.1 下一个更大元素I</h3><p><a href="https://leetcode.cn/problems/next-greater-element-i/">T496 下一个更大元素 I</a></p><p>这个很符合单调栈使用的场景的，所以主要是需要熟悉单调栈求解问题的方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">nextGreaterElement</span>(<span class="hljs-params">self, nums1: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], nums2: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-comment"># 第二种使用单调递增栈，因为nums1是num2的子集，所以可以遍历nums2</span><br>        <span class="hljs-comment"># 构造单调递增栈，求出nums2每个元素右侧下一个最大的元素，然后存储在哈希表中</span><br>        <span class="hljs-comment"># </span><br>        <span class="hljs-comment"># 【具体做法】</span><br>        <span class="hljs-comment"># res存储答案，使用stack表示单调递增栈，使用哈希表num-map存储nums2中比下一个当前</span><br>        <span class="hljs-comment"># 元素大的数值，当前数值：下一个比当前元素大的数值</span><br>        <span class="hljs-comment"># </span><br>        <span class="hljs-comment"># 遍历nums2，对于当前元素，如果小则入栈，如果元素大则一直出栈，出栈元素是第一个大</span><br>        <span class="hljs-comment"># 于当前元素值的元素</span><br>        <span class="hljs-comment"># </span><br>        <span class="hljs-comment"># 遍历玩数组nums2周，建立好哈希表之后，遍历数组1</span><br>        <span class="hljs-comment"># </span><br>        <span class="hljs-comment"># 从num-map中取出对应的值</span><br>        res=[]<br>        stack=[]<br>        num_map=<span class="hljs-built_in">dict</span>()<br><br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums2:<br>            <span class="hljs-keyword">while</span>(stack) <span class="hljs-keyword">and</span> num&gt;stack[-<span class="hljs-number">1</span>]:<br>                num_map[stack[-<span class="hljs-number">1</span>]]=num<br>                stack.pop()<br>            stack.append(num)<br>            <span class="hljs-built_in">print</span>(stack)<br><br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums1:<br>            res.append(num_map.get(num,-<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><h3 id="6-2-下一个更大元素II"><a href="#6-2-下一个更大元素II" class="headerlink" title="6.2 下一个更大元素II"></a>6.2 下一个更大元素II</h3><p><a href="https://leetcode.cn/problems/next-greater-element-ii/">T503 下一个更大元素II</a></p><p>和上面一样，遍历两次就好了</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def nextGreaterElements(self, nums: List[<span class="hljs-keyword">int</span>]) -&gt; List[<span class="hljs-keyword">int</span>]:<br>        <span class="hljs-keyword">res</span>=[]<br>        stack=[]<br>        temp=[-<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums))]<br>        stack_index=[]<br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)*<span class="hljs-number">2</span>):<br>            <span class="hljs-keyword">while</span> stack <span class="hljs-built_in">and</span> nums[i%<span class="hljs-built_in">len</span>(nums)]&gt;stack[-<span class="hljs-number">1</span>]:<br>                <span class="hljs-built_in">index</span>=stack_index.<span class="hljs-keyword">pop</span>()<br>                stack.<span class="hljs-keyword">pop</span>()<br>                temp[<span class="hljs-built_in">index</span>]=nums[i%<span class="hljs-built_in">len</span>(nums)]<br>            stack_index.<span class="hljs-keyword">append</span>(i%<span class="hljs-built_in">len</span>(nums))<br>            stack.<span class="hljs-keyword">append</span>(nums[i%<span class="hljs-built_in">len</span>(nums)])<br>        <span class="hljs-keyword">return</span> temp<br></code></pre></td></tr></table></figure><h3 id="6-3-每日温度"><a href="#6-3-每日温度" class="headerlink" title="6.3 每日温度"></a>6.3 每日温度</h3><p><a href="https://leetcode.cn/problems/daily-temperatures/">T739 每日温度</a></p><p>也是单调栈解题的常规思路</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-function">def <span class="hljs-title">dailyTemperatures</span><span class="hljs-params">(temperatures)</span>:</span><br><span class="hljs-function">        # 因为等价于找第i元素后，高于这个元素的index之差</span><br><span class="hljs-function">        # 可以考虑使用一个stack来记录递减元素的下标</span><br><span class="hljs-function">        size=</span><span class="hljs-built_in">len</span>(temperatures)<br>        ans=[<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ in <span class="hljs-built_in">range</span>(size)]<br>        stack=[]<br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(size):<br>            <span class="hljs-keyword">while</span>( stack <span class="hljs-keyword">and</span> temperatures[i]&gt;temperatures[stack[<span class="hljs-number">-1</span>]]):<br>                index=stack.<span class="hljs-built_in">pop</span>()<br>                ans[index]=i-index<br>            stack.<span class="hljs-built_in">append</span>(i)<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-20252b09ed3a25c8f958da2fa8ef0321_1440w.jpg" alt="img"></p><p>image-20220708222328976</p><h3 id="6-4-股票时间跨度"><a href="#6-4-股票时间跨度" class="headerlink" title="6.4 股票时间跨度"></a>6.4 股票时间跨度</h3><p><a href="https://leetcode.cn/problems/online-stock-span/">T901 股票价格跨度</a></p><p>单调栈可以存储值，也可以存储对应的下标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">StockSpanner</span>:<br>    <span class="hljs-comment"># 小于或等于今天价格的最大连续日数</span><br>    <span class="hljs-comment"># 等价于求解左侧最近的一次大于价格的日数</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br><br>        <span class="hljs-variable language_">self</span>.stack=[]<br>        <span class="hljs-variable language_">self</span>.day=[]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">next</span>(<span class="hljs-params">self, price: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        dayT=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">while</span>(<span class="hljs-variable language_">self</span>.stack <span class="hljs-keyword">and</span> price&gt;=<span class="hljs-variable language_">self</span>.stack[-<span class="hljs-number">1</span>]):<br>            dayT+=<span class="hljs-variable language_">self</span>.day.pop()<br>            <span class="hljs-variable language_">self</span>.stack.pop()<br>        <span class="hljs-variable language_">self</span>.day.append(dayT)<br>        <span class="hljs-variable language_">self</span>.stack.append(price)<br>        <span class="hljs-keyword">return</span> dayT<br></code></pre></td></tr></table></figure><h3 id="6-5-去除重复字母"><a href="#6-5-去除重复字母" class="headerlink" title="6.5 去除重复字母"></a>6.5 去除重复字母</h3><p><a href="https://leetcode.cn/problems/remove-duplicate-letters/">T316 去除重复字母</a></p><p>这一题看上去是和单调栈没有关系的，这里就需要将现实问题转换称为单调栈的问题；首先结果的字典序最小，就是尽可能的将a放到b前面、c放到d前面，因此我们每次遍历的时候需要对应字符放入结果并排序。但是我们要警惕后面是否还剩字符，因此就需要先遍历字符串中字符对应的数量，然后在遍历删除的时候根据字符串是否有剩余来决定删不删字符。amazing！</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs axapta"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    def removeDuplicateLetters(self, s: <span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-meta"># 首先需要统计各种字符串出现的次数</span><br>        <span class="hljs-meta"># 对s进行遍历</span><br>        <span class="hljs-meta"># 要求字典序最小，也就是a要尽可能在b前面，b要尽可能在c前面</span><br>        <span class="hljs-meta"># 这就需要用单调栈来约束，但是需要注意单调栈中元素是否存在</span><br>        charMap=dict()<br>        resStack=[]<br><br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">char</span> <span class="hljs-keyword">in</span> s:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">char</span> <span class="hljs-keyword">in</span> charMap:<br>                charMap[<span class="hljs-built_in">char</span>]+=<span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                charMap[<span class="hljs-built_in">char</span>]=<span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">char</span> <span class="hljs-keyword">in</span> s:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">char</span> not <span class="hljs-keyword">in</span> resStack:<br>                <span class="hljs-meta"># 如果不在resStack中,我需要设置一个尽可能小的序列</span><br>                <span class="hljs-meta"># 同时需要保证还有数</span><br>                <span class="hljs-keyword">while</span> resStack and <span class="hljs-built_in">char</span>&lt;resStack[<span class="hljs-number">-1</span>] and charMap[resStack[<span class="hljs-number">-1</span>]]&gt;<span class="hljs-number">0</span>:<br>                    resStack.pop()<br>                resStack.append(<span class="hljs-built_in">char</span>)<br>                charMap[<span class="hljs-built_in">char</span>]-=<span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                charMap[<span class="hljs-built_in">char</span>]-=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.<span class="hljs-keyword">join</span>(resStack)<br></code></pre></td></tr></table></figure><h3 id="6-6-最短无序连续子数组"><a href="#6-6-最短无序连续子数组" class="headerlink" title="6.6 最短无序连续子数组"></a>6.6 最短无序连续子数组</h3><p><a href="https://leetcode.cn/problems/shortest-unsorted-continuous-subarray/">T581 最短无序子数组</a></p><p>同样的这个需要先分析问题，我们需要找到什么，隐含的就是序列一定是先递增、然后乱序、然后递增，这个第一个递增序列的最大值是后面的最小值、第二个递增序列的最小值是后面的最大值；由此就可以用单调栈的方式来求解问题</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs vim">class Solution:<br>    def findUnsortedSubarray(self, nums: List[<span class="hljs-keyword">int</span>]) -&gt; <span class="hljs-keyword">int</span>:<br>        # 如果必然存在这样的一个数组，那么肯定是单调递增，然后***、然后单调递增<br>        # 从小到大，找到左边界<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(nums)&lt;<span class="hljs-number">2</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        stack=[]<br>        size=<span class="hljs-built_in">len</span>(nums)<br>        <span class="hljs-keyword">left</span>=size-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">right</span>=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(size):<br>            <span class="hljs-keyword">while</span>(stack <span class="hljs-built_in">and</span> nums[i]&lt;nums[stack[-<span class="hljs-number">1</span>]]):<br>                <span class="hljs-keyword">left</span>=<span class="hljs-built_in">min</span>(<span class="hljs-keyword">left</span>,stack.<span class="hljs-keyword">pop</span>())<br>            stack.<span class="hljs-keyword">append</span>(i)<br>        stack[:]=[]<br>        <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(size-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">while</span>(stack <span class="hljs-built_in">and</span> nums[i]&gt;nums[stack[-<span class="hljs-number">1</span>]]):<br>                <span class="hljs-keyword">right</span>=<span class="hljs-built_in">max</span>(<span class="hljs-keyword">right</span>,stack.<span class="hljs-keyword">pop</span>())<br>            stack.<span class="hljs-keyword">append</span>(i)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">right</span>-<span class="hljs-keyword">left</span>+<span class="hljs-number">1</span>&gt;<span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">right</span>-<span class="hljs-keyword">left</span>+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法基础</tag>
      
      <tag>堆栈</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法设计 Vol5</title>
    <link href="/posts/3d7504b6.html"/>
    <url>/posts/3d7504b6.html</url>
    
    <content type="html"><![CDATA[<h2 id="0x01-数组排序"><a href="#0x01-数组排序" class="headerlink" title="0x01   数组排序"></a>0x01   数组排序</h2><h3 id="1-1-冒泡排序"><a href="#1-1-冒泡排序" class="headerlink" title="1.1 冒泡排序"></a>1.1 冒泡排序</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">bubbleSort</span>(<span class="hljs-params">arr</span>):<br>    <span class="hljs-comment"># 冒泡排序的思想</span><br>    <span class="hljs-comment"># 相邻元素之间的比较和变换，将值较小的元素逐步从后面移到前面，值较大的元素从前面移到后面</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 冒泡排序的步骤</span><br>    <span class="hljs-comment"># 逐步将i和i+1元素相比较，如果大小不合适则交换，这样重复一次可以保证下标为n的值最大</span><br>    <span class="hljs-comment"># 之后对n-2元素重复操作，一直到排序结束</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(arr)):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(arr)-i-<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> arr[j]&gt;arr[j+<span class="hljs-number">1</span>]:<br>                arr[j],arr[j+<span class="hljs-number">1</span>]=arr[j+<span class="hljs-number">1</span>],arr[j]<br><br>    <span class="hljs-keyword">return</span> arr<br></code></pre></td></tr></table></figure><p>## 1.2 选择排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">selectSort</span>(<span class="hljs-params">arr</span>):<br>    <span class="hljs-comment"># 选择排序的思想</span><br>    <span class="hljs-comment"># 每一次排序中，从剩余未排序元素中选择一个最小的元素，未排好序的元素最前面的那个元素交换位置</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 选择排序算法步骤</span><br>    <span class="hljs-comment"># 在算法中设置整型变量i，既可以作为排序树木的计算、同时i也作为执行第i次排序的时候，参加排序的后n-i+1元素的位置</span><br>    <span class="hljs-comment"># 整型变量 min_i记录最小元素的下标</span><br>    <span class="hljs-comment"># 结束之中交换两者之间的顺序</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(arr)-<span class="hljs-number">1</span>):<br>        min_i=i<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i+<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(arr)):<br>            <span class="hljs-keyword">if</span> arr[j]&lt;arr[min_i]:<br>                min_i=j<br><br>        <span class="hljs-keyword">if</span> i!=min_i:<br>            arr[i],arr[min_i]=arr[min_i],arr[i]<br><br>    <span class="hljs-keyword">return</span> arr<br></code></pre></td></tr></table></figure><h3 id="1-3-插入排序"><a href="#1-3-插入排序" class="headerlink" title="1.3 插入排序"></a>1.3 插入排序</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">def <span class="hljs-keyword">insertSort(arr):</span><br><span class="hljs-keyword"></span>    <span class="hljs-comment"># 插入排序的基本思想</span><br>    <span class="hljs-comment"># 每一次排序中，将剩余无序列序列的第一个元素，插入到有序序列的适当位置上</span><br>    # <br>    <span class="hljs-comment"># 插入排序的基本步骤</span><br>    <span class="hljs-comment"># 将第一个元素看作一个有序序列</span><br>    <span class="hljs-comment"># 从头到尾扫描无序序列，将扫描到的每个元素插入到有序序列的适当位置上</span><br>    for i in range(<span class="hljs-number">1</span>,len(arr)):<br>        temp=arr[i]<br>        <span class="hljs-keyword">j=i</span><br><span class="hljs-keyword"></span>        <span class="hljs-comment"># 0-（i-1）都是有序数组</span><br>        while <span class="hljs-keyword">j&gt;0 </span><span class="hljs-keyword">and </span>arr[<span class="hljs-keyword">j-1]&gt;temp:</span><br><span class="hljs-keyword"></span>            arr[<span class="hljs-keyword">j]=arr[j-1]</span><br><span class="hljs-keyword"></span>            <span class="hljs-comment"># 因为肯定需要移动一个位置</span><br>            <span class="hljs-keyword">j-=1</span><br><span class="hljs-keyword"></span>        arr[<span class="hljs-keyword">j]=temp</span><br><span class="hljs-keyword"></span><br>    return arr<br></code></pre></td></tr></table></figure><h3 id="1-4-希尔排序（没搞懂干什么）"><a href="#1-4-希尔排序（没搞懂干什么）" class="headerlink" title="1.4 希尔排序（没搞懂干什么）"></a>1.4 希尔排序（没搞懂干什么）</h3><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs stan">def shellSort(arr):<br>    <span class="hljs-comment"># 希尔排序的基本思想</span><br>    <span class="hljs-comment"># 按照一定的间隔取值划分为若干个子序列，每个子序列按照插入排序，然后逐渐缩小间隔进行</span><br>    <span class="hljs-comment"># 下一轮划分子序列和插入排序，一直到最后一轮排序间隔为1</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 希尔排序是在插入排序的基础上进行改进的，因为我们可以看出插入排序在已经排好序的效率非常高</span><br>    <span class="hljs-comment"># 但是插入排序效率比较低的原因是每次只能将数据移动以为</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 希尔排序的算法步骤</span><br>    <span class="hljs-comment"># 1. 确定元素间隔Gap，将序列按照1开始划分为若干个子序列，之间元素的间隔为一个gap</span><br>    <span class="hljs-comment"># 2. 减少间隔数，并重新将整个序列按照新的间隔数分成若干个子序列，并对每个子序列进行排序</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-built_in">size</span>=len(arr)<br>    gap=<span class="hljs-built_in">size</span><span class="hljs-comment">//2</span><br><br>    <span class="hljs-keyword">while</span> gap&gt;<span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(gap,<span class="hljs-built_in">size</span>):<br>            temp=arr[i]<br>            j=i<br>            <span class="hljs-keyword">while</span> j&gt;=gap and arr[j-gap]&gt;temp:<br>                arr[j]=arr[j-gap]<br>                j-=gap<br>            arr[j]=temp<br>        gap=gap<span class="hljs-comment">//2</span><br>    <span class="hljs-keyword">return</span> arr<br></code></pre></td></tr></table></figure><h3 id="1-5-归并排序（很喜欢的排序）"><a href="#1-5-归并排序（很喜欢的排序）" class="headerlink" title="1.5 归并排序（很喜欢的排序）"></a>1.5 归并排序（很喜欢的排序）</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">def mergeSort(arr):<br>    <span class="hljs-comment"># 归并排序的基本思想：</span><br>    <span class="hljs-comment"># 采用经典的分治策略，先递归将当前序列平均分成两半，然后将有序序列合并，最终合并成一个有序序列</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 【算法步骤】</span><br>    <span class="hljs-comment"># 1. 将数组中的所有数据堪称n有序的子序列</span><br>    <span class="hljs-comment"># 2. 将当前序列组中的有序序列两两归并，完成一遍之后序列组里的排序序列的个数减版，每个子序列的长度加倍</span><br>    <span class="hljs-comment"># 3. 重复上述操作得到一个长度为n的有序序列</span><br>    <span class="hljs-comment"># </span><br>    def <span class="hljs-built_in">merge</span>(left_arr,right_arr):<br>        arr=[]<br>        <span class="hljs-keyword">while</span> left_arr <span class="hljs-keyword">and</span> right_arr:<br>            <span class="hljs-keyword">if</span> left_arr[<span class="hljs-number">0</span>]&lt;=right_arr[<span class="hljs-number">0</span>]:<br>                arr.append(left_arr.pop(<span class="hljs-number">0</span>))<br>            <span class="hljs-keyword">else</span>:<br>                arr.append(right_arr.pop[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-keyword">while</span> left_arr:<br>            arr.append(left_arr.pop(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-keyword">while</span> right_arr:<br>            arr.append(right_arr.pop(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-literal">return</span> arr<br><br>    size =<span class="hljs-built_in">len</span>(arr)<br><br>    <span class="hljs-comment"># 边界情况</span><br>    <span class="hljs-keyword">if</span> size&lt;<span class="hljs-number">2</span>:<br>        <span class="hljs-literal">return</span> arr<br><br>    <span class="hljs-keyword">mid</span> =siz<span class="hljs-comment">e//2</span><br><br>    left_arr,right_arr=arr[<span class="hljs-number">0</span>:<span class="hljs-keyword">mid</span>],arr[<span class="hljs-keyword">mid</span>:]<br>    <span class="hljs-literal">return</span> <span class="hljs-built_in">merge</span>(mergeSort(left_arr),mergeSort(right_arr))<br></code></pre></td></tr></table></figure><h3 id="1-6-快速排序（第二喜欢）"><a href="#1-6-快速排序（第二喜欢）" class="headerlink" title="1.6 快速排序（第二喜欢）"></a>1.6 快速排序（第二喜欢）</h3><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs axapta">def quickSort(arr,low,high):<br>    <span class="hljs-meta"># 快速排序，排序的边界条件 low比high小</span><br><br>    def resort(arr,low,high):<br>        <span class="hljs-meta"># 这里的数组中，目标数已经放到的最右边</span><br>        <span class="hljs-meta"># 现在需要把大于目标值的数据放到右边</span><br><br>        flag=low<span class="hljs-number">-1</span><br><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(low,high+<span class="hljs-number">1</span>):<br>            <span class="hljs-meta"># 双指针</span><br>            <span class="hljs-keyword">if</span> arr[j]&lt;=arr[high]:<br>                flag+=<span class="hljs-number">1</span><br>                arr[flag],arr[j]=arr[j],arr[flag]<br><br>        <span class="hljs-keyword">return</span> flag<br><br><br>    def random_index(arr,low,high):<br>        <span class="hljs-meta"># 当然还需要进行一些小的操作</span><br>        <span class="hljs-keyword">index</span>=(low+high)<span class="hljs-comment">//2</span><br><br>        <span class="hljs-meta"># 默认左边都是最小的</span><br>        arr[<span class="hljs-keyword">index</span>],arr[high]=arr[high],arr[<span class="hljs-keyword">index</span>]<br>        <span class="hljs-keyword">return</span> resort(arr,low,high)<br><br>    <span class="hljs-keyword">if</span> low&lt;high:<br>        <span class="hljs-meta"># 每次按照队列排序</span><br>        <span class="hljs-meta"># 需要返回标杆的下标</span><br>        <span class="hljs-keyword">index</span> =random_index(arr,low,high)<br><br>        <span class="hljs-meta"># 之后还需要继续分解</span><br>        quickSort(arr,low,<span class="hljs-keyword">index</span><span class="hljs-number">-1</span>)<br>        quickSort(arr,<span class="hljs-keyword">index</span>+<span class="hljs-number">1</span>,high)<br>    <span class="hljs-keyword">return</span> arr<br></code></pre></td></tr></table></figure><h3 id="1-7-堆排序（没看懂，感觉太难了）"><a href="#1-7-堆排序（没看懂，感觉太难了）" class="headerlink" title="1.7 堆排序（没看懂，感觉太难了）"></a>1.7 堆排序（没看懂，感觉太难了）</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> heapSort(arr):<br>    <span class="hljs-comment"># 借用堆结构所设计的排序算法，将数组转换为大顶堆，重复从大顶堆中取出数值最大的节点，并让剩余的堆维持大顶堆的性质</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 【堆的定义】</span><br>    <span class="hljs-comment"># 大顶堆，根节点值大于子节点值</span><br>    <span class="hljs-comment"># 小顶堆，根节点值小于等于子节点值</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># 【算法步骤】</span><br>    <span class="hljs-comment"># 1. 首先将无序序列构造成第1个大顶堆，使得m个元素的最大值在序列的第一个值</span><br>    <span class="hljs-comment"># 2. 交换序列的最大值元素与最后一个元素的位置</span><br>    <span class="hljs-comment"># 3. 将前面n-1元素组成的序列调整称为一个新的大顶堆，这样得到第2个最大值元素</span><br>    <span class="hljs-comment"># 4. 如此循环下去，知道称为一个有序序列</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-attribute">arrLen</span> =len(arr)<br><br>    <span class="hljs-attribute">def</span> heapify(arr,i):<br>        <span class="hljs-attribute">left</span>=<span class="hljs-number">2</span>*i+<span class="hljs-number">1</span><br>        <span class="hljs-attribute">right</span>=<span class="hljs-number">2</span>*i+<span class="hljs-number">2</span><br>        <span class="hljs-attribute">largest</span>=i<br>        <span class="hljs-attribute">if</span> left&lt;arrLen and arr[left]&gt;arr[largest]:<br>            <span class="hljs-attribute">largest</span>=left<br>        <span class="hljs-attribute">if</span> right&lt;arrLen and arr[right]&gt;arr[largest]:<br>            <span class="hljs-attribute">largest</span>=right<br><br>        <span class="hljs-attribute">if</span> largest!=i:<br>            <span class="hljs-attribute">arr</span>[i],arr[largest]=arr[largest],arr[i]<br>            <span class="hljs-attribute">heapify</span>(arr,largest)<br>        <span class="hljs-attribute">print</span>(arr)<br><br>    <span class="hljs-attribute">def</span> buileMaxHeap(arr):<br>        <span class="hljs-attribute">for</span> i in range(len(arr)//<span class="hljs-number">2</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>):<br>            <span class="hljs-attribute">heapify</span>(arr,i)<br><br>    <span class="hljs-attribute">buileMaxHeap</span>(arr)<br>    <span class="hljs-attribute">print</span>(&#x27;buil&#x27;)<br>    <span class="hljs-attribute">for</span> i in range(arrLen-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,-<span class="hljs-number">1</span>):<br>        <span class="hljs-attribute">arr</span>[<span class="hljs-number">0</span>],arr[i]=arr[i],arr[<span class="hljs-number">0</span>]<br>        <span class="hljs-attribute">arrLen</span>-=<span class="hljs-number">1</span><br>        <span class="hljs-attribute">heapify</span>(arr,<span class="hljs-number">0</span>)<br><br>    <span class="hljs-attribute">return</span> arr<br></code></pre></td></tr></table></figure><h3 id="1-8-计数排序-没意思"><a href="#1-8-计数排序-没意思" class="headerlink" title="1.8 计数排序(没意思)"></a>1.8 计数排序(没意思)</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">def</span> countingSort(arr):<br>    <span class="hljs-comment"># 【基本思想】</span><br>    <span class="hljs-comment"># 使用一个额外的数组counts，其中counts元素是排序数组中arr等于i的个数</span><br>    <span class="hljs-comment"># 根据数组counts来将arr的元素排列到正确位置</span><br>    <span class="hljs-attribute">min_arr</span>,max_arr=min(arr),max(arr)<br><br>    <span class="hljs-attribute">counts</span> =[<span class="hljs-number">0</span> for _ in range(max_arr-min_arr+<span class="hljs-number">1</span>)]<br><br>    <span class="hljs-attribute">for</span> num in arr:<br>        <span class="hljs-attribute">counts</span>[num-min_arr]+=<span class="hljs-number">1</span><br><br>    <span class="hljs-attribute">for</span> j in range(<span class="hljs-number">1</span>,max_arr-min_arr+<span class="hljs-number">1</span>):<br>        <span class="hljs-attribute">counts</span>[j]+=counts[j-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-attribute">res</span>=[<span class="hljs-number">0</span> for _ in range(len(arr))]<br>    <span class="hljs-attribute">for</span> i in range(len(arr)-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>):<br>        <span class="hljs-attribute">res</span>[counts[arr[i]-min_arr]-<span class="hljs-number">1</span>]=arr[i]<br>        <span class="hljs-attribute">counts</span>[arr[i]-min_arr]-=<span class="hljs-number">1</span><br>    <span class="hljs-attribute">return</span> res<br></code></pre></td></tr></table></figure><h3 id="1-9-基数排序（很有意思，注意有负数的情况）"><a href="#1-9-基数排序（很有意思，注意有负数的情况）" class="headerlink" title="1.9 基数排序（很有意思，注意有负数的情况）"></a>1.9 基数排序（很有意思，注意有负数的情况）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">raidxSort</span>(<span class="hljs-params">arr</span>):<br>    <span class="hljs-comment"># 基数排序radix sort【基本思想】</span><br>    <span class="hljs-comment"># 将整数按照位切割称为不同的数字，然后按照个位数来从小到达进行排列</span><br>    <span class="hljs-comment"># 注意这个排序方式是先比较个位数，然后逐渐向高位数</span><br><br>    <span class="hljs-comment"># 首先需要了解到基数排序中最大位数</span><br>    max_radix=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> arr:<br>        <span class="hljs-keyword">if</span> num&gt;<span class="hljs-number">0</span>:<br>            temp=<span class="hljs-built_in">len</span>(<span class="hljs-built_in">str</span>(num))<br>            <span class="hljs-keyword">if</span> max_radix&lt;temp:<br>                max_radix=temp<br>        <span class="hljs-keyword">else</span>:<br>            temp=<span class="hljs-built_in">len</span>(<span class="hljs-built_in">str</span>(num))-<span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> max_radix&lt;temp:<br>                max_radix=temp<br><br>    <span class="hljs-keyword">for</span> radix <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_radix):<br>        <span class="hljs-comment"># 从最低位到最高位开始计算</span><br>        <span class="hljs-comment"># 按照位数来生成</span><br>        buckets=[[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)]<br>        <span class="hljs-comment"># 按照个位数，放到每个篮子，再按照十位数放到篮子</span><br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> arr:<br>            <span class="hljs-comment"># 提取radix对应的位数，0代表个位</span><br>            <span class="hljs-comment"># 这里使用转换为字符串的方式来方便理解</span><br><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">str</span>(<span class="hljs-built_in">abs</span>(num)))&lt;(radix+<span class="hljs-number">1</span>):<br>                <span class="hljs-comment"># 位数不够的时候，说明这个位置没有0，就需要放到第一格</span><br>                buckets[<span class="hljs-number">0</span>].append(num)<br>            <span class="hljs-keyword">else</span>:<br>                index=<span class="hljs-built_in">str</span>(<span class="hljs-built_in">abs</span>(num))[<span class="hljs-built_in">len</span>(<span class="hljs-built_in">str</span>(<span class="hljs-built_in">abs</span>(num)))-<span class="hljs-number">1</span>-radix]<br>                buckets[<span class="hljs-built_in">int</span>(index)].append(num)<br>        <span class="hljs-comment"># 提取到buckets之后需要重新解析arr中</span><br>        arr.clear()<br>        <span class="hljs-keyword">for</span> bukcet <span class="hljs-keyword">in</span> buckets:<br>            <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> bukcet:<br>                arr.append(num)<br>    neg_arr=[]<br>    pos_arr=[]  <br><br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> arr:<br>        <span class="hljs-keyword">if</span> num&gt;<span class="hljs-number">0</span>:<br>            pos_arr.append(num)<br>        <span class="hljs-keyword">else</span>:<br>            neg_arr.append(num)<br>    <span class="hljs-keyword">return</span> neg_arr[::-<span class="hljs-number">1</span>]+pos_arr<br></code></pre></td></tr></table></figure><h3 id="1-10-桶排序（听着就没意思）"><a href="#1-10-桶排序（听着就没意思）" class="headerlink" title="1.10 桶排序（听着就没意思）"></a>1.10 桶排序（听着就没意思）</h3><p>跳过</p><p>排序相关的题目：</p><p><a href="https://leetcode.cn/problems/rotate-array/">189 轮转数组</a></p><p><a href="https://leetcode.cn/problems/plus-one/">66 加一</a></p><p><a href="https://leetcode.cn/problems/find-pivot-index/">724 寻找数组中心下标</a></p><p><a href="https://leetcode.cn/problems/max-consecutive-ones/">485 最大连续1的个数</a></p><p><a href="https://leetcode.cn/problems/product-of-array-except-self/">238除自身以外乘积</a></p><p><a href="https://leetcode.cn/problems/diagonal-traverse/">498对角线遍历</a></p><p><a href="https://leetcode.cn/problems/rotate-image/">48旋转图像</a></p><p><a href="https://leetcode.cn/problems/pascals-triangle/">118 杨辉三角</a></p><p><a href="https://leetcode.cn/problems/pascals-triangle-ii/">119 杨辉三角2</a></p><p><a href="https://leetcode.cn/problems/set-matrix-zeroes/">73矩阵置零</a></p><p><a href="https://leetcode.cn/problems/spiral-matrix/">54螺旋数组</a></p><p><a href="https://leetcode.cn/problems/spiral-matrix-ii/">59 螺旋数组2</a></p><p><a href="https://leetcode.cn/problems/move-zeroes/">移动零</a></p><p><a href="https://leetcode.cn/problems/kth-largest-element-in-an-array/">215数组第K大元素</a></p><p><a href="https://leetcode.cn/problems/sort-colors/">75颜色分类</a></p><p><a href="https://leetcode.cn/problems/relative-ranks/">506 相对名次</a></p><p><a href="https://leetcode.cn/problems/sort-an-array/">912排序数组</a></p><p><a href="https://leetcode.cn/problems/merge-sorted-array/">88合并两个有序数组</a></p><p><a href="https://leetcode.cn/problems/majority-element/">169 多数元素</a></p><p><a href="https://leetcode.cn/problems/maximum-gap/">最大间距</a></p><h2 id="0x02-二分查找"><a href="#0x02-二分查找" class="headerlink" title="0x02 二分查找"></a>0x02 二分查找</h2><p>注意二分查找不同边界条件可以求解不同的问题，可以简单的查找数组中存在元素的位置，也可以查找第一个大于等于数组的下标，也可以查找最后一个小于等于该数据的下标，也可以查询相关的元素的值</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs julia"><span class="hljs-comment"># ====================================================================#</span><br><span class="hljs-comment"># ==========================二分查找====================================#</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 【算法概述】</span><br><span class="hljs-comment"># 确定待查找元素所在的区间范围，再逐步缩小范围，直到找到元素或者找不到该元素为止</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 【算法思想】</span><br><span class="hljs-comment"># 经典的减而治之的思想，减小问题规模来解决问题，每一次查找排除掉一定不存在目标元素的</span><br><span class="hljs-comment"># 区间，在剩下可能存在的目标元素的区间中继续查找，每一次通过一些条件判断，将待搜索的</span><br><span class="hljs-comment"># 区间逐渐缩小，来达到减少问题规模的目的</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 【算法过程】</span><br><span class="hljs-comment"># 1. 每次查找从数组的中间元素开始，如果中间元素正好是查找的元素，则搜索过程结束</span><br><span class="hljs-comment"># 2. 如果特定元素大于或者小于中间元素，则在大于或者小于的元素中查找</span><br><span class="hljs-comment"># 3. 如果在某一步骤数组为空则代表找不到</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 【算法重点】</span><br><span class="hljs-comment"># 1. 区间的开闭问题？</span><br><span class="hljs-comment"># 2. mid的取值问题</span><br><span class="hljs-comment"># 3. 出界条件的判断？</span><br><span class="hljs-comment"># 4. 搜索区间的范围选择问题？</span><br><span class="hljs-comment"># =====================================================================#</span><br><span class="hljs-comment"># </span><br><br>def baseBinarySearch(nums,target):<br>    <span class="hljs-comment"># nums是一个升序</span><br>    <span class="hljs-comment"># 存在下标就返回</span><br>    <span class="hljs-comment"># 不存在就返回-1</span><br>    <span class="hljs-comment"># 因为要返回下标，所以采用index采用</span><br>    left=<span class="hljs-number">0</span><br>    right=len(nums)-<span class="hljs-number">1</span><br>    <span class="hljs-comment">#=========================================#</span><br>    <span class="hljs-comment"># 【二分查找的开闭问题】</span><br>    <span class="hljs-comment"># 第一种：左闭右闭，区间中所有的点都可以得到</span><br>    <span class="hljs-comment"># 第二种：左闭右开，右边界的点不能被取到</span><br>    <span class="hljs-comment">#=========================================#</span><br><br>    <span class="hljs-keyword">while</span>(left&lt;=right):<br>    <span class="hljs-comment">#=========================================#</span><br>    <span class="hljs-comment"># 【出界条件的判断】</span><br>    <span class="hljs-comment"># left&lt;=right</span><br>    <span class="hljs-comment"># 说明查找的元素不存在</span><br>    <span class="hljs-comment"># left&lt;right</span><br>    <span class="hljs-comment"># 此时查找的区间不</span><br>    <span class="hljs-comment"># </span><br>    <span class="hljs-comment">#=========================================#</span><br>        mid=(left+right)//<span class="hljs-number">2</span><br>    <span class="hljs-comment">#=========================================#</span><br>    <span class="hljs-comment"># 【MID的取值问题】</span><br>    <span class="hljs-comment"># 第一种：(left+right)//2</span><br>    <span class="hljs-comment"># 第二种：left+（right-left）//2</span><br>    <span class="hljs-comment"># 前者是常见写法，后者是为了防止整型溢出。//2的代表的中间数是向下取整</span><br>    <span class="hljs-comment"># 同时这种倾向于寻找左边的数组，这里可以选择</span><br>    <span class="hljs-comment"># mid=(left+right+1)//2或者mid=left（right-left+1）//2</span><br>    <span class="hljs-comment"># 同时这里的查找过成功1/2，也可以选择靠左一点、或者靠右一点</span><br>    <span class="hljs-comment"># 从一般的意义上来说，趣中间位置元素在平均意义中达到的效果最好</span><br>    <span class="hljs-comment">#=========================================#</span><br>        <span class="hljs-keyword">if</span> nums[mid]&lt;target:<br>    <span class="hljs-comment">#=========================================#</span><br>    <span class="hljs-comment"># 【搜索区间的选择】</span><br>    <span class="hljs-comment"># 直接法，在循环体中元素之间返回</span><br>    <span class="hljs-comment"># 排除法，在循环体中排出目标元素一定不存在区间</span><br>    <span class="hljs-comment">#=========================================#</span><br><br>            <span class="hljs-comment"># 说明target在右边</span><br>            left=mid+<span class="hljs-number">1</span><br>        elif nums[mid]&gt;target:<br>            <span class="hljs-comment"># 说明target在左边</span><br>            right=mid-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> mid<br><br>    <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br><br>nums=[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">44</span>]<br>target=<span class="hljs-number">4</span><br><span class="hljs-comment"># print(baseBinarySearch(nums,target))</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/binary-search/">704 二分查找</a></p><p><a href="https://leetcode.cn/problems/guess-number-higher-or-lower/">374 猜数字大小</a></p><p><a href="https://leetcode.cn/problems/search-insert-position/">35 搜索插入位置</a></p><p><a href="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/">34 排序数组中查找元素的第一个和最后一个元素</a></p><p><a href="https://leetcode.cn/problems/two-sum-ii-input-array-is-sorted/">167两数之和</a></p><p><a href="https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array/">153 寻找旋转数组最小值</a></p><p><a href="https://leetcode.cn/problems/search-a-2d-matrix/">74 搜索二维矩阵</a></p><p><a href="https://leetcode.cn/problems/sqrtx/">x的平方根</a></p><h2 id="0x3-双指针"><a href="#0x3-双指针" class="headerlink" title="0x3 双指针"></a>0x3 双指针</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># ====================================================================#</span><br><span class="hljs-comment"># ==========================数组双指针====================================#</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 【算法概述】</span><br><span class="hljs-comment"># 在遍历元素的过程中，不是使用单个指针进行访问，而是使用双指针来访问达到目的</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># 【算法思想】</span><br><span class="hljs-comment"># 根据指针的方向，可以分为</span><br><span class="hljs-comment"># 对撞指针：两个指针的方向相反</span><br><span class="hljs-comment"># 快慢指针：指针方向相同</span><br><span class="hljs-comment"># 分离双指针：如果两个指针分别属于不同的数组或者链表</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment"># =====================================================================#</span><br><span class="hljs-comment"># </span><br>def threeSum(nums):<br>    res=[]<br>    def twoSum(arr,target):<br>        <span class="hljs-attribute">left</span>=0<br>        <span class="hljs-built_in">print</span>(arr)<br>        <span class="hljs-built_in">print</span>(target)<br>        <span class="hljs-attribute">right</span>=len(arr)-1<br><br>        <span class="hljs-keyword">while</span>(left&lt;right):<br>            <span class="hljs-attribute">temp_sum</span>=arr[left]+arr[right]<br>            <span class="hljs-keyword">if</span> <span class="hljs-attribute">temp_sum</span>==target:<br>                res.append([target*(-1),arr[left],arr[right]])<br>                left+=1<br>                <span class="hljs-attribute">right-</span>=1<br>            elif temp_sum&lt;target:<br>                left+=1<br>            elif temp_sum&gt;target:<br>                <span class="hljs-attribute">right-</span>=1<br>    nums.sort()<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(nums)):<br>        <span class="hljs-keyword">if</span> nums[i]&gt;0:<br>            break<br>        <span class="hljs-keyword">if</span> i&gt;0 <span class="hljs-keyword">and</span> nums[i]==nums[i-1]:<br>            continue<br>        twoSum(nums[i+1:],(-1)*nums[i])        <br>    return res<br>nums = [-1,0,1,2,-1,-4]<br><span class="hljs-comment"># print(threeSum(nums))</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/two-sum-ii-input-array-is-sorted">167 两数之和</a></p><p><a href="https://leetcode.cn/problems/reverse-string">344 反转字符串</a></p><p><a href="https://leetcode.cn/problems/reverse-vowels-of-a-string">345反转字符串中的元音字母</a></p><p><a href="https://leetcode.cn/problems/container-with-most-water">11 最多的容器</a></p><p><a href="https://leetcode.cn/problems/3sum">15三数之和</a></p><p><a href="https://leetcode.cn/problems/3sum-closest">16最接近的三数之和</a></p><h2 id="0x04-滑动窗口"><a href="#0x04-滑动窗口" class="headerlink" title="0x04 滑动窗口"></a>0x04 滑动窗口</h2><p>下周任务</p><h2 id="0x05"><a href="#0x05" class="headerlink" title="0x05"></a>0x05</h2><p>简单题中也会蕴含着一些基本的操作</p><p>比如设置一些标志位、边界条件的设计、辅助元素的设置，这些需要潜移默化的刷题的过程中才能看到</p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>数组</tag>
      
      <tag>算法设计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络 Vol4 ｜ 网络层协议</title>
    <link href="/posts/d93fb6.html"/>
    <url>/posts/d93fb6.html</url>
    
    <content type="html"><![CDATA[<p>在应用层我们可以解释数据传输的目的、在运输层分析可以看出两个主机进程之间逻辑通信、在本次的网络层将涉及主机与主机之间的通信服务是如何实现的。由于网络层的复杂性，其可以分为数据层面和控制层面。</p><h3 id="0x01-运输层overview"><a href="#0x01-运输层overview" class="headerlink" title="0x01 运输层overview"></a>0x01 运输层overview</h3><p>假设从H1向H2转送信息 </p><p>H1中的网络层取得来自于H1运输层的报文段，将每个报文段封装成一个数据报，然后向相邻路由器R1发送数据， 在网络中经过多个路由器的转发和控制，到达接收方的路由器；</p><p>在网络的过程中每台路由器的功能是从输入链路向输出链路的转变；控制层面是协调本地的路由器转发动作，然后整个过程的数据报沿着origin和end主机之间的路由器路径进行主机之间端到端传送。 </p><p>在接收方主机H2收到来自相邻路由器的R2的数据报，提取出运输层的数据段，并将其上交给H2的运输层，在传递给应用层</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ebed608b9c5291b117a61b30d80b1e71_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>转发（forwarding）：指的将分组从一个输入链路接口转移到适当的输出链路接口的路由本地工作</p><p>路由选择（routing）：确定分组从origin到end次的网络范围处理过程</p><p>转发表（forwarding table）：路由器检查到达分组首部的一个或多个字段值，进而实现这些首部值在其转发表中索引，通过这种方法来转发分组</p><p>转发表制定的方法？？</p><ol><li>传统方法，路由器中物理存在的所有转发表的内容是由人类网络操作员直接配置的</li><li>SDN方法，远程控制器计算和分发转发表来供每台路由器选择；这个需要设计在具有高可靠性和冗余性的远程数据中心，由IDP或者第三方管理。此时路由器和远程控制器之间的通信通过软件定义网络（software defined networking，SDN）来控制</li></ol><h3 id="0x02-运输层分析"><a href="#0x02-运输层分析" class="headerlink" title="0x02 运输层分析"></a>0x02 运输层分析</h3><p>我们可以用开阔的视野来专注于我们引入的新东西并考虑网络层可能提供的不同类型的服务；在这样一个复杂的情况中可能存在的问题包括：</p><ol><li>运输层如何将多个分组交付给目的地？</li><li>运输层之后是否可以保持顺序？</li><li>发送两个分组和接收分组的时间间隔相同吗？</li><li>网络层如何提供拥塞的信息给运输层？</li><li>在发送主机和接收主机之间连接运输层通道的抽象特性是什么？</li><li>&#x3D;&#x3D;&#x3D;</li></ol><p>这些答案都是对网络层提供的服务模型进行描述所决定。网络服务模型（network service model）定义数据分组在分送与接收端系统之间端到端的运输特性</p><p>我们<strong>首先</strong>考虑网络层所能提供可能的服务：</p><ol><li>确保交付，该服务确保分组将最终到达目的地</li><li>时延上界，在特定的主机与主机之间的最大延迟</li><li>有序分组交付，确保分组按照发送顺序到达目的地</li><li>确保最小带宽，模仿发送和接收按照特定速率传输链路</li><li>安全性，网络层能够在源加密所有数据报，并在目的地解密这些分组</li><li>以及一些其他的方式</li></ol><p>但是！因特网的网络层只提供一种服务，尽力而为服务（best- effort service）；其他的网络体系结构定义和实现了超过因特网尽力而为服务的服务模型，例如ATM网络体系实现了确保顺序恶化时间延迟、有边界时延迟的确保足校带宽、以及一些继承服务体系结构提供端到端的时延保证。但是因特网这种尽力而为的服务模型已经被证明足够好。</p><p>后面我们将讨论：</p><ul><li>路由器内部硬件操作，包括输入和输出分组、内部交换机制以及分组排队和调度 </li><li>传统的ip转发，其中分组基于它们的目的IP地址转发到输出端口，将学习到IP寻址</li></ul><p>以及更近一步的一般转发，基于大量首部值来转发到输出端口</p><p>转发和交换两个术语；约定</p><ul><li>分组交换机：指的是一台通用分组交换，根据分组首部字段的值，从输入链路转移到输出分组 </li><li>链路层交换机：根据链路帧中的字段值做出转发决定 </li><li>路由器（router）：其他的分组交换机称为路由器，基于网络层的数据报中的首部字段值做出转发决定</li></ul><h3 id="0x03-路由器工作原理"><a href="#0x03-路由器工作原理" class="headerlink" title="0x03 路由器工作原理"></a>0x03 路由器工作原理</h3><ol><li>输入端口（input port）（硬件）</li><li>执行进入 物理链路的物理层功能</li><li>链路远端的数据链路层交互来执行数据链路层功能</li><li>输入端口需要执行查找功能</li><li>对于控制分组从输入端口转发到路由选择处理器</li><li>交换结构（switch）（硬件）</li><li>将路由器的输入端口连接到输出端口</li><li>网络路由器内部网络</li><li>输出端口（output port）（硬件）</li><li>存储从交换结构接收的分组，并执行必要的链路层和物理层功能</li><li>路由选择处理器（router select process）</li><li>传统控制，执行路由选择协议、维护路由选择表与关联链路状态信息</li><li>SDN控制，与远程控制器通信、输入输出端口安装表项</li></ol><p>为什么路由器需要使用硬件实现？比如10Gpbs和64字节为单位的IP数据报，其输入端口在另一个数据报到达之间只有51.2ns来处理数据报，如果N各端口结合在一块线路卡上，数据处理速率必须以N倍速率来运行，远远超过软件实现的速率；数据层面以ms或者s的方面运行，执行包括路由选择协议、上下线路响应、远程控制通信和执行管理功能</p><p>在深入讨论路由器内部细节之前，我们需要明确路由器处理需要什么信息？</p><ol><li>有目的地转发</li><li>通用转发</li></ol><h3 id="3-1-输入端口处理和基于目的地转发"><a href="#3-1-输入端口处理和基于目的地转发" class="headerlink" title="3.1  输入端口处理和基于目的地转发"></a>3.1  输入端口处理和基于目的地转发</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9a5ed44fb2d5310ac9b15c4f09cd34e0_1440w.jpg" alt="img"></p><p>image-20220628112516078</p><ol><li>线路端接</li><li>数据链路处理（协议、拆封）</li><li>查找、转发、排队</li><li>交换结构</li></ol><p>首先看出一个最基本的例子，路由器的输入端口分为四条链路，我们不必对路由器的映射包括40亿可能存在的地址；我们可以设置不同的匹配原则来进行交换，但是由于IP地址编码的存在，我们需要使用最大前缀匹配规则（longest prefix matching rule）来进行匹配</p><p>在转发表确定的此技术，查找是简单的，也就是实现硬件逻辑下的最大前缀IP，但是在Gpbs速率下，这种查找必须在ns执行，因此必须要用硬件执行查找、还需要对大型转发表使用超出简单线性搜索技术，硬件常时间DRAM、SRAM、TCAM等</p><p>确定分组的输出端口，该分组能够发送进入交换结构，如果其他分组正在使用，我们在进入交换结构时候暂时阻塞，因此一个被阻塞的分组必须在输入端口出排队，并等待稍后被及时调度来通过交换结构</p><ol><li>必须出现物理层和链路层处理</li><li>必须检查分组的版本号、检验和以及寿命字段，并且重写这两个字段</li><li>必须更新用户网络管理的计数器</li></ol><p>匹配+动作</p><h3 id="3-2-交换"><a href="#3-2-交换" class="headerlink" title="3.2  交换"></a>3.2  交换</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-34f3a8e09cc2b824110415ea6a36a920_1440w.jpg" alt="img"></p><p>image-20220628112544656</p><p>三种交换技术</p><ol><li>内存交换</li><li>总线交换</li><li>互联网络交换</li></ol><h3 id="3-3-输出端口处理"><a href="#3-3-输出端口处理" class="headerlink" title="3.3 输出端口处理"></a>3.3 输出端口处理</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5bc238a749b53544103f694aca367258_1440w.jpg" alt="img"></p><p>image-20220628112552623</p><h3 id="3-4-排队分析"><a href="#3-4-排队分析" class="headerlink" title="3.4 排队分析"></a>3.4 排队分析</h3><p>在输入端口和输出端口都会出现排队的现象，当路由器的缓存空间耗尽的时候就会出现丢包的现象</p><p>那么在什么情况下（输入速率、N和交换速率）在多少的时候会出现拥堵？如何缓解拥堵现象？</p><ol><li>输入排队：线路前部阻塞（HOL head of the line）</li><li>输出排队，弃尾（drop tail）、以及提供主动队列管理（active queue management，AQM算法）、随机早期检测（Random Early Detection，RED）</li></ol><h3 id="3-5-分组调度"><a href="#3-5-分组调度" class="headerlink" title="3.5  分组调度"></a>3.5  分组调度</h3><p>排队的分组如何经输出链路传输的问题，这里存在一些基本的规则</p><ol><li>先进先出</li><li>优先权排序 priority queuing</li><li>循环排序规则 round robin queueing discipline</li></ol><h3 id="0x04-网络层协议-–-网际协议（IPv4、IPv6）"><a href="#0x04-网络层协议-–-网际协议（IPv4、IPv6）" class="headerlink" title="0x04 网络层协议 – 网际协议（IPv4、IPv6）"></a>0x04 网络层协议 – 网际协议（IPv4、IPv6）</h3><h3 id="4-1-IPv4数据报格式"><a href="#4-1-IPv4数据报格式" class="headerlink" title="4.1 IPv4数据报格式"></a>4.1 IPv4数据报格式</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c9e704fe4f780c176e3f54f2534ff665_1440w.jpg" alt="img"></p><p>image-20220628112708344</p><p>关键字段分析</p><ol><li>版本号</li><li>首部长度</li><li>服务类型TOS</li><li>数据报长度，因为该字节为16bit，所以IP数据报理论长度为65536；但是一般很少超过1500</li><li>标识、标志、片偏移，与IP分区有关，但是IPv6不允许分组</li><li>寿命TTL，time to live</li><li>协议，比如6标识TCP、17表示UDP；协议号类似运输层报文段中端口号字段起到的作用</li><li>首部检验和</li><li>源和目标IP地址</li><li>选项，意味着IP首部允许被拓展</li><li>数据，有效载荷，处理TCP、UDP还包括ICMP等协议</li></ol><h3 id="4-2-IPv4数据报切片"><a href="#4-2-IPv4数据报切片" class="headerlink" title="4.2 IPv4数据报切片"></a>4.2 IPv4数据报切片</h3><p>网络层下面的链路层的不同，其承载网络分组的长度也不同，有的协议能承载大数据报、有的协议只能承载小的分组。比如以太网不能超过1500字节、某些广域网不能超过576字节，这个被称为最大传送单元（Maximum Transmission Unit，MTU）</p><p>为了保证在所以的链路中都可以传输，我们需要将IP数据报中的数据分片成多个较小的IP数据报，用单独的链路层帧（Frame）来封装这些较小的IP数据报，然后通过输出链路发送这些帧，每个较小的数据报称为片（Fragment）</p><h3 id="4-3-IPv4编址"><a href="#4-3-IPv4编址" class="headerlink" title="4.3 IPv4编址"></a>4.3 IPv4编址</h3><p>在讨论IP编址之前，我们简要了解主机与路由器接入网络的方法，一台主机通常只有一条链路连接到网络，当主机中的IP想发送一个数据报，他就需要在该链路发送，这件称为接口（interface）；路由器由于是网络中的设置，因此具有多个接口；同时因为主机和路由器都可以发送和接收IP数据报，因此都需要拥有自己的iP地址，从技术上说，IP地址与一个接口相关联，而不是与包括该接口的主机或路由器相关联</p><p>IPv4采用点分十进制记法，比如192.168.0.1</p><p>在全球因特网中每台主机和路由器上的每个接口都必须有全球唯一的IP地址（在NAT后面的接口除外），这些地址不能随意自由选择</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f267f2d0a0ae085421d2eb45ec5f472a_1440w.jpg" alt="img"></p><p>image-20220628112808159</p><p>互联这主机接口和路由器接口形成一个子网（subnet），IP编址为这个子网分配一个地址，223.1.1.0&#x2F;24.称为子网掩码（network mask）</p><p>因特网的地址分配策略为无类别域间路由选择（Classless interDomain routing，CIDR）将子网寻址概念化为a.b.c.d&#x2F;x 构成IP地址的网络部分；网络中剩余的32-x比特被认为是用于区分该组织内部设置的，其中的额所有的设备具有相同的前缀</p><p>在CIDR被采用之前，IP地址的网络部分为选址为8、16、24比特，称为分类编址（classful addressing），称为A、B和C类网络，但是C类网络只能使用254台、B类可以65534太多了</p><p>特殊的IP地址，255.255.255.255为IP广播地址，该数据交付给同一个网络的所有主机</p><p>在详学习IP编址，我们需要知道主机或者子网最初是如何得到地址的</p><p>第一步：获取地址，可以从ISP获得、ISP从因特网名字和编号分配机构（Internet corporation for assigned Names and Numbers，ICANN）管理，其还管理DNS服务器</p><p>第二部，主机与路由器接口分配IP地址，系统管理员通常手工配置IP地址，主机地址也可以手动配置但是通常使用动态主机配置协议（Dynamic Host configuration，DHCP），允许主机自动获取一个IP地址，允许主机每次和网络连接的时候都会获得一个相同的IP地址或者临时的IP地址</p><h3 id="4-4-地址转换与通信过程"><a href="#4-4-地址转换与通信过程" class="headerlink" title="4.4 地址转换与通信过程"></a>4.4 地址转换与通信过程</h3><p>对于家庭网络等专用网络（private network）和具有专用地址的地域（realm with private address），其中的IP地址只有在专用网络中才有作用；但是在专用网中至少需要有一个公网IP地址的设备，此时这个路由器是如何获取自己的iP地址的？使用的是ISP提供的DHCP服务</p><p>之间通信的过程是这样的：</p><p>专用网10.0.01:3345想和公网IP的128.119.40.186:80进行通信；专用网中的公网IP路由器为138.76.29.7，这个时候可以将专用网和公网IP路由器新增一个映射，10.0.0.1:3345和128.76.29.7:5001（5001任意），然后数据通过128.76.29.7：5001和128.119.40.186：80进行通信，路由器负责两者之间的转换，同时路由的NAT协议支持超过60000个并行连接</p><p>NAT协议的批评者指出：</p><ol><li>路由器或者设备是用来进行进程寻址、而不是主机寻址的，这种违规用法在运行家庭网络中的服务器会出现问题；这里需要NAT穿越（traversal）或者通用即插即用（Universal Plug and play，UPnP协议）</li><li>违反了主机之间应当直接对话的原则</li></ol><h3 id="4-5-IPv6"><a href="#4-5-IPv6" class="headerlink" title="4.5 IPv6"></a>4.5 IPv6</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-876d5895df299a9ee96cac23a5c2ca0a_1440w.jpg" alt="img"></p><p>image-20220628112904862</p><h3 id="0x05-控制层面"><a href="#0x05-控制层面" class="headerlink" title="0x05 控制层面"></a>0x05 控制层面</h3><p>其中算法过于硬核，这里不做介绍，可以看出转发表的制作分为两种</p><ol><li>预先物理设置</li><li>由SDN来软件控制</li></ol><p>软件控制面临集中还是分散的区别、面临静态路由和动态路由、面临负载敏感或迟钝的问题</p><h3 id="0x06-实际应用和感悟"><a href="#0x06-实际应用和感悟" class="headerlink" title="0x06 实际应用和感悟"></a>0x06 实际应用和感悟</h3><h3 id="6-1-一个实际的例子"><a href="#6-1-一个实际的例子" class="headerlink" title="6.1 一个实际的例子"></a>6.1 一个实际的例子</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ac47b57633fdade989eef483214d55e7_1440w.jpg" alt="img"></p><p>image-20220628113107112</p><h3 id="6-2-一些实际的应用"><a href="#6-2-一些实际的应用" class="headerlink" title="6.2 一些实际的应用"></a>6.2 一些实际的应用</h3><p>ssh连接远程主机</p><p>搭建自己的webdav云盘</p><p>开放自己的MC服务器</p><p>利用腾讯云服务器实现内网穿透</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>网络层</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络 Vol3 ｜ 运输层协议</title>
    <link href="/posts/f3860fe5.html"/>
    <url>/posts/f3860fe5.html</url>
    
    <content type="html"><![CDATA[<p>尝试将课本中的知识重新整理成为自己的post，虽然用自己的理解去描述可能会带来错误，但是“太对的话并没有新的价值”，本次主要描述位于计算就网络应用层和网络层之间的运输层的通信协议，以及其在传递过程中的：可靠性和拥塞控制两个基本问题。</p><h3 id="0x01-背景"><a href="#0x01-背景" class="headerlink" title="0x01 背景"></a>0x01 背景</h3><h3 id="1-1-运输层概述"><a href="#1-1-运输层概述" class="headerlink" title="1.1 运输层概述"></a>1.1 运输层概述</h3><p>运输层为运行在不同主机上的应用进程之间提供逻辑通信（logic communication），从应用程序的角度来说，运行不同进程的主机之间好像直接相连一样，而不用考虑承载这些物理基础设施的细节。</p><p>运输层协议是在端系统中而不是路由器中实现的：</p><p><strong>在发送端，运输层将从发送应用程序进程接收到报文转换成运输层分组成为运输层报文段（segment）</strong>，（后续网络层可能会封装成网络层分组（数据报）向目的地发送，在这个过程中网络路由器仅作用于该数据报的网络层字段），<strong>在接收端网络层从数据报中提取出运输层字段，并将报文上交给运输层</strong></p><p>网络应用程序中可以有多种运输层协议，在因特网中包括UDP和TCP协议。</p><p>？运输层和网络层区别？</p><p>运输层主要负责两个host之间不同进程之间的通信；而网络层提供host之间的逻辑通信；以寄快递的方式来描述：</p><blockquote><p> 应用层报文是快递里面的东西，运输层主要负责收件人的具体信息，运输层协议主要负责各地区收集和分发快递人员；网络层主要负责收货地址，网络层协议主要负责运输车辆、方式的分配</p></blockquote><p>这种解耦的过程可以为不同进程通信之间规定不同的协议，而不是每个人都安排一辆车或者一种地址编码方式来造成杂乱的情况；同时不同的运输层协议也可以搭配不同的网络层协议。</p><h3 id="1-2-因特网运输层概述"><a href="#1-2-因特网运输层概述" class="headerlink" title="1.2 因特网运输层概述"></a>1.2 因特网运输层概述</h3><p>UDP：用户数据报协议，这个进程提供一种不可靠、无连接的服务</p><p>TCP：传输控制协议，提供可靠的、面向连接的服务</p><p>结合IP层理解可能会更好，这里需要了解的是：因特网网络协议的为IP协议（网际协议），提供主机与主机之间的逻辑通信，服务模型是尽力而为角度，并不作任何确保，也就是称为不可靠服务，在这一章我们需要知道的是每一台主机都有一个IP地址</p><p>UDP和TCP的基本责任是将两个端系统之间的IP交付服务拓展到端系统上两个进程之间的交付服务，这个称为多路复用（transport- layer multiplexing）和多路分解（demultiplexing）。除此之外TCP还会提供可靠数据传输（reliable data transfer）和拥塞控制（congestion control）</p><blockquote><p> 举个例子：当你坐在计算机下载web页面，这个时候需要运行一个FTP会话和两个Telnet会话，这样4个网络应用进程在运行，在这个时候我们需要将主机接受到的信息分配给这4个进程中的一个，这就是运输层负责事情。应用层里面是通过套接字（socket）将数据从应用层传向运输层和向运输层进程传递数据的门户，因此在任何一个时刻主机上存在多个套接字，套接字具有唯一的标识符。</p></blockquote><p>？那么主机如何将一个到达的运输层报文定向到适当的套接字？</p><p>这个就需要我们在运输层报文段中定义一些字段，在接收端运输层来解析这些字段来将报文定向到套接字，这个就是多路分解（demultiplexing）的过程；在源主机上将报文段传递到运输层这个工作称为多路复用（multiplexing）；但是当然TCP相比较UDP的解析过程会更加复杂</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6c8c385610b9eb78a9fd7a6a732587e4_1440w.jpg" alt="img"></p><p>image-20220624104913355</p><h3 id="1-3-规范语言"><a href="#1-3-规范语言" class="headerlink" title="1.3 规范语言"></a>1.3 规范语言</h3><p>报文段（segment）：经过网络层到达运输层后生成的分组报文</p><p>数据报（datagram）：运输层的报文到达网络层生成的分组报文</p><p>socket：应用层进程和运输层之间数据通信的方式，同一个host存在多个套接字，每个套接字具有唯一标识符</p><p>多路复用：多个数据传输到运输层</p><p>多路分解：收到下层数据转移到上层多个进程</p><h3 id="0x02-基本原理"><a href="#0x02-基本原理" class="headerlink" title="0x02 基本原理"></a>0x02 基本原理</h3><h3 id="2-1-可靠数据传输原理（Reliable-Data-Transfer-Protocol）"><a href="#2-1-可靠数据传输原理（Reliable-Data-Transfer-Protocol）" class="headerlink" title="2.1 可靠数据传输原理（Reliable Data Transfer Protocol）"></a>2.1 可靠数据传输原理（Reliable Data Transfer Protocol）</h3><p>在数据传输的过程中，我们会遇到差错、损坏和丢失的现象，想要按照原来数据复原就是可靠数据传输协议需要规范的事情，这种协议可以看作是策略方面的method，因为它不仅可以适用于运输层、也可以网络层或者应用层，更重要的是内在思想，而TCP相比较UDP的一方面就是使用了可靠数据传输的东西</p><p>可以参考<a href="https://blog.csdn.net/qq_40177015/article/details/112836182">https://blog.csdn.net/qq_40177015/article/details/112836182</a></p><p>最基本的模型如下图所示，后面我们可以层层递进来看这个是如何实现的：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-805d5b08c3b34503233bc09e571d4086_1440w.jpg" alt="img"></p><p>image-20220624105512085</p><h3 id="2-1-1-RDT1-0-–-底层信道可靠的情况"><a href="#2-1-1-RDT1-0-–-底层信道可靠的情况" class="headerlink" title="2.1.1 RDT1.0 – 底层信道可靠的情况"></a>2.1.1 RDT1.0 – 底层信道可靠的情况</h3><p>那就只需要</p><p>发送端：接受上层的data、转换（make-pkt）成数据段packet、然后发送（send）</p><p>接收端：接受下层的packet、提取extract出数据data、然后移交deliver给上层</p><h3 id="2-1-2-RDT2-0-–-底层差错的情况（但不会丢失）"><a href="#2-1-2-RDT2-0-–-底层差错的情况（但不会丢失）" class="headerlink" title="2.1.2 RDT2.0 – 底层差错的情况（但不会丢失）"></a>2.1.2 RDT2.0 – 底层差错的情况（但不会丢失）</h3><p>出现差错我们的第一反应是需要：如何检查出差错、以及如果告诉发送方错了、以及重新发送，这种称为基于自动重传请求（ARQ、automatic repeat request）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ac84b7325043d7f123734cd4bae10ad3_1440w.jpg" alt="img"></p><p>image-20220624110157727</p><h3 id="2-1-3-RDT2-1-2-2"><a href="#2-1-3-RDT2-1-2-2" class="headerlink" title="2.1.3 RDT2.1&amp;2.2"></a>2.1.3 RDT2.1&amp;2.2</h3><p>但是RDT2.0并没有考虑ACK和NAK错误的情况，这个时候发生发送端忘记重传或者过度重传，这个时候我们需要对报文进行标记，也就是需要增加数据分组编号（sequence number）来帮助区分，这也就是RDT2.1的工作</p><p>由于同时检验ACK和NAK很麻烦，因此可以设计单纯只用ACK的确认和否定就可。</p><h3 id="2-1-4-RDT3-0-–-经过比特差错和丢包信道的可靠信息传输"><a href="#2-1-4-RDT3-0-–-经过比特差错和丢包信道的可靠信息传输" class="headerlink" title="2.1.4 RDT3.0 – 经过比特差错和丢包信道的可靠信息传输"></a>2.1.4 RDT3.0 – 经过比特差错和丢包信道的可靠信息传输</h3><p>针对丢包，我们需要设置定时器（？如何针对每个报文设置），在一定的时间间隔（？后面介绍这么确定）下，没有收到ACK就重新发送</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c7499fadd40ad08d71b76e9a26b1ba7c_1440w.jpg" alt="img"></p><p>image-20220624110956831</p><h3 id="2-1-4-暂停思考RDT"><a href="#2-1-4-暂停思考RDT" class="headerlink" title="2.1.4 暂停思考RDT"></a>2.1.4 暂停思考RDT</h3><p>RDT是一个功能完全正确的协议，但是它是一个停等协议，在于我每个分组报文需要等到上一个ACK到来的时候再重新发送，信道中只有一个报文必然是浪费的</p><p>因此我们可以采用流水线的方式，不停的发送报文，但是这个时候又会面临新的问题</p><h3 id="2-1-5-流水线-退回N步（GBN）"><a href="#2-1-5-流水线-退回N步（GBN）" class="headerlink" title="2.1.5 流水线-退回N步（GBN）"></a>2.1.5 流水线-退回N步（GBN）</h3><p>由于流水线允许发送发连续发送多个分组而不需要等待确认，在这个过程中出现某个数据分组差错或者丢失的情况就非常麻烦，GBN设置是错误就退回N步重新发送，这样的缺点是通信系统可能会被许许多多的无用报文充斥</p><h3 id="2-1-6-流水线-选择重传（SR）"><a href="#2-1-6-流水线-选择重传（SR）" class="headerlink" title="2.1.6 流水线-选择重传（SR）"></a>2.1.6 流水线-选择重传（SR）</h3><p>为了解决GBN的问题，可以使用选择重传的协议，对于错误之后的序列号接收端保存</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-e6e7aae007da732b88dfac9db88b5962_1440w.jpg" alt="img"></p><p>image-20220624111436365</p><h3 id="2-1-7-总结"><a href="#2-1-7-总结" class="headerlink" title="2.1.7 总结"></a>2.1.7 总结</h3><p>在经过上述可靠数据传输机制的描述之后，存在一些基本概念：</p><ul><li>差错检验（checksum）</li><li>定时器（timer）</li><li>序号（sequence number）</li><li>确认（acknowledgement）</li><li>否定确认（NAK）</li><li>窗口、流水线（windows、pipeline）</li></ul><h3 id="2-2-拥塞控制原理"><a href="#2-2-拥塞控制原理" class="headerlink" title="2.2 拥塞控制原理"></a>2.2 拥塞控制原理</h3><p><a href="https://zhuanlan.zhihu.com/p/37379780">https://zhuanlan.zhihu.com/p/37379780</a></p><h3 id="2-2-1-什么是拥塞"><a href="#2-2-1-什么是拥塞" class="headerlink" title="2.2.1 什么是拥塞"></a>2.2.1 什么是拥塞</h3><p>作用于网络的，避免过多的无效重传数据充斥在传输网络中</p><p>第一种情况：两台机器之间+无限缓存的路由器</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3d1d3b4c48a25e1134c54f4222b5cb22_1440w.jpg" alt="img"></p><p>image-20220624112144178</p><p>第二种情况：两台机器之间+有限缓存的路由器</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c5543caad0790ac9a9982e115cd9b0c9_1440w.jpg" alt="img"></p><p>image-20220624112159474</p><p>第三种情况：多台机器之间+有限缓存的路由器</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-aff3974a2d4d84d9970e4e5ad4cfd275_1440w.jpg" alt="img"></p><p>image-20220624112217633</p><h3 id="2-2-2-拥塞控制方法"><a href="#2-2-2-拥塞控制方法" class="headerlink" title="2.2.2 拥塞控制方法"></a>2.2.2 拥塞控制方法</h3><p>第一种方法：端到端的拥塞控制，比如根据报文丢包情况来观察网络拥塞状态，进而缩小自己的窗口（TCP采用的）</p><p>第二种方法：网络辅助拥塞控制，网络中路由器等传输给端设备，高速拥堵情况</p><h3 id="0x03-UDP-TCP"><a href="#0x03-UDP-TCP" class="headerlink" title="0x03 UDP&#x2F;TCP"></a>0x03 UDP&#x2F;TCP</h3><h3 id="3-1-UDP传输协议"><a href="#3-1-UDP传输协议" class="headerlink" title="3.1 UDP传输协议"></a>3.1 UDP传输协议</h3><h3 id="3-1-1-UDP报文"><a href="#3-1-1-UDP报文" class="headerlink" title="3.1.1 UDP报文"></a>3.1.1 UDP报文</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7f81073f885e5550c0eef862c70e4d86_1440w.png" alt="img"></p><p>img</p><p>源端口：发送方的进程对应的socket的端口</p><p>目的端口：接收方的进程对应的socket端口</p><p>报文长度：用来检查报文是否会出错</p><p>检验和：最基本的差错的checksum检验</p><h3 id="3-1-2-UDP套接字编程"><a href="#3-1-2-UDP套接字编程" class="headerlink" title="3.1.2 UDP套接字编程"></a>3.1.2 UDP套接字编程</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs stata">UDPClient.py<br><br>from socket import *<br>serverName=&#x27;hostname;<br>serverPort=12000<br><br><span class="hljs-comment">// 常见客户端的套接字</span><br>clientSocket=socket(AF_INET，SOCK_DGRAM)<br><span class="hljs-comment">// 第一个参数代表使用了ipv4；第二个参数代表该套接字是SOCK——DGRAM类型的</span><br>message=raw_input(&#x27;<span class="hljs-keyword">Input</span> your message:&#x27;)<br><br><span class="hljs-comment">//有了信息和发送端socket</span><br>clientSocket.sendto(message.<span class="hljs-keyword">encode</span>(),(serverName,serverPort))<br><br><span class="hljs-comment">//之后等待服务器的数据</span><br>modifiedMessage,serverAddreess =clientSocket.recvfrom(2048)<br><span class="hljs-comment">// 2048代表分组信息到达该客户端套接字，该分组的数据的放到变量modified message中</span><br><span class="hljs-comment">// 源地址放置到变量server Address，取缓存长度2048作为输入</span><br><span class="hljs-keyword">print</span>(modifiedMessage.<span class="hljs-keyword">decode</span>())<br>clientSocket.<span class="hljs-keyword">close</span>()<br></code></pre></td></tr></table></figure><h3 id="3-2-TCP传输协议"><a href="#3-2-TCP传输协议" class="headerlink" title="3.2 TCP传输协议"></a>3.2 TCP传输协议</h3><h3 id="3-2-1-TCP报文"><a href="#3-2-1-TCP报文" class="headerlink" title="3.2.1 TCP报文"></a>3.2.1 TCP报文</h3><ul><li>首部包括16比特源端口号和16比特目的端口号，被用于多路复用&#x2F;分解来自或者送到上层应用的数据</li><li>32比特的序号字段（sequence number field）和32比特的确认号字段（acknowledgment number field）用于实现可靠数据传输服务，随机的初始序列号还是很有意思的；讨论见后</li><li>4比特的首部字段长度（header length field），指示以32比特的字为单位的TCP首部长度，由于TCP选项字段的原因，TCP首部的长度是可变的，一般为空则说明典型长度为20字节</li><li>8比特的标志字段（flag field）</li><li>ACK，指示确认字段的值是有效的</li><li>RST、SYN、FIN，用于连接和拆除</li><li>CWR、ECE，拥塞通知里面使用</li><li>PSH，有值时，应当指示接收方立即将数据交给上层</li><li>URG，指示报文段中存在的被发送端的上层设置为紧急的数据，最后一个字节有紧急数据指针字段给出</li><li>16比特的接受窗口（receive windows field），用于流量控制，用于指示接收方愿意接受的字节数量</li><li>校验和字段（checksum field）</li><li>16比特的紧急数据指针（urgent data pointer field）</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ddad00336a9331c8a1ca2ae5d5b8f1f8_1440w.jpg" alt="img"></p><p>image-20220624134417983</p><h3 id="3-2-2-TCP整个过程"><a href="#3-2-2-TCP整个过程" class="headerlink" title="3.2.2 TCP整个过程"></a>3.2.2 TCP整个过程</h3><p>TCP连接是在复杂的物理网络中建立两台主机（host）之间的点对点（point to point）之间的连接；两台主机之间连接的简约过程是通过三次握手（three-way handshake）进行的：</p><p>客户端发送一个特殊的TCP报文段，服务器使用另一个特殊的TCP报文段来响应，之后客户端利用第三个特殊报文段作为响应，第三个可以包含有效负荷（也就是应用层数据），整个过程称为三次握手；之后包括四次挥手的过程；详细如下：</p><ul><li>第一步，client首先向服务器TCP发送一个特殊的TCP报文段，SYN报文段设计为1，同时client随机选择一个初始序号，包装在一个IP数据报发送给server</li><li>第二步，这个特殊的TCP报文段到达server后，该TCP连接分配缓存和变量，并向client发送允许连接的报文段，这个允许的报文段包括三个重要的信息：SYN比特为1、确认号设置client-isn+1、设置服务器的server-isn字段，这个被称为SYNACK segment</li><li>client在受到回复之后，为该连接分配缓存和变量，client向server发送另外一个报文段，对于服务器的允许连接的报文段进行确认，server-isn+1设置到TCP报文段首部确认字节，这里可以报文段中携带数据</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-aa03cc5dc97404641d622ad761fa23f1_1440w.jpg" alt="img"></p><p>image-20220624134710294</p><p>拆除：</p><p>通过FIN字段来设置关闭报文段，client发送包含FIN报文，server发送ACK；之后server发送FIN，client回复ACK，如果没有关闭设置超时重传机制</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4109489368e428396027ae7f506af6f2_1440w.jpg" alt="img"></p><p>image-20220624134748694</p><p>之后应用层通过套接字（socket）来传递数据流，这些数据被引导到连接到发送缓存（send buffer）中，之后TCP会时不时地从发送缓存中取出来一块数据，并将数据传递到网络层，这里取出多少数据受限于最大报文段长度（maximum segment size，MSS），这个通过由最初确定的最大链路层帧长度（MTU）确定</p><h3 id="3-2-3-TCP的可靠数据传输"><a href="#3-2-3-TCP的可靠数据传输" class="headerlink" title="3.2.3 TCP的可靠数据传输"></a>3.2.3 TCP的可靠数据传输</h3><p>定时器的重要设计是需要了解多长时间才是超时？因此需要对两个host之间的传送时间进行确认，这个需要考虑的这个超时时间间隔重传的设置，它必须要大于该链接的往返时间（RTT），以及是否需要为所有未确认的报文段各设置一个定时器</p><p>TCP连接为每一个时间段做出一次SampleRTT，结合历史估计的Estimate RTT来做移动平均（EWMA，exponential weighed moving average）；同时定义偏差DevRTT</p><p>EstimateRTT&#x3D;0.875EstimateRTT+0.125SampleRTT</p><p>DevRTT&#x3D;0.75DevRTT+0.25（SampleRTT- EstimateRTT）</p><p>设置时间间隔为</p><p>Timeout Interval&#x3D;EstimateRTT+4DevRTT</p><p>在可靠性传输章节我们可以看到对于丢包最重要的是定时器的设置、对于发送方最重要的是定义如何重传数据，因此在TCP中采用的方法如图：也就是采用一个定时器的超时重传机制。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-aa33dee36152450e71aa1f5e551692b5_1440w.jpg" alt="img"></p><p>image-20220624135014852</p><ol><li>超时间隔加倍</li><li>定时器时限过期之后超时间间隔的长度，在这种修改中，没当超时事件发生后，TCP重传具有最小序号的还未被确认的报文段，只是每次TCP重传都会将以下一次超时间隔设置为前面的2倍；这种修改提供了一种形式受限的拥塞控制，因为时延增加并且重传会导致拥塞更严重</li><li>快速重传</li><li>上述的超时重传的问题在于超时周期可能相对较长，当一个报文段丢失时，这种长超时周期使得发送方延迟重传丢失的分组，从而增加了端到端的时延，这里可以采用冗余ACK（duplicate ACK）来帮助检测丢包</li><li>GBN or SR</li><li>TCP更多的事选择确认（selective acknowledgement），属于两者之间balance</li></ol><h3 id="3-2-4-TCP的拥塞控制"><a href="#3-2-4-TCP的拥塞控制" class="headerlink" title="3.2.4 TCP的拥塞控制"></a>3.2.4 TCP的拥塞控制</h3><p>还没看懂～～</p><p><a href="https://zhuanlan.zhihu.com/p/37379780">https://zhuanlan.zhihu.com/p/37379780</a></p><p>这位博主总结为</p><ol><li>慢开始、拥塞避免</li><li>快重传、快恢复</li></ol><p><a href="https://zhangbinalan.gitbooks.io/protocol/content/tcpde_yong_sai_kong_zhi.html">https://zhangbinalan.gitbooks.io/protocol/content/tcpde_yong_sai_kong_zhi.html</a></p><p>慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )。</p><h3 id="3-3-两者对比"><a href="#3-3-两者对比" class="headerlink" title="3.3 两者对比"></a>3.3 两者对比</h3><p>既然TCP比UDP多出来这么多功能？两者之间的优劣在哪里呢，UDP提供了速度快、报文传递多的特点，而TCP虽然点对点安全传递但是计算开销大，适合用于安全性较强的业务。</p><ul><li>关于发送什么数据以及何时发送的应用层控制更为精细；采用UDP主需要进程将数据传送给UDP即可，但是TCP具有拥塞控制机制；在一些应用中更加强调实时性，而不必要数据的完整性</li><li>无须连接建立，对于网页HTTP连接中可靠性是重要的，同时实时性也是重要的，所以谷歌的QUIC协议是在UDP协议的基础上在应用层协议中实现了可靠性</li><li>无连接状态，TCP需要保持连接状态，这个包括接受和发送缓存、拥塞控制i参数以及序号与确认好的参数，而UDP不需要追踪这些参数，因此在某些特定应用使用UDP可以支持更多的并发用户</li><li>分组首部开销小，TCP报文段20字节的首部开销、UDP仅有8字节</li></ul><p>同时在UDP不可靠的基础上，可以在应用层上增加可靠传输协议来实现可靠UDP（比如Google的QUIC协议）</p><h3 id="0x04-回顾、收获和展望"><a href="#0x04-回顾、收获和展望" class="headerlink" title="0x04 回顾、收获和展望"></a>0x04 回顾、收获和展望</h3><p>将本科信息传输原理、通信网络基础没有听懂的数据报整明白了出处；并且在描述一件事情的过程最好先简化一个基本模型、然后逐步添加。同时理清楚其中的原理、策略和具体操作之间的区别和联系</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>运输层</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络 Vol2 ｜ 应用层协议</title>
    <link href="/posts/52fccb61.html"/>
    <url>/posts/52fccb61.html</url>
    
    <content type="html"><![CDATA[<p>网络应用是计算机网络存在的理由，如果我们不能构想出任何游有用的应用，也就没有必要支持它们的网络协议来，正是因特网的全面发展以来，的确开发众多有用的、有趣的网络应用，正是这些应用程序成为因特网成功的驱动力。</p><p>一些非常有意思的历史：</p><p>20世纪70年年代到80年年代，流行的经典的基于文本的应用，比如电子邮件、远程访问计算机等；</p><p>20世纪90年代中期引入的万维网，包括Web冲浪、搜索和电子商务</p><p>20世纪末，引入的即时通讯和对等P2P文件分享</p><p>之后出现来：IP电话、视频电话、视频网站、在线游戏</p><p>再到智能手机发展：签到、约会和地图导航等等</p><p>贲本章主要学习网络应用的原理和实现方面的只是，从关键的应用层概念开始，描述网络服务、客户和服务器、进程和运输层接口，之后设计开发运行在TCP和UDP的网络应用程序，之后学习Socket接口来编写一些简单的客户-服务器应用程序。</p><h3 id="2-1-应用层协议原理"><a href="#2-1-应用层协议原理" class="headerlink" title="2.1 应用层协议原理"></a>2.1 应用层协议原理</h3><p>在应用层程序开发过程中，最重要的一点是不必要关心网络中路由设置等等情况。</p><h3 id="2-1-1-网络应用程序体系结构（-Application-architecture）"><a href="#2-1-1-网络应用程序体系结构（-Application-architecture）" class="headerlink" title="2.1.1 网络应用程序体系结构（ Application architecture）"></a>2.1.1 网络应用程序体系结构（ Application architecture）</h3><p>应用程序研发者设计，规定如何在各种端系统上组织该应用程序，在选择应用程序体系结构时，应用程序研发者很可能利用现代网络应用程序所使用的两种主流体系结构之一：客户-服务器体系结构和对等P2P体系结构</p><ol><li>Client-server architecture，可能会出现多台客户端想服务器响应，在这个过程服务器的IP地址是固定的，但是会出现单独的服务器主机跟不上所有客户请求的情况，因此需要设置多个数据中心Data center用来创建强大的虚拟服务器，通常一个数据中心可能存在数十万台服务器</li><li>P2P architecture，应用程序对于专用服务器的具有最小的依赖，相反应用程序在间断连接的主机之间使用直接通信，这些主机对称为对等方，服务器用于跟踪用户的IP地址，但是用户到用户的报文在用户主机之间无须通过中间服务器直接发送，一个比较大的特点是具有自拓展性（self-scalability）</li></ol><h3 id="2-1-2-进程通信（Process-communication）"><a href="#2-1-2-进程通信（Process-communication）" class="headerlink" title="2.1.2 进程通信（Process communication）"></a>2.1.2 进程通信（Process communication）</h3><p>在构建网络应用需要对运行在多个端系统上的程序是如何进行通信的情况进行了解，⚠️计算机网络中并不关注于同一台主机的进程，而是不同端系统的进程（Process）之间的通信机制</p><p>在进程之间的通信会话场景中，发起通信的称为客户（Clinet）、在会话开始等待联系的是服务器（Server）</p><p>进程与网络之间通信的接口需要使用套接字（Socket）的软件接口向网络发送message和接受message；调用系统提供的socket api和操作系统，将其转换为不同的流量的tCP再发送。我们可以在Socket中选择什么？</p><ol><li>选择运输层的协议</li><li>设定几个运输层参数，比如最大缓存和最大报文长度</li></ol><p>同时在进程需要考虑地址IP和端口Port的选择</p><p><a href="https://zh.m.wikipedia.org/zh/TCP/UDP%E7%AB%AF%E5%8F%A3%E5%88%97%E8%A1%A8">TCP&#x2F;UDP端口列表 - 维基百科，自由的百科全书</a></p><h3 id="2-1-3-可供应用程序使用的运输服务"><a href="#2-1-3-可供应用程序使用的运输服务" class="headerlink" title="2.1.3 可供应用程序使用的运输服务"></a>2.1.3 可供应用程序使用的运输服务</h3><p>针对不同的Application自然需要不同的运输方式，比如实际的火车、飞机各有不同，计算机网络传输方式也是相同。需求分析需要从之间的计算机网络的基本特性出发</p><ol><li>可靠性数据传输（reliable data transfer）</li><li>吞吐量（Bandwidth）</li><li>定时（Time delay）</li><li>安全性（Security）</li></ol><h3 id="2-1-4-因特网（或者抽象模型下次层）提供的运输服务"><a href="#2-1-4-因特网（或者抽象模型下次层）提供的运输服务" class="headerlink" title="2.1.4 因特网（或者抽象模型下次层）提供的运输服务"></a>2.1.4 因特网（或者抽象模型下次层）提供的运输服务</h3><p>提供TCP和UDP传输协议，这个是运输层提供的选择</p><ul><li>TCP的主要特点是面向连接的服务，会有握手的过程，同时提供可靠的数据传输服务以及拥塞控制机制。</li><li>UDP提供不必要服务的轻量级运输协议，面向无连接的</li></ul><p>额外的会有Security Socket Layer来将进行加密，提供关键进程到进程的安全性服务</p><h3 id="2-1-5-应用层协议（application-layer-protocol）"><a href="#2-1-5-应用层协议（application-layer-protocol）" class="headerlink" title="2.1.5 应用层协议（application-layer protocol）"></a>2.1.5 应用层协议（application-layer protocol）</h3><p>学习如何构造报文发送到Socket实现网络进程之间的相互通信，如何构造这些报文？如何明确这些报文中字段的含义？在何时发送报文？这些都是由协议规定的：</p><ol><li>交换的报文类型，利用request和response报文</li><li>报文中字段是如何描述</li><li>字段的语义</li><li>确定一个进程何时以及如何发送报文</li></ol><h3 id="2-2-Web和HTTP"><a href="#2-2-Web和HTTP" class="headerlink" title="2.2 Web和HTTP"></a>2.2 Web和HTTP</h3><p>Web是一个引起公众注意的因特网应用，极大的改变了人与工作环境交流的方式，它将人们从被动接受转向按需操作</p><h3 id="2-2-1-HTTP概况（HyperText-Transfer-Protocol，HTTP）"><a href="#2-2-1-HTTP概况（HyperText-Transfer-Protocol，HTTP）" class="headerlink" title="2.2.1 HTTP概况（HyperText Transfer Protocol，HTTP）"></a>2.2.1 HTTP概况（HyperText Transfer Protocol，HTTP）</h3><p>超文本传输协议是Web的核心，主要由客户端程序和服务器程序组成，在两个不同的host中，通过交换HTTP报文进行会话，需要回顾Web术语。</p><p>Web页面（web page）是由对象（Object）组成的，包括HTML文件、JPEG图片、JAVA程序或者视频等</p><p>URL是这些文件寻找地址的东西，多数Web页面包含HTML文本和5个JPEG图形，浏览器（Web browser）实现了HTTP的客户端、服务器（Web server）实现了HTPP的服务器端，用于存储web对象</p><p>HTTP也定义了客户端请求和回复Web页面的方式，HTTP使用TCP作为支撑运输协议，即HTTP一定能保证报文可以传递，同时在请求和回复的状态中，服务器并不会保存关于客户的任何信息，因此是一个无状态协议（Stateless protocol）</p><h3 id="2-2-2-非持续连接和持续连接（yes-or-no-persistent-connection）"><a href="#2-2-2-非持续连接和持续连接（yes-or-no-persistent-connection）" class="headerlink" title="2.2.2 非持续连接和持续连接（yes or no persistent connection）"></a>2.2.2 非持续连接和持续连接（yes or no persistent connection）</h3><p>也就是客户端和服务器之间会存在一个持续文件交互是采用单独的TCP连接，还是经过不同的TCP连接，经过激烈的讨论希望在一定时间间隔内采用持续性连接的方式，超过之后采用非持续性连接的方式</p><h3 id="2-2-3-HTTP报文格式"><a href="#2-2-3-HTTP报文格式" class="headerlink" title="2.2.3 HTTP报文格式"></a>2.2.3 HTTP报文格式</h3><p>第一种请求报文，第一行是request line，后面的行header line，</p><p>请求行包括方法字段、URL字段和HTTP版本</p><p>常用的方法有GET、POST、HEAD、PUT、DELETE</p><p>HOST提供信息是web代理高速缓存所需要的</p><p>Connection：高速服务器不要使用持续连接</p><p>User-agent：指明用户代理，也就是服务器的类型</p><p>Accept-language：表示拥护想要该对象的版本</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-e35524cf73fae114320a7c4d3a322acc_1440w.jpg" alt="img"></p><p>image-20220618194455618</p><p>三个部分：初始状态行status line、六个首部行header line、实体行entity body</p><p>HTTP以及状态码</p><p>connection 表示发送完之后将关闭tcp连接</p><p>200 OK请求成功</p><p>301对象被永久转移了</p><p>400通用差错代码</p><p>404 请求的文档不在服务器上</p><p>505  服务器不支持请求报文使用的HTTP协议版本</p><p>Date 表示服务器从文件系统中检索到该对象，并插入响应报文并发送该报文的时间</p><p>Server：表示服务器产生</p><p>Last-modified：表示对象创建或者最后修改的日期和时间</p><p>Content-Length：包含的字节数</p><p>content-type：对象是HTML文本</p><h3 id="2-2-4-服务器缓存-cookie"><a href="#2-2-4-服务器缓存-cookie" class="headerlink" title="2.2.4 服务器缓存 cookie"></a>2.2.4 服务器缓存 cookie</h3><p>无状态的HTTP服务器简化了设计，但是为了识别用户，还是希望将请求内容与用户身份联系起来，为此使用Cookie，其主要由四个组件构成</p><ol><li>在HTTP响应报文中一个cookie首部行</li><li>在HTTP请求报文中一个cookie首部行</li><li>在用户端系统中保留有一个cookie文件，并由用户的浏览器进行管理</li><li>位于web站点的一个后端数据库</li></ol><p>但是结合cookie用户提供的账户信息，web站点可以知道许多用户的信息</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d15c7aca00ee82e7dc9b7748ad523618_1440w.jpg" alt="img"></p><p>image-20220618194508226</p><h3 id="2-2-5-Web缓存（cache）"><a href="#2-2-5-Web缓存（cache）" class="headerlink" title="2.2.5 Web缓存（cache）"></a>2.2.5 Web缓存（cache）</h3><p>也叫做代理服务器（proxy server），是能够代表初始Web服务器来满足HTTP请求的网络实体</p><p>web缓存可以大大减少客户请求的响应时间，</p><p>同时可以大大减少一个机构的接入链路到因特网的通信量，通过内容分发网络（Content Distribution Network，CDN），web缓存器正在让因特网发挥越来越多的作用</p><p>但是web缓存的一个重要的问题可能是，存放在缓存器中的对象副本可能是陈旧的，这就需要缓存器证实对象是最新的，也就是条件GET（conditional  GET方法）</p><h3 id="2-3-Mail和SMTP、POP3、IMAP、HTTP"><a href="#2-3-Mail和SMTP、POP3、IMAP、HTTP" class="headerlink" title="2.3 Mail和SMTP、POP3、IMAP、HTTP"></a>2.3 Mail和SMTP、POP3、IMAP、HTTP</h3><p>当因特网还在襁褓之中，电子邮件已经称为来最流行的应用程序，是一种异步通信媒介，也就是人们方便的时候可以收发邮件，不必与他人计划进行协调。同时电子邮件的优势在于具有附件、超链接、html格式文本的图片的报文。</p><p>首先看电子邮件的总体结构以及关键组成部分</p><ol><li>用户代理 user-agent</li><li>邮件服务器 mail-server</li><li>简单邮件传输协议 simple-Mail-Transfer-Protocol，SMTP</li></ol><p>重要的场景： SMTP一般不使用中间邮件服务器发送邮件，即时这两个邮件服务器位于地球两端也是这样的，即使另一个服务器没有开机，这样的报文也会保留在ALice的邮件服务器上等待进行新的尝试</p><h3 id="2-3-1-SMTP协议"><a href="#2-3-1-SMTP协议" class="headerlink" title="2.3.1 SMTP协议"></a>2.3.1 SMTP协议</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-597558600ec427c71897171ac297fe5e_1440w.jpg" alt="img"></p><p>image-20220618194530174</p><p>2.3.2 SMTP协议与HTTP协议对比</p><p>HTTP 从Web服务器向客户端传送文件；但是HTTP是一个pull protocol，在方便的时候，某些人在Web服务器上装载信息，用户使用HTTP协议从服务器上拉取这些协议</p><p>SMTP 从一个邮件服务器向另一个邮件服务器传送文件，也就是发送邮件服务器将文件推向接受邮件服务器</p><p>其他的区别：</p><ol><li>STMP要求每个报文按照7比特ASCII格式，如果含有非法的7比特或者二进制的图形文件，需要按照7比特ASCII进行编码，HTTP不受这样的限制</li><li>如何处理包含文本和图形的文档，HTTP将其封装在HTTP响应报文中，SMTP则把所有报文对象放在一个报文中</li></ol><h3 id="2-3-3-邮件报文格式"><a href="#2-3-3-邮件报文格式" class="headerlink" title="2.3.3 邮件报文格式"></a>2.3.3 邮件报文格式</h3><p>类似与SPI、CAN，无线传递都需要使用首部行来作为标识，只是作为应用层的标识可能更人性化一点，但是这样的人性化可能在一定程度上让人产生费解，之后理解背后的机理才能更好理解别人。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f87f5da997501045edd0c81b5b125bad_1440w.jpg" alt="img"></p><p>image-20220618194546104</p><h3 id="2-3-4-邮件访问协议"><a href="#2-3-4-邮件访问协议" class="headerlink" title="2.3.4 邮件访问协议"></a>2.3.4 邮件访问协议</h3><p>之前讨论的SMTP协议，必须要求发送端发送的时候，接收端在线，但是由于PC或者手机不能一种保持在线，因此通常需要一个共享的邮件服务器，但是这个服务器和代理之间不能通过SMTP传送，因为SMTP只能Push，不能PULL文件，因此需要一些流行的邮件访问协议，包括</p><ol><li>第三版的邮局协议，POST-OFFCIE-PROTOCOL，POP3</li><li>第一个阶段：特许（Authorization）</li><li>第二个阶段：事务处理</li><li>第三个阶段：更新</li><li>特点，一旦将邮件下载到本地主机就能简历邮件文件夹，并将下载的邮件放入该文件夹中，之后Bob可以删除报文，可以在文件夹之间移动报文，并查询报文，但是POP3协议没有给用户提供任何创建远程文件夹并为报文指派文件夹的方法</li><li>因特网邮件访问协议 Internet Mail Access Protocol，IMAP</li><li>IMAP协议是一个邮件访问协议，每个报文与一个文件夹联系起来，当报文第一次到达服务器时，与收件人的inbox的文件夹相关联，收件人则能够把邮件转移到一个新的、用户创建的文件夹中，阅读邮件、删除邮件等</li><li>还提供了在远程文件夹中查询邮件的命令</li><li>维护了IMAP会话的用户状态信息</li><li>（重要）允许用户代理获取报文某些部分的命令</li><li>HTTP</li><li>在上述选择和获取的过程采用的是http的协议</li><li>但是邮件传输的过程中仍然采用的是SMTP协议</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-88bae216c679b3f13d6e8a2abeb0c832_1440w.jpg" alt="img"></p><p>image-20220618194556513</p><p>2.4 URL和DNS</p><p>一件事物通常可以通过多种方式来进行标识，我们可以通过姓名来标识，也可以通过身份证来标识、也可以通过社保账号来有限的、通过某个学校的学号来确定的认识一个人，在不同的场景下可以采用不同的识别方式，对于计算机网络中的主机有多种方式</p><ol><li>主机名称（Host name），<a href="http://www.baidu.com等,但是主机几乎没哟提供主机在因特网中的位置信息/">www.baidu.com等，但是主机几乎没哟提供主机在因特网中的位置信息</a></li><li>IP地址，利用4字节组成，具有严格的层次结构，这里将在第四章中进行讨论</li></ol><h3 id="2-4-1-DNS（Domain-Name-System，域名系统）"><a href="#2-4-1-DNS（Domain-Name-System，域名系统）" class="headerlink" title="2.4.1 DNS（Domain Name System，域名系统）"></a>2.4.1 DNS（Domain Name System，域名系统）</h3><p>人们容易记住主机名标识方式，但是路由器喜欢定长的、有着层次结构的IP地址，为了这些不同的偏好，我们需要一种将主机名称转换到IP地址的目录服务，这就是域名系统的主要功能</p><ol><li>一个分层的DNS服务器实现的分布式数据库</li><li>一个是的主机能够查询分布式数据库的额应用层协议</li></ol><p>DNS服务器通常是运行BIND（Berkeley internet name domain）软件的UNIX，使用53端口，考虑它如何使用，可以观察一个客户端发送URL请求会发生什么样的过程：</p><ol><li>用户主机上运行DNS应用的客户端</li><li>浏览器从上述URL中抽取出主机名，也就是URL，传递给DNS应用的客户端</li><li>DNS客户端向DNS服务器发送包含主机的请求</li><li>返回ip地址的报文</li><li>一旦浏览器没收到来自DNS的IP地址，向该IP地址的80端口的HTTP服务器进程发送一个TCP连接</li></ol><p>除进行主机到IP地址的转换之外还提供服务</p><ol><li>主机名到ip地址转换</li><li>主机别名 host aliasing</li><li>邮件服务器别名 mail server aliasing</li><li>负载分配 load distribution 用于在冗余的服务器之间进行负载分配，繁忙的站点被冗余分布在多台服务器中，每台服务器均运行在不同的端系统高上，因此ip地址集合可以与同一个规范主机名相联系。一些内容分发公司也以更加复杂的方式使用DNS来提供web内容分发</li></ol><h3 id="2-4-2-DNS工作原理概述"><a href="#2-4-2-DNS工作原理概述" class="headerlink" title="2.4.2 DNS工作原理概述"></a>2.4.2 DNS工作原理概述</h3><p>DNS客户端使用gethostbyname() 的方法可以简单的得到IP地址的报文，但是对于DNS内部机理复杂，因为它是由分布在全球大量的DNS服务器以及定义DNS服务器与查询主机通信方式的应用层协议组成</p><p>如果一个网络只存在一个DNS服务器</p><ol><li>单点故障 a single point of failure</li><li>通信容量 traffic volume</li><li>远距离的集中式数据库 distant centralized database</li><li>维护 maintenance，因此需要采用不同层次方式组织</li></ol><p>需要分布式、层次数据库</p><ol><li>根DNS</li><li>顶级域 Top Level Domain TLD</li><li>权威DNS服务器</li><li>本地DNS服务器，这个由ISP提供，本地DNS服务器通常使用DHCP与其他本地DNS服务器连接，在本次网络中可能与用户的服务器临近，起到了代理的作用</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-69dcdb60269a969be44e03da76331ef5_1440w.jpg" alt="img"></p><p>image-20220618194611062</p><p>这里使用到了递归查询（recursive query）和迭代查询（iterative query）两种方式</p><p>   DNS缓存  </p><p>为了改善时延和减少互联网上的DNS报文数量，广泛使用缓存的概念，一般情况下DNS服务器在同一段时间内会丢弃缓存的信息，这样会让的根服务器或者TLD的服务器被绕过</p><p>   DNS记录和报文  </p><p>共同实现了DNS分布式数据库的所有DNS服务器存储的资源记录，只有查询和回答报文</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ad0bd0cced669e35e9bc1d4e96a1bd84_1440w.jpg" alt="img"></p><p>image-20220618194625034</p><p>2.5 File Share和P2P</p><p>到目前为止，本章描述的应用包括web、电子邮件和DNS都采用了客户-服务器体系结构，极大依赖于总是打开的基础u设施服务器，使用P2P体系结构对于总是打开的基础设置设施服务器有最小的依赖，与之相反，成间歇连接的主机批次直接通信，这些对等方并不为服务提供商ISP所拥有，而是受用户控制的桌面计算机和个人计算机</p><p>本次章节将研究一个非常自然的P2P应用，即从单一服务器向大量对等方发送大文件，这里可能是Linux操作系统，也有可能是MP3音乐文件等，在客户-服务器文件分发中，该服务器必须向每个对等方发送该文件的副本，服务器承担了极大的负担，并且消耗了大量的服务器带宽；在P2P文件分发中，每个对等方能够收到任何其他对等方重新分发已经收到的该文件的任何部分，而从在分发过程中协助该服务器，2016年中最重要的是BitTorrent</p><h3 id="2-5-1-P2P体系结构的拓展性"><a href="#2-5-1-P2P体系结构的拓展性" class="headerlink" title="2.5.1 P2P体系结构的拓展性"></a>2.5.1 P2P体系结构的拓展性</h3><p>为了将客户-服务器体系结构与P2P体系结构进行比较，阐述P2P的内在自拓展性，我们现在考虑一个用于两种体系结构类型的简单定量模型</p><h3 id="2-6-VIdeo-Stream和内容分发网CDN"><a href="#2-6-VIdeo-Stream和内容分发网CDN" class="headerlink" title="2.6 VIdeo Stream和内容分发网CDN"></a>2.6 VIdeo Stream和内容分发网CDN</h3><p>在本节我们将对如何在今天的因特网实现流行的视频流服务进行概述，实现的方式是通过应用层协议和像高速缓存那样方式运行的服务器。</p><h3 id="2-6-1-因特网视频"><a href="#2-6-1-因特网视频" class="headerlink" title="2.6.1 因特网视频"></a>2.6.1 因特网视频</h3><p>video是一系列的图像，通常是按照一种恒定的速率来，这些是通过像素阵列组成，通过比特编码来实现亮度或者颜色，因此比特率越高、图像质量越好，用户的总体视觉感受越好，因此视频网站也会根据视频设置不同清晰度的版本</p><h3 id="2-6-2-HTTP流和DASH（dynamic-adaptive-streaming-over-HTTP）"><a href="#2-6-2-HTTP流和DASH（dynamic-adaptive-streaming-over-HTTP）" class="headerlink" title="2.6.2 HTTP流和DASH（dynamic adaptive streaming over HTTP）"></a>2.6.2 HTTP流和DASH（dynamic adaptive streaming over HTTP）</h3><p>HTTP流中，视频只是存储在HTTP服务器中作为一个普通的文件，每个文件都有一个特定的URL，当用户查看这个视频的时候，客户与服务器创建一个TCP连接并发送对该URL的get请求，服务器则以底层网络协议和流量条件允许的尽可能快的速率，发送视频文件，另一端字节被收集到客户应用缓存中，一段超过预先设定的门限，客户应用程序就可以开始播放</p><p>对于相同的客户的不同时间也有可能存在不同的带宽，因此需要根据不同时段，设计不同的比特率，对应于不同的质量水平，客户动态选择来自高速率版本的快，当可用带宽量较低的时候，客户可以自然选择来自低速率版本的快</p><h3 id="2-6-3-内容分发网-content-distribution-network"><a href="#2-6-3-内容分发网-content-distribution-network" class="headerlink" title="2.6.3 内容分发网 content distribution network"></a>2.6.3 内容分发网 content distribution network</h3><p>对于一个因特网视频公司，最直观的是建立单一的大规模数据中心，在数据中心中存储所有的视频，并直接从该数据中心向世界范围的客户传输流式视频，会存在一些问题</p><ol><li>如果客户远离数据中心，服务器到客户的分组将跨越许多通信链路并肯呢个经过不同的ISP，导致烦人的通信时延</li><li>经过多条链路会浪费网络带宽</li><li>单个数据中心代表着单点故障</li></ol><h3 id="2-7-课后习题"><a href="#2-7-课后习题" class="headerlink" title="2.7 课后习题"></a>2.7 课后习题</h3><p><a href="https://blog.51cto.com/rickyigoogle/1857294">课后习题和问题_与君共勉的技术博客_51CTO博客</a></p><p><a href="https://blog.51cto.com/rickyigoogle/1857301">课后习题和问题 复习题 2.2～2.5节_与君共勉的技术博客_51CTO博客</a></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">nslookup</span> blog.tjdata.site<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>应用层</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>工科生研0的Macbook Air M1 13inch（16+256）深度使用感受</title>
    <link href="/posts/9148fb62.html"/>
    <url>/posts/9148fb62.html</url>
    
    <content type="html"><![CDATA[<p>在深度使用M1一年之后，在即将比如研究生生活之前，介绍一次Mac。同时也会结合自己的华为Mate40pro介绍macOS+ipadOS+harmonyOS在日常使用的习惯，说明常用的软件、硬件的配件、文件管理和同步以及password以及GTD管理、环境管理中Linux和python环境以及Git使用等，作为一次Summary</p><p>本次介绍的主体是2020年发布的M1芯片的Mac，也经历了结构更迭之苦，当然一年之后能看到ARM架构的优越性。具体规格配置如图</p><p><a href="https://www.apple.com.cn/macbook-air-m1/specs/">MacBook Air (M1 芯片机型) - 技术规格</a></p><p>【写作时间】2022年6月12日</p><h2 id="01-Software-management"><a href="#01-Software-management" class="headerlink" title="01 Software management"></a>01 Software management</h2><p>在使用的过程中，很多软件都没有兼容M1，兼容M1的破解版也少的可怜</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-617065001909f0346e299d2839b64853_1440w.jpg" alt="img"></p><p>软件思维导图</p><p>常用的Mac软件下载网站：</p><p><a href="https://www.macwk.com/">MacWk - 精品mac软件下载</a></p><h3 id="1-1-处理不同格式文件软件"><a href="#1-1-处理不同格式文件软件" class="headerlink" title="1.1 处理不同格式文件软件"></a>1.1 处理不同格式文件软件</h3><p>Office 三件套：docx、ppt、xlsx WPS ：同office，虽然功能会少一点，但是带来的是性能的提升 Typora：Markdown Sublime Text：打开、轻编辑 【code】、markdown、latex（快） Skim：打开PDF文件，搭配Sublime Text使用（快） PDF Expert：编辑PDF文件 iMovie：编辑视频文件 IINA：视频播放软件（快） Capture One：修图软件</p><h3 id="1-2-牛刀类工具"><a href="#1-2-牛刀类工具" class="headerlink" title="1.2 牛刀类工具"></a>1.2 牛刀类工具</h3><p>Notion：非常好用的笔记软件 Zotero：文献管理软件，可以搭配notion使用 Marginnote3 : 阅读PDF和知识管理软件，不太好用 OmniGraffle ： 流程图绘制软件（平替 draw.io网站） Xmind：思维导图软件，正在用的 Edge：很不错的浏览器 Axure RP：原型设计软件 Parallels Desktop：虚拟机管理 Terminus：终端增强</p><h3 id="1-3-润物细无声的小工具"><a href="#1-3-润物细无声的小工具" class="headerlink" title="1.3 润物细无声的小工具"></a>1.3 润物细无声的小工具</h3><p>ClashX：代理软件简介 Rectangle：窗口管理 欧路词典：查单词 iStat：硬件状态 iPic Free：markdown的图片上传 Aldente：充电管理 GifSki：压缩生成GIF文件 SVGview：查看SVG Hidden bar：状态栏设置隐藏 iShot：截图软件，可以长截图和标记（command shift 4&#x2F;5可以截图和录屏） Better Zip：解压缩文件 ClipborderManagement：剪贴板管理 坚果云+Nextcloud：Webdav文件管理 cheetsheet：快捷键提醒 Monitor Control：管理显示器的亮度 Mail+Calendar+Reminder+Notes+Music：自带的软件都非常高</p><h3 id="1-4-娱乐类工具"><a href="#1-4-娱乐类工具" class="headerlink" title="1.4 娱乐类工具"></a>1.4 娱乐类工具</h3><p>B站客户端 腾讯视频 虚拟机内的植物大战僵尸 微信-QQ 百度网盘+阿里云盘</p><h2 id="02-Hardware-management"><a href="#02-Hardware-management" class="headerlink" title="02 Hardware management"></a>02 Hardware management</h2><p>Anker Nano II 65W充电头</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-835bdbe7ebd0732a725b5258d74dfeff.jpg" alt="img"></p><p>广告</p><p>Anker 安克  USB-C PIQ3.0 PD20W快充头</p><p>知乎</p><p>¥55.00</p><p>去购买</p><p>小米100w快充线（2m）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5d7500ce9dd672ca6731d147feea5a07.jpg" alt="img"></p><p>广告</p><p>ROCK 三合一数据线    一拖三</p><p>知乎</p><p>¥39.00</p><p>去购买</p><p>Belkin七合一拓展坞</p><p>罗技Master 2s</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-cb6b35636e7282f932af01caeb5a399e.jpg" alt="img"></p><p>广告</p><p>Razer 雷蛇 炼狱蝰蛇 V2 游戏鼠标</p><p>知乎</p><p>¥249.00</p><p>去购买</p><p>DP1.4绿联转接线</p><p>西数移动硬盘*2T</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7989088113b7c2078c1689abe1904e38.jpg" alt="img"></p><p>广告</p><p>ROCK typec 多功能转换器 十合一</p><p>知乎</p><p>¥259.00</p><p>去购买</p><p>华为原装TypeC耳机</p><h2 id="03-File-management-and-workflow"><a href="#03-File-management-and-workflow" class="headerlink" title="03 File management and workflow"></a>03 File management and workflow</h2><h3 id="3-1-Mac本地文件管理和备份"><a href="#3-1-Mac本地文件管理和备份" class="headerlink" title="3.1 Mac本地文件管理和备份"></a>3.1 Mac本地文件管理和备份</h3><p>256G的固态硬盘用起来比较着急，好在速度很快，全新 M1 MacBook Air 的固态硬盘写入速度为 2190 MB&#x2F;s，读取速度为 2675 MB&#x2F;s；相比较手机的sfs的速度可发现该机连续读取约为 1966M&#x2F;s，持续写入 1280M&#x2F;s；相比较UFS3.1通常为1200M&#x2F;s和800M&#x2F;s，速度感觉非常不错。</p><p>在日常使用的文件中主要分为：</p><ol><li>大文件，比如下载的4K电影、爱死机系列片等，这一类文件在使用之后可以放到移动硬盘作为存储，或者上传到百度网盘</li><li>不常用的文件，比如下载的文件PDF、下载的PPT、不用的课程PPT，这一类文件通常在一个阶段结束之后可以备份，比如放在移动硬盘或者放到百度网盘中</li><li>小而杂的文件，临时做的ppt、下载的通知，这是管理的重点</li></ol><p>管理方法Tag+Folder：</p><p>作为一个对macOS的的Finder热爱的人来说，非常喜欢提供tag的选项，但是常常利用Tag也会让我感到苦恼，因为会因为一个文件满足多个tag而不知道如何区分，然后新建一个Temp文件夹，久而久之所有的文件都会被放到这个临时文件夹中。就在苦苦思考的时候，我们会发现天然的无论是windows还是macOS都给我提供好了应该有的方法Folder，按照Document、Picture、Video、Download、Desktop的文件夹方式来进行分类，这里的管理原则是：</p><blockquote><p> 首先按照文件的时效性区分开： </p></blockquote><ol><li>时效较短的 download desktop</li><li>时效较长的 document、picture、video</li></ol><p> 再根据自己的Project来对不同文件类型的文件进行标记Tag，方便自己查找 Tag存在的意义是让自己容易找到他，而不是分类，因为Folder天然具有分类的作用 </p><h3 id="3-2-基于Webdav的多端同步Sync"><a href="#3-2-基于Webdav的多端同步Sync" class="headerlink" title="3.2 基于Webdav的多端同步Sync"></a>3.2 基于Webdav的多端同步Sync</h3><p>！！macOS12还没有适配坚果云的云桥模式！！</p><p>网盘通常可以分为两种</p><ol><li>大容量，作为资源库仅限App操作的，比如百度网盘、阿里云盘、夸克云盘等</li><li>同步网盘，比如iCloud、坚果云、DropBox、nextcloud、one drive等</li></ol><p>针对第一种网盘纯粹可以当作备份盘处理，虽然百度网盘推出同步空间，体验依旧非常不好。第二种网盘主要针对写作办公和同步，如果你是iPhone+iPad+Mac当然iCloud是最佳的选择，没有之一，在国内的环境使用非常的舒服，但是如果没有则需要挑选，主要的选择有：</p><ol><li>时不时抽风的one drive</li><li>会员当道的坚果云</li><li>自建webdav协议的nextcloud</li></ol><h3 id="3-2-1-第一种one-drive"><a href="#3-2-1-第一种one-drive" class="headerlink" title="3.2.1 第一种one drive"></a>3.2.1 第一种one drive</h3><p>速度有时候莫名的快，有时候莫名的慢，使用很难受。同步必须在自己的电脑中新建一个文件夹作为同步，不能实现同步其他任意文件夹的功能，比较鸡肋。但是可以释放本地空间，以快捷方式保存在本地，同时作为学生可以享受超大容量。</p><h3 id="3-2-2-第二种-坚果云"><a href="#3-2-2-第二种-坚果云" class="headerlink" title="3.2.2 第二种 坚果云"></a>3.2.2 第二种 坚果云</h3><p>目前开了一年的会员，在尝试其中的使用技巧，利用坚果云来构建Mac+iPad+Mate40pro之间的文件共享体系（重点！华为居然可以将文件夹放在桌面，太好用），在国内的同步速度非常快，快到看不见。但是目前macOS12应该是不支持云桥模式的api，也就是我文件都是同步的，而不能释放本地空间</p><h3 id="3-2-3-第三种-webdav协议自建nextcloud"><a href="#3-2-3-第三种-webdav协议自建nextcloud" class="headerlink" title="3.2.3 第三种 webdav协议自建nextcloud"></a>3.2.3 第三种 webdav协议自建nextcloud</h3><p>因为本人自己有一台腾讯云服务器，所以尝试使用nextcloud构建自己的云盘，但是因为功能比较初级，想要在Mac系统中集成需要走HTTPS协议，暂时还没有搞好，但是webdav协议除了可以下载文件之外，还可以同步日历等，构建macOS+iPados+HarmonyOS的同步，但是目前https协议还没有搞好，期待后面的使用</p><h3 id="3-3-Password管理"><a href="#3-3-Password管理" class="headerlink" title="3.3 Password管理"></a>3.3 Password管理</h3><p>密码管理的重要性，无论是个人信息中需要的密码管理，还是掌握服务器中的使用，都需要掌握良好的米阿们</p><ul><li>拖库:拖库就是指黑客通过各种社工手段、技术手段将数据库中敏感信息非法获取，一般这些敏感信息包括用户的账号信 息如用户名、密码;身份信息如真实姓名、证件号码;通讯信息如电子邮箱、电话、住址等。</li><li>洗库:在拖库后，取得大量的用户数据之后，黑客会通过一系列的技术手段和黑色产业链将有价值的用户数据变现。</li><li>撞库:撞库是黑客通过收集互联网已泄露的用户和密码信息，生成对应的字典表，尝试批量登陆其他网站 后，得到一系列可以登录的用户。很多用户在不同网站使用的是相同的帐号密码，因此黑客可以通过获取 用户在A网站的账户从而尝试登录B网址，这就可以理解为撞库攻击。</li></ul><p>手机内部华为自带的密码管理器真的是太差劲了，为了满足多端同步的功能，既然用不了Apple的钥匙串服务，Google密码在华为中用不了，只能使用微软的EDGE，电脑和平板端使用浏览器，手机端开启浏览器外部填充功能。</p><p>当时实际的密码最好还是用备忘录或者笔记本记下来！！</p><h3 id="3-4-GTD管理"><a href="#3-4-GTD管理" class="headerlink" title="3.4 GTD管理"></a>3.4 GTD管理</h3><p>它的所有方法都围绕着把所有事情都从大脑中弄出来并体系化地管理，让人专注于眼前的工作，完全信任外部体系的事务管理。</p><p>因此，GTD的方法最适用以下情况：</p><ul><li>事务繁杂，容易忘记和混淆</li><li>害怕忘记任务而反复回想，浪费时间造成压力</li><li>杂务纷扰，重要的事常来不及做</li></ul><p>通常使用reminder来将需要做的事情记录下，收集所有的事情，然后划分到长期和短期应用，之后在日历calendar中记录准确的事情，在同步过程中，依旧采用微软的outlook邮箱来实现多端同步，手机上需要下载一个Microsoft ToDo，很简洁！</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b79640844d6dfb329cdf3aca60765b0d_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="3-5-Notes管理"><a href="#3-5-Notes管理" class="headerlink" title="3.5 Notes管理"></a>3.5 Notes管理</h3><p>笔记整理中，通常的来源包括</p><ol><li>讲座、会议等的会议记录和总结</li><li>对于视频、文档、课本的阅读和总结</li><li>个人博客</li><li>自己的代码仓库</li></ol><p>由于使用不同文件就让自己的笔记整理困难，因此个人选择的ALl in one的Notion来管理自己的笔记，作为一个基于Web的笔记管理，很好用，个人笔记的特征管理选择</p><ul><li>Area-user ：笔记中主要分为person（自己的思考、自己的整理、自己的读书记录）；或者是school（课题组的项目、讲座）；以及一些第三方的东西，我是去study，或者做一些record </li><li>Group-Tag：用于分类的依据，将自己的笔记分为四种：Library（个人的汇总表）、Post（一些单独的文章）、Wiki（完整知识体系，帮助学习）、Log（项目的日志） </li><li>Name：该页面的主要名称，汇总表建议带上汇总两字、series的课程或者希望可以带上&lt;名称&gt;作为标识 </li><li>Create Date：创建日期 </li><li>Resource： 这个页面的根源是什么，包括个人的管理类或者思考类、school的项目日志或一些杂事、study的课程或文档 </li><li>Area：随便的一些tag</li></ul><h2 id="04-Environment-management"><a href="#04-Environment-management" class="headerlink" title="04 Environment management"></a>04 Environment management</h2><h3 id="4-1-Homebrew-—软件包管理工具"><a href="#4-1-Homebrew-—软件包管理工具" class="headerlink" title="4.1 Homebrew —软件包管理工具"></a>4.1 Homebrew —软件包管理工具</h3><p><a href="https://brew.sh/index_zh-cn.html">Homebrew</a></p><p>Homebrew是一款macOS的软件包管理工具，通过其可以很方便的install or uninstall软件工具，类似与Linux下的apt-get、node的npm等包管理工具、centos下的yum，使用homebrew可以安装系统没有预装的东西，，同时会将软件包安装到独立目录，也就是local文件夹下面的homebrew自己安装的文件，然后再通过软链接到usr&#x2F;local</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/bin/bash -c <span class="hljs-string">&quot;<span class="hljs-subst">$(curl -fsSL &lt;https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh&gt;)</span>&quot;</span><br></code></pre></td></tr></table></figure><p>（可选的，如果希望将homebrew添加到环境变量）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;&#x27;</span> &gt;&gt; ~/.zprofile<br><span class="hljs-built_in">eval</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(/opt/homebrew/bin/brew shellenv)</span>&quot;</span><br></code></pre></td></tr></table></figure><p>常用的一些命令</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">brew install 包名<br>brew uninstall 包名<br>brew upgrade 包名<br><br>brew <span class="hljs-keyword">update</span><br>brew upgrade<br>brew outdated<br><br>// 查看Homebrew版本<br>brew <span class="hljs-comment">--version</span><br>brew list<br>brew home<br>brew <span class="hljs-keyword">info</span> 包名<br>brew doctor<br></code></pre></td></tr></table></figure><h3 id="4-2-Mini-forge3-—-macOS的conda"><a href="#4-2-Mini-forge3-—-macOS的conda" class="headerlink" title="4.2 Mini forge3 — macOS的conda"></a>4.2 Mini forge3 — macOS的conda</h3><ol><li>一种是下载sh脚本，然后自己安装</li><li>使用homebrew安装mini forge3</li></ol><p>Anaconda和Miniconda到现在都没有提供M1处理器的conda环境，可以使用conda-forge提供miniforge，用来支持Apple Silicon的版本软件</p><p>下载软件的地址</p><p><a href="https://github.com/conda-forge/miniforge/#download">GitHub - conda-forge&#x2F;miniforge: A conda-forge distribution.</a></p><p>下载好mini forge3中对应的macOS+apple silicon的脚本，假设下载咋download文件夹</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> download<br><br><span class="hljs-string">//</span> 之后安装脚本<br><span class="hljs-string">//</span> 假如之前安装过，可以重新卸载再安装<br><span class="hljs-string">//</span> 卸载的脚本 <br><span class="hljs-string">//</span> rm -rf ~<span class="hljs-string">/miniforge3</span> <br><span class="hljs-string">//</span> rm -rf <span class="hljs-string">.conda</span> <br><span class="hljs-string">//</span> rm -rf <span class="hljs-string">.condarc</span><br><span class="hljs-string">//</span> 没有以上可以略过<br>bash Miniforge3-MacOSX-arm64.sh<br></code></pre></td></tr></table></figure><p>完成安装之后，需要重新激活配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> .zshrc<br></code></pre></td></tr></table></figure><p>完成之后可以测试conda是否安装完成</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">conda <span class="hljs-comment">--version</span><br></code></pre></td></tr></table></figure><p>第二种方法是使用homebrew安装</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>miniforge<br></code></pre></td></tr></table></figure><p>conda常见命令的使用</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-string">//</span> conda环境的创建、激活<br><span class="hljs-comment"># 查看所有conda环境</span><br>conda info -e<br><span class="hljs-comment"># 创建虚拟环境</span><br>conda create -n env_name python=python_<span class="hljs-keyword">version</span><br>Eg<span class="hljs-function">:conda</span> create -n py39 python=3.9<br><span class="hljs-comment"># 激活虚拟环境</span><br>conda activate env_name<br>Eg: conda activate py39<br><span class="hljs-comment"># 退出虚拟环境</span><br>conda deactivate<br><br><span class="hljs-comment"># 删除虚拟环境</span><br>conda remove -n env_name <span class="hljs-params">--all</span><br><br><span class="hljs-string">//</span> 包的查看、安装、删除和更新<br><span class="hljs-comment"># 查看环境下所有安装的包</span><br>conda activate env_name<br>conda list<br><span class="hljs-comment"># 查看是否安装特定的包</span><br>conda activate env_name<br>conda list package_name<br><span class="hljs-comment"># 安装包</span><br>conda antivate env_name<br>conda insatall package_name<br><span class="hljs-comment"># 删除包</span><br>conda antivate env_name<br>conda remove package_name <br><span class="hljs-comment"># 更新包</span><br>conda antivate env_name<br>conda update package_name<br><br><span class="hljs-string">//</span> conda 安装本地包<br><span class="hljs-comment"># 进入包所在路径 cd /d路径</span><br><span class="hljs-keyword">cd</span> <span class="hljs-string">/dE</span>:\\Download<br><span class="hljs-comment"># 安装包 conda install --use-local 包名</span><br>conda install <span class="hljs-params">--use-local</span> pytorch-1.8.0-py3.7_cuda11.1_cudnn8_0.tar.bz2<br><br><span class="hljs-string">//</span> conda清理环境<br>conda clean -p<br>conda clean <br></code></pre></td></tr></table></figure><h3 id="4-3-Git-—-版本控制"><a href="#4-3-Git-—-版本控制" class="headerlink" title="4.3 Git — 版本控制"></a>4.3 Git — 版本控制</h3><h3 id="4-3-1-常见指令"><a href="#4-3-1-常见指令" class="headerlink" title="4.3.1 常见指令"></a>4.3.1 常见指令</h3><p><a href="https://git-scm.com/book/zh/v2">Book</a></p><p><a href="http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html">常用 Git 命令清单</a></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-comment">// 新建代码库</span><br><span class="hljs-meta"># 在当前目录新建一个 Git 代码库</span><br>$ git <span class="hljs-keyword">init</span><br><span class="hljs-meta"># 新建一个目录，将其初始化为 Git 代码库</span><br>$ git <span class="hljs-keyword">init</span> [project-name]<br><span class="hljs-meta"># 下载一个项目和它的整个代码历史</span><br>$ git clone [url]<br><br><span class="hljs-comment">// 配置git，配置文件为gitconfig，可以在用户主目录，也可以在项目目录</span><br><span class="hljs-meta"># 显示当前的 Git 配置</span><br>$ git config --list<br><span class="hljs-meta"># 编辑 Git 配置文件</span><br>$ git config -e [--<span class="hljs-keyword">global</span>]<br><span class="hljs-meta"># 设置提交代码时的用户信息</span><br>$ git config [--<span class="hljs-keyword">global</span>] user.name <span class="hljs-string">&quot;[name]&quot;</span><br>$ git config [--<span class="hljs-keyword">global</span>] user.email <span class="hljs-string">&quot;[email address]&quot;</span><br><br>等等<br><br><span class="hljs-meta"># 添加所有文件</span><br><br>git commit -m <span class="hljs-string">&#x27;[your message]&#x27;</span><br><span class="hljs-meta"># 上传，应该是上一次add的文件夹的信息</span><br><br><br><span class="hljs-meta"># GitHub的本地ssh的设置</span><br><span class="hljs-meta"># GitHub的本地</span><br><br>git config --<span class="hljs-keyword">global</span>  --list<br>git config --<span class="hljs-keyword">global</span>  user.name <span class="hljs-string">&quot;这里换上你的用户名&quot;</span><br>git config --<span class="hljs-keyword">global</span> user.email <span class="hljs-string">&quot;这里换上你的邮箱&quot;</span><br>ssh-keygen -t rsa -C <span class="hljs-string">&quot;这里换上你的邮箱&quot;</span><br><span class="hljs-meta"># 后续确认就行，在GitHub上进行GUI的设置就OK</span><br>Settings -- SSH <span class="hljs-keyword">and</span> GPG keys<br><br><span class="hljs-meta"># 为dnf添加</span><br>sudo dnf config-manager --<span class="hljs-keyword">add</span>-repo https:<span class="hljs-comment">//cli.github.com/packages/rpm/gh-cli.repo</span><br><span class="hljs-meta"># 安装就好</span><br>dnf install gh<br><span class="hljs-meta"># 之后认证</span><br>gh auto login<br><span class="hljs-meta"># 按照规定输入选择输入token就行了</span><br></code></pre></td></tr></table></figure><h3 id="4-3-2-Git-PyCharm-GitHub操作"><a href="#4-3-2-Git-PyCharm-GitHub操作" class="headerlink" title="4.3.2 Git - PyCharm - GitHub操作"></a>4.3.2 Git - PyCharm - GitHub操作</h3><p>首先安装好自己的git，然后在pycharm中的version control system中为当前的项目构建自己的git同步目录，其中的 git每次的文件会存储在相对应的.git文件夹的目录下，所以也会导致在每次提取的过程会逐渐变的越来越大，同时为自己的GitHub账户中的一些操作</p><ol><li><a href="http://chenxia31.github.io/">chenxia31.github.io</a> ,主要是个人blog的使用，在其中使用markdown来撰写相关的推文，使用hexo g、hexo d，以及GitHub page来管理域名来完成自己的公开</li><li>code_study_pratice 这个是过去的一些操作，这里就删除，算是最开始的algorithm的东西，因为名字不好听以后就删除来，为啥不换个名字，算了换一个名字重新开始</li><li>Mercury这里以后就是代码学习的地方，主要分为BaseSkill（比如pandas的操作）、Course（比如CS229、CS61A、李宏毅之类的）、Kaggle（其中比赛的名称）、Leetcde（包括力扣官网的，还有一些关于算法导论中习题的刷）、latex中的一些模版、如果想要更近一步还需要有其他代码语言比如web开发、C++语言等属于Special Area</li></ol><h3 id="4-3-3-Git-VsCode-GitHub操作"><a href="#4-3-3-Git-VsCode-GitHub操作" class="headerlink" title="4.3.3 Git - VsCode - GitHub操作"></a>4.3.3 Git - VsCode - GitHub操作</h3><p>暂时不需要</p><h3 id="4-4-（Python）Tensorflow-Pytorch"><a href="#4-4-（Python）Tensorflow-Pytorch" class="headerlink" title="4.4 （Python）Tensorflow + Pytorch"></a>4.4 （Python）Tensorflow + Pytorch</h3><p>2022-04-09 pytorch暂时没有支持GPU加速，仅仅是讨论阶段，tensorflow已经支持GPU加速，使用的是apple提供的metal的toolkit</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> -c pytorch pytorch torchvision<br></code></pre></td></tr></table></figure><p><a href="https://developer.apple.com/metal/tensorflow-plugin/">Tensorflow Plugin - Metal - Apple Developer</a></p><p>在conda管理中，直接使用homebrew安装的miniforge可能会有问题，可以选择第一种下载之后手动安装的方式来管理</p><p>之后是新建一个conda环境，然后activate进去</p><p>利用conda安装相关依赖</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf">// 可以管理安装的版本<br>conda install -c apple tensorflow-deps<span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-number">2.6</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>然后在环境内使用pip安装相对应的tensorflow和插件</p><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs mel"><span class="hljs-comment">// 安装</span><br><span class="hljs-keyword">python</span> -m pip install tensorflow-macos<br><span class="hljs-keyword">python</span> -m pip install tensorflow-metal<br><br><span class="hljs-comment">// 如果需要卸载之后重新安装</span><br><span class="hljs-keyword">python</span> -m pip uninstall tensorflow-macos<br><span class="hljs-keyword">python</span> -m pip uninstall tensorflow-metal<br></code></pre></td></tr></table></figure><p>测试代码</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> tensorflow.keras import layers<br><span class="hljs-keyword">from</span> tensorflow.keras import models<br>model = models.Sequential()<br>model.<span class="hljs-built_in">add</span>(layers.Conv2D(32, (3, 3), <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(28, 28, 1)))<br>model.<span class="hljs-built_in">add</span>(layers.MaxPooling2D((2, 2)))<br>model.<span class="hljs-built_in">add</span>(layers.Conv2D(64, (3, 3), <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.<span class="hljs-built_in">add</span>(layers.MaxPooling2D((2, 2)))<br>model.<span class="hljs-built_in">add</span>(layers.Conv2D(64, (3, 3), <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.<span class="hljs-built_in">add</span>(layers.Flatten())<br>model.<span class="hljs-built_in">add</span>(layers.Dense(64, <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.<span class="hljs-built_in">add</span>(layers.Dense(10, <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br>model.summary()<br><br><span class="hljs-keyword">from</span> tensorflow.keras.datasets import mnist<br><span class="hljs-keyword">from</span> tensorflow.keras.utils import to_categorical<br>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()<br>train_images = train_images.reshape((60000, 28, 28, 1))<br>train_images = train_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / 255<br>test_images = test_images.reshape((10000, 28, 28, 1))<br>test_images = test_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / 255<br>train_labels = to_categorical(train_labels)<br>test_labels = to_categorical(test_labels)<br>model.compile(<span class="hljs-attribute">optimizer</span>=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              <span class="hljs-attribute">loss</span>=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(train_images, train_labels, <span class="hljs-attribute">epochs</span>=5, <span class="hljs-attribute">batch_size</span>=64)<br>test_loss, test_acc = model.evaluate(test_images, test_labels)<br>test_acc<br></code></pre></td></tr></table></figure><p><a href="https://zhuanlan.zhihu.com/p/408877901">https://zhuanlan.zhihu.com/p/408877901</a>)</p><h3 id="4-5-Linux使用"><a href="#4-5-Linux使用" class="headerlink" title="4.5 Linux使用"></a>4.5 Linux使用</h3><p>Linux 系统启动过程</p><ul><li>内核的引导</li><li>BIOS开机自检，按照BIOS中设置的启动设备来启动，操作系统接管硬件之后，首先读入&#x2F;boot目录下的内核文件</li><li>运行init</li><li>init是系统所有进程的起点，首先读取配置文件 &#x2F;etc&#x2F;inittab</li><li>系统初始化</li><li>建立终端</li><li>用户登陆系统</li><li>命令行</li><li>ssh</li><li>图形界面</li></ul><p>Linux 系统目录结构</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span>：查看文件目录<br>常见目录的解释：<br>/bin =binaries 二进制文件的缩写，这个目录存放最常用使用的明林<br>/boot 存放启动Linux的核心文件，包括一些连接文件以及镜像文件<br>/dev 是device的缩写，存放的是Linux的外部设备，Linux中饭昂问设备和访问文件的方式是相同的<br>/etc 是etcetera的缩写，用来存放所有的系统管理所需要的配置文件和子目录<br>/home 用户的主目录，一般该目录是用用户的账号名称<br>/lib 是library，存放在和系统最基本的动态链接库<br>/media 系统会将识别的设备挂载到这个目录下<br>/mnt 让用户临时挂载到别的文件系统<br>/opt optional，是主机额外安装软件所摆放的目录，弄人是空的<br>/proc 是processes的缩写，是虚拟文件下i痛，这个目录的netting不再硬盘上而是在内存里面，可以直接修改里面的文件来屏蔽命令<br>/root 系统管理员的用户主目录<br>/sbin superuser binaries的缩写，存放的额是系统管理员使用的系统管理程序<br>/selinux 是centos特有的目录，类似于Windows的额防火墙<br>/usr unix shared resources的缩写，用户的许多应用程序和文件<br>/usr/bin 系统用户使用的应用程序<br>/usr/sbin <br>/usr/src 内核源代码默认的存放目录<br></code></pre></td></tr></table></figure><p>Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限，为来保护系统的安全性，Linux系统对不同的用户访问相同的文件的权限做出了不同的规定，在Linux中常用一下两个命令来修改文件或者目录所述的用户与权限</p><ul><li>chown（change owner）修改所属的user和group</li><li>chmod（change mode）修改用户的权限</li></ul><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mel">ll 或者<span class="hljs-keyword">ls</span>-l<br>来显示一个文件的属性以及文件所属于的user和<span class="hljs-keyword">group</span><br></code></pre></td></tr></table></figure><p>Linux 文件和目录管理</p><p>Linux的目录结构为tree结构，最顶级的目录为根目录，其他目录通过挂载可以添加到tree中，通过解除挂载可以移除他们，首先区分绝对路径和相对路径</p><ul><li>绝对路径 &#x2F;usr&#x2F;share&#x2F;doc</li><li>相对路径 ..&#x2F;man</li></ul><p>处理目录的常用命令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">ls</span>：<span class="hljs-keyword">list</span> <span class="hljs-keyword">file</span> 列出目录及文件名<br><span class="hljs-keyword">cd</span>：<span class="hljs-keyword">change</span> directory 切换目录<br><span class="hljs-keyword">pwd</span>：佩内塔 work directory 显示目前的目录<br><span class="hljs-built_in">mkdir</span>：<span class="hljs-keyword">make</span> directory 创建一个新的目录<br>rmdir：<span class="hljs-built_in">remove</span> directory 删除一个空的目录<br><span class="hljs-keyword">cp</span>：<span class="hljs-keyword">copy</span> <span class="hljs-keyword">file</span>复制文件<br>rm：<span class="hljs-built_in">remove</span>删除文件或者目录<br>mv：<span class="hljs-keyword">move</span> <span class="hljs-keyword">file</span> 移动文件与目录<br>man 【<span class="hljs-keyword">command</span>】来查看使用文档<br></code></pre></td></tr></table></figure><p>Linux的文件内容查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> 由第一行开始显示文本内容<br><span class="hljs-built_in">tac</span> 从最后一行开始显示<br><span class="hljs-built_in">nl</span> 显示的时候输出行号<br>more 一页一页的显示内容<br>less 可以<br><span class="hljs-built_in">head</span> 前几行<br><span class="hljs-built_in">tail</span> 只看后几行<br></code></pre></td></tr></table></figure><p>Linux的连接link的概念，一种是硬连接，另外一种是符号连接</p><ul><li>hard link 通过索引界面的来连接，保存在磁盘分区汇总的文件不管是什么类型都给它分配一个编号成为inode index，在Linux汇总多个文件名指向同一个索引届电视存在的，硬连接允许一个文件拥有多个有效的文件名</li><li>symbolic link：类似于快捷方式</li></ul><p>Linux 用户和用户组管理</p><p>Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户都必须首先向系统管理员申请一个账号，然后用账号的身份进入系统</p><p>用户的账号一方面可以帮助系统管理员对使用系统得到用户进行跟踪并控制他们对系统资源的访问，另一方面也可以帮助用户组织文件，并为用户提供安全性防护</p><p>实现用户账号的管理，要完成的工作主要有几个方面</p><ul><li>用户账号的添加、删除和修改</li><li>用户口令的管理</li><li>用户组的管理</li></ul><p>Linux磁盘管理好坏直接关系到整个系统的性能问题</p><p>Linux磁盘管理常用的命令为df、du、fidsk</p><ul><li>disk full（df）</li><li>disk used（du）</li><li>fdisk 用于磁盘分区</li></ul><p>我的base环境是我之前用的</p><p>首先安装anaconda：</p><p>wget -P &#x2F;tmp <a href="https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh">https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh</a></p><p>wget是工具的名称，后面的 是对应的下载地轴</p><p>后面的&#x2F;tmp是让下载文件存储在这个文件夹下面</p><p>常见的command</p><p>yum yellow dog update modified 是一个在fedora和redhat以及suse的shell前端软看包管理器，基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次性安装所有依赖软件包，无须繁琐对的一次次下载</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">grammar <span class="hljs-keyword">of</span> yum：<br>yum 【<span class="hljs-keyword">option</span>】 【command】<br>* <span class="hljs-keyword">option</span> 可选，包括-h（help） -y（yes） -q（不显示安装过程）<br>* command 要进行的操作<br>* package 安装的包名<br><br>常见的命令：<br><span class="hljs-keyword">check</span>-<span class="hljs-keyword">update</span><br><span class="hljs-keyword">update</span><br>install &lt;package-<span class="hljs-type">name</span>&gt;<br><span class="hljs-keyword">update</span> &lt;package -<span class="hljs-type">name</span>&gt;<br>yum list<br>yum remove &lt;package -<span class="hljs-type">name</span>&gt;<br>yum <span class="hljs-keyword">search</span> &lt;keyword&gt;<br>yum clean packages<br>yum clean headers<br>yum clean oldheaders<br></code></pre></td></tr></table></figure><p>apt (advanced packageing tool 是一个在debian和ubuntu终端额shell前端软件挂你了工具</p><p>利用bash Anaconda3-2021.11-Linux-x86_64.s h 来安装对应的文件</p><p>安装的位置会默认在root&#x2F;anaconda的目录下面</p><p>从yum安装dnf</p><h3 id="4-6-Docker使用"><a href="#4-6-Docker使用" class="headerlink" title="4.6 Docker使用"></a>4.6 Docker使用</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs vim">yum <span class="hljs-keyword">list</span> installed | <span class="hljs-keyword">grep</span> docker<br><br>卸载安装<br>yum -<span class="hljs-keyword">y</span> <span class="hljs-built_in">remove</span> docker-<span class="hljs-keyword">ce</span>-cli.x86_64<br>yum -<span class="hljs-keyword">y</span> <span class="hljs-built_in">remove</span> docker-<span class="hljs-keyword">ce</span>.x86_64<br>yum -<span class="hljs-keyword">y</span> <span class="hljs-built_in">remove</span> containerd.io<br><br>安装依赖<br>yum install -<span class="hljs-keyword">y</span> yum-utils device-mapper-persistent-data lvm2<br>安装yum圆头<br><br>yum-config-manager --<span class="hljs-built_in">add</span>-repo &lt;https://mirrors.cloud.tencent.<span class="hljs-keyword">com</span>/docker-<span class="hljs-keyword">ce</span>/linux/centos/docker-<span class="hljs-keyword">ce</span>.repo&gt;<br><br>安装Docker<br><br>yum install -<span class="hljs-keyword">y</span> docker-<span class="hljs-keyword">ce</span> docker-<span class="hljs-keyword">ce</span>-cli containerd.io<br><br>docker <span class="hljs-keyword">version</span><br></code></pre></td></tr></table></figure><p>启动</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-params">system</span>ctl start docker<br><br>设置为开机启动<br><span class="hljs-params">system</span>ctl enable docker<br><br>查看运行状态<br></code></pre></td></tr></table></figure><p>配置</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">mkdir -<span class="hljs-selector-tag">p</span> /etc/docker<br><br><span class="hljs-comment">//配置加速</span><br>tee /etc/docker/daemon<span class="hljs-selector-class">.json</span> &lt;&lt;-<span class="hljs-string">&#x27;EOF&#x27;</span><br>&#123;<br>  <span class="hljs-string">&quot;registry-mirrors&quot;</span>: <span class="hljs-selector-attr">[<span class="hljs-string">&quot;&lt;https://mirror.ccs.tencentyun.com&gt;&quot;</span>]</span><br>&#125;<br>EOF<br><br><span class="hljs-comment">// 重启守护进程并重启docker</span><br>systemctl daemon-reload &amp;&amp; systemctl restart docker<br></code></pre></td></tr></table></figure><p>运行第一个容器</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">docker <span class="hljs-built_in">run</span> <span class="hljs-attribute">--name</span>=hello hello-world<br><br>查看<br>docker ps -a<br></code></pre></td></tr></table></figure><p>或者一步一步运行一个容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull johngong/calibre-web<br><br>查看现有<br>docker images<br><br>根据创建镜像 <br>docker create --name=calibre-web -p 80:8083 -v /data/calibre-web/library:/library -e WEBLANGUAGE=zh_CN johngong/calibre-web<br><br>docker create 是创建容器的命令<br>--name=calibre-web 表示创建的容器的名称<br>-p 80:8083 表示该容器将 80 端口映射到 8083 端口<br>-v /data/calibre-web/librery:/libray 表示该容器将 /data/calibre-web/library 目录映射为 /library 目录<br>-e WEBLANGUAGE=zh_CN 表示该容器定义了一个变量，变量名是 WEBLANGUAGE，变量值是 zh_CN<br>johngong/calibre-web 是容器的镜像，这里也就是我们前面拉取的镜像<br><br>启动刚才创建好的镜像<br><br>docker start calibre-web<br></code></pre></td></tr></table></figure><p>停止镜像</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">docker stop calibre-web<br><br>或者<br><br>docker kill calibre-web<br><br>删除容器<br>docker rm calibre-web<br><br>docker rmi hello-world<br><br>docker rmi `docker images -q`删除所有<br></code></pre></td></tr></table></figure><p>Docker部署python 网站</p><ul><li>安装Docker虚拟机环境</li><li>学习docke常用命令</li><li>安装Python SDK和MySQL镜像</li><li>为Flask添加SQL Alchemy拓展库</li></ul><p>为什么要Docker?虚拟化环境的重要性</p><ol><li>直接程序安装或者部署在Linux操作系统上面，容易引发资源冲突</li><li>程序卸载不干净，导致无法安装或者部署新程序</li><li>每次部署之前要安装很多软件，修改复杂的配置文件</li><li>无法让服务器硬件资源实现多租户服务</li><li>增大了在Linux系统上部署集群和分布式的难度</li></ol><p>在Linux上安装docker，了解docker hub的</p><p>设置dockerhub的加速器DaoCloud</p><p>了解Docker镜像与容器的关系，了解镜像和容器之间的关系，学习常用的docker命令</p><p>什么是docker镜像？</p><p>为了快速打包和部署软件环境，docker引入image机制</p><p>镜像是一个配置好的只读层软件环境</p><p>可以通过dockerfile文件创建镜像，也可以从docker仓库中下载到镜像中</p><p>什么是docker容器</p><p>容器是在镜像基础上创建出的虚拟实例，内容可读可写</p><p>一个docker image 可以创建多个container，而且container之间相互隔离，部署的程序之间不会相互干扰，所有的容器直接使用宿主的主机的Linux内核</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-11aa44143b5c4a55178f7e67eb55b57e_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>docker run -it —name&#x3D;p1 python:3.8 bash</p><p>docker 会为每个容器创建网址</p><p>学习docker网络，docker的容器端口映射，目录挂载技术，FTP程序来管理文件，可以将宿主机挂载在目录挂载</p><p>docker network create —subnet&#x3D;172.18.0.0&#x2F;16 mynet</p><p>网段的ip</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">docker<span class="hljs-built_in"> network </span>create <span class="hljs-attribute">--subnet</span>=172.18.0.0/16 mynet<br>docker<span class="hljs-built_in"> network </span>rm mynet<br>docker <span class="hljs-built_in">run</span> -it <span class="hljs-attribute">--name</span>=p2 --net mynet --ip 172.18.0.2 python:3.8 bash<br></code></pre></td></tr></table></figure><p>默认情况下，除来宿主机之外，任何主机无法访问远程访问的docker容器，通过端口映射可以把容器端口映射到宿主机的端口，这样其他的主机就能访问容器来，映射到宿主机的端口，不需要设置防火墙规则</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">docker</span> run -it -p9500:<span class="hljs-number">5000</span> -p9600:<span class="hljs-number">3306</span> python3.<span class="hljs-number">8</span>:bash<br></code></pre></td></tr></table></figure><p>为能把一部分业务数据保存在docker环境中，可以使用ftp，但是比较麻烦，因此可以使用目录映射。因为ftp会导致文件删除，只支持文件目录挂载，不支持文件挂载，而且一个容器可以挂载多个目录</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apple</tag>
      
      <tag>MacBook</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络 Vol1 ｜ 什么是因特网</title>
    <link href="/posts/764a4687.html"/>
    <url>/posts/764a4687.html</url>
    
    <content type="html"><![CDATA[<p>暑假新坑，自顶向下的学习计算机网络，虽然作为交通人学过了信号与系统、信息传输原理、嵌入式系统中，了解到了比特传输以及协议的初步概念，但是和实际应用中的计算机网络还存在比较大的GAP，自顶向下成功的介绍了如何去看计算机网络，而且作者行文非常有趣！<a href="http://101.43.56.17:84/s/J22BYyj7sFKyDyj">Computer Networking_ A Top-Down Approach, Global Edition, 8th Edition.pdf</a>,自己nextcloud搭建的webdav网盘分享！</p><h2 id="chapter01-计算机网络和因特网"><a href="#chapter01-计算机网络和因特网" class="headerlink" title="chapter01 计算机网络和因特网"></a>chapter01 计算机网络和因特网</h2><p>概述计算机网络和因特网，这章的目标是从整体上粗线条勾勒出计算机网络的概貌，并且描述这本书的内容框架，包含大量计算机网络的背景知识，并放到整个网络的大环境中讨论结果。</p><p>在介绍基本术语和概念后，将首先查看构成网络的基本硬件和软件组建，从网络的边缘开始考察网络中运行的端设备；之后探究网络的核心，包括传输数据的链路和交换机，以及将端系统和网络核心相连接的接入网和物理媒体</p><p>后半部分将从更广泛、更抽象的角度来考察计算机网络，包括其中的数据的时间延迟、丢包和吞吐量；介绍其中的关键体系结构原则，如协议分层和服务模型，我们将了解到计算机网络对于许多不同类型的攻击来说是脆弱的</p><h3 id="1-1-是什么是因特网（Internet）"><a href="#1-1-是什么是因特网（Internet）" class="headerlink" title="1.1 是什么是因特网（Internet）"></a>1.1 是什么是因特网（Internet）</h3><h3 id="1-1-1-硬件角度描述"><a href="#1-1-1-硬件角度描述" class="headerlink" title="1.1.1 硬件角度描述"></a>1.1.1 硬件角度描述</h3><p>computer network似乎有点过时，因为除了电脑、服务器智能手机、电视、游戏机、温度调节设备都逐渐连接到因特网中，而这些设备通常称为主机（Host）或者端系统（End system）</p><p><strong>端系统（End system）</strong>通过<strong>通信链路（Communication link）</strong>和<strong>分组交换（Packet switch）</strong>连接到一起，</p><p>其中通信链路由不同的物理媒介组成，包括同轴电缆、铜线、光纤等，最重要的是传输速率</p><p>其中分组交换是将从一组输入链路到达另外一个输出链路，在这个过程由许多特色的分组交换机，其中最著名的包括路由器（router）和链路层交换机（link-layer switch）</p><p>### 1.1.2 软件角度描述</p><p>互联网应用处理传统的电子邮件、web冲浪等传统应用外，还包括移动智能手机和平板电脑应用程序，其中包括即时通讯、地图导航、流媒体等，这些应用程序涉及多个相互交互数据的端系统，因此称为分布式应用系统（distributed application）</p><p>​    ？？假设你希望对分布式因特网有一个激动人心的想法，如何将这种想法转换称为一种实际的因特网应用呢？？也就是运行在一个端系统上的应用程序怎样才能通过互联网获得在另一个端系统上的软件发送的数据？    与因特网的端系统提供了一个<strong>套接字 socket interface，规定了运营在一个端系统上的程序 请求因特网基础设施向运行在另一个端系统上的特定目的地程序交付数据的方式，是发送程序必须遵循的规则集合</strong>    </p><blockquote><p> 协议（protocol）定义了两个或多个通信实体之间交换的报文的格式和顺序，以及报文发送和或接受一条报文或其他时间所采取的动作 </p></blockquote><p>不同的协议用于完成不同的通信任务</p><h3 id="1-2-网络详细-—-网络边缘"><a href="#1-2-网络详细-—-网络边缘" class="headerlink" title="1.2 网络详细 — 网络边缘"></a>1.2 网络详细 — 网络边缘</h3><p>端系统称为主机（host），也被叫做端系统（End system），主机可以进一步划分为客户（Client）和服务器（Server）</p><p>传输的物理媒介到底是什么？也就是信息传输原理中传递信息的方式，从发射器到接收器中的物理媒介和引导型媒介</p><ol><li>物理媒介（Physical medium） </li><li>双绞铜线，用来减少临近类似的双绞线的嗲你去干扰，无屏蔽双绞线（UTP）通常用于建筑物内的计算机网络（LAN） </li><li>同轴电缆，由两个铜导体组成的，但是这两个导体是同心的而不是并行的，可以达到较快的数据传输速率 </li><li>光纤，一种细而柔软的能够引导光脉冲的媒介，衰减低同时难以窃听，由于光频率的特殊性，以51.8Mpbs作为标准版 </li><li>导引性媒介（Guided media） </li><li>陆地无线电信道，极大的依赖于传播环境和信号传输的举例，分为短、中、长三类 </li><li>卫星无线信道，通常使用两类卫星，同步卫星（geostationary satellite）和近地轨道（low-earth satellite）</li></ol><p>边缘设备和网络链接需要考虑接入网，也就是将端系统物理设备连接到边缘路由器（Edge router），常见的方式有：</p><ol><li>家庭接入，DSL、电缆、FTTH、拨号和卫星 </li><li>DSL（Digital Subscriber Line）数字用户线，受限于价格或者物理贷款，上行速率和下行速率并不相同 </li><li>电缆（Cable Internet Access）利用有线电视公司现有的有线电视基础设施，通常使用同轴电缆和光纤混合的方式（Hybrid Fiber Coax，HFC） </li><li>光纤到户（Fiber to the home，FTTH)，分为主动光纤网络和被动光纤网络 </li><li>非常慢、适合偏远地区的卫星和拨号（传统电话线） </li><li>公共场所，以太网和WiFi </li><li>使用局域以太网将设备连接到交换机，比如使用双绞铜线连接到减缓集 </li><li>一种基于IEEE802.11的LAN介入方式 </li><li>虽然以太网和Wi-Fi接入网最早出现在企业或大学，但是今年来也已经成为家庭网络中相当常见的部件 </li><li>移动网络，3G、LTE、5G </li><li>利用基站布设来获取速度 </li><li>LTE（long term evolution）最差首字母缩写词，取得了10Mpbs的速率</li></ol><h3 id="1-3-网络详细-—网络核心"><a href="#1-3-网络详细-—网络核心" class="headerlink" title="1.3 网络详细 —网络核心"></a>1.3 网络详细 —网络核心</h3><p>由通信链路和交换机组成的网状系统，在网络链路和交换机移动数据的两种基本方法，电路交换（circuit switching）和分组交换（packet switching）</p><h3 id="1-3-1-分组交换"><a href="#1-3-1-分组交换" class="headerlink" title="1.3.1 分组交换"></a>1.3.1 分组交换</h3><p>传递报文，但是会根据数据包有时延，因为交换机需要存储文件然后发出去</p><h3 id="1-3-2-电路交换"><a href="#1-3-2-电路交换" class="headerlink" title="1.3.2 电路交换"></a>1.3.2 电路交换</h3><p>面向电路的连接，无论时时分复用、频分复用还是码分复用</p><h3 id="1-4-网络详细-—-评价指标：时延、丢包和吞吐量"><a href="#1-4-网络详细-—-评价指标：时延、丢包和吞吐量" class="headerlink" title="1.4 网络详细 — 评价指标：时延、丢包和吞吐量"></a>1.4 网络详细 — 评价指标：时延、丢包和吞吐量</h3><p>在理想情况下，我们希望因特网服务能够在任意两个端系统之间随心所欲瞬间移动数据而没有任何数据丢失，但是受限于系统的实际传递速率，可能会丢到分组的报文，一方面这个是网络的巨大问题，另一方面也是计算机网络存在的意义</p><h3 id="1-4-1-时延的概述"><a href="#1-4-1-时延的概述" class="headerlink" title="1.4.1 时延的概述"></a>1.4.1 时延的概述</h3><p>当分组报文从一个节点沿着这条路到达下一个节点，该分组在沿途中的每个节点经受了几种不同类型的时延，其中最重要的包括</p><ol><li>节点处理时延（Nodal processing delay），比如检查比特差错所需要的时间、等时间 </li><li>排队时延（Queuing delay），在队列中，在链路上等待传世的时间，如果流量很大，则不能传输无需等待 </li><li>传输时延（Transmission delay），这个适用于报文分组所需要的时间毫秒级到微秒级别 </li><li>传播时延（Propagation delay），这个是实际上在路径上运行的额记录，一旦一个比特走向了链路，该比特所需要的路由器传播</li></ol><p>不同业务场景对于计算机网络的性能处理方式并不相同。</p><h3 id="1-4-2-排队时延和丢包的关系"><a href="#1-4-2-排队时延和丢包的关系" class="headerlink" title="1.4.2 排队时延和丢包的关系"></a>1.4.2 排队时延和丢包的关系</h3><p>当然是时延越长，丢包越多啦</p><h3 id="1-4-3-端到端时延"><a href="#1-4-3-端到端时延" class="headerlink" title="1.4.3 端到端时延"></a>1.4.3 端到端时延</h3><p>略</p><h3 id="1-4-4-计算机网络的吞吐量"><a href="#1-4-4-计算机网络的吞吐量" class="headerlink" title="1.4.4 计算机网络的吞吐量"></a>1.4.4 计算机网络的吞吐量</h3><p>略</p><h3 id="1-5-网络详细-—-协议层次和服务模型"><a href="#1-5-网络详细-—-协议层次和服务模型" class="headerlink" title="1.5 网络详细 — 协议层次和服务模型"></a>1.5 网络详细 — 协议层次和服务模型</h3><p>在计算机网络中虽然存在大量的饮用程序协议、各种个样的端系统、分组交换机以及各种类型的链路级媒体，面对这种复杂性，还存在组织网络体系结构的希望吗？答案是肯定的，OSI参考模型</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-007e9fe3c1971465e6c29f6d66795486_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="1-5-1-分组的体系结构"><a href="#1-5-1-分组的体系结构" class="headerlink" title="1.5.1 分组的体系结构"></a>1.5.1 分组的体系结构</h3><p>利用分层的体系结构就可以讨论一个大而复杂系统的定义良好的特定部分，这种简化本身由于提供模块化而具有很高价值，这是的某层所提供的服务容易改变，只要该曾对上面提供相同的服务并且使用来自下层的相同服务，就可以保持不断更新，这种改变服务的实现而不影响该系统其他组件是分组的另一种重要优点。但是分层的一个潜在缺点是一层可能容易较低层的功能。将这些综合起来，各层的所有协议被成为协议栈（protocol stack）</p><ol><li>应用层，比如HTTP、SMTP、FTP、SFTP、DNS解析 </li><li>运输层，TCP、UDP </li><li>网络层，将数据报datagram从一台主机移动到另一太主机，包括著名的网际协议你IP，也包括路由选择协议等，统一称作IP层 </li><li>链路层，包括以太网、WIFI、电缆接入网DOCSIS协议，这里的分组成为frame </li><li>物理层，通过物理协议传输</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-28c6192c2fa407bc7f2b7c3312b25693_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 boosting 方法 ｜ Vol10</title>
    <link href="/posts/eb8928f5.html"/>
    <url>/posts/eb8928f5.html</url>
    
    <content type="html"><![CDATA[<p>对决策树中有关的boosting方法进行总结，包括最基本的Adaboost、然后了解从提升树（Boosting Tree）到之后的梯度提升回归树（Gradient boosting decision tree），希望可以了解其中的Boosting方法的脉络，也为之后的XGBoost和LightGBM的学习奠定基础</p><h2 id="0x01-背景知识"><a href="#0x01-背景知识" class="headerlink" title="0x01 背景知识"></a>0x01 背景知识</h2><h3 id="1-1-决策树（Decision-tree）"><a href="#1-1-决策树（Decision-tree）" class="headerlink" title="1.1 决策树（Decision tree）"></a>1.1 决策树（Decision tree）</h3><p>决策树是一种基本的机器学习模型，具有分类和回归的功能。它的特点是具有分类速度快，模型容易可视化的解释，缺点是容易发生过拟合</p><p>决策可以分为分类树和回归树</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7f621ec131db6dcd99ec51e617d3e6cc_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="1-2-集成学习-（Ensemble-learning）"><a href="#1-2-集成学习-（Ensemble-learning）" class="headerlink" title="1.2 集成学习 （Ensemble learning）"></a>1.2 集成学习 （Ensemble learning）</h3><p>为了将弱学习器（Weak learner）集中其中形成这个具有多学习器综合的条件，我们可以采用不同的算法和数据集融合的方式来构建模型、也可以采用不同的融合方法比如平均法投票法stacking、或者采用特殊的技巧比如bagging和boosting的方式</p><p>其中XGboost和GD都属于boosting的方法范畴，其工作原理是初始训练集训练得到一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得之前基学习器出错的训练样本后续可以得到更多的关注，然后基于调整后的样本分布来重新训练学习器，最终继承多个学习器并根据错误进行加权来得到最终的学习器</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d97a0e2319b0f01e61f42b9307fb1bd4_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="1-3-回顾机器学习"><a href="#1-3-回顾机器学习" class="headerlink" title="1.3 回顾机器学习"></a>1.3 回顾机器学习</h3><p>机器学习从组成部分中最重要可以说分为四个部分</p><ol><li>数据集（dataset）</li><li>模型（model）和参数（parameters）</li><li>目标函数（objective）&#x3D; 损失（loss）+正则化（regularization）</li><li>优化方法（optimization method）</li></ol><h2 id="0x02-AdaBoost算法"><a href="#0x02-AdaBoost算法" class="headerlink" title="0x02 AdaBoost算法"></a>0x02 AdaBoost算法</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>每次学习之后，迭代更新样本数据集中的权重来重新生成一颗决策树，之后根据准确率来组合所有的学习器实例得到最终的结果</p><h3 id="2-2-算法过程"><a href="#2-2-算法过程" class="headerlink" title="2.2 算法过程"></a>2.2 算法过程</h3><p>给定一个二分类的数据集$T&#x3D;{(x_i,y_i)}_{i&#x3D;1}^{i&#x3D;N}$，X是特征集合，y是标记集合。</p><p>算法的流程(<strong>AdaBoost) 输出为G（X）</strong></p><p>1-1 首先，初始化训练数据中的权重分布，最初都相同权重</p><p>之后训练M颗树，对于M&#x3D;1，2，…，M</p><p>2-1 根据权重分布$D_m$训练数据集来得到基本分类器</p><p><strong>《李航统计学习》没有提到的部分就是如何根据权重来重新计算分类器；个人推测是在决策树分类的过程中，对不同类别的比例按照权重来重新计算，比如均匀分布的时候1和-1的比例为6:4；如果有权重需要根据对其中错误的类别进行分类：（3<strong><strong>0.07+3</strong></strong>0.16）：4*0.07</strong></p><p>2-2 计算得到的分类器在训练数据集中的误差率</p><p>2-3 由此来得到分类器对应的权重</p><p>2-4 在进行下一步计算的过程中，更新下一次的训练的权重，$Z_m$的作用是为来归一化</p><p>3-1 这样反复迭代得到基本分类器的线性组合</p><p>G(x)就是最终的输出，有时候为了防止过拟合，也可以使用学习率来控制$f(x)$的增加速度</p><h3 id="2-3-AdaBoost例子"><a href="#2-3-AdaBoost例子" class="headerlink" title="2.3 AdaBoost例子"></a>2.3 AdaBoost例子</h3><p>见《李航统计学习131页》</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-2a0ea3d85544f1c8cc4db9d8c4739c22_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="2-4-算法的分析和解释"><a href="#2-4-算法的分析和解释" class="headerlink" title="2.4 算法的分析和解释"></a>2.4 算法的分析和解释</h3><p>关于AdaBoost的解释可以分为两种</p><ol><li>是通过统计学习的理论来推到出AdaBoost的训练误差边界、和相关推广的定理</li><li>认为它是模型为集成策略为additive、损失函数为exp、学习算法为前向分布算法（forward Stagewise algorithm）</li></ol><p>其实AdaBoost中一个问题就是权重的选取为什么是这个样子的？</p><p>这里我们可以结合前向分布来进行推导,这里的目标函数是</p><p>也正是因为这个限制，AdaBoost只能作为二分类来学习，</p><p>1-1 加法模型，就是我们可以</p><p>其中 $\alpha_m$为权重，$f_m(x)$是根据第M次训练得到的分类器，接下来就是确定这两者的取值，我们的目标是</p><p>由于在M次训练的时候，$G_{m-1}(X)$没有影响，所以可以分离开来作为比例项</p><p>根据加权数据来重新训练得到$f_m(X)$。</p><p>对于$\alpha_m$可根据求导得到答案</p><p><strong>注意这里只是一个二分类的问题，所以化简之后得到的答案</strong></p><h3 id="2-5-优缺点"><a href="#2-5-优缺点" class="headerlink" title="2.5 优缺点"></a>2.5 优缺点</h3><p>优点：</p><p>分类精度高，可以用各种回归分类模型作为构建学习，非常灵活，不容易过拟合</p><p>缺点：</p><p>对于异常值敏感，会获得较高的权重</p><h2 id="0x03-梯度提升简介-Gradient-Boosting"><a href="#0x03-梯度提升简介-Gradient-Boosting" class="headerlink" title="0x03 梯度提升简介 Gradient Boosting"></a>0x03 梯度提升简介 Gradient Boosting</h2><h3 id="3-0-直觉background"><a href="#3-0-直觉background" class="headerlink" title="3.0 直觉background"></a>3.0 直觉background</h3><p>参考链接5，下文给出，写的非常好</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8c7cf57b7e15849359b48ec1d757cd82_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="3-1-提升树-Boosting-Tree"><a href="#3-1-提升树-Boosting-Tree" class="headerlink" title="3.1 提升树 Boosting Tree"></a>3.1 提升树 Boosting Tree</h3><p>提升树是按照分类树或者回归树作为基本分类器的提升方法，提升树被认为是统计学习中性能最好的方法之一，针对不同问题的提升树的学习算法主要在于损失函数不同，包括平方误差损失函数、指数损失函数的分类问题和一般损失函数的一般决策问题</p><p><strong>回归问题中的提升树算法</strong></p><p>1-1 初始化模型</p><p>2-1 对于m&#x3D;1，2，…，M不断迭代</p><p>2-2 计算上一个模型留下来的残差</p><p>2-3 重新计算一个回归树</p><p>2-4 更新现有的模型</p><p>2-5 得到最终的回归树</p><p>[gradient_boosting.pdf](<a href="https://zhuanlan.zhihu.com/Gradient">https://zhuanlan.zhihu.com/Gradient</a> Boosting算法的原理解析 adea61fa3573481aaf9534834231c2c8&#x2F;gradient_boosting.pdf)</p><p>每一次建立模型是在之前建立模型损失函数的梯度下降方法，损失函数是评价模型性能，如果可以让损失函数可以持续下降，就能够使得模型可以不断改变提升性能，其中最好的方法是让损失函数可以沿着梯度的下降进行更新迭代</p><h3 id="3-2-梯度提升-Gradient-Boosting（for-regression-DT）"><a href="#3-2-梯度提升-Gradient-Boosting（for-regression-DT）" class="headerlink" title="3.2 梯度提升 Gradient Boosting（for regression DT）"></a>3.2 梯度提升 Gradient Boosting（for regression DT）</h3><p>最开始GBDT只能解决回归问题，但是后续工业的调整之后允许分类</p><p>从上述我们可以看出对于分类和回归模型下来的思路，但是后续我们应该怎么办？对于其他类型的损失函数，我们如何去训练整个过程，这个时候就需要利用梯度下降（Gradient decent的思路）</p><p>残差其实是根据很直觉的想法来得到的，但是对于更加复杂的损失函数比如交叉熵之类的，简单的残差是没有意义的，所以为了保证整个算法可以走下去，使用梯度下降的方法</p><p>在其中使用shrinkage的方法，来保证每次仅仅改变一丝，来逐渐的得到梯度提升回归树</p><p>可以看出梯度提升（Gradient Boosting）和AdaBoost之间的细微的区别在于：对于错误的数据，AdaBoost采用改变其样本分布的权重增大比例，而梯度提升是利用残差的大小来提高错误数据在模型中的重要性</p><h2 id="reference："><a href="#reference：" class="headerlink" title="reference："></a>reference：</h2><p>《统计学习方法》-李航</p><p>《机器学习》-周志华</p><p><a href="https://blog.csdn.net/shine19930820/article/details/65633436">https://blog.csdn.net/shine19930820/article/details/65633436</a></p><p><a href="https://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf">https://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf</a></p><p>[<a href="https://blog.csdn.net/shine19930820/article/details/65633436]">https://blog.csdn.net/shine19930820/article/details/65633436]</a>(</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法基础 Vol4</title>
    <link href="/posts/b8a375fb.html"/>
    <url>/posts/b8a375fb.html</url>
    
    <content type="html"><![CDATA[<p>适合初始算法与数据结构的新手和想要在短时间内高效提升的人，熟练掌握这100道题，可以具备在代码世界通行的基本能力。想借这一百道题来了解一下算法的一些基本思想。本次主要为题号1～20共11道题。第四题是重点。</p><h3 id="0x00-自我总结"><a href="#0x00-自我总结" class="headerlink" title="0x00 自我总结"></a>0x00 自我总结</h3><p>数据结构和算法是看待一道题目的不同解读。数据结构更多的是如何去表述一件事情，不同的数据结构具有不同的时间空间复杂度的性能和不同的接口，这也是不同数据结构的特点所在，算法更多的是解决问题的流程，如何把手里面已经有的数据结构通过三种基本的逻辑运算结合形成高效的操作流程。</p><p>在刷题之前掌握具有哪些常见的数据结构和算法是有必要的，数据结构可以分为线性数据结构：栈和队列、数据与矩阵；半线性结构：树，非线性结构：链表、数组、图、位运算等；从算法的角度可以分为基本：分治、贪心、动态规划、回溯、分枝定界；和一些特殊的技巧双指针、二分查找、搜索法等，以及特定的场景排序、树的搜索等</p><h3 id="0x01-T1-两数之和"><a href="#0x01-T1-两数之和" class="headerlink" title="0x01 T1 两数之和"></a>0x01 T1 两数之和</h3><p>标签：数组、哈希表</p><p>难度：简单</p><blockquote><p> 给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。你可以按任意顺序返回答案。 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        hashtable=<span class="hljs-built_in">dict</span>()<br>        <span class="hljs-comment"># 新建一个哈希表，来记录每个数字它对应的补数</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> target-nums[i] <span class="hljs-keyword">in</span> hashtable:<br>            <span class="hljs-comment"># 在这个哈希表中就OK了，而且这个时间复杂度并不高</span><br>                <span class="hljs-keyword">return</span> [hashtable[target-nums[i]],i]<br>            hashtable[nums[i]]=i<br>        <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure><h3 id="0x02-T2-两数相加"><a href="#0x02-T2-两数相加" class="headerlink" title="0x02 T2 两数相加"></a>0x02 T2 两数相加</h3><p>标签：递归、链表、标签</p><p>难度：中等</p><blockquote><p> 给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。 请你将两个数相加，并以相同形式返回一个表示和的链表。 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">addTwoNumbers</span>(<span class="hljs-params">self, l1,l2</span>):<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">addTwoNumbers2</span>(<span class="hljs-params">l1, l2, flag</span>):<br>            <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">            三种情况</span><br><span class="hljs-string">            情况1：链表都存在，l1!=None</span><br><span class="hljs-string">            情况2：链表不存在，赋值为0</span><br><span class="hljs-string">            计算两数之和</span><br><span class="hljs-string">            如果大于10，则val为余数</span><br><span class="hljs-string">            flag值为1</span><br><span class="hljs-string">            链表不存在并且flag==0，则循环终止，输出最终的链表</span><br><span class="hljs-string">            链表存在，则继续下一个循环</span><br><span class="hljs-string">            &#x27;&#x27;&#x27;</span><br>            <span class="hljs-keyword">if</span> l1!=<span class="hljs-literal">None</span>:<br>                num1=l1.val<br>                l1=l1.<span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">else</span>:<br>                num1 = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> l2!=<span class="hljs-literal">None</span>:<br>                num2=l2.val<br>                l2=l2.<span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">else</span>:<br>                num2 = <span class="hljs-number">0</span><br>            <span class="hljs-comment"># 更新求和</span><br>            sum_result = num1 + num2 + flag<br>            <span class="hljs-comment"># 结果求余</span><br>            val = sum_result % <span class="hljs-number">10</span><br>            <span class="hljs-comment"># 更新flag，大于9设置为1，其他的则为0</span><br>            <span class="hljs-keyword">if</span> sum_result &gt; <span class="hljs-number">9</span>:<br>                flag = <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                flag = <span class="hljs-number">0</span><br>            <span class="hljs-comment"># 终止条件，l1为空，并且flag为0</span><br>            <span class="hljs-keyword">if</span> l1 == <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> l2==<span class="hljs-literal">None</span> <span class="hljs-keyword">and</span>  flag == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">return</span> ListNode(val)<br>            <span class="hljs-keyword">return</span> ListNode(val, addTwoNumbers2(l1, l2, flag))<br>        <span class="hljs-keyword">return</span> addTwoNumbers2(l1,l2,<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h3 id="0x03-T3-无重复字符的最长子串"><a href="#0x03-T3-无重复字符的最长子串" class="headerlink" title="0x03 T3 无重复字符的最长子串"></a>0x03 T3 无重复字符的最长子串</h3><p>标签：字符串，滑动窗口哈希表</p><p>难度：中等</p><blockquote><p> 给定一个字符串  s ，请你找出其中不含有重复字符的 <strong>最长子串</strong> 的长度 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lengthOfLongestSubstring</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>            <span class="hljs-comment"># 感觉比较简单</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> s:<span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        left = <span class="hljs-number">0</span><br>        lookup = <span class="hljs-built_in">set</span>()<br>        n = <span class="hljs-built_in">len</span>(s)<br>        max_len = <span class="hljs-number">0</span><br>        cur_len = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            cur_len += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> s[i] <span class="hljs-keyword">in</span> lookup:<br>                lookup.remove(s[left])<br>                left += <span class="hljs-number">1</span><br>                cur_len -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> cur_len &gt; max_len:max_len = cur_len<br>            lookup.add(s[i])<br>        <span class="hljs-keyword">return</span> max_len<br></code></pre></td></tr></table></figure><h3 id="0x04-T4-寻找两个正序数组的中位数"><a href="#0x04-T4-寻找两个正序数组的中位数" class="headerlink" title="0x04 T4 寻找两个正序数组的中位数"></a>0x04 T4 寻找两个正序数组的中位数</h3><p>标签：数组、二分查找、分治</p><p>难度：困难</p><blockquote><p> 给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的 中位数 。 算法的时间复杂度应该为 O(log (m+n)) 。  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">findMedianSortedArrays</span>(<span class="hljs-params">self, nums1, nums2</span>):<br>        <span class="hljs-comment"># even 需要找（m+n+1）/2</span><br>        <span class="hljs-comment"># odd 需要找（m+n）/2 （m+n）/2+1</span><br>        m=<span class="hljs-built_in">len</span>(nums1)<br>        n=<span class="hljs-built_in">len</span>(nums2)<br>        <span class="hljs-keyword">if</span> (m+n)%<span class="hljs-number">2</span> ==<span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># odd</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.getK(nums1,nums2,(m+n+<span class="hljs-number">1</span>)/<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># even</span><br>            a=<span class="hljs-variable language_">self</span>.getK(nums1,nums2,(m+n)/<span class="hljs-number">2</span>)<br>            b=<span class="hljs-variable language_">self</span>.getK(nums1,nums2,(m+n)/<span class="hljs-number">2</span>+<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">return</span> (a+b)/<span class="hljs-number">2</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">getK</span>(<span class="hljs-params">self,nums1,nums2,k</span>):<br>        <span class="hljs-comment"># nums1：第一个正序的数组</span><br>        <span class="hljs-comment"># nums2：第二正序的数组</span><br>        <span class="hljs-comment"># k：两个并列数组中第k大的数组</span><br>        <span class="hljs-comment"># 终止条件为，k为1</span><br>        k=<span class="hljs-built_in">int</span>(k)<br>        m=<span class="hljs-built_in">len</span>(nums1)<br>        n=<span class="hljs-built_in">len</span>(nums2)<br>        index1,index2=<span class="hljs-built_in">int</span>(<span class="hljs-built_in">min</span>(k//<span class="hljs-number">2</span>,m))-<span class="hljs-number">1</span>,<span class="hljs-built_in">int</span>(<span class="hljs-built_in">min</span>(k//<span class="hljs-number">2</span>,n))-<span class="hljs-number">1</span><br>        <span class="hljs-comment"># 两个正序数组中的值</span><br>        <span class="hljs-keyword">if</span> m==<span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 第一个数组到头</span><br>            <span class="hljs-keyword">return</span> nums2[k-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> n==<span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 说明第二个数组到头</span><br>            <span class="hljs-keyword">return</span> nums1[k-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> k==<span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">min</span>(nums1[<span class="hljs-number">0</span>],nums2[<span class="hljs-number">0</span>])<br>        <span class="hljs-comment"># 下面来看如何更新迭代</span><br>        value1=nums1[index1]<br>        value2=nums2[index2]<br>        <span class="hljs-keyword">if</span> value1 &lt;= value2:<br>            k=k-index1-<span class="hljs-number">1</span><br>            nums1=nums1[index1+<span class="hljs-number">1</span>:]<br>        <span class="hljs-keyword">else</span>:<br>            k=k-index2-<span class="hljs-number">1</span><br>            nums2=nums2[index2+<span class="hljs-number">1</span>:]<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.getK(nums1,nums2,k)<br></code></pre></td></tr></table></figure><h3 id="0x05-T5-最长回文子串"><a href="#0x05-T5-最长回文子串" class="headerlink" title="0x05 T5 最长回文子串"></a>0x05 T5 最长回文子串</h3><p>标签：字符串，动态规划</p><p>难度：中等</p><blockquote><p> 给你一个字符串  s ，找到 s 中最长的回文子串。 </p></blockquote><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">class</span> Solution:<br>    <span class="hljs-attribute">def</span> longestPalindrome(self, s: str) -&gt; str:<br>        <span class="hljs-comment">## dynamic programing ,基本步骤是确定下标的含义和递推公式的含义</span><br>        <span class="hljs-comment"># 第一步，确定dp数据，用dp【i,j】来表示区间范围内的淄川是否为回文子串dp[i][j]是否为true</span><br>        <span class="hljs-comment"># 第二步，确定递推的三种情况，回文数取真的情况分为</span><br>        <span class="hljs-comment"># 1. i=j相等，true</span><br>        <span class="hljs-comment"># 2. i和j相差等于1，true</span><br>        <span class="hljs-comment"># 3. i和j相差大于1的情况，如果i和j相等，需要查看i和j的区间内部是不是相等，也就是dp[i+1][j-1]是不是true</span><br>        <span class="hljs-comment"># 第三步，确定dp数组如何初始化，当然都是false</span><br>        <span class="hljs-comment"># 第四步，为了保证dp[i+1][j-1]要最新开始计算,所以可以先将第一种情况和第二种情况先得到</span><br><br>        <span class="hljs-attribute">dp</span>=[[<span class="hljs-number">0</span> for i in range(len(s))] for j in range(len(s))]<br>        <span class="hljs-attribute">left</span> = <span class="hljs-number">0</span><br>        <span class="hljs-attribute">right</span> = <span class="hljs-number">0</span><br>        <span class="hljs-attribute">maxlength</span>=<span class="hljs-number">0</span><br>        <span class="hljs-attribute">for</span> i in range(len(s)-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>):<br>            <span class="hljs-attribute">for</span> j in range(i, len(s)):<br>                <span class="hljs-attribute">if</span> s[i]==s[j]:<br>                    <span class="hljs-attribute">if</span> (j - i &lt;= <span class="hljs-number">1</span>):<br>                        <span class="hljs-attribute">dp</span>[i][j] = <span class="hljs-number">1</span><br>                    <span class="hljs-attribute">elif</span> dp[i + <span class="hljs-number">1</span>][j - <span class="hljs-number">1</span>] == <span class="hljs-number">1</span>:<br>                        <span class="hljs-attribute">dp</span>[i][j] = <span class="hljs-number">1</span><br>                <span class="hljs-attribute">if</span> dp[i][j] == <span class="hljs-number">1</span> and j - i + <span class="hljs-number">1</span> &gt; maxlength:<br>                    <span class="hljs-attribute">maxlength</span> = j - i + <span class="hljs-number">1</span><br>                    <span class="hljs-attribute">left</span> = i<br>                    <span class="hljs-attribute">right</span> = j<br>        <span class="hljs-attribute">return</span> s[left:right + <span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><h3 id="0x06-T10-正则表达式匹配"><a href="#0x06-T10-正则表达式匹配" class="headerlink" title="0x06 T10 正则表达式匹配"></a>0x06 T10 正则表达式匹配</h3><p>标签：字符串、递归、动态规划</p><p>难度：困难</p><blockquote><p> 给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 ‘.’ 和 ‘<em>‘ 的正则表达式匹配。 ‘.’ 匹配任意单个字符 ‘</em>‘ 匹配零个或多个前面的那一个元素 所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isMatch</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span>, p: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        m, n = <span class="hljs-built_in">len</span>(s), <span class="hljs-built_in">len</span>(p)<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">matches</span>(<span class="hljs-params">i: <span class="hljs-built_in">int</span>, j: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">if</span> p[j - <span class="hljs-number">1</span>] == <span class="hljs-string">&#x27;.&#x27;</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">return</span> s[i - <span class="hljs-number">1</span>] == p[j - <span class="hljs-number">1</span>]<br><br>        f = [[<span class="hljs-literal">False</span>] * (n + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m + <span class="hljs-number">1</span>)]<br>        f[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m + <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n + <span class="hljs-number">1</span>):<br>                <span class="hljs-keyword">if</span> p[j - <span class="hljs-number">1</span>] == <span class="hljs-string">&#x27;*&#x27;</span>:<br>                    f[i][j] |= f[i][j - <span class="hljs-number">2</span>]<br>                    <span class="hljs-keyword">if</span> matches(i, j - <span class="hljs-number">1</span>):<br>                        f[i][j] |= f[i - <span class="hljs-number">1</span>][j]<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">if</span> matches(i, j):<br>                        f[i][j] |= f[i - <span class="hljs-number">1</span>][j - <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">return</span> f[m][n]<br></code></pre></td></tr></table></figure><h3 id="0x07-T11-盛最多水的容器"><a href="#0x07-T11-盛最多水的容器" class="headerlink" title="0x07 T11 盛最多水的容器"></a>0x07 T11 盛最多水的容器</h3><p>标签：贪心、数组、双指针</p><p>难度：中等</p><blockquote><p> 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 说明：你不能倾斜容器。  </p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs sql">class Solution:<br>    def maxArea(self, height):<br>        # 找出其中的两条线，找出其中与x轴构成可以容纳最多的水<br>        <span class="hljs-keyword">left</span><span class="hljs-operator">=</span><span class="hljs-number">0</span><br>        <span class="hljs-keyword">right</span><span class="hljs-operator">=</span>len(height)<span class="hljs-number">-1</span><br>        <span class="hljs-keyword">result</span><span class="hljs-operator">=</span><span class="hljs-number">0</span><br>        while <span class="hljs-keyword">left</span> <span class="hljs-operator">&lt;</span> <span class="hljs-keyword">right</span>:<br>            <span class="hljs-keyword">result</span><span class="hljs-operator">=</span><span class="hljs-built_in">max</span>(<span class="hljs-keyword">result</span>,<span class="hljs-built_in">min</span>(height[<span class="hljs-keyword">right</span>],height[<span class="hljs-keyword">left</span>])<span class="hljs-operator">*</span>(<span class="hljs-keyword">right</span><span class="hljs-operator">-</span><span class="hljs-keyword">left</span>))<br>            # 更新<span class="hljs-keyword">right</span>和<span class="hljs-keyword">left</span><br>            # 提升的意义，在于删除小于当前值<br>            if height[<span class="hljs-keyword">right</span>]<span class="hljs-operator">&gt;</span>height[<span class="hljs-keyword">left</span>]:<br>                <span class="hljs-keyword">left</span><span class="hljs-operator">=</span><span class="hljs-keyword">left</span><span class="hljs-operator">+</span><span class="hljs-number">1</span>   <br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">right</span><span class="hljs-operator">=</span><span class="hljs-keyword">right</span><span class="hljs-number">-1</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">result</span><br></code></pre></td></tr></table></figure><h3 id="0x08-T15-三数之和"><a href="#0x08-T15-三数之和" class="headerlink" title="0x08 T15 三数之和"></a>0x08 T15 三数之和</h3><p>标签：数组、双指针、排序</p><p>难度：中等</p><blockquote><p> 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c &#x3D; 0 ？请你找出所有和为 0 且不重复的三元组。  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        hashtable=<span class="hljs-built_in">dict</span>()<br>        <span class="hljs-comment"># 新建一个哈希表，来记录每个数字它对应的补数</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>            <span class="hljs-keyword">if</span> target-nums[i] <span class="hljs-keyword">in</span> hashtable:<br>            <span class="hljs-comment"># 在这个哈希表中就OK了，而且这个时间复杂度并不高</span><br>                <span class="hljs-keyword">return</span> [hashtable[target-nums[i]],i]<br>            hashtable[nums[i]]=i<br>        <span class="hljs-keyword">return</span> []<br></code></pre></td></tr></table></figure><h3 id="0x09-T17电话号码的字符组合"><a href="#0x09-T17电话号码的字符组合" class="headerlink" title="0x09 T17电话号码的字符组合"></a>0x09 T17电话号码的字符组合</h3><p>标签：字符串、哈希表、回溯</p><p>难度：中等</p><blockquote><p> 给定一个仅包含数字  2-9 的字符串，返回所有它能表示的字母组合。答案可以按 <strong>任意顺序</strong> 返回。 </p></blockquote><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs prolog">class <span class="hljs-symbol">Solution</span>:<br>    def letterCombinations(self, digits: str) -&gt; <span class="hljs-symbol">List</span>[str]:<br>        if not digits: return []<br><br>        phone = &#123;<span class="hljs-string">&#x27;2&#x27;</span>:[<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>],<br>                 <span class="hljs-string">&#x27;3&#x27;</span>:[<span class="hljs-string">&#x27;d&#x27;</span>,<span class="hljs-string">&#x27;e&#x27;</span>,<span class="hljs-string">&#x27;f&#x27;</span>],<br>                 <span class="hljs-string">&#x27;4&#x27;</span>:[<span class="hljs-string">&#x27;g&#x27;</span>,<span class="hljs-string">&#x27;h&#x27;</span>,<span class="hljs-string">&#x27;i&#x27;</span>],<br>                 <span class="hljs-string">&#x27;5&#x27;</span>:[<span class="hljs-string">&#x27;j&#x27;</span>,<span class="hljs-string">&#x27;k&#x27;</span>,<span class="hljs-string">&#x27;l&#x27;</span>],<br>                 <span class="hljs-string">&#x27;6&#x27;</span>:[<span class="hljs-string">&#x27;m&#x27;</span>,<span class="hljs-string">&#x27;n&#x27;</span>,<span class="hljs-string">&#x27;o&#x27;</span>],<br>                 <span class="hljs-string">&#x27;7&#x27;</span>:[<span class="hljs-string">&#x27;p&#x27;</span>,<span class="hljs-string">&#x27;q&#x27;</span>,<span class="hljs-string">&#x27;r&#x27;</span>,<span class="hljs-string">&#x27;s&#x27;</span>],<br>                 <span class="hljs-string">&#x27;8&#x27;</span>:[<span class="hljs-string">&#x27;t&#x27;</span>,<span class="hljs-string">&#x27;u&#x27;</span>,<span class="hljs-string">&#x27;v&#x27;</span>],<br>                 <span class="hljs-string">&#x27;9&#x27;</span>:[<span class="hljs-string">&#x27;w&#x27;</span>,<span class="hljs-string">&#x27;x&#x27;</span>,<span class="hljs-string">&#x27;y&#x27;</span>,<span class="hljs-string">&#x27;z&#x27;</span>]&#125;<br><br>        def backtrack(conbination,nextdigit):<br>            if len(nextdigit) == <span class="hljs-number">0</span>:<br>                res.append(conbination)<br>            else:<br>                for letter in phone[nextdigit[<span class="hljs-number">0</span>]]:<br>                    backtrack(conbination + letter,nextdigit[<span class="hljs-number">1</span>:])<br><br>        res = []<br>        backtrack(<span class="hljs-string">&#x27;&#x27;</span>,digits)<br>        return res<br></code></pre></td></tr></table></figure><h3 id="0x10-T19-删除链表的倒数第N个节点"><a href="#0x10-T19-删除链表的倒数第N个节点" class="headerlink" title="0x10 T19 删除链表的倒数第N个节点"></a>0x10 T19 删除链表的倒数第N个节点</h3><p>标签：链表，双指针</p><p>难度：中等</p><blockquote><p> 给你一个链表，删除链表的倒数第  n 个结点，并且返回链表的头结点。 </p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeNthFromEnd</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, <span class="hljs-symbol">head:</span> <span class="hljs-title class_">ListNode</span>, <span class="hljs-symbol">n:</span> int</span>) -&gt; <span class="hljs-title class_">ListNode</span>:<br>        dummy = <span class="hljs-title class_">ListNode</span>(<span class="hljs-number">0</span>, head)<br>        first = head<br>        second = dummy<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n):<br>            first = first.<span class="hljs-keyword">next</span><br><br>        <span class="hljs-keyword">while</span> <span class="hljs-symbol">first:</span><br>            first = first.<span class="hljs-keyword">next</span><br>            second = second.<span class="hljs-keyword">next</span><br><br>        second.<span class="hljs-keyword">next</span> = second.<span class="hljs-keyword">next</span>.<span class="hljs-keyword">next</span><br>        <span class="hljs-keyword">return</span> dummy.<span class="hljs-keyword">next</span><br></code></pre></td></tr></table></figure><h3 id="0x11-T20-有效的括号"><a href="#0x11-T20-有效的括号" class="headerlink" title="0x11 T20 有效的括号"></a>0x11 T20 有效的括号</h3><p>标签：栈、字符串</p><p>难度：简单</p><blockquote><p> 给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串 s ，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isValid</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        stack = []<br><br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> s:<br>            <span class="hljs-keyword">if</span> item == <span class="hljs-string">&#x27;(&#x27;</span>:<br>                stack.append(<span class="hljs-string">&#x27;)&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> item == <span class="hljs-string">&#x27;[&#x27;</span>:<br>                stack.append(<span class="hljs-string">&#x27;]&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> item == <span class="hljs-string">&#x27;&#123;&#x27;</span>:<br>                stack.append(<span class="hljs-string">&#x27;&#125;&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> stack <span class="hljs-keyword">or</span> stack[-<span class="hljs-number">1</span>] != item:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">else</span>:<br>                stack.pop()<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> stack <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法设计 Vol3</title>
    <link href="/posts/d416a183.html"/>
    <url>/posts/d416a183.html</url>
    
    <content type="html"><![CDATA[<p>二叉树中三种遍历方式：前序遍历、后序遍历和中序遍历；在普通二叉搜索树中的搜索search、插入insert、删除remove中的算法效率或者说复杂度和树的深度有关。因此在树的等价交换和基本操作的基础上，提出了平衡二叉树（BBST，Balanced binary search tree）的概念，由此延伸得到AVL、伸展树（Splay tree）、B树、红黑树、KD-树的概念，并可以看到这些在实际生活中的应用。</p><h2 id="0x01-二叉树的遍历traversal"><a href="#0x01-二叉树的遍历traversal" class="headerlink" title="0x01 二叉树的遍历traversal"></a>0x01 二叉树的遍历traversal</h2><p>对二叉树的访问可以抽象为如下形式：按照某种约定的次序，对节点访问且仅一次。遍历之于二叉树的意义，同样在于为相关算法的实现提供通用的框架，此外这一过程等效于将半线性的树形结构转换为线性结构，但是二叉树不在属于线性结构，因此遍历过程更为复杂。</p><h3 id="1-1-递归式遍历-recursively-traversal"><a href="#1-1-递归式遍历-recursively-traversal" class="headerlink" title="1.1 递归式遍历 recursively traversal"></a>1.1 递归式遍历 recursively traversal</h3><p>二叉树本身并不具有天然的全局次序，故为实现遍历，首选需要在各节点与其孩子之间约定某种局部次序，从而间接定义全局次序。分为左（L，left）、右（R，right）、节点（R，root），因此分为VLR、LVR、LRV三中心选择，也就是先序遍历（preorder）、中序遍历（inorder）、后序遍历（postorder）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-e034dd6b3af2fa7a976adbb6b61ca50c_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>先序遍历（preorder）</li></ul><p>首先核对X是不是空集，若x为空则直接退出</p><p>若x非空，则按照先序遍历，优先访问根节点，然后访问左子树和右子树</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">travPre_R</span>(set,visit):<br>    <span class="hljs-built_in">if</span>(!x) return;<br>    <span class="hljs-built_in">visit</span>(x.data);<br>    <span class="hljs-built_in">travPre_R</span>(X.lc,visit)<br>    <span class="hljs-built_in">travPre_R</span>(x.rc,visit)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-acad6074bf8ffd1f320a402499d4aa4c_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>中序遍历（inorder）</li></ul><p>各节点在中序遍历序列中</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a22dbcfb4b387c3184bec913ea3f211c_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>后序遍历（postorder）</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5d9253a7a9325895531537614c476ca3_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>递归遍历算法和迭代遍历算法都需要渐进的线性时间，而且相对而言，前者更加简明；但是迭代算法的时间和空间复杂度的常系数相比较递归更小，同时从迭代遍历完成可以加深对相关过程和技巧的理解</p><h3 id="1-2-递归式遍历-recursively-traversal"><a href="#1-2-递归式遍历-recursively-traversal" class="headerlink" title="1.2 递归式遍历 recursively traversal"></a>1.2 递归式遍历 recursively traversal</h3><p>比较难，但可以加深理解</p><p>先序遍历可以分解为两段，沿着最左侧通路自上而下的访问各节点，以及自底向上遍历的对应右子树</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-953b2444f355cac74f23b59085acb090_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="1-3-一些算法的实现"><a href="#1-3-一些算法的实现" class="headerlink" title="1.3 一些算法的实现"></a>1.3 一些算法的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Definition for a binary tree node.</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TreeNode</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, val=<span class="hljs-number">0</span>, left=<span class="hljs-literal">None</span>, right=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-variable language_">self</span>.val = val<br>        <span class="hljs-variable language_">self</span>.left = left<br>        <span class="hljs-variable language_">self</span>.right = right<br><br><span class="hljs-comment"># 题目debug地址：https://leetcode-cn.com/problems/binary-tree-inorder-traversal/</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inorderTraversal</span>(<span class="hljs-params">self, root</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        题目描述：给定一个二叉树的根节点root，返回它的中序遍历</span><br><span class="hljs-string">        二叉树的遍历分为深度遍历和广度遍历</span><br><span class="hljs-string">        深度遍历分为前序、中序和后序三种遍历方法</span><br><span class="hljs-string">        - 前序，根节点、左子树、右子树</span><br><span class="hljs-string">        - 中序，左子树、根节点、右子树</span><br><span class="hljs-string">        - 后序，左子树、右子树、根节点</span><br><span class="hljs-string">        广度遍历就是常说的层次遍历的方法</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param root:</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        stack,ret=[],[]<br>        <span class="hljs-comment"># 设计一个栈作为中间变量</span><br>        <span class="hljs-comment"># ret为最终结果</span><br>        cur=root <span class="hljs-comment"># 初始化最初的值为cur</span><br>        <span class="hljs-keyword">while</span> stack <span class="hljs-keyword">or</span> cur:<br>            <span class="hljs-comment"># 当stack为空并且cur为空停止，也就是所有都遍历完，同时也不存在左子树</span><br>            <span class="hljs-keyword">if</span> cur:<br>                <span class="hljs-comment"># 如果当前节点非空，则取这个节点到stack中</span><br>                <span class="hljs-comment"># 将当前节点设置为节点的left node</span><br>                stack.append(cur)<br>                cur=cur.left<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># 如果stack非空，但是当前节点为空。说明遍历到最左边的</span><br>                <span class="hljs-comment"># 取当前节点的上一个</span><br>                cur=stack.pop()<br>                ret.append(cur.val)<br>                cur=cur.right<br>        <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><h2 id="0x02-二叉搜索树（Binary-Search-tree"><a href="#0x02-二叉搜索树（Binary-Search-tree" class="headerlink" title="0x02 二叉搜索树（Binary Search tree)"></a>0x02 二叉搜索树（Binary Search tree)</h2><p>二叉搜索树是满足顺序性的二叉树</p><blockquote><p> 任一节点r的左右子树中，所有的节点（若存在）均不大于（不小于）r 为回避边界情况，暂定假设所有节点不想等，因此可以简化为 任意节点r的左（右）子树中，所有的节点（若存在）均小于（大于）r </p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-cf8df58728c8ca6daf5e39e9a305684e_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>从本章开始，讨论的重点将逐步转向查找技术，实际上在之前的章节已经就此做过一些讨论，比如在vector和list等结果中，已经给出了对应的ADT结构，但是遗憾的是这种接口的效率无法令人满意。</p><p>比如vector向量模版，针对无序和有序向量的查找提供了find()和search（）接口。前者的实现策略是将目标对象与向量存放的对象逐个对比，硬刺需要O(n)时间，后者利用二分查找策略可以确保O(logN)时间内完成单次查找，但是一旦向量本身需要修改，无论是插入还是删除，在最坏情况下每次需要O（n）的时间。</p><p>所以对于线性结构来说，既要求对象集合的组成可以高效率地动态调整，同时也要求能够高效率查找，lInear data structure很难胜任，<strong>那么高效率的动态修改和高效率的静态查找能否同时兼顾，如有可能又应该采用什么样的数据结构？</strong></p><p>之后两章希望逐步了解其中的故事，涉及到的数据结构种类比较多，按照基本和高级两章分别进行讲解</p><blockquote><p> 本章首先介绍树式查找的总体构思、基本算法以及数据结构，通过对二分查找策略的抽象与推广来定义并实现二叉搜索树（Binary search tree)结构，虽然在最坏情况下渐进时间复杂度与之前并无实质性改变，但是这给出来一种基于半线性的树形结构。 之后提出理想平衡和适度平衡等概念，并相应引入和实现AVL树这种典型的平衡二叉搜索树（Balanced binary search tree)，借助精巧的平衡调整算法，AVL树可以保证即使在最坏情况下，单次动态修改和静态查找也可以在O(logN)的时间内完成，在之后的选择中给出balanced m-way search trees </p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4503e8914079cf613df6dd7a1b23740b_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="2-1-查找算法（Search）"><a href="#2-1-查找算法（Search）" class="headerlink" title="2.1 查找算法（Search）"></a>2.1 查找算法（Search）</h3><p>采用减而治之的思路与策略，执行的过程可以描述为</p><p>从树根出发，逐步缩小查找范围，直到发现目标或缩小到空树。节点的插入和删除操作，都需要首先调用查找算法，并根据查找结果确定后序的处理方式，因此这里引用方式传递子树根节点，为后续操作提供必要的信息</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b5608983626d2ddc209de34cfda228e8_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>效率：在二叉搜索树的每一层，查找算法至多访问一个节点，且只需常数时间，因此总体所需要时间应线性正比于查找路径的长度，或最终返回节点的深度，在最坏的情况下可以达到\omiga n的复杂度；由此我们得到汽水，若要控制单次查找在最坏情况下的运行时间，需要从二叉搜索树的高度入手，后面讨论的平衡二叉搜索树正是基于这个思路的改进</p><h3 id="2-2-插入算法（Insert）"><a href="#2-2-插入算法（Insert）" class="headerlink" title="2.2 插入算法（Insert）"></a>2.2 插入算法（Insert）</h3><p>一般在二叉搜索树中插入新节点e的过程，可以描述为函数insert(): 它的过程是首先调用search() 查找e，如果返回位置非空，则说明已有雷同节点从，插入操作失败，否则x必然是_hot节点的某一个空孩子，于是创建这个孩子并存入e，之后更新全树的规模记录，并调用updateHeightAbove（）来更新x和历代祖先的高度</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-eee0f2d5e3d18221f28f22575828bdd5_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><strong>效率</strong>： 主要小孩在对算法search()和updateHeightAbove()的调用，时间复杂度同样取决与新节点的深度，在最坏情况下不超过全树的高度</p><h3 id="2-3-删除算法（Remove）"><a href="#2-3-删除算法（Remove）" class="headerlink" title="2.3 删除算法（Remove）"></a>2.3 删除算法（Remove）</h3><p>从二叉搜索树中删除节点，首先需要调用BST::search（）来判断目标点是否的确存在树中，如果存在则需要返回其位置，方能对其进行具体实施删除操作，在删除的过程中分为单分支情况和双分支情况</p><p><strong>效率</strong>： 删除操作所需要的时间主要消耗在对search()、succ（）和updateHeightAbove（） 的调用，在树中的任何高度，它们都至多消耗O（1）时间，故总体的渐进时间复杂度也不会超过全树的高度</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-629e73c3dde09316fc07a5325b8b2d15_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="0x03-平衡二叉搜索树（Balanced-binary-search-tree）"><a href="#0x03-平衡二叉搜索树（Balanced-binary-search-tree）" class="headerlink" title="0x03 平衡二叉搜索树（Balanced binary search tree）"></a>0x03 平衡二叉搜索树（Balanced binary search tree）</h2><h3 id="3-1-树高和性能"><a href="#3-1-树高和性能" class="headerlink" title="3.1 树高和性能"></a>3.1 树高和性能</h3><p>根据对二叉搜索树的实现与分析，search()、insert()和remove（）等主要接口的运行时间，均线性正比于二叉搜索树的高度，在最坏情况下，二叉搜索树可能彻底退化为列表，此时查找效率甚至会降低O(n)，因此如果不能有效控制树高，实际性能比之前的向量和列表并没有明显的区别。</p><p>为了描述出现这种最坏的情况的概率，这里使用平均复杂度的概念来看二叉搜索树的性能，使用两种常见的随机统计口径来进行分析</p><ul><li>随机生成(randomly generated)，在这种情况下可以看出二叉搜索树的平均高度为\Omega(LogN)</li><li>随机组成(randomly composed)， 平均查找长度为 \Omega(\Sqrt(N))</li></ul><p><strong>对比两种情况，可以看出两种不同的统计口径中对于平衡二叉树的统计次数的重叠，在第一种平衡中会越是平衡的树越会被统计多次，因此相对而言，按照后面口径多得到的估计值更加可信。</strong></p><h3 id="3-2-理想平衡和适度平衡"><a href="#3-2-理想平衡和适度平衡" class="headerlink" title="3.2 理想平衡和适度平衡"></a>3.2 理想平衡和适度平衡</h3><p>在之前看对于二叉树（Binary tree）的基本操作，比如search()、insert（）、remove（）的性能主要取决与树的高度，因此在节点数目固定的情况下，应当尽可能的降低高度，也就是尽可能的让兄弟子树的高度彼此接近，让全树更加平衡，比如对于包含n节点的二叉树，最理想的情况是高度为log_2N,这种就是理想平衡的情况；但是适当放松要求可以让我们得到比较好的效果，也就是适度平衡。</p><p>比如将树高限制在渐进不超过O(logN),相比较严格的理想平衡二叉树会更加放松，这里介绍的AVL树、伸展树、红黑树、KD树等都属于适度平衡的类别，因此也可以归纳为平衡二叉搜索树（Balanced binary search tree）</p><p><strong>平衡二叉搜索树的适度平衡性就是通过对树中每一个局部增加某种限制条件来保证的，</strong></p><p>比如在红黑树中，从树根到叶节点的通路总是包含一样多的黑节点；</p><p>比如在AVL树中，兄弟节点的高度相差不过1</p><p>这种限制条件设定是非常精妙的，除了适度平衡性，还具有局部性</p><ol><li>经过单次动态修改操作后，至多之后O（logN）处局部不在满足限制条件</li><li>可以在O(logN)的时间内，让这O（logN）处局部乃至全树重新满足限制条件</li></ol><p>由此来让失去平衡的二叉搜索树，必然可以迅速转换称为一颗等价的平衡二叉搜索树，等价二叉搜索树之间的转换过程称为等价交换</p><h3 id="3-3-等价交换"><a href="#3-3-等价交换" class="headerlink" title="3.3 等价交换"></a>3.3 等价交换</h3><p>首先定义等价，也就是中序遍历相同的二叉树之间是等价的，这种特点可以概括为“上下可变，左右不乱”</p><p>虽然二叉搜索树可以转换为理想平衡的完全二叉树，但是这种转换也是需要时间的，如何实现这种局部失衡的调整的同时来保证修复的速度？</p><h3 id="3-4-旋转调整"><a href="#3-4-旋转调整" class="headerlink" title="3.4 旋转调整"></a>3.4 旋转调整</h3><p>最基本的修复手段就是通过围绕特定节点的选装来实现等价前提下的局部拓扑调整，也就是单旋和双旋，zig和zag</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a1e602f68196efaecacdfd80cc9e558e_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>zig和zag均属于局部操作，仅涉及常熟个节点及其之间的链接关系，故可以在常数时间内完成，在后面实现各种二叉搜索数平衡化算法是支撑性的基本操作。</p><h2 id="0x04-平衡二叉搜索树（BBST，Balanced-binary-search-tree）的总结"><a href="#0x04-平衡二叉搜索树（BBST，Balanced-binary-search-tree）的总结" class="headerlink" title="0x04 平衡二叉搜索树（BBST，Balanced binary search tree）的总结"></a>0x04 平衡二叉搜索树（BBST，Balanced binary search tree）的总结</h2><p>本来直接看AVL的，但是由于太难了，不如在此对这一类的平衡二叉搜索树做一个总结，防止以后概念的混淆，同时在学习这些BBST的时候要注意基本概念产生的树的深度和算法（基本操作insert()、remove()、search() 之间的关</p><ul><li>AVL树，平衡二叉树之一，应用相对其他数据结构比较少，windows对进程地址空间的管理用到了AVL；由G. M. Adelson-Velsky和E. M. Landis不1962年収明[36] ，并以他们名字的首字母命名</li><li>伸展树（Splay tree），按照“最常用者优先”的启发式策略，引入并实现伸展树，；由D. D. Sleator和R. E. Tarjan亍1985年发明</li><li>B、B-、B+树，平衡多路查找树（查找路径不只两个），主要用在文件系统以及数据库中做索引等</li><li>红黑树，广泛应用在C++STL中，比如map和set、Java的TreeMap</li><li>KD-树结构，在四叉树（quadtree）和八叉树（octree）的一般性推广，是在计算几何类问题的求解的模式</li></ul><h3 id="4-1-AVL树"><a href="#4-1-AVL树" class="headerlink" title="4.1 AVL树"></a>4.1 AVL树</h3><p>在二叉查找树中，任一节点对应的两棵子树的最大高度差为 1，这样的二叉查找树称为平衡二叉树</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ebb252aab56216a47855577065619060_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>特点：</p><p>尽管可以保证最坏情况下的单次操作速度，但需在节点中嵌入平衡因子等标识；更重要的是，删除操作之后的重平衡可能需做多 达(logn)次旋转，从而频繁地导致全树整体拓扑结构的大幅度变化。</p><p>保持树平衡的目的是可以控制查找、插入和删除在平均和最坏情况下的时间复杂度都是O(log n)，相比普通二叉树最坏情况的时间复杂度是 O(n) ，AVL树把最坏情况的复杂度控制在可接受范围，非常合适对算法执行时间敏感类的应用。</p><h3 id="4-2-伸展树（Splay-tree）"><a href="#4-2-伸展树（Splay-tree）" class="headerlink" title="4.2 伸展树（Splay tree）"></a>4.2 伸展树（Splay tree）</h3><p>1）刚刚被访问过的节点，极有可能在不久之后再次被访问到</p><p>2）将被访问的下一节点，极有可能就处于不久之前被访问过的某个节点的附近</p><p>特点：</p><p>伸展树实现简便、无需修改节点 结构、分摊复杂度低，但可惜最坏情况下的单次操作需要(n)时间，故难以适用于核电站、医 院等对可靠性和稳定性要求极高的场合。</p><h3 id="4-3-B、B-、B-树"><a href="#4-3-B、B-、B-树" class="headerlink" title="4.3 B、B-、B+树"></a>4.3 B、B-、B+树</h3><p>为此，需要充分利用磁盘之类外部存储器的另一特性：就时间成本而言，读取物理地址连续 的一千个字节，与读取单个字节几乎没有区别。既然外部存储器更适宜于批量式访问，不妨通过 时间成本相对极低的多次内存操作，来替代时间成本相对极高的单次外存操作。相应地，需要将 通常的二叉搜索树，改造为多路搜索树在中序遍历的意义下，这也是一种等价变换。</p><p>特点：</p><p>B树是所有节点的平衡因子均等于0的多路查找树（AVL树是平衡因子不大于 1 的二路查找树）。B 树节点可以保存多个数据，使得 B 树可以不用像 AVL 树那样为了保持平衡频繁的旋转节点。B树的多路的特性，降低了树的高度，所以B树相比于平衡二叉树显得矮胖很多。B树非常适合保存在磁盘中的数据读取，因为每次读取都会有一次磁盘IO，高度降低减少了磁盘IO的次数。</p><h3 id="4-4-红黑树"><a href="#4-4-红黑树" class="headerlink" title="4.4 红黑树"></a>4.4 红黑树</h3><p>为此首先需在AVL树“适度平衡”标准的基础上，进一步放宽条件。实际上，红黑树 所采用的“适度平衡”标准，可大致表述为：任一节点左、右子树的高度，相差不得超过两倍</p><p>特点：</p><p>而节点的路径长度决定着对节点的查询效率，这样我们确保了，最坏情况下的查找、插入、删除操作的时间复杂度不超过O(log n)，并且有较高的插入和删除效率。</p><h3 id="4-5-KD-树结构"><a href="#4-5-KD-树结构" class="headerlink" title="4.5 KD-树结构"></a>4.5 KD-树结构</h3><p>需要在理解多维查询的基础上查询</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5384ce42123e409bad5fc4f0d6a1e58b_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>循着上一节采用平衡二叉搜索树实现一维查询的构思，可以将待查询的二维点集组织为所谓 的kd-树（kd-tree）⑥结构。在任何的维度下，kd-树都是一棵递归定义的平衡二叉搜索树。</p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法设计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>操作系统导论</title>
    <link href="/posts/3d85131.html"/>
    <url>/posts/3d85131.html</url>
    
    <content type="html"><![CDATA[<p>希望跟着李老师来了解一下，嵌入式系统和微机原理没有了解的地方，来见证计computer是如何从计算机变成电脑的。可能需要看一下&lt;计算机组成原理&gt;,但是因为太下饭了，就看一下这个吧,掌握概念就好。Learning OS concepts by coding them!</p><h3 id="0x01-什么是操作系统"><a href="#0x01-什么是操作系统" class="headerlink" title="0x01 什么是操作系统"></a>0x01 什么是操作系统</h3><p>计算机是用来帮助人们解决一些实际问题的，它需要通过总线来连接输入输出设备、存储、CPU和内存等设备。其中最重要的CPU（Central Processing Unit，中央处理器）主要是解释计算机中的指令并处理数据的单元。</p><p>比如计算机在终端输入hello的过程</p><ol><li>cpu发出指令，取内存</li><li>内存取对应地址的内容</li><li>通过控制器、总线来传递到图形控制器</li></ol><p>操作系统（operation system）将计算机硬件设备管理，在这个基础上实现应用软件。基本的软件包括CPU、内存、终端、磁盘、文件；在之外的高级操作系统中添加不同计算机之前的使用。</p><p>我们需要学习</p><ol><li>从应用软件出发看操作系统，只能是仅仅停留在操作系统的接口</li><li>从应用软件出发进入到操作系统，理解其中命令的过程，理解量才能知道背后的功能并开发</li><li>从硬件出发设计并实现操作系统</li></ol><p>希望了解操作系统是如何运转的，八个实验</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-cf3082f7581af213634bbd3b33ada1ef_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="0x02-bootsect-s"><a href="#0x02-bootsect-s" class="headerlink" title="0x02 bootsect.s"></a>0x02 bootsect.s</h3><h3 id="2-1-计算机发展的历史"><a href="#2-1-计算机发展的历史" class="headerlink" title="2.1 计算机发展的历史"></a>2.1 计算机发展的历史</h3><ol><li>从白纸到图灵机，1936年，通过模拟人对计算过程得到计算模型 </li><li>从图灵机到通用图灵机的过程</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-96c7d9ee79026f191a52038c0271ba63_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ol><li>从通用图灵机到计算机（1946年，冯·洛依曼）</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-553efc6291e83e1d280d842dddb850b7_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="2-2-计算机开机的过程"><a href="#2-2-计算机开机的过程" class="headerlink" title="2.2 计算机开机的过程"></a>2.2 计算机开机的过程</h3><ol><li>计算机中开始电源的最初的IP是由硬件设计者决定，比如x86的开机CS&#x3D;0xffff，ip&#x3D;0x0000，以及寻址（Bios映射区），检查RAM、键盘、显示器和软硬磁盘，汇编中的每一条指令中有着操作</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d90354da3c2b3bad16e719a7a4d4afed_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ol><li>BIOS的引导扇区代码（bootsect.s）</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4b8d815850ae1c5e7d38b47966394fa6_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="0x03-setup-s-and-head-s"><a href="#0x03-setup-s-and-head-s" class="headerlink" title="0x03 setup.s and head.s"></a>0x03 setup.s and head.s</h3><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1fd5720a53de5564fe64c42c294cb76c_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>在上述bootsec.s运行完之后，可以是导入setup.s，它主要做了两件事情</p><ol><li>添加操作系统代码到内存中，并为了保证操作系统的完整性，移动到0地址,第一个代码是head.s</li><li>读取硬件信息并初始化，mem_init</li></ol><p>运行setup.s的过程中会短暂的定义GDT和IDT表格，之后系统会进行模式的转换，将16位模式来变为32位模式，提高对内存使用的权限</p><p>16位模式的寻址解析电路：CS&lt;&lt;4+ip  （称为实模式）</p><p>32位模式的寻址解析电路：根据CS查表+IP（称位保护模式）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-1cafe52eec8c51fdc889bea21fcd2571_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>在运行完head.s之后便可以孩子选哪个main.c,这里是根据栈来进行处理的，也就是内存中函数的记录和返回</p><h3 id="0x04-Interface"><a href="#0x04-Interface" class="headerlink" title="0x04 Interface"></a>0x04 Interface</h3><p>接口就是系统提供的一些可以调用的函数，因为接口是函数，函数是通过调用实现的，因此也称为系统调用</p><p>在具体到操作系统中，系统的接口需要一些统一的规范，最基本的是POSIX中的接口列表</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f7b4020b32b912a8eb47669af3581dce_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>操作系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pyomo 简单示例学习</title>
    <link href="/posts/450e6b54.html"/>
    <url>/posts/450e6b54.html</url>
    
    <content type="html"><![CDATA[<p>pyomo是Python中的一个建模求解语言 Pyomo(Python Optimization Modeling Objects):优化建模对象，支持复杂优化应用的数学模型的建立和分析,是一种功能齐全的高级编程语言，包含一组丰富的支持库；建模的过程是科学研究、工程和商业许多方面的基本过程，建模涉及系统或现实世界对象的简化表示的制定。可以调用cplex这样的求解器来进行计算。</p><p><a href="https://zhuanlan.zhihu.com/p/479835520">Cplex12.10–Linux–(Py3~Py38安装过程（附安装包）)</a></p><p>因此像Pyomo的建模工具可以用于：</p><ol><li>解释系统中出现的现象</li><li>预测系统的未来状态</li><li>识别系统中可能的最坏情况或者最低成本的极值点</li><li>分析权衡来给予决策者支持</li></ol><p>常见的求解器：</p><ul><li>CBC，开源求解器（COIN-OR）开发的线性规划求解器，性能不足</li><li>GLPK（GNU Linear Programmed Kit）是GNU维护一个线形规划工具包，对于求解大规模的额线性规划问题速度缓慢</li><li>CPLEX，是IBM开发的商业线性规划求解器，可以求解LP,QP,QCQP,SOCP等四类基本问题和对应的MIP，社区版仅支持低于1000变量的使用，教育版无限制，建模时间长，但是求解速度非常快</li><li>Gurobi，是CPLEX团队创始人重新创建的商业求解器，和cplex速度差别不大，相对来说比Cplex好一点</li></ul><p>Pyomo的学习网站</p><p><a href="https://pyomo.readthedocs.io/en/stable/installation.html">Installation - Pyomo 6.4.0 documentation</a></p><h3 id="0x01-Pyomo-overview"><a href="#0x01-Pyomo-overview" class="headerlink" title="0x01 Pyomo overview"></a>0x01 Pyomo overview</h3><h3 id="1-1-Mathematical-modeling-数学建模"><a href="#1-1-Mathematical-modeling-数学建模" class="headerlink" title="1.1 Mathematical modeling 数学建模"></a>1.1 Mathematical modeling 数学建模</h3><p>数学模型是用形式化的语言表示系统只是，以下数学概念是现代建模活动的核心</p><ol><li>变量variable，变量代表模型的未知或变化部分，（例如是否做出决定，或系统结果的特征），变量所取的值通常称为解，通常是优化过程的输出</li><li>参数parameters，参数表示为执行优化必须提供的数据，实际上在某些设置中data用来代替parameters</li><li>关系relations，这里是在定义模型的不同部分如何相互链接的方程、不等式或者其他的数学关系</li><li>目标goal，反应被建模系统的目标和功能</li></ol><p>在计算机求解的过程中可以分为建模和求解两个方面，计算性能的替僧是的数学模型的数值分析称为一项司空见惯的活动，但是如果没有建模语言，设置输入文件、执行求解器以及从求解器中输出中提取结果的过程即繁琐又容易出错，在发生错误时难以调试，在大规模的实际应用程序中这种困难更加复杂，此外优化软件包使用的格式很多，而许多优化器实际识别的格式很少，因此应用多个优化器求解来分析模型会带来额外的复杂性</p><p>在这个过程中pyomo希望拓展用于python的数学建模，并调用对应的优化求解器来辅助求解，在这个过程中其具有的特征有</p><ul><li>开源</li><li>可定制</li><li>求解器集成</li><li>具有非常robust的语言、广泛的文档、丰富的标准库集，对现代编程中类和函数的支持</li></ul><h3 id="1-2-Overview-of-modeling-components-and-processes建模组建和过程描述"><a href="#1-2-Overview-of-modeling-components-and-processes建模组建和过程描述" class="headerlink" title="1.2 Overview of modeling components and processes建模组建和过程描述"></a>1.2 Overview of modeling components and processes建模组建和过程描述</h3><p>简单建模过程的基本步骤</p><ol><li>创建模型并声明组建</li><li>实例化模型</li><li>应用求解器</li><li>询问求解器结果</li></ol><p>在整个过程中pyomo的建模组件定义来模型的不同方面，包括现代AML通常支持的建模组件，</p><p>sets, symbolic parameters, decision variables, objectives，constraints</p><p>在pyomo中定义相关的python类</p><ul><li>Set，用于定义模型实例的数据</li><li>Param，用于定义模型实例的参数数据</li><li>Var。模型中的决策变量</li><li>Objective，模型中最小化或最大化的目标函数</li><li>Constraint，对模型中的Var施加约束限制的表达式</li></ul><h3 id="0x02-Abstract-versus-concrete-models抽象模型和具体模型"><a href="#0x02-Abstract-versus-concrete-models抽象模型和具体模型" class="headerlink" title="0x02 Abstract versus concrete models抽象模型和具体模型"></a>0x02 Abstract versus concrete models抽象模型和具体模型</h3><p>使用类AbstractModel（）可以使用表示数据值的符号来定义数学模型，，例如下列表示一个线性程序，用于找到向量的最优值x，包括参数n和b，和参数向量a和c</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-386b10c9d37e5fdac8c32aae59460624_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>使用类ConcreteModel（）来建立具体模型</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4e46c5aada9960304314c7536a55b5d7_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>python程序可能更喜欢 编写具体模型，而其他的一些代数建模语言的用户可能更喜欢编写抽象模型</p><h3 id="2-1-Simple-models简单模型"><a href="#2-1-Simple-models简单模型" class="headerlink" title="2.1 Simple models简单模型"></a>2.1 Simple models简单模型</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> pyomo.environ as pyo<br><span class="hljs-attribute">model</span>=pyo.ConcreteModel()<br><span class="hljs-attribute">model</span>.x=pyo.Var([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],domain=pyo.NonNegativeReals)<br><span class="hljs-attribute">model</span>.Obj=pyo.Objective(expr=<span class="hljs-number">2</span>*model.x[<span class="hljs-number">1</span>]+<span class="hljs-number">3</span>*model.x[<span class="hljs-number">2</span>])<br><span class="hljs-attribute">model</span>.Constraint1=pyo.Constraint(expr=<span class="hljs-number">3</span>*model.x[<span class="hljs-number">1</span>]+<span class="hljs-number">4</span>*model.x[<span class="hljs-number">2</span>]&gt;=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>虽然规则函数也可以用于指定Constraint和Objective，但是在此示例中使用expr来仅在具体模型中可用的选项，来直接指定表达式</p><h3 id="2-2-Abstract-Models抽象模型"><a href="#2-2-Abstract-Models抽象模型" class="headerlink" title="2.2 Abstract Models抽象模型"></a>2.2 Abstract Models抽象模型</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> _future_ <span class="hljs-keyword">import</span> division<br><span class="hljs-keyword">import</span> pyomo.environ <span class="hljs-keyword">as</span> pyo<br><br>model=pyo.AbstractModel()<br><br>//  定义一些参数的集合<br>// <span class="hljs-keyword">within</span> 用于验证分配给参数数据值的选项的使用<br>model.m=pyo.Param(<span class="hljs-keyword">within</span>=pyo.NonNegativeReals)<br>model.n=pyo.Param(<span class="hljs-keyword">within</span>=pyo.NonNegativeReals)<br><br>// 使用range<span class="hljs-keyword">set</span>来声明集合是一个整数序列<br>Model.I=Pyo.Range<span class="hljs-keyword">Set</span>(<span class="hljs-number">1</span>,model.m)<br>Model.J=Pyo.Range<span class="hljs-keyword">Set</span>(<span class="hljs-number">1</span>,model.n)<br><br>// 当集合<span class="hljs-keyword">Set</span>作为Param的组件参数给出之后，表明该集合将索引参数<br>model.a=pyo.Param(model.I,model.J)<br>model.b=pro.Param(model.I)<br>model.c=pyo.Param(model.J)<br><br>//根据参数j定义变量，第二个参数表示Variable指定的参数域<br>model.x=pyo.Var(model.J,<span class="hljs-keyword">domain</span>=pyo.NonNegativeReals)<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">在抽象模型中，promo表达式通常通过def来定义函数给定目标和约束声明，该def为函数及其参数建立一个名称，当pyomo使用函数来获取objective或者constraint时候，总是将模型作为第一个参数输入</span><br><span class="hljs-comment">因此在pyomo中声明此类函数时，模型始终是第一个形式参数，如果需要请遵循其他参数</span><br><span class="hljs-comment">定义的过程中有，常见的</span><br><span class="hljs-comment">sum函数，来表示求和</span><br><span class="hljs-comment">summation（）表示两个参数在他们的索引的乘积之和，默认最小化，如果希望最大化需要加入senese=pyo.maximize</span><br><span class="hljs-comment">*/</span><br><br>//定义目标函数<br>def obj_experssion(m):<br>    <span class="hljs-keyword">return</span> pyo.summation(m.c,m.x)//??这里应该是矩阵相乘<br>model.OBJ=pyo.Objective(<span class="hljs-keyword">rule</span>=obj_experssion)//？？这里的参数输入为什么是<span class="hljs-keyword">rule</span><br><br>def ax_constraint_rule(m, i):<br>    # <span class="hljs-keyword">return</span> the expression <span class="hljs-keyword">for</span> the <span class="hljs-keyword">constraint</span> <span class="hljs-keyword">for</span> i<br>    <span class="hljs-keyword">return</span> sum(m.a[i,j] * m.x[j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> m.J) &gt;= m.b[i]<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">从抽象模型中可以看出，我们需要为每个值设置一个约束i从1到m</span><br><span class="hljs-comment">为参数化表达式i，我们将它作为形式参数包含在声明约束表达式的函数中，使用pyomo.constraint参数</span><br><span class="hljs-comment">因为我们模型具有许多相同形式的约束，并且我们创建来一个集合，model.I这些约束可以在其上被索引，因此这是约束声明的第一个参数，</span><br><span class="hljs-comment">下个参数给出来将用于生成约束表达式的规则，</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">这个约束声明model.I将创建一个由集合索引的约束列表，并且对于每个成员model.I都会调用函数ax-constraint-rule并传递给模型对象以及成员model.I</span><br><span class="hljs-comment">*/</span><br># the next <span class="hljs-type">line</span> creates one <span class="hljs-keyword">constraint</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">each</span> member <span class="hljs-keyword">of</span> the <span class="hljs-keyword">set</span> model.I<br>model.AxbConstraint = pyo.<span class="hljs-keyword">Constraint</span>(model.I, <span class="hljs-keyword">rule</span>=ax_constraint_rule)<br></code></pre></td></tr></table></figure><p>为了使用这个模型，必须给出参数值的数据，可以提供数据的文件，格式为dat</p><h3 id="0x03-Pyomo-model-setting"><a href="#0x03-Pyomo-model-setting" class="headerlink" title="0x03 Pyomo model setting"></a>0x03 Pyomo model setting</h3><ul><li>Any &#x3D; 所有可能的值</li><li>Reals &#x3D; 浮点值</li><li>PositiveReals &#x3D; 严格的正浮点值</li><li>NonPositiveReals &#x3D; 非正浮点值</li><li>NegativeReals &#x3D; 严格的负浮点值</li><li>NonNegativeReals &#x3D; 非负浮点值</li><li>PercentFraction &#x3D; 区间 [0,1] 中的浮点值</li><li>UnitInterval &#x3D; PercentFraction 的别名</li><li>Integers &#x3D; 整数值</li><li>PositiveIntegers &#x3D; 正整数值</li><li>NonPositiveIntegers &#x3D; 非正整数值</li><li>NegativeIntegers &#x3D; 负整数值</li><li>NonNegativeIntegers &#x3D; 非负整数值</li><li>Boolean 布尔值，可以表示为 False&#x2F;True、0&#x2F;1、’False’&#x2F;‘True’ 和 ‘F’&#x2F;‘T’</li><li>Binary &#x3D; 整数 {0, 1}</li></ul><h3 id="3-1-Sets"><a href="#3-1-Sets" class="headerlink" title="3.1 Sets"></a>3.1 Sets</h3><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs gams">pyo.Set()<br><span class="hljs-comment">* dimen,集合参数的维度</span><br><span class="hljs-comment">* doc，描述集合的字符串</span><br><span class="hljs-comment">* filter，在构造过程中使用布尔函数</span><br><span class="hljs-comment">* initialize，包含set的初始成员的可迭代对象</span><br><span class="hljs-comment">* ordered，一个bool来表示集合是有序的</span><br><span class="hljs-comment">* validate，验证新成员数据的bool</span><br><span class="hljs-comment">* within，用于验证的集合</span><br><br>pyo.rangeSet()<br>生成一个【<span class="hljs-built_in">min</span>,<span class="hljs-built_in">max</span>,step】<br></code></pre></td></tr></table></figure><h3 id="3-2-Parameters"><a href="#3-2-Parameters" class="headerlink" title="3.2 Parameters"></a>3.2 Parameters</h3><p>使用该词表示必须提供的数据，以便为决策变量找到最佳的分配</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">pyo.Param()<br><span class="hljs-bullet">* </span>default<br><span class="hljs-bullet">* </span>doc<br><span class="hljs-bullet">* </span>initialize 返回用于初始化参数值的数据函数<br><span class="hljs-bullet">* </span>mutable，bool来设置是否允许在param初始化后更改值<br><span class="hljs-bullet">* </span>validate，一个回调函数，接受模型、建议值和建议值的索引<br><span class="hljs-bullet">* </span>within，用于验证的集合，指定有效参数值的域<br></code></pre></td></tr></table></figure><h3 id="3-3-Varibles"><a href="#3-3-Varibles" class="headerlink" title="3.3 Varibles"></a>3.3 Varibles</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stata">pyo.<span class="hljs-keyword">Var</span>()<br><span class="hljs-comment">* bounds</span><br><span class="hljs-comment">* domain</span><br><span class="hljs-comment">* initialize,为变量提供初始值，或者提供初始索引值</span><br></code></pre></td></tr></table></figure><h3 id="3-4-Objectives"><a href="#3-4-Objectives" class="headerlink" title="3.4 Objectives"></a>3.4 Objectives</h3><p>返回试图最大化或最小化值的变量函数，可以构建一个默认参数位model的函数来作为输入</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stan">def ObjRule(<span class="hljs-title">model</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span>*<span class="hljs-title">model</span>.x[<span class="hljs-number">1</span>] + <span class="hljs-number">3</span>*<span class="hljs-title">model</span>.x[<span class="hljs-number">2</span>]<br><span class="hljs-title">model</span>.obj1 = pyo.Objective(rule=ObjRule)<br></code></pre></td></tr></table></figure><p>可以使用sense参数来最大化</p><h3 id="3-5-Constraints"><a href="#3-5-Constraints" class="headerlink" title="3.5 Constraints"></a>3.5 Constraints</h3><p>大多数约束是使用规则创建的等式或不等式指定表达的</p><p>约束可以通过列表或集合来索引，当声明中包含列表或集合作为参数，元素会迭代传递给规则函数</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stan"><span class="hljs-title">model</span>.A = RangeSet(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)<br><span class="hljs-title">model</span>.a = Param(<span class="hljs-title">model</span>.A, within=PositiveReals)<br><span class="hljs-title">model</span>.ToBuy = Var(<span class="hljs-title">model</span>.A)<br>def bud_rule(<span class="hljs-title">model</span>, i):<br>    <span class="hljs-keyword">return</span> <span class="hljs-title">model</span>.a[i]*<span class="hljs-title">model</span>.ToBuy[i] &lt;= i<br>aBudget = Constraint(<span class="hljs-title">model</span>.A, rule=bud_rule)<br></code></pre></td></tr></table></figure><h3 id="求解实例"><a href="#求解实例" class="headerlink" title="求解实例"></a>求解实例</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 学习promo中建模的基本过程</span><br><span class="hljs-comment"># min 2*x+3*y</span><br><span class="hljs-comment"># s.t. 3x+4y&gt;=1</span><br><span class="hljs-comment"># x,y&gt;=0</span><br><span class="hljs-comment"># 尝试构建模型，求解并得到最终的结果</span><br><span class="hljs-keyword">from</span> pyomo.environ import *<br>import pyomo.environ as pyo<br><br><span class="hljs-comment"># 建立实际模型</span><br><span class="hljs-attribute">model</span>=ConcreteModel()<br>model.<span class="hljs-attribute">x</span>=Var([1,2],<span class="hljs-attribute">domain</span>=pyo.NonNegativeReals)<br>model.<span class="hljs-attribute">Obj</span>=Objective(expr=2*model.x[1]+3*model.x[2])<br>model.<span class="hljs-attribute">Constraint1</span>=Constraint(expr=3*model.x[1]+4*model.x[2]&gt;=1)<br><br><span class="hljs-attribute">opt</span>=pyo.SolverFactory(&#x27;cplex&#x27;)<br>opt.solve(model)<br><span class="hljs-built_in">print</span>(pyo.value(model.x[1]))<br><span class="hljs-built_in">print</span>(pyo.value(model.x[2]))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>交通工程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>运筹优化</tag>
      
      <tag>pyomo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法设计 Vol2</title>
    <link href="/posts/a3119115.html"/>
    <url>/posts/a3119115.html</url>
    
    <content type="html"><![CDATA[<p>这里主要借鉴Tsinghua的2020fall&lt;数据结构与算法&gt;这本书，包含对于一些基本数据结构的探索，先从定义梳理，在整理题目，之后是实际训练。结合之前在程序设计或嵌入式中对于数据在实际存储的例子来学习可能会更好。从中也能体悟到算法一些含义</p><p>本次主要对图（Graph）的基本知识点，和一些实际应用的算法进行描述，但是在另外一方面，图也不仅仅存在于数据结构中，在运筹学中对图也有相关的描述，因此希望在这里整合两者之间的概念，并分别介绍两者的应用，挺有意思的。</p><h2 id="0x01-数据结构中的-图-Graph"><a href="#0x01-数据结构中的-图-Graph" class="headerlink" title="0x01 数据结构中的 图 Graph"></a>0x01 数据结构中的 图 Graph</h2><p>例如之前例子中的迷宫，借助绳索掌握迷宫内各通道之间的相互关系，在很多应用中我们需要准确有效描述和利用这类信息，这类信息往往可以表述为定义与一组对象之间的二元关系，比如城市交通图、比如互联网中的IP地址，尽管上一章的树 Tree结构也可以用来表示这种二元关系，但是仅限与Parent Node 和Child Node之间，这种一般性的二元关系属于图论 Graph Theory的范畴</p><p>本章的主要目的</p><ol><li>简要介绍图（Graph）的基本概念和术语</li><li>如何实现作为抽象数据类型（ADT）的图结构，主要讨论邻接矩阵（Adjacent matrix），和邻接表（Adjacent list）两种实现方式</li><li>从遍历（traverse）的角度介绍将图（Graph）转换为树（Tree）的典型方法，包括广度优先搜索（BFS）和深度优先搜索（DFS）</li><li>分别以拓扑排序和双连通域分解为例子，介绍利用基本数据结构并基于遍历模式，设计图算法的主要方法</li><li>（数据结构决定遍历次序）的观点出发，将遍历算法概括并统一为最佳优先遍历这一模式，如此来更加准确和深刻理解不同图算法之间的共性与联系，更可以学会通过选择和改进数据结构，高效设计并实现各种图算法</li></ol><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>图（Graph）定义为G&#x3D;（V，E）。集合V中的元素为顶点（Vertex），集合E中的元素分别对应于V总的某一对顶点（u,v），表示它们之间存在的某种关系，因此可以称为边（Edge），从计算需求的角度约定都是有限集，规模分别记为n和e</p><ul><li>无向图（undirected graph），若边(u,v)所对应顶点的u和v的次序无所谓，称作无向边（undirect edge）；</li><li>有向图（directed graph）。如果不对等，称为directed edge，如果从u指向v，则其中u是该边的origin或者尾顶点tail，v是该边的终点destination或头顶点head</li><li>如果包含有向边和无向边，可以称为混合图mixed graph</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7f8e5237259c925d67906127dc6440e1_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>度（Degree），对于任何一边e&#x3D;，可以称顶点u和v彼此邻接（adjacent），同时与边e彼此关联（incident）。在无向图中，与顶点v关联的边数称为v的度数（degree），记deg（v）；在有向图中，出边总数称为出度（out-degree）、入边总数称为（in-degree）</p><p>简单图（Simple Graph），指的是不包含任何自环的图（Self-loop），自环指的是联接与同一顶点之间的边，在某些场景中具有特定意义，但在简单图中不讨论。</p><p>通路（path）就是由m+1个顶点于m条边交替而成的一个序列，也就是说这些边依次首尾相连，其中沿途的边的总数m是通路的长度。注意通路中的边必须互异，但是顶点可能重复，当顶点都不相同称为简单通路（Simple path）</p><p>环路（cycle）就是在通路的基础上加上起止顶点相同，对于不包含任何环路的有向图称为有向无环图（directed acrylic graph，DAG），如果环路中除了起点和终点的顶点相同，其他都是一样的称为简单环路（Simple  cycle）。一些特殊的环路，欧拉环路（Eulerian tour）是图中各边一次且恰好一次的环路；对偶的，哈密尔顿环路（Hamiltonian tour）是经过图中各顶点一次且恰好一次的环路</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d55423bf03ac246770e7e9ec5859e078_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>带权网络（weighted network，G（V，E，wt（）））需要通过一个权值函数，为每一边e指定一个权重weight，wt（e）指定边e的权重</p><p>复杂度（complexity），在这个过程中n各顶点，最多有e&#x3D;O（n^2)来进行描述</p><h2 id="1-2-抽象数据类型的实现"><a href="#1-2-抽象数据类型的实现" class="headerlink" title="1.2 抽象数据类型的实现"></a>1.2 抽象数据类型的实现</h2><p>也就是图（Graph）操作的一些方法，一般常见的实际方法，具体实现过程略过（看不懂，暂时觉得没必要）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d0bf6d868d5fa0f8f947a5b88a4f6e23_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="1-3-邻接矩阵-Adjacent-matrix"><a href="#1-3-邻接矩阵-Adjacent-matrix" class="headerlink" title="1.3 邻接矩阵 Adjacent matrix"></a>1.3 邻接矩阵 Adjacent matrix</h2><p>邻接矩阵是图（Graph）的ADT（Abstract Data Type），使用方阵A[n][n] 表示由n顶点构成的图，其中的每个单元负责描述顶点之间可能存在的邻接（Adjacent）关系</p><p>对于无权图，存在和不存在使用0-1来表示</p><p>对于带权网络，此时可以将矩阵中各单元从bool转换为int或者float类型，对于不存在的边使用0或者∞来代替</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-73bcc73bb74fafe42c874d3035441960_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><strong>代码实现过程略过</strong></p><h3 id="1-3-1-时间性能"><a href="#1-3-1-时间性能" class="headerlink" title="1.3.1 时间性能"></a>1.3.1 时间性能</h3><p>各顶点编号可以直接转换为其Adjacent matrix中对应的Rank，从而使得ADT中所有的静态操作接口的时间只需要O（1）时间</p><p>另外边的静态和动态操作只需要O（1）的时间，代价是Adjacent matrix的空间冗余</p><p>但是这种方法并非完美无缺，不足点在于顶点的动态操作接口十分耗费时间，为来插入新的顶点，V[]中需要添加一个元素，同时边集向量E[][]也需要增加一行，且每行都需要添加一个元素，顶点的删除操作也是类似；在通常的算法中顶点的动态操作远少于其他操作，而且计入向量扩容的代价，单次操作的耗时不过O（n）</p><h3 id="1-3-2-空间性能"><a href="#1-3-2-空间性能" class="headerlink" title="1.3.2 空间性能"></a>1.3.2 空间性能</h3><p>对于无向图而言，邻接矩阵必须为对称阵，其中除自环以外的每条边都被重复存放来两次，因此有一半的单元都是冗余的，也就是O（n^2）</p><h2 id="1-4-邻接表（adjacent-list）"><a href="#1-4-邻接表（adjacent-list）" class="headerlink" title="1.4 邻接表（adjacent list）"></a>1.4 邻接表（adjacent list）</h2><p>在上述邻接矩阵（Adjacent matrix）的空间解释中可以看出其空间的仍存在改进的余地，在实际应用中处理的图所含有的边通常远远少于O（n^2)，比如在平面图之类的稀疏图（sparse graph）中，边数渐进不超过O（n），仅仅与顶点总数大致相当</p><p>为了完善这种静态空间管理策略所导致的问题，因此可以使用图结构的另外一种表示与实现方式，可以将邻接矩阵转换称邻接表的方式，分别记录每个顶点的关联边（或者等价的邻接顶点）来构成邻接表（adjacent list）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3a64490ff0bfe85b8c81dad6e011c5f3_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="1-4-1-时间复杂度分析"><a href="#1-4-1-时间复杂度分析" class="headerlink" title="1.4.1 时间复杂度分析"></a>1.4.1 时间复杂度分析</h3><p>复杂度分析，adjacent list中的列表等于顶点总数N，每条边在其中仅存放一次（for direct graph）或者两次（for undirected graph），因此空间总量为O(n+e),与图自身的规模相当，相比较adjacent matrix有很大的改进</p><p>在空间性能的改进，相对应的是时间性能的降低为代价，比如判断顶点v与顶点u的边是否存在，需要O(n)时间</p><p>与顶点相关的动态操作，顶点的插入操作可以在O（1）的时间内完成，顶点的删除操作需要遍历所有的adjacent list，需要O（e）时间</p><h3 id="1-4-2-空间复杂度分析"><a href="#1-4-2-空间复杂度分析" class="headerlink" title="1.4.2 空间复杂度分析"></a>1.4.2 空间复杂度分析</h3><p>adjacent list在访问单条边的效率并不算高，但是适合用循环的额方式，处理同一顶点的所有关联边比如为了枚举从顶点v出发的所有边，仅需要theta(1+outdegreee(v))而不是theta(n)的时间</p><h2 id="1-5-图（Graph）遍历算法概述–BFS和DFS"><a href="#1-5-图（Graph）遍历算法概述–BFS和DFS" class="headerlink" title="1.5 图（Graph）遍历算法概述–BFS和DFS"></a>1.5 图（Graph）遍历算法概述–BFS和DFS</h2><p>图算法中大部分成员的主体框架都归结于图的遍历，与树的遍历相似，图的遍历需要访问所有顶点一次且仅有一次，此外图遍历同时还需要访问所有的边一次且仅一次</p><p>同时无论采用何种策略和算法，图遍历都可以理解为<strong>将非线性结转化为半线性结构的过程，经遍历而确定的边类型中，最重要的一类就是所谓的树边，它们与所有顶点共同构成来原图的一颗支撑树，称为遍历树traversal tree</strong></p><p>图中顶点之间可能存在多条通路，为来避免对顶点重复访问，在遍历的过程中，通常还需要动态设置不同顶点的状态，并随着遍历的进程不断转换状态，直至最后的“访问完毕”，图的遍历更加强调对处于特定状态顶点的甄别与查找，因此也称为图搜索（Graph search）</p><p>深度优先搜索、广度优先搜索、最佳优先等基本而典型的图搜索，都可以在线性时间内完成，也就睡这些算法仅需要O（n+e）的时间来访问所有的顶点和边</p><p>各种图搜索之间的区别体现在边分类结果的不同，以及所得遍历树的结构差异，其决定因素在于搜索过程的每一步迭代，将依照何种策略来选取下一接受访问的顶点</p><h3 id="1-5-1-广度优先搜索-breadth-first-search-BFS-越早被访问到的顶点，其邻居被优先选用"><a href="#1-5-1-广度优先搜索-breadth-first-search-BFS-越早被访问到的顶点，其邻居被优先选用" class="headerlink" title="1.5.1  广度优先搜索 breadth-first-search BFS 越早被访问到的顶点，其邻居被优先选用"></a>1.5.1  广度优先搜索 breadth-first-search BFS 越早被访问到的顶点，其邻居被优先选用</h3><p>搜索的过程：反复从前沿集（frontier）中找到最早被访问到顶点v，若其邻居均已访问到，则将其逐出前沿集，否则随意选出一个尚未访问到的邻居，并将其加入到前沿集（frontier）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3e899cb521c1ac0d03a1b33c219ef61e_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>由于每一步迭代都有一个顶点被访问，因此最多迭代O（n），另一方面因为不会泄漏每个刚被访问顶点的任何邻居，因此对应无向图（undirected graph）必能覆盖s所属的连通分量（connected component），对于有向图必能覆盖以s为起点的可达分量（reachable component）</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs php"><span class="hljs-comment">// 图的广度优先搜索算法</span><br><span class="hljs-keyword">void</span> <span class="hljs-title class_">Graph</span>::<span class="hljs-title function_ invoke__">bfs</span>(<span class="hljs-keyword">int</span> s)&#123; <span class="hljs-comment">//assert 0,n</span><br><span class="hljs-title function_ invoke__">reset</span>();<br><span class="hljs-keyword">int</span> clock=<span class="hljs-number">0</span>;<br><span class="hljs-keyword">int</span> v=s; <span class="hljs-comment">//初始化</span><br><span class="hljs-keyword">do</span><br>    <span class="hljs-keyword">if</span> (UNDISCOVERED == <span class="hljs-title function_ invoke__">status</span>(v))<span class="hljs-comment">// 一旦遇到尚未发现的顶点</span><br>        <span class="hljs-title function_ invoke__">BFS</span>(v,clock);<br><span class="hljs-keyword">while</span> (s !=(v=(++v %n))<span class="hljs-comment">// 按照序号检查</span><br><br><span class="hljs-keyword">void</span> <span class="hljs-title class_">Graph</span>::<span class="hljs-title function_ invoke__">BFS</span>(<span class="hljs-keyword">int</span> v, <span class="hljs-keyword">int</span>&amp; clock)&#123;<br>    Queue Q;<span class="hljs-comment">//辅助队列</span><br>    <span class="hljs-title function_ invoke__">status</span>(V)=DISCOVERED;<br>    Q.<span class="hljs-title function_ invoke__">enqueue</span>(v);<span class="hljs-comment">//初始化起点</span><br>    <span class="hljs-keyword">while</span>(!Q.<span class="hljs-keyword">empty</span>())&#123;<br>        <span class="hljs-keyword">int</span> v=Q.<span class="hljs-title function_ invoke__">dequeue</span>();<br>        <span class="hljs-title function_ invoke__">dTime</span>(v)=++clock;<span class="hljs-comment">//取出队首顶点v</span><br>        <span class="hljs-keyword">for</span>(u=<span class="hljs-title function_ invoke__">firstNbr</span>(v);-<span class="hljs-number">1</span>&lt;u;u=<span class="hljs-title function_ invoke__">nextNbr</span>(v,u))<br>            <span class="hljs-keyword">if</span>(<span class="hljs-title function_ invoke__">status</span>(u)==UNDISCOVERED)&#123;<br>                <span class="hljs-title function_ invoke__">status</span>(u)=DSICOVERED;<br>                Q.<span class="hljs-title function_ invoke__">enqueue</span>(u);<span class="hljs-comment">//发现顶点</span><br>                <span class="hljs-title function_ invoke__">type</span>(v,u)=TREE;<br>                <span class="hljs-built_in">parent</span>(u)=v;<br>            &#125;<span class="hljs-keyword">else</span><br>                <span class="hljs-title function_ invoke__">type</span>(v,u)=CROSS;<br>    &#125;<br>    <span class="hljs-title function_ invoke__">status</span>(v)=VISITED<br>&#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><p>### 1.5.2 深度优先搜索 depth-first-search，DFS 优先选取最后一个被访问到的顶点的邻居</p><p>以顶点s为基点的DFS搜索，首先访问顶点s，再从s所有尚未访问到的邻居中任取其一，并以之为基点，递归地执行DFS搜索，故各顶点被访问到的次序，类似与树的先序遍历，而各顶点被访问完毕的次序，则类似于树的后序遍历</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-eca9e84f77339f9de049189d47825e0c_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="0x02-交通运筹学中的图"><a href="#0x02-交通运筹学中的图" class="headerlink" title="0x02 交通运筹学中的图"></a>0x02 交通运筹学中的图</h2><h2 id="2-1-交通中图的例子"><a href="#2-1-交通中图的例子" class="headerlink" title="2.1 交通中图的例子"></a>2.1 交通中图的例子</h2><ul><li>轨道交通中的图</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-09530d19000494ddc864f0f2cb72f574_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>18世纪哥尼斯堡城的，普雷格尔的两岸和河中两个岛之前有七座桥</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c2e79f28ed745cbf0dadb6161cc3e928_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>经典的收发问题</li></ul><h2 id="2-2-基本概念–无向图G"><a href="#2-2-基本概念–无向图G" class="headerlink" title="2.2 基本概念–无向图G"></a>2.2 基本概念–无向图G</h2><p>无向图，设 V 是一个有n顶点的非空集合：V&#x3D;{v1，v2，v3，…，vn}；E是一个有m条无向边的集合：E&#x3D;{e1，e2，..，em}，则称V和E这两个集合组合一个无向图，记做G&#x3D;（V，E）</p><p>基于无向图的G的结构特点，给出下列术语</p><ul><li>平行边，如果两条不同的边具有相同的端点，称为e和e‘是G的平行边</li><li>简单图，如果图G中没有平行边则为简单图（！！有点不一样）</li><li>完备图，若图G中任何两个顶点之间恰好有一条边相关联，则称图G是完备图</li><li>子图，G1&#x3D;（V1，E1）并且V1属于V、E1属于E</li><li>生成子图，G1是G的子图，并且V1&#x3D;V，则称G1是G的生成子图</li><li>导出子图，若非空E1属于E，则E1及包含的顶点时G的导出子图</li><li>链，无向图中一个由顶点和边交替而成的非空有限序列，Q&#x3D;v0e0v1e1v2，如果起始点相同是闭链，不想等位开链</li><li>初等链，若开链Q中诸顶点都不相同，则称Q为一条初等链</li><li>回路，若一个闭链除链第一个顶点和最后一个顶点相同之外，没有其他相同的顶点和相同的边则称为回路</li><li>连通图，若图G中任意两顶点u和v之间存在一条链，则称为图G为连通图。 否则被称为分离图</li><li>割边，若G为连通图，将G中边e取走后所得图为分离图，则称e为图G的割边</li></ul><h2 id="2-3-基本概念–有向图D"><a href="#2-3-基本概念–有向图D" class="headerlink" title="2.3 基本概念–有向图D"></a>2.3 基本概念–有向图D</h2><p>有向图，设 V 是一个有n顶点的非空集合：V&#x3D;{v1，v2，v3，…，vn}；E是一个有m条有向边的集合：E&#x3D;{e1，e2，..，em}，则称V和E这两个集合组合一个有向图，记做D&#x3D;（V，E）</p><p>类似的有向图D也有术语</p><ul><li>平行边，不同有向边的e与e‘的起点与终点相同</li><li>孤立点，V中不与E任一条边关联的点称为D的孤立点</li><li>简单图，无平行边的称为简单图</li><li>完备图，图中任何两个顶点，存在有向边（u，v）和（v，u），则该有向图D为完备图</li><li>基本图，将有向图D的每条边除定向就得到一个相应的无向图G，则G位D的基本图</li><li>子图，同上</li><li>导出子图，同上</li><li>导出生成子图，同上</li><li>同构图，如下图，顶点和边是相等的，由于同构图被认为是相同的，这就给我们在网络规划中建立网络模型带来许多方便，当我们使用几何图来构建网络模型时，点的位置可以任意布置。边的长短曲直可以任意，因此需要尽量设计这种反应问题清晰、简练的几何图</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3c5062b5717ce3b3a4f34da227125fcc_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>链，若Q是有向图D的基本图G中的一条链，则Q为D的一条链</li><li>初等链，若Q是有向图D的基本图G的一条初等链，则Q被称为D的一条初等链</li><li>路，若Q是有向图D的基本图G中一条链</li><li>路径</li><li>回路，第一个顶点和最后一个顶点相同</li></ul><h2 id="2-4-基本概念–矩阵表示"><a href="#2-4-基本概念–矩阵表示" class="headerlink" title="2.4 基本概念–矩阵表示"></a>2.4 基本概念–矩阵表示</h2><p>关联矩阵，行表示顶点，列表示边，使用1表示是否链接</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7a527be03926e147f04d538ca5bdd4ab_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>邻接矩阵，行表示顶点，列表示顶点，使用数目表示两个顶点之间的链接数量</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9276e9a3f616942d0d5033def810d71e_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="2-5-树"><a href="#2-5-树" class="headerlink" title="2.5 树"></a>2.5 树</h2><p>不做赘述</p><h2 id="0x03-数据结构中图的应用"><a href="#0x03-数据结构中图的应用" class="headerlink" title="0x03 数据结构中图的应用"></a>0x03 数据结构中图的应用</h2><h2 id="3-1-拓扑排序（topological-sorting）"><a href="#3-1-拓扑排序（topological-sorting）" class="headerlink" title="3.1 拓扑排序（topological sorting）"></a>3.1 拓扑排序（topological sorting）</h2><p>以教材的编写实际问题为例子，作者可以借助有向图的结构整理出相关知识点之间的依赖关系，因向量是散列表和查找表的基础知识点，等等得到一份有向图，在这个基础上如何将这个有向图（undirected graph）来得到具有前端的顺序的额拓扑排序（topological sorting）</p><p>相容：每一个顶点都不会通过边，指向其在此序列中的前驱顶点，这样的一个线性序列，称作原图的拓扑序列</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f6f5a16c8a9aade41e700382fc170cc8_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>针对有向无环图（directed noloop graph），问题在于topological sorting是否必然存在，如果存在是否唯一？</p><p>同理，有限偏序集中也必然存在极小元素（同样，未必唯一）。该元素作为顶点，出度必然 为零比如图6.10(b)中的顶点D和F。而在对有向无环图的DFS搜索中，首先因访问完成而转 换至VISITED状态的顶点m，也必然具有这一性质；反之亦然。</p><p>166</p><p>进一步地，根据DFS搜索的特性，顶点m（及其关联边）对此后的搜索过程将不起任何作用。 于是，下一转换至VISITED状态的顶点可等效地理解为是，从图中剔除顶点m（及其关联边）之 后的出度为零者在拓扑排序中，该顶点应为顶点m的前驱。由此可见，DFS搜索过程中各顶 点被标记为VISITED的次序，恰好（按逆序）给出了原图的一个拓扑排序。</p><p>此外，DFS搜索善于检测环路的特性，恰好可以用来判别输入是否为有向无环图。具体地， 搜索过程中一旦发现后向边，即可终止算法并报告“因非DAG而无法拓扑排序”。</p><h2 id="3-2-双连通域分解"><a href="#3-2-双连通域分解" class="headerlink" title="3.2 双连通域分解"></a>3.2 双连通域分解</h2><p>考查无向图G。若删除顶点v后G所包含的连通域增多，则v称作切割节点（cut vertex）或 关节点（articulation point）。如图6.13中的C即是一个关节点它的删除将导致连通域 增加两块。反之，不含任何关节点的图称作双连通图。任一无向图都可视作由若干个极大的双连 通子图组合而成，这样的每一子图都称作原图的一个双连通域（bi-connected component）。 例如图6.14(a)中的无向图，可分解为如图(b)所示的三个双连通域。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-79e89290433cfffdbb4064dd9cbe4e39_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="3-3-优先级搜索"><a href="#3-3-优先级搜索" class="headerlink" title="3.3 优先级搜索"></a>3.3 优先级搜索</h2><p>上面的图搜索应用虽然各有特点，但是其基本框架缺基本相似，总体而言都是通过迭代逐步发现各顶点，将其纳入遍历树中并做相应处理，同时根据应用问题的需求适当给出解答，各算法在功能上的差异，主要体现在每一步迭代中对新顶点的选取策略不同，比如BFS搜索会优先考察更早被发现的顶点，而DFS会更侧重于最后被发现的顶点</p><p>总结上述策略，可以看出其本质上是给所有顶点赋予不同的优先级，随着算法的推进不断调整，而每一步迭代所选取的顶点都是当时的优先级最高者，按照这样的理解，可以将BFS和DFS纳入统一的框架，称为PFS（priority-first search）</p><p>在实际应用中，引导优化方向的指标，往往对应于某种有限的资源或成本（比如通讯带宽、机票价格等）因此这里不妨约定优先级越大顶点的优先级越低</p><p>通过最小支撑树和最短路径这两个经典的图算法深入介绍这个框架的具体应用，在复杂度的分析中，PFS由两层循环构成，其中内层循环又含并列的两个循环，采用邻接表的方式，总体的复杂度为O（n^2)</p><h2 id="3-4-最小支撑树"><a href="#3-4-最小支撑树" class="headerlink" title="3.4 最小支撑树"></a>3.4 最小支撑树</h2><p>连通图G的某一无环连通子树T若覆盖G中所有的顶点，则称为G的一颗支撑树或生成树（spanning tree）</p><p>就保留原图中边的数目而言，支撑树是禁止环路前提下的极大子图，也是保持连通前提下的最小子图，其往往对应系统中最经济的连接方案</p><h3 id="3-4-1-蛮力算法"><a href="#3-4-1-蛮力算法" class="headerlink" title="3.4.1 蛮力算法"></a>3.4.1 蛮力算法</h3><h3 id="3-4-2-prim算法（属于PFS的特例，时间复杂度为O（n-2）"><a href="#3-4-2-prim算法（属于PFS的特例，时间复杂度为O（n-2）" class="headerlink" title="3.4.2 prim算法（属于PFS的特例，时间复杂度为O（n^ 2）"></a>3.4.2 prim算法（属于PFS的特例，时间复杂度为O（n^ 2）</h3><h2 id="3-5-最短路径树"><a href="#3-5-最短路径树" class="headerlink" title="3.5  最短路径树"></a>3.5  最短路径树</h2><p>可以看下面问题描述</p><p>Dijkstra算法（时间复杂度O(n^2)</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20220423203235868.png" alt="image-20220423203235868"></p><h2 id="0x04-交通运筹学中的图的应用"><a href="#0x04-交通运筹学中的图的应用" class="headerlink" title="0x04 交通运筹学中的图的应用"></a>0x04 交通运筹学中的图的应用</h2><h2 id="4-1-最短路径问题"><a href="#4-1-最短路径问题" class="headerlink" title="4.1 最短路径问题"></a>4.1 最短路径问题</h2><p>在生产实践、运输管理和工程时间的很多活动中都需要面临寻找“图的最短路径”，这是网络规划中的一个基本问题</p><p>假设对于有向图D&#x3D;（V，E），设图D的每条边(u,v)都和一个实数权重W（e）&#x3D;W（u，v）对应，则称为赋权图。这里所说的权是与边有关的数量指标，可以根据问题的需要不同的含义，比如距离、时间和费用等</p><p>$W（P）&#x3D;\Sigma W(e)$</p><p>常见的解决方法</p><h3 id="4-1-1-Dijkstra算法-略"><a href="#4-1-1-Dijkstra算法-略" class="headerlink" title="4.1.1 Dijkstra算法(略)"></a>4.1.1 Dijkstra算法(略)</h3><h3 id="4-1-2-Bellman-Ford算法（略"><a href="#4-1-2-Bellman-Ford算法（略" class="headerlink" title="4.1.2 Bellman-Ford算法（略"></a>4.1.2 Bellman-Ford算法（略</h3><h2 id="4-2-最长路径问题"><a href="#4-2-最长路径问题" class="headerlink" title="4.2 最长路径问题"></a>4.2 最长路径问题</h2><h2 id="4-3-第k短路径问题"><a href="#4-3-第k短路径问题" class="headerlink" title="4.3 第k短路径问题"></a>4.3 第k短路径问题</h2><p>在最短路径的基础上，考虑第k路径感兴趣</p><h2 id="4-4-最小生成树"><a href="#4-4-最小生成树" class="headerlink" title="4.4 最小生成树"></a>4.4 最小生成树</h2><p>给定连通赋权无向图G&#x3D;（V，E），若T为G的生成树，T中边e的权</p><p>$W(T)&#x3D;\Sigma W(e)$</p><p>其中最小的称为最小生成树，一些常见的算法</p><h3 id="4-4-1-Prim算法（略）"><a href="#4-4-1-Prim算法（略）" class="headerlink" title="4.4.1 Prim算法（略）"></a>4.4.1 Prim算法（略）</h3><h3 id="4-4-2-Kruskal-算法（略）"><a href="#4-4-2-Kruskal-算法（略）" class="headerlink" title="4.4.2 Kruskal 算法（略）"></a>4.4.2 Kruskal 算法（略）</h3><h2 id="4-5-中国邮路问题"><a href="#4-5-中国邮路问题" class="headerlink" title="4.5 中国邮路问题"></a>4.5 中国邮路问题</h2><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20220423194702621.png" alt="image-20220423194702621"></p><h2 id="4-6-运输网络"><a href="#4-6-运输网络" class="headerlink" title="4.6 运输网络"></a>4.6 运输网络</h2><p>许多系统中存在流的问题，运输系统有物资流、公交系统中有车辆流、供水系统中存在水流，因此可以用图来描述网络，因此给定一些数学定义</p><ul><li>运输网络</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/image-20220423195949264.png" alt="image-20220423195949264"></p><ul><li>网络流</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8066d8f9dfd90a17d86d80091faabaf5_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>割</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-dc151c272ee97c94f688589124d836f0_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>最小割</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-535005f5498a0f3c725c00cddfe32c1c_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>最大流</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0d50fecaa4587ff1399b17cc5424f56b_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="4-7-最大流（4-6）"><a href="#4-7-最大流（4-6）" class="headerlink" title="4.7 最大流（4.6）"></a>4.7 最大流（4.6）</h2><h2 id="4-8-最小代价流问题（4-6）"><a href="#4-8-最小代价流问题（4-6）" class="headerlink" title="4.8 最小代价流问题（4.6）"></a>4.8 最小代价流问题（4.6）</h2>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图 graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法设计 Vol1</title>
    <link href="/posts/3a18c0af.html"/>
    <url>/posts/3a18c0af.html</url>
    
    <content type="html"><![CDATA[<p>这里主要借鉴Tsinghua的2020fall&lt;数据结构与算法&gt;这本书，包含对于一些基本数据结构的探索，先从定义梳理，在整理题目，之后是实际训练。结合之前在程序设计或嵌入式中对于数据在实际存储的例子来学习可能会更好。从中也能体悟到算法一些含义</p><p>本次主要对绪论、线性结构的数组（Array）为基础的向量（Vector）、以链表（Link）为基础的列表（List）、栈（Stack）与队列（Queue）；半线性结构的数（Tree）、非线形结构的图（Graph）做定义的记录和梳理。</p><h2 id="Chapter01-绪论"><a href="#Chapter01-绪论" class="headerlink" title="Chapter01 绪论"></a>Chapter01 绪论</h2><h2 id="1-1-瞎说"><a href="#1-1-瞎说" class="headerlink" title="1.1 瞎说"></a>1.1 瞎说</h2><p>计算机是人类从事计算的工具，也是抽象计算模型的具体物化，基于图灵模型的现代计算机，即是人类现代文明的标志与基础，更是人脑思维的拓展与延伸。尽管计算机的性能日益提高，但是这种能力在解决实际应用问题能够真正得到发挥，决定性的因素在于人类如何深入思考与分析获得对问题本质的透彻理解，</p><ul><li>需要按照长期积淀的框架和模式来设计出合乎问题内在规律的算法，</li><li>选用、改进或者定制足以支撑算法高效实现的数据结构，</li><li>并在真实的应用环境中充分测试、调试和改进</li></ul><p>那么什么是算法？</p><ul><li>埃及人的绳索是算法</li><li>尺规作图是算法</li><li>冒泡排序bubble sort也是算法</li></ul><p>宏观的定义是<strong>基于特定的计算模型，在解决某一信息处理问题而设计的一个指令序列，本书所说的算法还需要具有以下的特点</strong></p><ul><li>有input和output</li><li>可操作、确定性与可行性</li><li>有穷性和正确性，finiteness and correctness</li><li>退化与鲁棒性，degeneracy and robustness</li><li>重用性，reuse</li></ul><p>如何评价一个算法呢？</p><ul><li>可计算性computability</li><li>难解性 intractability，本书将更多的关注于非“不可解和难解”的一般性问题，并讨论如何效率的解决这一层面的计算问题，因此需要<strong>确定一种尺度，从时间和空间方面度量算法的计算成本，并由此对不同算法进行比较和评判，当然最重要的是在研究和归纳算法设计月实现过程中的一般性规律与技巧，以编写出效率更高、能够处理更大规模数据的程序，这是本书的基本主题也是贯穿主题的脉络</strong></li><li>计算效率</li><li>数据结构</li></ul><h2 id="1-2-复杂度complexity度量"><a href="#1-2-复杂度complexity度量" class="headerlink" title="1.2 复杂度complexity度量"></a>1.2 复杂度complexity度量</h2><p>   时间复杂度（Time complexity），运行时间是由多种综合因素作用而决定的，因为对于同种算法，对于不同的输入所需的运行时间并不相同，比如对于排序问题，输入序列的规模其中各元素的元素以及次序均不确定，这些因素都将影响到排序算法最终的运行时间，因此<strong>为来运行时间建立一种可行、可信的评估标准，需要考虑其中的关键因素 因此可以将这个问题转化成为：随着输入模式的扩大，算法的执行时间将如何增长？执行时间的这一变化趋势可表示为输入模式的一个函数，可以讲起称为时间复杂度time complexity，对于特定算法处理规模为n的问题所需要的时间可以作T(n)</strong>     渐进复杂度（asymptotic analysis），一些算法适用于小规模输入，一些算法适用于大规模输入，在评价算法运行效率时，我们往往可以忽略其处理小规模问题的差异，转而关注在处理更大规模问题时的表现，在其中我们可以尝试用一些方法，1. 大O记号 2 大Omega记号 3 大Theta记号  </p><ol><li>渐进上界 ，也就是最差情况big-O notation，对于规模为n的任意输入，算法的运行时间都不高于O（f（n））</li><li>渐进下界，也就是最好情况big-Omega-Notation，对于规模为n的任意输入，算法的运行时间都不低于Omega（g（n））</li><li>如果恰好渐进上界和渐进下界之间先等，O&#x3D;Omega，因此可以使用big-theta-notation做记号</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-863fa8d7378424e742ba82d67f047f0f_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>   空间复杂度（space complexity），为了更加客观地评价算法性能的优劣，除非特别申明，空间复杂度通常并不原始输入本身所占用的空间，对于同一问题，这一指标对任何算法都是相同的，反之其他的各种方面所小孩的空间都应该计算  </p><p>另外在很多时候我们都是更好甚至仅仅关注于算法的时间复杂度，而不必对空间复杂度多专门的考察，这种简单评测方式的一句都来自于一下事实：就渐进复杂度的意义而言，在任一算法的任何一次运行过程中所消耗的存储空间都不会多余期间所执行基本操作的累计次数；但是空间复杂度的分析的意义在于对空间效率非常在乎的应用场合中，当问题的输入规模极为庞大时，由时间复杂度所确立的平凡上界已经难以令人满意</p><h2 id="1-3-复杂度分析"><a href="#1-3-复杂度分析" class="headerlink" title="1.3 复杂度分析"></a>1.3 复杂度分析</h2><p>在1.2 中确定算法复杂度的度量标准，分析算法的复杂度，可一般使用大O算法，来将各种算法的复杂度从低到高划分为不同的级别</p><ul><li>常数O（1）</li><li>对数O(log N)</li><li>线性 O（n）</li><li>多项式 O（polynomial（n））</li><li>指数O（2^n）</li></ul><h2 id="1-4-递归recursive"><a href="#1-4-递归recursive" class="headerlink" title="1.4 递归recursive"></a>1.4 递归recursive</h2><p>递归的价值在于，许多应用问题都可以简介而准确地描述为递归形式，同时也是一种基本而典型的算法设计模型，这模式可以对实际问题中反复出现的结构和形式loop variant进行高度概括，并从本质层面加以描述与刻画，进而导出高效的算法</p><p>本篇文章从递归的基本模式入手，循序渐进介绍如何选择和应用（线性递归、二分递归和多分支递归等）不同的递归形式，来实现（遍历、分治等）算法策略，以及如何利用递归跟踪和递推方程等方法分析递归算法的复杂度</p><ul><li>线性递归，base case of recursion，避免因无限递归导致系统溢出，往往对应减而治之decrease and conquer的算法策略，<strong>递归每深入一层，待求解问题的规模都缩减一个常熟，直至最终蜕化为平凡的简单问题</strong></li><li>递归分析，可以使用递归跟踪与递归方程两种主要的方法</li><li>算法的每一个递归实例都表示为一个方框，其中著名该实例调用的参数</li><li>若实例M调用实例N，则在M与N对应的方框之间添加一条有向联线</li><li>递归模式</li><li>递归消除</li><li>二分递归 divide-and-conquer</li></ul><h2 id="Chapter02-（Static）向量vector"><a href="#Chapter02-（Static）向量vector" class="headerlink" title="Chapter02 （Static）向量vector"></a>Chapter02 （Static）向量vector</h2><p>数据结构是数据项的结构化集合，其结构表现为数据项之间的相互系列与作用，也可以理解为定义与数据项之间的某种逻辑次序，根据这种逻辑次序的复杂程度，可以将数据结构划分为线性结构、半线性结构和非线性结构三个部分</p><p>在线性结构中，数据项按照一个线性次数构成一个整体，其中最基本的线性结构为sequence，根据数据项的逻辑次序和物理存储地址的不同可以区分为向量vector和列表list</p><p>本章的讲解将围绕向量结构的高效实现而逐步展开，包括作为抽象数据类型的接口规范以及对应的额算法，尤其是高效维护动态向量的技巧</p><h2 id="2-1-从数组和向量"><a href="#2-1-从数组和向量" class="headerlink" title="2.1 从数组和向量"></a>2.1 从数组和向量</h2><p>   数组 Array  </p><p>若集合S由n个元素组成，且各元素之间具有一个线性次序，则可以将他们存在在开始地址A、物理位置连续的一段存储空间，</p><p>$A&#x3D;[a_0,a_1,…,a_{n-1}]$</p><p>q其中i小于j，A[i]都是A[j]的predecessor，后者是successor，相邻的成为immediate predecessor；immediate successor，元素的所有前去构成前缀prefix，所有的后继构成suffix</p><p>采用这样的元素规范，不仅使得每个元素都通过下标唯一指代，而且可以让我们直接访问任何元素，这里所说的访问包括 读取和修改等基本操作，“直接”的意思是这些操作可以在常数时间内完成</p><p>在其中元素的物理地址与其下标之间满足这种线性关系，因此被称作Linear Array</p><p>   向量Vector  </p><p>对数组array基础上做一般性的推广。也是具有线性次序的一组元素构成的集合</p><p>$V&#x3D;[v_0,v_1,…,v_{n-1}]$其中的元素分别由Rank秩相互区分</p><p>在这样抽象之后，我们不在限定同一个向量中的元素都属于同一基本类型，本身可以是来自于更具一般性的某一类的对象，另外各元素也不见得同时具有某一数值属性，故并不保证之间可以相互比较大小</p><h2 id="2-2-Interface接口"><a href="#2-2-Interface接口" class="headerlink" title="2.2 Interface接口"></a>2.2 Interface接口</h2><p>操作接口</p><p>具有一些基本操作，</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c76a0e1108a73f2b192d5ef176fbfd40_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>介绍其中某些操作，比如size（）、get（）等静态操作可以在常数时间内完成</p><p>insert（）、remove（）等动态操作可能需要线性时间</p><h2 id="Chapter03-（Dynamic）列表List"><a href="#Chapter03-（Dynamic）列表List" class="headerlink" title="Chapter03 （Dynamic）列表List"></a>Chapter03 （Dynamic）列表List</h2><p>上一个章节中的元素访问为call-by-rank访问</p><p>另外一种形象的元素访问时间可以是call-by-link</p><p>不同数据结构内部的存储与组织方式差异，其操作接口的作用方式和时空性能也不相同，在设计和选用数据结构时，应该从实际应用的需求出发，先确定功能规范及性能指标，比如引入LIst的目的就是为了弥补Vector在解决某些应用问题的功能和性能方面的不足，两者之间的差异体现在对外的操作不同，根源在于内部存储方式的不同</p><h2 id="3-1-从Vector到List"><a href="#3-1-从Vector到List" class="headerlink" title="3.1 从Vector到List"></a>3.1 从Vector到List</h2><p>这也代表这从Static到Dynamic之间的转变</p><p>Static Storage Policy 静态存储策略</p><p>之前的Vector采用的就是静态存储的策略，“各元素物理地址连续”，由此导致的size（）get（）静态操作均可以在常数时间完成，而insert（）、remove（）等动态操作却都可能需要线性时间；得益于这种策略，我们可以O（1）时间内由Rank确定向量元素的物理地址，但是反过来在insert和remove的 过程中又不得不移动 O（n）元素，可以看出静态存储策略的静态操作的效率达到了极致，但是动态操作，局部的修改会引起整个范围甚至整个数据结构的调整</p><p>Dynamic Storage Policy 动态存储策略</p><p>List作为代表，尽管要求元素在逻辑上具有线性次序，但对物理地址并未做出任何限制，具体的是在生命周期内，此类数据结构将随着内部数据的需要，相应分配或回收局部的数据空间，如此元素之间的逻辑关系得以延续，却不必与物理次序相关，作为补偿，这类结构将通过指针或引用等机制来确定各元素的实际物理地址</p><p>比如链表（Linked list）就是一种典型的动态存储结构，其中的数据分散为一系列称为节点（Node）的单位，节点之间通过指针相互索引和访问，引入新的节点就可以删除原有的节点，在局部调整少量相关节点之间的指针</p><p>但是这样并不是没有问题</p><p>在提高动态动态操作效率的同时，却不不得不舍弃原静态存储策略中使用Rank访问的方式，从而造成来静态操作性能的下降，虽然Link list的访问可以通过Rank来查询，但是其本质上是通过多次的指针递归得到的，因此平均访问时间为O（n）</p><h2 id="Chapter04-（更基本）栈Stack-与队列Queue"><a href="#Chapter04-（更基本）栈Stack-与队列Queue" class="headerlink" title="Chapter04 （更基本）栈Stack 与队列Queue"></a>Chapter04 （更基本）栈Stack 与队列Queue</h2><p>本章的目标是实现更加基本，也更加常用的两类数据结构—栈和队列，属于线性序列结构，古其中存放的数据对象之间也具有线性次序，相对于一般的序列结构，栈与队列的数据操作范围仅限于逻辑上的特定某段，得益于其简洁行于规范性，他们可以构建更复杂、更高级数据结构的基础，也是算法设计的基本方法</p><p>相对于向量和列表，栈于队列的外部接口更为简化和紧凑，得益于此，本章的重点将不在拘泥对数据结构内部实现机制的展现，并转而更多从外部特性出发，结合若干典型的实际问题介绍栈和队列的具体应用</p><p>在队列的应用方面，贲张将介绍如何实现基于轮值策略的通用循环分配器，并以银行窗口服务为例实现基本的调度算法</p><h2 id="4-1-栈（Stack）"><a href="#4-1-栈（Stack）" class="headerlink" title="4.1 栈（Stack）"></a>4.1 栈（Stack）</h2><p>栈是存放数据对象的一种特殊容器，其中数据元素按照线性逻辑次序排列，因此可以定义首、末元素，不过尽管栈结构也支持对象的插入和删除操作，<strong>其操作的范围仅限于栈的某一特定端，也就是说，若约定的元素只能从某一端插入其中，则反过来也只能从这一端删除已有的元素</strong></p><p>类似于这种操作，栈中可以操作的一段成为栈顶Stack Top，另一段无法直接操作的盲端称为Stack Bottom，最常用的插入和删除操作分别称为入栈Push和出栈Pop</p><h2 id="4-2-栈与递归"><a href="#4-2-栈与递归" class="headerlink" title="4.2 栈与递归"></a>4.2 栈与递归</h2><p>递归算法所需的空间量主要决定于最大递归深度，在达到这个深度的时刻，同时活跃的递归实例达到最多，</p><p>关于操作系统是如何实现函数递归调用的？如何记录调用与被调用函数（递归）之间的关系？如何实现函数（递归）调用的返回？又是如何维护同时活跃的所有函数（递归）实例的实现的？</p><p><strong>这些所有问题的答案，都可以归结于栈Stack</strong></p><p>在windows操作系统中，每个运行中的binary executable的二进制程序都配有一个调用栈（call stack）或执行栈execution stack，借助调用栈可以跟踪属于同一程序的所有函数，记录它们之间的相互调用关系，并保证在每一调用实例执行完毕之后，可以准确返回</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-de15347aaed8400b805e062952cb0809_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>函数调用</li></ul><p>Call Stack的基本单位是帧（Frame）、以及局部变量、输入参数，并将该栈Push调用栈，并在该函数返回之前发生新的调用，同样将与新函数对应的一帧Push栈顶，函数一旦运行完毕，对应的帧随即弹出，运行控制权将被交还给该函数的上层调用，并按照Frame记录的返回地址确定二进制程序中继续执行的位置，当位于栈底的main（）被弹出意味着整个程序的运行结束，此后控制权将交还给OS</p><p><a href="https://www.bilibili.com/video/BV1FY411J7s7?spm_id_from=333.337.search-card.all.click">CPU眼里的：{函数} | 栈帧 | 堆栈 | 栈变量 | 调用栈_哔哩哔哩_bilibili</a></p><ul><li>递归</li></ul><p>作为函数调用的特殊形式，递归也可以借助上述调用栈得以实现，可以看出同一函数可能同时拥有多个实例，并在调用栈中各自占有一栈，这些栈的结构完全相同</p><p>一些没有用的建议</p><h2 id="4-3-栈的典型应用"><a href="#4-3-栈的典型应用" class="headerlink" title="4.3 栈的典型应用"></a>4.3 栈的典型应用</h2><p>4.3.1 逆序输出</p><p>逆序输出中的共同特征：首先，虽有明确的算法，但解答却以线性序列的形式给出，其次无论是递归还是迭代实现，该序列都是按照逆序计算输出的，最后输入和输出规模不确定，事先难以确定盛放输出数据的容器大小，因此具有特有的“后进先出”的特点，以及在容量方面的自适应性，因此可以使用Stack来解决这类问题</p><p>   进制转换问题，可以将十进制整数n转换为lambda进制的表现形式  </p><p>比较简单的除呗</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">convert</span>(<span class="hljs-params">S,<span class="hljs-built_in">int</span>：n,<span class="hljs-keyword">base</span></span>)</span>&#123;<br>    <span class="hljs-keyword">static</span> <span class="hljs-built_in">char</span> digit[]=&#123;<span class="hljs-string">&#x27;0&#x27;</span>,<span class="hljs-string">&#x27;1&#x27;</span>,...,<span class="hljs-string">&#x27;E&#x27;</span>,<span class="hljs-string">&#x27;F&#x27;</span>&#125;;<br>    <span class="hljs-keyword">if</span> (n&gt;<span class="hljs-number">0</span>)&#123;<br>    S.push(digit[n%<span class="hljs-keyword">base</span>]&#125;<br>    convert(S,n/<span class="hljs-keyword">base</span>,<span class="hljs-keyword">base</span>)<span class="hljs-comment">//在C中这里的n/base会自动输出为整数</span><br>&#125;<br>&#125;<br><span class="hljs-comment">//新的base进制下由高到低的各数位，自顶而下保存与栈S中</span><br></code></pre></td></tr></table></figure><p>4.3.2 递归嵌套</p><p>具有自相似性的问题可以用多嵌套递归描述，但因为分支位置和嵌套深度并不固定，其递归算法的复杂度不易控制，栈结构及其操作天然具有递归嵌套性，因此可用于高效解决这类问题</p><p>栈混洗（Stack Permutation）</p><p>假设三个栈A、B、S，其中BS开始为空，A中包含n元素，S作为中间转移栈，将A中元素转移到B中，其中仅允许</p><p>S.Push(A.Pop())</p><p>B.Push(S.Pop())</p><p>   括号匹配问题，对源程序的检查是代码compile中重要而基本的一个步骤，而对表达式括号匹配的检查又是语法检查中必需的一个环节 其任务是，对任何一个程序块，需要判断其中的括号是否在嵌套的意义下完全匹配，  </p><h2 id="4-4-试探回溯法（Probing-and-BackTracking）"><a href="#4-4-试探回溯法（Probing-and-BackTracking）" class="headerlink" title="4.4 试探回溯法（Probing and BackTracking）"></a>4.4 试探回溯法（Probing and BackTracking）</h2><p>古希腊神话中半人半牛的怪物弥诺陶洛斯（Minotaur），藏身于一个精心设计、结构极其 复杂的迷宫之中。因此，找到并消灭它绝非易事，而此后如何顺利返回而不致困死更是一个难题。 不过，在公主阿里阿德涅（Ariadne）的帮助下，英雄忒修斯（Theseus）还是想出了个好办法， 他最终消灭了怪物，并带着公主轻松地走出迷宫。</p><p>实际上，忒修斯所使用的法宝，只不过是一团普通的线绳。他将线绳的一端系在迷宫的入口 处，而在此后不断检查各个角落的过程中，线团始终握在他的手中。线团或收或放，跟随着忒修 斯穿梭于蜿蜒曲折的迷宫之中，确保他不致迷路。</p><p>忒修斯的高招，与现代计算机中求解很多问题的算法异曲同工。事实上，很多应用问题的解， 在形式上都可看作若干元素按特定次序构成的一个序列。 以 经典 的旅行商问题（ traveling salesman problem, TSP）为例，其目标是计算出由给定的n个城市构成的一个序列，使得按此 序列对这些城市的环游成本（比如机票价格）最低。尽管此类问题本身的描述并不复杂，但遗憾 的是，由于所涉及元素（比如城市）的每一排列都是一个候选解，它们往往构成一个极大的搜索 空间。通常，其搜索空间的规模与全排列总数大体相当，为n! &#x3D; O(nn )。因此若采用蛮力策略， 逐一生成可能的候选解并检查其是否合理，则必然无法将运行时间控制在多项式的范围以内。</p><p><strong>这样的算法对应这样的模式：</strong></p><p>从零开始，尝试逐步增加候选解的长度，更准确地是在这个过程中成批的考察具有特定前缀的所有候选解，这种从长度上逐步向目标解靠近的尝试，成为试探 Probing</p><p>作为解得到局部特征，特征前缀在试探的过程中一旦被发现与目标解不适合，则收缩到此前一步的长度，然后试探下一可能的组合，特征前缀长度缩减的这类操作成为，回溯BackTracking，其效果等同于剪枝Pruning</p><p>在上述的故事中，这种方法依赖于有型的物质基础，也就是需要保证搜索过的过程不被重复搜索，办法就是在剪枝的位置留下某种标记</p><p>   八皇后，国际象棋中皇后的势力范围覆盖所在的水平线、垂直线和两条对角线，现在考察如下的问题 在n*n的棋盘中，如何使得她们之间彼此互不攻击， 此时称她们构成一个可行的棋局，对于任何整数n大于等于4  </p><p>首先分析问题可以看出，n皇后可以在n*n的棋盘上可行，首先需要设计Queen这个类，也就是坐标，通过重载判断等操作符来实现对皇后位置是否相互冲突的便捷判断</p><p>若每行能且只能放置一个皇后，不妨首先将每个皇后分配给每一行，然后从空棋盘开始逐个尝试将她们放置到无冲突的某列，没放置好一个皇后才继续试探下一个，若当前皇后在任何列都会造成冲突，则后续皇后的试探都是徒劳的，因此应该回溯到上一皇后</p><p>可以借助栈solu来动态记录各皇后的列号，当该栈的规模增至N得到全局解</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c8ae7d95c379322be40613a37c59ae67_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>   迷宫寻路，要求依照预定的行进规则，在具有特定集合结构的空间区域内，找到从起点到终点的一条通路，这样的简化版本 在空间区域限定为由n*n的方格组成的迷宫，除来四周的围墙，还有分布其间的若干障碍物，只能水平或垂直移动 我们的任务是在任意制定的Origin和Des中找到一条通路  </p><h2 id="4-5-队列Queue"><a href="#4-5-队列Queue" class="headerlink" title="4.5 队列Queue"></a>4.5 队列Queue</h2><p>与栈Stack一样，队列Queue也是存放数据对象的一种容器，其中的数据对象也按照线性的逻辑次序排列，队列结构同样支持对象Insert和delete，但是两种操作的范围分别被限制于队列的两端（若约定新对象只能从某一端插入其中，则只能从另一端删除已有的元素，允许取出元素的一端成为队头Front，而允许插入元素的另一端成为队尾Rear</p><p>对于的插入和删除操作，从队的角度可以称为enqueue和dequeue的操作</p><p>对应的一些基本操作</p><p>   循环分配器，为在客户client群体共享某一资源，比如共享CPU，一套公平且高效的分配规则必不可少，而队列结构则非常适于定义和实现这样的一套分配规则  </p><p>可以使用所谓的轮值round robin算法中，首先令所有参与资源分配的客户组成一个队列Q，接下来是一个反复轮回式的调度过程</p><p>这里，每位用户持续占用资源的时间对算法的成败至关重要，一方面，未来保证响应速度，这个时间值通常不能过大；另一方面，因占有权限的切换也需要消耗一定时间，因此这个时间取的过小，切换过于频繁也会造成整体效率的下降</p><p>   银行服务模拟，介绍如何使用Queue结构实现顾客服务的调度于优化，  </p><h2 id="Chapter05-二叉树-Tree"><a href="#Chapter05-二叉树-Tree" class="headerlink" title="Chapter05 二叉树 Tree"></a>Chapter05 二叉树 Tree</h2><p>如果以平衡二叉搜索数为例子，若其中包含n个元素，则每次查找、更新、插入和删除操作等都可以在O(log N)的时间内完成</p><p>数结构有不计其数的变种，在算法理论以及实际应用中扮演着最为关键的角色，得益于独特而又普适的逻辑结构。树作为一种分层结构，而层次化这个特征几乎蕴含在所有食物及其联系之中</p><p>作为树的特例，二叉树实际上并不失其一般性，就本章而言，无论是逻辑结构还是算法功能来说，任何有跟有序的多叉树都可以等价转化并实现为二叉树，本章的终点在二叉树中，我们将用通讯编码算法的实现这个应用实例作为线索贯穿全章</p><h2 id="5-1-二叉树及其表示"><a href="#5-1-二叉树及其表示" class="headerlink" title="5.1 二叉树及其表示"></a>5.1 二叉树及其表示</h2><h3 id="5-1-1-树（Tree）"><a href="#5-1-1-树（Tree）" class="headerlink" title="5.1.1 树（Tree）"></a>5.1.1 树（Tree）</h3><p>从图论的角度来看，树等价于连通无环图，因此与一般的图相同，树可以用</p><ul><li>一组定点Vertex，及其连接与其间的若干条边edge组成</li><li>在计算机科学，在这个基础上在制定某一特定定点成为根root，实现rooted tree</li><li>在程序实现的角度上，更多将定点vertex称做节点node</li></ul><p>由于树的连通性，每个节点与根之间都有一条路相连，而根据树的无环性，由根通往每个节点的路径必然唯一，因此在每个节点vertex到根root的唯一通路所经过边的数目，称作的深度depth，依据深度排序可以对所有节点做分层归类</p><ul><li>depth；沿每个节点vertex到根root的唯一通路所经过边的数目成为该节点的深度</li><li>祖先ancestor&#x2F;后代descendant；这个是包含vertex节点本身</li><li>真祖先proper ancestor&#x2F;真后代proper descendant；不包含vertex节点本身</li><li>parent&#x2F;child；相邻的成为父母节点，和孩子节点</li><li>degree，vertex的孩子总数称为度树或者度，记做deg（v）</li><li>leaf，无孩子的节点被成为叶节点，包括根在内的其余节点都被称内部节点internal node</li><li>subtree；vertex的所有后代及其之间的联边称作子树，subtree（v）</li></ul><h3 id="5-1-2-二叉树-（Binary-Tree）"><a href="#5-1-2-二叉树-（Binary-Tree）" class="headerlink" title="5.1.2 二叉树 （Binary Tree）"></a>5.1.2 二叉树 （Binary Tree）</h3><p>二叉树中的每个节点的degree均不超过2，因此在Binary Tree中同一父节点中的汉子都可以使用左、右相互区分，因此可以被称为有序二叉（Ordered Binary Tree）；特别的，不包含一度节点的二叉树称为真二叉树（Proper Binary Tree）</p><h3 id="5-1-3-多叉树（K-ary-Tree）"><a href="#5-1-3-多叉树（K-ary-Tree）" class="headerlink" title="5.1.3 多叉树（K-ary Tree）"></a>5.1.3 多叉树（K-ary Tree）</h3><p>一般来说，树中各节点的degree的数目并不确定，每个节点的degree均不超过k个的有根树，称为K叉树</p><p>在这个过程过，多叉树的表现方式有多种</p><ol><li>父节点表示方法</li><li>孩子节点表示方法</li><li>父节点和孩子节点表示方法</li><li>有序多叉树&#x3D;二叉树</li><li>长子和兄弟 </li><li>父节点表示方法</li></ol><p>可以将各节点组织为向量Vector或列表List，其中每个元素除保存节点本身的信息Data之外，还需要保存父节点Parent的Rank或Position</p><p>特别的，可以为Root指定一个虚构的父节点-1或Null，便于统一判断</p><p>如此，所有向量或列表所占的空间总量为O（N），线性正比于节点总数N，时间方面，仅需常数时间就可以确定任一节点的父节点Parent；但是反过来，孩子节点的查找却不得不花费O（n）时间访遍所有节点</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-52cb976a0d58248e7de70ca619a19edd_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ul><li>孩子节点表示方法</li></ul><p>可以让每个节点将其所有的孩子组织为一个向量或列表，由此对于拥有r个孩子的节点，可以在O（r+1）的时间内列举出其所有的孩子</p><ul><li>父节点和孩子节点</li></ul><p>上面两种方式各有所长，也各有所短，为了综合两者之间的优势，消除缺点，可以使用各节点记录父节点，也记录序列保存所有孩子</p><p>但是这种方法并不是没有缺点；尽管如此可以高效兼顾对父节点和孩子定位，但在节点插入与删除操作频繁的场合，为了动态维护和更新树的拓扑结构，不得不反复遍历和调整一些节点所对应的孩子序列，然而Vector和List等线性结构的此类操作都需要耗费大量时间，势必影响到整体的效率</p><ul><li>有序多叉树和二叉树</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5a4bfd357bbea207fa62385725dedf0b_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>采用从支持高效动态调整的二叉树结构，为此必须首先建立从多叉树到二叉树之间的某种转换关系，在这种转换的意义下，任何多叉树都等价与某颗二叉树</p><ul><li>长子和兄弟</li></ul><p>多序多叉树中任一非叶节点都有唯一的长子，而且可以该长子出发，按照约定或指定的次序遍历所有的孩子节点</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7024ca6eff8be28ae39d8a1931783753_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><strong>一个非常有趣的现象出现：尽管二叉树只是多叉树的一个子集，但是对应用问题的描述和刻画能力并不低于后者，实际上以下我们还能进一步发现，即便就计算效率而言，二叉树也并不逊色一般意义上的树，反过来，得益与定义的简洁性以及结构的规范性，二叉树所所支撑的算法往往可以更好地得到描述，更加便捷实现</strong></p><h2 id="5-2-编码树"><a href="#5-2-编码树" class="headerlink" title="5.2 编码树"></a>5.2 编码树</h2><p>通过将通讯编码算法的实现作为二叉树的应用实例，通讯理论的基本问题是在尽可能低的成本下，用最高的速度来真实的实现信息在空间和时间上的复制和转移；在信道转移的信息大多按照二进制比特形式表示和存在，而每一个具体的编码方案都对应一颗二叉编码树</p><p>讲解的不深，不值得看，可以去看《信息传输远离》</p><h2 id="5-3-二叉树的实现"><a href="#5-3-二叉树的实现" class="headerlink" title="5.3 二叉树的实现"></a>5.3 二叉树的实现</h2><p>作为图（Graph）的特殊形式，二叉树的基本组成单元是节点与边，作为数据结构，其基本的组成实体是二叉树节点（Binary Tree Node），边作用与节点之间的相互作用</p><h2 id="Chapter06-图-Graph"><a href="#Chapter06-图-Graph" class="headerlink" title="Chapter06 图 Graph"></a>Chapter06 图 Graph</h2><p>例如之前例子中的迷宫，借助绳索掌握迷宫内各通道之间的相互关系，在很多应用中我们需要准确有效描述和利用这类信息，这类信息往往可以表述为定义与一组对象之间的二元关系，比如城市交通图、比如互联网中的IP地址，尽管上一章的树 Tree结构也可以用来表示这种二元关系，但是仅限与Parent Node 和Child Node之间，这种一般性的二元关系属于图论 Graph Theory的范畴</p><p>本章的主要目的</p><ol><li>简要介绍Graph的基本概念和术语</li><li>如何实现作为抽象数据类型的图结构，主要讨论邻接矩阵，和邻接表两种实现方式</li><li>从遍历的调度介绍将图转换为树的典型方法，包括广度优先搜索和深度优先搜索</li><li>分别以拓扑排序和双连通域分解为例子，介绍利用基本数据结构并基于遍历模式，设计图算法的主要方法</li><li>（数据结构决定遍历次序）的观点出发，将遍历算法概括并统一为最佳优先遍历这一模式，如此来更加准确和深刻理解不同图算法之间的共性与联系，更可以学会通过选择和改进数据结构，高效设计并实现各种图算法</li></ol><h2 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1 概述"></a>6.1 概述</h2><h3 id="6-1-1-图"><a href="#6-1-1-图" class="headerlink" title="6.1.1 图"></a>6.1.1 图</h3><p>图（Graph）定义为G&#x3D;（V，E）。集合V中的元素为顶点Vertex，集合E中的元素分别对应于V总的某一对顶点（u,v），表示它们之间存在的某种关系，因此可以称为边（Edge），从计算需求的角度约定都是有限集，规模分别记为n和e</p><ul><li>无向图undirected graph，若边(u,v)所对应顶点的u和v的次序无所谓，称作无向边direct edge；</li><li>有向图directed graph。如果不对等，称为directed edge，如果从u指向v，则其中u是该边的origin或者尾顶点tail，v是该边的终点destination或头顶点head</li><li>如果包含有向边和无向边，可以称为混合图mixed graph</li></ul><p>度（Degree），对于任何一边e&#x3D;，可以称顶点u和v彼此邻接（adjacent），同时与边e彼此关联（incident）。在无向图中，与顶点v关联的边数称为v的度数（degree），记deg（v）；在有向图中，出边总数称为出度（out-degree）、入边总数称为（in-degree）</p><p>简单图，是联接与同一顶点之间的边，称为自环self-loop，在某些特定的应用，这类边的确具有意义，但是不含任何自环的图称为简单图 simple graph</p><p>（之后巴拉巴拉一大堆。。。掠过）</p><p>带权网络（weighted network，G（V，E，wt（）））需要通过一个权值函数，为每一边e指定一个权重weight，wt（e）指定边e的权重</p><p>复杂度（complexity），在这个过程中n各顶点，最多有e&#x3D;O（n^2)来进行描述</p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法设计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How to promote sustainable travel behavior in the post COVID-19 period</title>
    <link href="/posts/4ccb03.html"/>
    <url>/posts/4ccb03.html</url>
    
    <content type="html"><![CDATA[<p>在疫情和定制公交的背景下，探讨如何促进人们可持续出行，也就是尽可能的转向定制公交，做出的主要贡献是研究了</p><ol><li>通勤者在疫情中转向定制公交服务的意愿</li><li>根据问卷调查的方法确定了不同群体的模型偏好</li><li>为在疫情中促进公交服务提供潜在的政策建议</li></ol><p><a href="https://www.sciencedirect.com/science/article/pii/S2046043021000836">论文链接</a></p><h2 id="01-引言和文献综述"><a href="#01-引言和文献综述" class="headerlink" title="01 引言和文献综述"></a>01 引言和文献综述</h2><p>首先大的背景是疫情，在疫情初期人们不可避免的受到政策和居家办公等影响在家，但是随着疫情慢慢结束，人们有着不可避免的出行需求，根据前人的研究，疫情前后出行的特点包括</p><ol><li>客流需求降低、货运需求提升</li><li>出行使用私家车、自行车和步行升高；使用其他公共交通方式降低</li></ol><p>但是过度的私家车出行会具有交通拥堵、环境污染等问题，因此促进疫情可持续出行是十分有必要的，而定制公交（customized bus，CB）是典型的需求响应式公交（demand response transit，DRT），利用类似公交服务于具有相同出行点和目的地的乘客，它具有比私家车更环保、比传统公共交通成本更低、同时具有私密、灵活和接驳的特点，从疫情的角度可以有效的避免路上的交叉感染，</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-77159311ae28d20c60c0d165ab6241bf_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>从定制公交和疫情中人们出行行为变化的发展来看</p><ol><li>定制公交的文献主要研究如何提升其服务水平，包括路径规划、车辆调度、服务质量等，没有研究定制公交具有吸引力的原因；因此可以研究疫情中定制公交如何影响人们的出行行为选择</li><li>在疫情前后，人们的出行方式可能发生变化，其中设计到人口特征、政策和个人观念，因此研究疫情汇总人们出行行为偏好的改变</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8c6c3edbce396b9882dd5c286d5b9429_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="02-方法和实验"><a href="#02-方法和实验" class="headerlink" title="02 方法和实验"></a>02 方法和实验</h2><p>因此本文使用调查问卷的方式，在问卷调查中主要分为三个部分：</p><ol><li>RP调查，是针对用户的真实场景进行提问，利用百度API爬取的通勤起终点步行、骑行、私家车、公交车、地铁和定制公交的时间、距离和费用等特征</li><li>SP调查时在给定场景中对用户的行为进行判断，对调查用户在收费和速度不同的四种场景下转向定制公交的意愿特征分析；四种正交场景包括票价和旅行速度的差异</li><li>基本的属性和社会学特征，包括性别、年龄、受教育程度、收入、是否有驾照、是否有私家车、是否有自行车&#x2F;摩托车</li></ol><p>之后使用多类别逻辑回归和嵌套逻辑回归分析，它们于逻辑回归的区别在于，</p><p><strong>逻辑回归</strong></p><p>假设</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-14e3f110e691536beb4d41958a3835ea_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>成本函数</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-36ad0366325d7287675911d9714c88e1_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><strong>多类别逻辑回归</strong></p><p>（ps：我觉得就是softmax regression），它受到不相关替代方案的独立性影响，会导致不同类别之间具有互斥性，因为我们暂时还不知道CB与其他交通方式是否具有相似性，导致不同类别之间具有互斥性</p><p><strong>嵌套逻辑回归</strong></p><p>允许某些备选模式之间存在相关性，放宽了不同类别之间的独立同分布的假设</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-318bc715b1bbeaee7cef83460c347d21_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="03-结果和讨论"><a href="#03-结果和讨论" class="headerlink" title="03 结果和讨论"></a>03 结果和讨论</h2><p>建立的模型为</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-90977abf3738677b825cadfe0becc524_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>根据假设检验判断NL并不显著，说明CB和其他交通方式有明显的差异，因此主要看MNL的结果就行</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6957accd0822e90835085a2494aeeea0_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ol><li>单程客制化票价系数为负（-0.1141）；定制巴士的出行时间对通勤者的出行方式选择概率产生负面影响（-1.3966） </li><li>大多数模式的旅行时间系数为负。汽车的出行时间系数分别是步行和乘坐公共汽车的2.41倍和2.16倍 </li><li>受访者对公共汽车的进出距离和地铁的出口距离很敏感。地铁的出口距离影响较大（-1.12），而公交车的出口距离不明显（-0.3321）。</li></ol><p>之后根据ASC，也就是受访者在不采取任何措施时候的取值，反映保持现状受访者的基准效用</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a0177f25376ceeb7436b70b4c4276e88_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>Ø年轻人更愿意乘坐地铁和公共汽车通勤，对定制公交比较积极</p><p>Ø中年人更喜欢乘坐定制公交出勤</p><p>Ø中等收入倾向于不步行通勤</p><p>Ø较高收入对私家车表现更高的偏好</p><p>Ø高等教育对定制公交具有强烈的偏好</p><p>Ø女性出行者更倾向于使用定制公交服务，比男性更讨厌步行和骑自行车</p><p>Ø男性对不同通勤方式的偏好没有显著差异</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-9cdc2f00d0128d8a7245c4d6a2f9dbe5_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>Ø无驾照人群更倾向于定制公交，同时对私家车出行消极</p><p>Ø有驾照人群对私家车出行和定制公交都积极</p><p>Ø有车群体愿意开车通勤，但比没有车的人更愿意改乘定制巴士</p><p>Ø有自行车的受访者使用自行车上班的可能性更高，但是定制公交对他们也很有吸引力</p><p>Ø有自行车的受访者使用自行车上班的可能性更高，但是定制公交对他们也很有吸引力</p><p>由上述给出的建议</p><ol><li>从短期来看，公众愿意接受具有个人防护设备和疫情防控的基本用品的定制公交服务 </li><li>从长远来看，由于后 COVID-19 时期的汽车通勤者更愿意使用定制巴士，而定制巴士的出行时间是一个重要变量，政府应实施相关政策来推动这一转变。 </li><li>关于每个群体的特点，重要的是要考虑中年（31 至 36 岁）或受过高等教育（硕士和博士学位）&#x2F;中等收入&#x2F;没有执照或同伴，并拥有汽车和自行车。应该为这些群体设计具体的政策。票价折扣和补贴可以被认为是一个起点。</li></ol>]]></content>
    
    
    <categories>
      
      <category>交通工程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Paper 阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 学习理论总结 ｜Vol9</title>
    <link href="/posts/9be9aaa.html"/>
    <url>/posts/9be9aaa.html</url>
    
    <content type="html"><![CDATA[<p>在目前没有足够做的经验或者说是机器学习素养的情况下来看这种文章无疑是一种折磨，因此过一遍概念就over惹，同时记录一下文献管理中的一些经验</p><h2 id="01-计算学习理论"><a href="#01-计算学习理论" class="headerlink" title="01 计算学习理论"></a>01 计算学习理论</h2><p>计算学习理论希望回答</p><ol><li>在什么样的条件下成功的学习是可能的</li><li>在什么条件下某个特定的学习算法可以保证成功运行</li></ol><p>在可能近似正确（probably approximately correct，PAC）下，我们确定若干假设类别，判断它们能否从多项式数量的训练集中得到，同时对假设空间的自然度量来界定归纳学习所需的样例的训练数目</p><h3 id="1-1-问题和错误率"><a href="#1-1-问题和错误率" class="headerlink" title="1.1 问题和错误率"></a>1.1 问题和错误率</h3><p>我们具有数据集合$(x^{(i)},y^{(i)}）$，以及做出的假设的集合H和学习算法L，在这个问题中，我们感兴趣的是刻画不同的学习器L的性能，这些学习器使用不同假设空间H来学习不同类别的C中的目标概念</p><p>为了描述学习器L输出的假设h对真实目标的逼近成句，我们定义真实错误率（true error）,注意这里的真实错误率的定义是在整个实例的分布上，而不只是训练样例上，因为它是在实际应用此假设h后遇到的真实错误率</p><p>$error(h)&#x3D;Pr[c(x)~&#x3D;h(x)]$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5db0e69419713fe1c2aeac8b36f4e015_1440w.jpg" alt="img"></p><p>image-20220403173130140</p><p>这个错误率的分布依赖于未知的概率分布，这个概率分布是和假设空间相关的；另外一方面h对于c的错误率不能直接由学习器观察到</p><p><strong>我们的目的是刻画一个：能够从合理数量的随机抽样训练样例中通过合理的计算来得到一个可靠的假设</strong>，那我们如何来描述这种可学习性？我们当然希望所有的真实错误率都为0，但是这样是不现实的（思维辩证就知道不合适），因此我们可以从这两个角度看</p><ol><li>将真实错误率限定在一个常数范围内，同时让这个常数可以任意小$\epsilon$</li><li>并不要求对所有的随机抽样序列都能成功，只要求其实拍的概率在某个常数$\sigma$</li></ol><p>一些PAC的定义：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d573fc13effe96fc3442050468ce2da9_1440w.jpg" alt="img"></p><p>image-20220403175054274</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-903182e8c2a0fcf1b4f96a4825753f48_1440w.jpg" alt="img"></p><p>image-20220403175125351</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-87986e8984f280d16b993aa0f44eb883_1440w.jpg" alt="img"></p><p>image-20220403175202889</p><p>在PAC学习定义之外，我们可以看出模型的可学习性很大程度由所需要的数据集数量确定，随着问题规模的增长所需要的训练样例的增长被称为该学习问题的样本复杂度（sample complexity）</p><p>对于一个假设空间如何确定和数据集之间的关系，可以用$\epsilon-exhousted$来刻画，表示于训练样例一致的所有假设的真实错误率都恰好小于$\epsilon$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-38baae2e2c0e9b5c645cbf09588432de_1440w.jpg" alt="img"></p><p>image-20220403175841000</p><p>由此计算的得到需要的训练实际例子</p><p>$m&gt;&#x3D;1&#x2F;(\epsilon)(In|H|+in(1&#x2F;\sigma ))$</p><p>同样对于一个假设空间的复杂程度，可以使用Vapnik-Chervonenkis维度来证明模型的相关复杂程度</p><ul><li>常见模型的复杂程度</li><li>神经网络的复杂程度</li></ul><h2 id="02-基于实例学习"><a href="#02-基于实例学习" class="headerlink" title="02 基于实例学习"></a>02 基于实例学习</h2><p>一个很简单的思路，通过强大的计算能力，我们可以比较希望预测的值和数据集中所有实例，当从这些实例中泛化的工作被推迟到必须分类新的实例时，每当学习器遇到一个新的查询实例就分析这个实例与之前存储的的关系，来由此区分目标函数的值，这样的方法包括：</p><ol><li>最近邻方法 nearest neighbor</li><li>局部加权回归 locally weighted regression</li></ol><h2 id="03-增强学习"><a href="#03-增强学习" class="headerlink" title="03 增强学习"></a>03 增强学习</h2><p>大坑，不学，死都不学</p><h2 id="04-文献管理"><a href="#04-文献管理" class="headerlink" title="04 文献管理"></a>04 文献管理</h2><p>这里就是作为一个简单的记录，看文献中可能的应用场景</p><ol><li>日常泛读来了解自己的研究现状（对于学术小白鼠不考虑555，所以没有这种需求）</li><li>文献综述</li><li>组会汇报的工作</li></ol><p>因此需要对论文整理成一个一个项目来管理，同时需要对其做笔记，更重要的是可以方便引用，</p><p>第一部分查找：</p><ol><li>单纯的找论文和管理的话，<a href="https://scholar.google.com.hk/?hl=zh-CN">google scholar</a>+<a href="https://sci-hub.se/">sci-hub</a>感觉就够了</li><li>在自己的google账号账图书馆中新建自己的文章档案库，这样可以方便管理</li></ol><p>第二部分阅读：</p><p>可以在文档下载导入到zotero，再链接到notion，利用notion来做笔记和一些comment</p><p>作为一个文献管理的过程</p><p>第三部分引用：</p><p>可以利用zotero配合word的插件</p><p>也可以google scholar中导出apa格式</p><p>也可以使用latex中的texbib格式</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 集成学习 ｜ Vol8</title>
    <link href="/posts/8c525481.html"/>
    <url>/posts/8c525481.html</url>
    
    <content type="html"><![CDATA[<p>集成学习是通过构建并结合多个学习器来完成学习任务，有时候也被称为多分类系统（multi- classifier system）、基于委员会的学习（committee based learning）等等之类的，在其中主要面临基本学习器的选择和训练过程，我们希望学习器可以在相同的数据集上训练，但是得到的最终的弱学习器最好的是相互独立的，这样才能提高我们最高算法的准确程度。</p><p>集成学习成立的基础是：基学习器的误差相互独立，因此这也是我们在集成方法中需要尽可能的去实现的，因此本文从基本的方法和以决策树作为一系列基学习器开始</p><h2 id="0x01-引言"><a href="#0x01-引言" class="headerlink" title="0x01 引言"></a>0x01 引言</h2><p>在任何应用中，我们都可以使用多个学习算法汇总的一个，使用确定的算法都在一组<strong>基本假设</strong>，并训练得到影响最终学习器的超参数，因此可以尝试使用构建多个彼此不同的学习器，尽可能的减少它们之间误差的相关性来通过某种策略的组合来提升模型的效果，如何构建不同的学习器？</p><ol><li>使用不同的算法和数据集，比如使用参数化的逻辑回归和非参数化的决策树模型，这样多算法的学习器之间的误差是可能独立的；对于相同的参数化的模型也可以采用不同的超参数，比如对于K-means算法可以在不同的算法选择不同的k值；同样对于不同的算法模型中选择不同的数据集是也可以产生不同的学习器，因此可以使用自助法等构建彼此相互独立数据集的基础上来训练得到不同的算法模型</li><li>使用一些特殊的策略，包括但不限于：平均法（averaging）、投票法（voting）、学习法（learning）</li><li>使用一些特殊的策略，包括但不限于：bagging（典型模型：随机森林random forest）和boosting（典型模型：Adaboost、GB）</li></ol><h2 id="0x02-集成学习-–-不同的模型"><a href="#0x02-集成学习-–-不同的模型" class="headerlink" title="0x02  集成学习 – 不同的模型"></a>0x02  集成学习 – 不同的模型</h2><p>比较简单，看0x01引言–第一点</p><h2 id="0x03-集成学习–-不同的策略"><a href="#0x03-集成学习–-不同的策略" class="headerlink" title="0x03 集成学习– 不同的策略"></a>0x03 集成学习– 不同的策略</h2><p>3.1 平均法averaging（回归器）</p><p>对于数值型常用的结合策略是平均法，又可以分为简单平均法（simple average）和加权平均法（weighted average）</p><p>虽然看名字可能就知道这个代表是什么，但是对于加权平均法其中最重要的是权重的确定，或许可以根据多元高斯分布来确定由近及远的权重、可以根据专家打分的德尔菲法、可以有迷糊的一致性矩阵计算得到的AHP、Kalman filter中检测和状态转移的估计等得到的权重，无论是何种方法最重要的是可以得到一个令自己满意、顶得住别人argue的处理方法</p><p>加权平均方法被用于集成学习可以被看作是集成学习研究的基本出发点，对给定的基学习器，不同的学习方法可以视作通过不同的方式来确定加权平均法中的基学习器的权重。</p><p>3.2 投票法voting（分类器）</p><p>投票的方式本质上和回归的平均法是类似的，但是因为class是可以数清楚的，因此可以分为绝对多数投票法（majority voting）、相对多数投票法（plurality voting）、加权投票法（weighed voting）、</p><p>3.3 学习法</p><p>当训练数据很多的时候，一种更为强大的结合策略是使用学习法，通过另外一种学习器来进行结合</p><p>stacking是学习法的典型代表，在这里将个体学习称为初级学习器，用于结合的学习器称为次级学习器或者元学习器（meta- learner），stacking先从初始数据集和中训练得到初级学习器，然后“生成“一个新的数据集来训练次级学习器，在这个新的数据集中，初级学习器的输出被当作样例来输入特征，而初始样本的标记被当作样例标记，这里的假设是初级学习器是使用不同学习算法得到的不同的模型，参考周志华-《机器学习》代码</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-52e362c38d83416ff5093fc9b7787116_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>可以看出，其中的次级学习算法可以看出是一种自适应adaptive的权重调整方法，同时这种权重是通过一个学习器来训练得到的，抛弃其中的数学原理不谈，这里常用的包括多响应线性回归Multi- response Linear regression和贝叶斯模型平均Bayes Model AVeraging BMA等效果较好</p><h2 id="0x04-集成学习–-特殊的概念"><a href="#0x04-集成学习–-特殊的概念" class="headerlink" title="0x04 集成学习– 特殊的概念"></a>0x04 集成学习– 特殊的概念</h2><p>在集成学习中最晦涩的就是boosting和bagging（bootstrap aggregating）两种方法</p><h3 id="4-1-bagging（有放回称为bagging、无放回的称为pasting）"><a href="#4-1-bagging（有放回称为bagging、无放回的称为pasting）" class="headerlink" title="4.1 bagging（有放回称为bagging、无放回的称为pasting）"></a>4.1 bagging（有放回称为bagging、无放回的称为pasting）</h3><p>同样是根据希望的得到不同基训练模型，因此我们可以对相同的学习器来赋予不同的训练集来得到不同的模型，但是由于数据集是有限的，因此如果数据集完全隔离开，这样训练得到的学习器必然是不好的（或者说是不够好的），如果我们想尽可能的好的得到独立同时效果也好的学习模型，[可以参考之前对模型评估](<a href="https://blog.tjdata.site/2022/02/25/Andrew">https://blog.tjdata.site/2022/02/25/Andrew</a> Wu-番外02-机器学习的实验设计&#x2F;)中的样本的自助法选择（bootstrap sampling）来得到相互独立数据集得到的不同学习模型；在此基础上得到的结果可以使用<strong>简单平均策略</strong>或者<strong>简单投票</strong>来得到最终的结果</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-16fc7815c5999f8082ba609838ca8c8a_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>从bias&#x3D;variance的角度来看，bagging主要集中在降低方差 </p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-25e39256fe8cd4502ade8a5a9c79d2f1_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="4-2-boosting"><a href="#4-2-boosting" class="headerlink" title="4.2 boosting"></a>4.2 boosting</h3><p>是一类可以将弱学习器提升为强学习器的算法，这种算法的工作机制类似：先从初始训练集训练得到一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本再后续受到跟多关注，然后基于调整后的样本分布是的先前基学习器做错的训练样本再后续收到更多的关注，然后使用基于调整后的样本分布来训练下一个基学习器，如此重复进行，直到基学习器数目达到来事先制定的值，最终将这些基学习器进行加权结合</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-fed68e0739c70d75816dc16bb49a5d81_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>从bias- variance的角度来看，boosting主要集中在降低偏差</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-7f7de7b989d0324742edb836942ed50d_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>（这里感觉书上讲的还是比较模糊的，主要在后面的实际例子中看）</p><h3 id="4-3-bagging和boosting的区别"><a href="#4-3-bagging和boosting的区别" class="headerlink" title="4.3 bagging和boosting的区别"></a>4.3 bagging和boosting的区别</h3><ol><li>从样本选择角度</li><li>从样本的权重</li><li>从预测函数的权重</li><li>从计算的角度</li></ol><h2 id="0x05-实际例子"><a href="#0x05-实际例子" class="headerlink" title="0x05 实际例子"></a>0x05 实际例子</h2><h3 id="5-1-投票法"><a href="#5-1-投票法" class="headerlink" title="5.1 投票法"></a>5.1 投票法</h3><p>一个比较简单的例子</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment">## 集成学习需要得到的是不相关的预测器</span><br><span class="hljs-comment"># 可以使用不同的学习器</span><br><span class="hljs-comment"># 可以使用不同的数据来训练得到不同的学习器</span><br><span class="hljs-comment"># 一些特殊的方法bagging和boosting</span><br><span class="hljs-comment">#%%</span><br><span class="hljs-keyword">from</span> sklearn.datasets import make_moons<br><span class="hljs-keyword">from</span> sklearn.model_selection import train_test_split<br>x,<span class="hljs-attribute">y</span>=make_moons(n_samples=100,noise=0.25)<br>x_train,x_val,y_train,<span class="hljs-attribute">y_val</span>=train_test_split(x,y,test_size=0.2)<br><span class="hljs-comment">#%%</span><br><span class="hljs-comment">## vote</span><br><span class="hljs-keyword">from</span> sklearn.ensemble import RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble import VotingClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model import LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.svm import SVC<br><br><span class="hljs-comment">#%%</span><br><span class="hljs-attribute">lr_clf</span>=LogisticRegression()<br><span class="hljs-attribute">rnd_clf</span>=RandomForestClassifier()<br><span class="hljs-attribute">svm_clf</span>=SVC()<br><span class="hljs-comment">#%%</span><br><span class="hljs-attribute">voting_clf</span>=VotingClassifier(estimators=[(<span class="hljs-string">&#x27;lr&#x27;</span>,lr_clf),(<span class="hljs-string">&#x27;rf&#x27;</span>,rnd_clf),(<span class="hljs-string">&#x27;svc&#x27;</span>,svm_clf),],<span class="hljs-attribute">voting</span>=<span class="hljs-string">&#x27;hard&#x27;</span>)<br><span class="hljs-comment">#%%</span><br>voting_clf.fit(x_train,y_train)<br><span class="hljs-keyword">from</span> sklearn.metrics import accuracy_score<br><span class="hljs-keyword">for</span> clf <span class="hljs-keyword">in</span> (lr_clf,rnd_clf,svm_clf,voting_clf):<br>    clf.fit(x_train,y_train)<br>    <span class="hljs-attribute">y_pred</span>=clf.predict(x_val)<br>    <span class="hljs-built_in">print</span>(accuracy_score(y_val,y_pred))<br></code></pre></td></tr></table></figure><h3 id="5-2-随机森林-random-forest（based-on-bagging）"><a href="#5-2-随机森林-random-forest（based-on-bagging）" class="headerlink" title="5.2 随机森林 random forest（based on bagging）"></a>5.2 随机森林 random forest（based on bagging）</h3><p>随机森林就是使用许多个决策树来集成得到最终的结果，论文最早发布与1997年的如下，他是如何将有放回的抽样应用到决策树CART中的呢？（这或许是随机森林学习的重点），以及具体的实践方法，其中的数学原理可能比较困难</p><p>Paper：<a href="https://link.springer.com/article/10.1023/A:1010933404324">Amit, Y., &amp; Geman, D. (1997). Shape quantization and recognition with randomized trees. Neural computation, 9(7), 1545-1588.</a></p><p>API:<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">sklearn.ensemble.RandomForestClassifier</a></p><p>回顾之前关于实现<a href="https://blog.tjdata.site/2022/02/20/Andrew-Wu%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B03-%E5%86%B3%E7%AD%96%E6%A0%91CART/">决策树模型的过程</a>，它是一个很简单的过程，一种采用递归思想的算法，在不考虑最终停止的条件，它总是从现有的特征集中选择最好的特征来进行模型的分割，然后删除这个特征进行下一次递归，那么随机森林在这个过程中做了什么优化呢？</p><ol><li>样本的角度，在训练决策树中并不选择所有的样本训练，利用bootstrap的方法来有放回的抽取数据集来训练不同的决策树（decision tree）</li><li>特征的角度，传统的decision tree由上述的描述可以看出它实际上是一种没有放回抽样（why？），那么随机森林的改进就是每次不删除特征集，而是随机选择一定属性的特征（比如原始特征集的10%）来选择其中最佳的特征来进行模型的分割一直到不能再分裂为止</li></ol><p>模型的优点</p><ol><li>有可以做出高纬度的数据、并且不用降纬，不需要做特征选择</li><li>可以判断特征的重要程度、特征之间的相互影响</li><li>训练速度可以很快，做成一种并行处理的方法</li><li>对于不均衡的数据可以平衡误差</li></ol><p>模型的缺点</p><ol><li>被证明在某些noise比较大的分类或回归问题中会产生过拟合</li><li>对于不同取之的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产生的属性权重是不可信的</li></ol><h3 id="5-3-Adaboost（based-on-boosting，short-for-adaptive-boosting）"><a href="#5-3-Adaboost（based-on-boosting，short-for-adaptive-boosting）" class="headerlink" title="5.3 Adaboost（based on boosting，short for adaptive boosting）"></a>5.3 Adaboost（based on boosting，short for adaptive boosting）</h3><p>自适应增强学习Adaboost是如何将boosting的方法使用在训练的过程中呢？总的来看它考虑如何通过更加规则（而不是随机）的方式来得到更加有效的基学习器的训练，并采用一定的策略来加权所有的预测</p><p>Paper：<a href="https://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf">Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139.</a></p><p>API：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html">sklearn.ensemble.AdaBoostClassifier</a></p><p>Adaboost的算法可以简述为三个步骤：</p><ol><li>初始化数据集，这里保证数据集中的每个样本的数据的权重是相同的</li><li>在第N次的训练中，得到原始标签和模型的输出，如果预测是正确的那么在下一次训练中需要降低它的权重、如果预测是错误的则需要提升它的权重，在进行N+1学习器的训练</li><li>在综合最终的结果就是对每个阶段的学习器进行加权平均，如何加权呢？根据正确率来判断，正确率高的权重大</li></ol><p>模型的优点：这里参考<a href="https://zhuanlan.zhihu.com/p/41536315">https://zhuanlan.zhihu.com/p/41536315</a></p><ol><li>很好的利用弱分类器进行级联cascade</li><li>可以将不同的分类算法作为弱分类器</li><li>AdaBoost具有很高的精度</li><li>可以充分考虑每个分类器的权重（也就是加权平均中权的选择合理）</li></ol><p>模型的缺点：</p><ol><li>模型的迭代次数不好设计，可以使用交叉验证来进行确定</li><li>数据不平衡导致分类精度下降</li><li>由于不能并行训练，因此计算时间比较长</li></ol><h3 id="5-4-GB（gradient-boosting，based-on-boosting-and-slightly-bagging）"><a href="#5-4-GB（gradient-boosting，based-on-boosting-and-slightly-bagging）" class="headerlink" title="5.4 GB（gradient boosting，based on boosting and slightly bagging）"></a>5.4 GB（gradient boosting，based on boosting and slightly bagging）</h3><p>有一点复杂2022-03-28还没有完全搞清楚梯度提升（gradient boosting），其算法实现主要分为XGBoost和LightGBM，从boosting算法的角度是必须串行计算的，但是在工程的实际应用中可以使用一些<strong>稀奇古怪</strong> 的方法来反复使用其中的一些东西来并行计算提高计算效率</p><p>Paper：<a href="http://luthuli.cs.uiuc.edu/~daf/courses/Opt-2017/Papers/2699986.pdf">Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232.</a></p><p>一些工业化的实现：</p><p>lightGBM：<a href="https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf</a></p><p>XGBoost：<a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">Chen, T., &amp; Guestrin, C. (2016, August). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp. 785-794).</a></p><p><strong>存疑</strong>：GBDT（采用decision tree作为基学习器的GB算法）的核心是学习之前每棵树结论和的残差，这个残差就是一个加预测值后能得到的真实值的累加量，这里相比Adaboost的优点</p><ol><li>与Adaboost算法不同的是，梯度提升在迭代每一步构建一个能够沿着梯度最陡的方向降低损失来弥补现有模型的不足</li><li>经典的Adaboost的算法只能采用指数损失函数的二分类学习任务，而梯度提升可以通过不同的可微损失函数来处理各类学习任务（multi-class、regression、rank）</li><li>Adaboost中对异常点比较敏感，而梯度提升算法通过引入bagging的思想、正则化可以提升robust</li></ol>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>zotero 文献管理实战</title>
    <link href="/posts/87767e37.html"/>
    <url>/posts/87767e37.html</url>
    
    <content type="html"><![CDATA[<p>一个简单的小记录，之前一直使用的Endnote管理文献，但是由于Endnote是破解版，同时其浏览器extension并不好用，并且最重要的是并不能和notion使用，因此转向了Zotero+Notion来管理自己的文献，同时保证可以在浏览器中快速导入文章和在notion中使用笔记的方式来链接</p><p>其中下载地址：</p><p><a href="https://www.zotero.org/www.zotero.org/">https://www.zotero.org/www.zotero.org/</a></p><p>mac推荐下载beta版本可以使用Safari extension，但是用不了Notion</p><p>下载5.X版本只能配合Chrome使用拓展，以及可以和Notion一起使用</p><p><a href="https://github.com/dvanoni/noterogithub.com/dvanoni/notero">https://github.com/dvanoni/noterogithub.com/dvanoni/notero</a></p><p>GitHub上大佬的API</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d9327ce64183325da1c94c97793a8e1a_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>在notion中新建一个数据库，推荐模版：<a href="https://slash-gem-e5d.notion.site/e629fd800387440b9e406b198b1520bd?v=f7e40abd4f2b4600a8510218efd9ed4f">论文整理模版</a></p><p>之后添加notion的机器人invite、以及得到机器人的token，之后在zotero中</p><p>安装插件之后设置notion的参数</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d372a71fb4bf723d6357ca8e6248159c_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>设置好之后就可以愉快的使用了！</p><p>参考这个👇</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-935ab98ff16f5ca27a89e9697126a045.jpg" alt="视频封面"></p><p>上传视频封面</p><p>使用过程</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>zotero</tag>
      
      <tag>文献管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 贝叶斯学习理论 ｜ Vol7</title>
    <link href="/posts/30f3d7d.html"/>
    <url>/posts/30f3d7d.html</url>
    
    <content type="html"><![CDATA[<p>[Andrew-Wu 基本理论01–贝叶斯学习（暨基本理论开篇）](<a href="https://blog.tjdata.site/2022/03/19/Andrew-Wu">https://blog.tjdata.site/2022/03/19/Andrew-Wu</a> 基本理论01–贝叶斯学习（暨基本理论开篇）&#x2F;)</p><p>Andrew-Wu 基本理论01–贝叶斯学习（暨基本理论开篇）blog.tjdata.site&#x2F;2022&#x2F;03&#x2F;19&#x2F;Andrew-Wu%20%E5%9F%BA%E6%9C%AC%E7%90%86%E8%AE%BA01–%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9A%A8%E5%9F%BA%E6%9C%AC%E7%90%86%E8%AE%BA%E5%BC%80%E7%AF%87%EF%BC%89&#x2F;</p><h2 id="0x00-计算学习理论的开篇"><a href="#0x00-计算学习理论的开篇" class="headerlink" title="0x00 计算学习理论的开篇"></a>0x00 计算学习理论的开篇</h2><ul><li>贝叶斯学习，贝叶斯推理提供来一种推理的概率方法，基于待考察的量遵循某种概率分布，且可根据这些概率及已观察到的数据进行推理得到最优的决策</li><li>集成学习，从多个弱学习器中得到一个较好的学习器</li><li>计算学习理论，致力于回答“在什么养的条件下成功的学习是可能的》”和“在 什么条件下某个特定的学习算法可保证成功运行？”，从近似正确（PAC）和对假设空间的自然度量两个角度来分析学习算法，在出错界限的框架下考察一个学习器在确定正确假设前可能产生的训练错误数量</li><li>基于实例的学习，每当学习器遇到一个新的实例，通过分析这个例子和之前存储的实例关系来将这个目标函数赋值给新的实例</li><li>基于解释的学习，先验知识用于分析观察到的学习样例是怎样满足目标概念的，然后这个解释被用于区分训练样例中哪些是相关的特征、哪些是不相关的，这样就可以基于逻辑推理进行泛化，而不是基于统计推理</li><li>增强学习，用来确定最优控制策略</li></ul><h2 id="0x01-贝叶斯学习简介"><a href="#0x01-贝叶斯学习简介" class="headerlink" title="0x01 贝叶斯学习简介"></a>0x01 贝叶斯学习简介</h2><p>贝叶斯学习具有两个特点：第一，贝叶斯学习算法能够计算显始的假设概率，比如朴素贝叶斯分类器，是解决相应学习问题的最实际的方法之一；第二，为理解多数学习算法提供了一种有效的手段，这些算法不一定直接操纵概率数据</p><p>在机器学习中，通常最感兴趣是在给定训练数据D的时候，确定假设空间H中最可能的假设：</p><blockquote><p> 使用$P(h)$ 来代表没有训练数据前假设h拥有的概率，通常被称为prior probability（先验概率） $P(D)$ 代表将要观察的训练数据D的先验概率，也就是在没有某一假设成立时D的概率 $P(h|D)$ 代表的是看到训练数据D后h成立的置信度，这里是posterior probability（后验概率） $P(h|D)&#x3D;\frac{P(D|h)P(h)}{P(D)}$ 极大后验（maximum a posterior，MAP）：$max P(D|h)P(h)$ 极大似然估计（maximum likelihood，MLE）：$max P(D|h)$ </p></blockquote><p>example：考虑一个医疗诊断问题，这里是h是是否患有癌症，这里是的数据是检测结果是正例+还是负例-</p><p>我们知道人群中患有癌症的概率是0.008；也就是没有患有癌症的概率是0.992</p><p>同时我们知道对于患有癌症的患者返回检测结果，0.98是+、0.02是-</p><p>对于没有患有癌症的患者，返回检测结果，0.03是+，0.97-</p><p>如果现在来了一个病人的检测结果是+，那么患有癌症的概率是多少</p><p>$P(h&#x3D;患有癌症｜D&#x3D;检测结果+)&#x3D;\frac{P（D&#x3D;+｜h&#x3D;患有癌症）P（h&#x3D;患有癌症）}{p（D&#x3D;+|h&#x3D;患）p（h&#x3D;患）+p（D&#x3D;+｜h&#x3D;不患）p（不患）}&#x3D;0.21$</p><p>example02～example03:</p><ul><li>线性回归中极大似然和最小误差平方假设</li><li>逻辑回归中极大似然和交叉熵</li></ul><h2 id="0x02-贝叶斯分类器"><a href="#0x02-贝叶斯分类器" class="headerlink" title="0x02 贝叶斯分类器"></a>0x02 贝叶斯分类器</h2><p>基于一个简单而又强大的假设：在给定目标值事，属性值之间的相互条件独立，该假定说明在给定实例的目标值情况下，观察到的联合属性a1、a2、a3… 的概念等于单个属性的概率乘积： $$ P（a_1,a_2,…,a_n|h）&#x3D;\prod P(a_i|h) $$ example：</p><p>我们是数据集和知道了关于一个🍉是否好坏的颜色和大小的属性：</p><p>对一个红色，大的🍉判断是否好坏：</p><p>P（好）&#x3D;0.8</p><p>P（坏）&#x3D;0.2</p><p>P（红色｜好）&#x3D;0.8，</p><p>P（红色｜坏）&#x3D;0.1，</p><p>P（大｜好）&#x3D;0.9</p><p>P（大｜坏）&#x3D;0.4</p><p>P（好）<em>P（红色｜好）</em>P（大｜好）&#x3D;0.576</p><p>同理，观察到数据中坏的可能行&#x3D;0.008</p><p>所以为好🍉</p><p>半进阶，需要考虑到独依赖估计（semi- naive bayes classifiers）</p><p>进阶：贝叶斯信念网</p><p>留个坑：贝叶斯分类器的文本估计</p><h2 id="0x03-期望最大化算法（EM）"><a href="#0x03-期望最大化算法（EM）" class="headerlink" title="0x03 期望最大化算法（EM）"></a>0x03 期望最大化算法（EM）</h2><p>在讨论中，一直假设训练样本所有属性变量的值度可以被观测到，但是在实际中往往会遇到特征只有一部分可以观察到，EM算法可以用于变量从来没有被直接观察到的情况，只要这些变量所遵循的概率分布的一段形式</p><p>example：理解EM算法可以从一个实际的例子推导来得到，假设一个数据是从k个高斯分布混合而得到的，注意这个k未知，但是这个k个高斯分布的方差是$\sigma ^2$    知道，现在需要预测是k个均值$&lt;\mu_1,\mu_2,,,\mu_k&gt;$,我们需要先假设均值的值，在重新估计隐藏变量的值来重新计算极大似然假设</p><p>Step01:当前的假设为$&lt;\mu_1…,\mu_k&gt;$     的值，计算属于每个类别的期望值</p><p>$E(z&#x3D;k|D)&#x3D;p(x&#x3D;x_i|\mu&#x3D;\mu_i)&#x2F;P(x&#x3D;x_i)$</p><p>Step02: 计算一个新的极大似然假设，计算每个点属于每周类别的最大概率来更新均值，来替换称为新的假设</p><ul><li>理论推导</li></ul><p>对于n个样本观察数据${x^i}<em>{i&#x3D;1}^m$ 我们需要找到样本的模型参数$\theta$ 来最大对数似然函数，其中包含没有观察到的隐含数据Z $$ \bar \theta&#x3D;argmax \Sigma log p(x^i;\theta)\ \bar \theta&#x3D;argmax \Sigma log p(x^i,z^i;\theta)\ &#x3D;\bar \theta&#x3D;argmax \Sigma log \Sigma</em>{z^i}\ p(x_i,z^i;\theta)\ &#x3D;\bar \theta&#x3D;argmax \Sigma log Q_i(Z^i) \Sigma_{z^i}\ \frac{p(x^i,z^i;\theta)}{Q_i(z^i)}引入一个新的分布\</p><blockquote><p> &#x3D;\Sigma\Sigma Q(Z_i) *log\frac{p(x_i,z_i;\theta)}{Q(z_i)}根据Jensen不等式 $$ 我们有许多的选择，但是这样Jesen会在后面的取常数的时候手链 $$ \frac{p(x^i,z^i;\theta)}{Q_i(z^i)}&#x3D;c\ 同时\Sigma Q_i(Z^i)&#x3D;1 $$ 由此可以看出 $$ Q_i(z^i)&#x3D;p(x^i,z^i;\theta)&#x2F;\Sigma(p(x^i,z^i;\theta))&#x3D;p(z^i|x^i,\theta) $$ E: 计算$Q_i(Z^i)$ </p></blockquote><p>M: $argmax\Sigma\Sigma Q_i(Z^i)log(p(x^i,z^i;\theta)&#x2F;Q_i(Z^i))$</p><p><a href="https://chenrudan.github.io/blog/2015/12/02/emexample.html">EM算法的例子</a></p><p>更实际的例子：假设AB用相同的概率选择一个硬币，进行实验，每次都是独立的，请问如何估计两个硬币正面出现的概率？（a情况是知道选择硬币A还是B；b情况不知道选择硬币A还是B）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ae5acf3b4881e03e3b98761cd390af20_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>E：我们假设A的概率是0.6；假设B的概率是0.5，这里仅仅是初始化的值，同时根据观测的数据来计算期望的分布 $$ Q_i(Z^i&#x3D;A)&#x3D;P（Z&#x3D;A｜y_i,\bar\theta_A）&#x3D;\frac{P（Z&#x3D;A,y_i|\bar\theta_A）P(A)}{P（Z&#x3D;A,y_i|\bar\theta_A）P(A)+P（Z&#x3D;B,y_i|\bar\theta_A）P(B)}\ &#x3D;[0.45,0.80,0.73,0.35,0.65] $$ Q: 其中的Q函数形式为 $$ Q(θ,θi)&#x3D;∑j&#x3D;1N∑zP(z|yj,θi)logP(yj,z|θ)&#x3D;∑j&#x3D;1Nμjlog(θyjA(1−θA)10−yj)+(1−μj)log(θyjB(1−θB)10−yj)] Q ( θ , θ i ) &#x3D; ∑ j &#x3D; 1 N ∑ z P ( z | y j , θ i ) l o g P ( y j , z | θ ) &#x3D; ∑ j &#x3D; 1 N μ j l o g ( θ A y j ( 1 − θ A ) 10 − y j ) + ( 1 − μ j ) l o g ( θ B y j ( 1 − θ B ) 10 − y j ) ]</p><p>$$ 求导来得到更新的$\theta_A,\theta_B$</p><p>比如混合高斯模型GMM、K-means聚类的方法</p><h2 id="0x04-卡尔曼滤波"><a href="#0x04-卡尔曼滤波" class="headerlink" title="0x04 卡尔曼滤波"></a>0x04 卡尔曼滤波</h2><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5965c8e2746b9ecdee4563ac43bd0743_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CPLEX 安装指南</title>
    <link href="/posts/37cef8cc.html"/>
    <url>/posts/37cef8cc.html</url>
    
    <content type="html"><![CDATA[<p>本文主要介绍在Centos7上的cplex安装，以及在python3.7和python3.8中的pythonAPI的调用，⚠️Cplex是一个单独的软件cplex studio、在python中的库为cplex，同时在此基础上封装的更面向对象的有docplex</p><p>注意 不同版本对cplex的要求，12.10官方支持python3.6和3.7，也可以通过修改版本验证文件来在python3.8使用！</p><p>更新链接：<a href="https://drive.google.com/file/d/14nnCQiNPJ4tEejqj5uJ2seiPDyN4N3xJ/view?pli=1">https://drive.google.com/file/d/14nnCQiNPJ4tEejqj5uJ2seiPDyN4N3xJ/view?pli=1</a></p><h2 id="0x01简介"><a href="#0x01简介" class="headerlink" title="0x01简介"></a>0x01简介</h2><p>Cplex是IBM公司开发的一款商业版的优化引擎，主要分为：</p><ol><li>免费版（在python3.6～3.7可以直接pip安装）</li><li>付费版，价格比较贵</li><li>教育版，需要申请，但由于IBM并不重视网速较慢，因此可以用别人的安装包</li></ol><p>Cplex专门用于求解大规模的线性规划（LP）、二次规划（QP）、带约束的二次规划（QCQP）、二阶锥规划（SOCP）等四类基本问题，以及相应的混合整数规划（MIP）问题。</p><p>这里是官网的一些简介</p><p>IBM Docs<a href="http://www.ibm.com/docs/en/icos/12.8.0.0?topic=cplex-setting-up-python-api">www.ibm.com/docs/en/icos/12.8.0.0?topic=cplex-setting-up-python-api</a></p><h3 id="0x02-安装Cplex-studio-（centos7）"><a href="#0x02-安装Cplex-studio-（centos7）" class="headerlink" title="0x02 安装Cplex studio （centos7）"></a>0x02 安装Cplex studio （centos7）</h3><p>安装包：</p><p><a href="https://drive.google.com/file/d/14nnCQiNPJ4tEejqj5uJ2seiPDyN4N3xJ/view?usp=sharingdrive.google.com/file/d/14nnCQiNPJ4tEejqj5uJ2seiPDyN4N3xJ/view?usp=sharing">https://drive.google.com/file/d/14nnCQiNPJ4tEejqj5uJ2seiPDyN4N3xJ/view?usp=sharingdrive.google.com/file/d/14nnCQiNPJ4tEejqj5uJ2seiPDyN4N3xJ/view?usp=sharing</a></p><p>安装JDK（属于基础环节配置，网上有很多教程）</p><p>参考这份博客</p><p><a href="https://www.timberkito.com/?p=12">CentOS 7 安装JDK 1.8 环境教程</a></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-comment">#查看云端yum库中支持安装的JDK库</span><br>yum search <span class="hljs-keyword">java|grep </span><span class="hljs-keyword">jdk</span><br><span class="hljs-keyword"></span><span class="hljs-comment">#选择版本安装JDK</span><br>yum <span class="hljs-keyword">install </span>-y <span class="hljs-keyword">java-1.8.0-openjdk*</span><br><span class="hljs-keyword"></span><span class="hljs-comment">#安装完成后是否成功</span><br><span class="hljs-keyword">java </span>-version<br></code></pre></td></tr></table></figure><p>创建安装包的目录（推荐在&#x2F;tmp在存储，作为暂存文件）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建目录</span><br><span class="hljs-built_in">mkdir</span> cplex<br><span class="hljs-comment"># 先上传文件到这个目录中</span><br><span class="hljs-comment"># 设置权限</span><br><span class="hljs-built_in">chmod</span> u=rwx,g=rwx,o=rx cplex_studio1210.linux-x86-64.bin <br><span class="hljs-comment">#开始安装</span><br> ./cplex_studio1210.linux-x86-64.bin <br><span class="hljs-comment"># 1. 语言选择English</span><br><span class="hljs-comment"># 2. 软件介绍</span><br><span class="hljs-comment"># 3. 协议介绍</span><br><span class="hljs-comment"># 4. 安装路径，推荐，也可以自己设置</span><br>/toot/cplex/cplex<br><span class="hljs-comment"># 5. 安装python引擎？确认就完事了</span><br><span class="hljs-comment"># 6. 是否收集用户数据</span><br><span class="hljs-comment"># 7. 安装成功</span><br></code></pre></td></tr></table></figure><p>配置环境变量</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs elixir">vi  ~/.bashrc <br><span class="hljs-comment"># vim编辑器基本操作：insert插入、：wq保存退出</span><br><span class="hljs-comment"># 在最后一行加入下列</span><br>export <span class="hljs-title class_">PATH</span>=<br><span class="hljs-variable">$PATH</span><span class="hljs-symbol">:/root/cplex/cplex/cplex/bin/x86-</span><span class="hljs-number">64_</span>linux/<br><span class="hljs-symbol">:/root/cplex/cplex/cpoptimizer/bin/x86-</span><span class="hljs-number">64_</span>linux/<br><span class="hljs-comment"># 保存退出后，保存文件</span><br>source ~/.bashrc<br><span class="hljs-comment"># 测试是否安装成功</span><br>cplex -c read /root/cplex/cplex/cplex/examples/data/afiro.mps<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0ff286604f1bd4116bed2eade167df4f_1440w.png" alt="img"></p><p>成功✅</p><h2 id="0x03-python中的使用"><a href="#0x03-python中的使用" class="headerlink" title="0x03 python中的使用"></a>0x03 python中的使用</h2><p>按照官网的说法是在cplex的目录下面找到set up.pyl来安装</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-24a65f36c12eaed0cf5e988e3483c107_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> install<br><span class="hljs-keyword">python</span> setup.<span class="hljs-keyword">py</span> install --home yourPythonPackageshome/cplex<br></code></pre></td></tr></table></figure><p>出现一大堆，也没有报错，但是就是import不了，所以我们可以自己将cplex中已经生成的python的cplex包复制粘贴到我们需要对应的python的site-package下，conda下的</p><p>在Anaconda-envs-【conda name】-lib-【python-vesion】</p><p>可以将cplex中的cplex&#x2F;cplex&#x2F;cplex&#x2F;python&#x2F;python&#x2F;3.7&#x2F;[系统]&#x2F;cplex目录；复制到conda的python的site- package下。这次测试的是root&#x2F;anaconda3&#x2F;envs&#x2F;[conda name]&#x2F;lib&#x2F;python3.8&#x2F;site-package&#x2F;里就行了</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-966cc875ea7331ca473619556bff9619_1440w.png" alt="img"></p><p>官方支持python3.6、python3.7</p><p>但是并不支持python3.8，可以在cplex包里面找到内置函数&#x2F;_internal中找到&#x2F;_p ycplex_platform.py来修改，可能会有风险，更改需要谨慎</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ------------------------------------------------------------------------------</span><br><span class="hljs-comment"># Licensed Materials - Property of IBM</span><br><span class="hljs-comment"># 5725-A06 5725-A29 5724-Y48 5724-Y49 5724-Y54 5724-Y55 5655-Y21</span><br><span class="hljs-comment"># Copyright IBM Corporation 2008, 2019. All Rights Reserved.</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># US Government Users Restricted Rights - Use, duplication or</span><br><span class="hljs-comment"># disclosure restricted by GSA ADP Schedule Contract with</span><br><span class="hljs-comment"># IBM Corp.</span><br><span class="hljs-comment"># ------------------------------------------------------------------------------</span><br><span class="hljs-string">&quot;&quot;&quot;Imports the shared library on supported platforms.&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> platform<br><br><span class="hljs-keyword">from</span> sys <span class="hljs-keyword">import</span> version_info<br><br>ERROR_STRING = <span class="hljs-string">&quot;CPLEX 12.10.0.0 is not compatible with this version of Python.&quot;</span><br><br><span class="hljs-keyword">if</span> platform.system() <span class="hljs-keyword">in</span> (<span class="hljs-string">&#x27;Darwin&#x27;</span>, <span class="hljs-string">&#x27;Linux&#x27;</span>, <span class="hljs-string">&#x27;AIX&#x27;</span>, <span class="hljs-string">&#x27;Windows&#x27;</span>, <span class="hljs-string">&#x27;Microsoft&#x27;</span>):<br>    <span class="hljs-keyword">if</span> version_info &lt; (<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>):<br>        <span class="hljs-keyword">raise</span> Exception(ERROR_STRING)<br>    <span class="hljs-keyword">elif</span> version_info &lt; (<span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>):<br>        <span class="hljs-keyword">from</span> cplex._internal.py36_cplex12100 <span class="hljs-keyword">import</span> *<br>    <span class="hljs-keyword">elif</span> version_info &lt; (<span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">0</span>):<br>        <span class="hljs-keyword">from</span> cplex._internal.py37_cplex12100 <span class="hljs-keyword">import</span> *<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> Exception(ERROR_STRING)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;The CPLEX Python API is not supported on this platform.&quot;</span>)<br></code></pre></td></tr></table></figure><p>后续安装docplex</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> DOcplex<br></code></pre></td></tr></table></figure><h3 id="0x04-测试"><a href="#0x04-测试" class="headerlink" title="0x04 测试"></a>0x04 测试</h3><p>来简单的测试一下，求解这种问题</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b0cf308e581e156bb48c875a8548de14_1440w.png" alt="img"></p><p>求解模型</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs prolog">from docplex.mp.model import <span class="hljs-symbol">Model</span>  #导出库，只用这一个就够了<br>model = <span class="hljs-symbol">Model</span>() #创建模型<br>var_list = [i for i in range(<span class="hljs-number">0</span>, <span class="hljs-number">7</span>)] #创建列表<br><span class="hljs-symbol">X</span> = model.binary_var_list(var_list, lb=<span class="hljs-number">0</span>, name=<span class="hljs-string">&#x27;X&#x27;</span>) #创建变量列表<br>#设定目标函数<br>model.maximize(<span class="hljs-number">11</span>* <span class="hljs-symbol">X</span>[<span class="hljs-number">0</span>] + <span class="hljs-number">9</span> * <span class="hljs-symbol">X</span>[<span class="hljs-number">1</span>] + <span class="hljs-number">29</span> * <span class="hljs-symbol">X</span>[<span class="hljs-number">2</span>]+<span class="hljs-number">9</span>* <span class="hljs-symbol">X</span>[<span class="hljs-number">3</span>]+<span class="hljs-number">21</span>*<span class="hljs-symbol">X</span>[<span class="hljs-number">4</span>]+<span class="hljs-number">31</span>*<span class="hljs-symbol">X</span>[<span class="hljs-number">5</span>]+<span class="hljs-number">22</span>*<span class="hljs-symbol">X</span>[<span class="hljs-number">6</span>])  <br>#添加约束条件<br>model.add_constraint(<span class="hljs-symbol">X</span>[<span class="hljs-number">0</span>]+<span class="hljs-symbol">X</span>[<span class="hljs-number">1</span>]+<span class="hljs-symbol">X</span>[<span class="hljs-number">2</span>] &lt;= <span class="hljs-number">2</span>)<br>model.add_constraint(<span class="hljs-symbol">X</span>[<span class="hljs-number">3</span>] + <span class="hljs-symbol">X</span>[<span class="hljs-number">4</span>] &gt;=<span class="hljs-number">1</span>)<br>model.add_constraint(<span class="hljs-symbol">X</span>[<span class="hljs-number">5</span>] + <span class="hljs-symbol">X</span>[<span class="hljs-number">6</span>] &gt;=<span class="hljs-number">1</span>)<br>model.add_constraint(<span class="hljs-number">10</span>* <span class="hljs-symbol">X</span>[<span class="hljs-number">0</span>] + <span class="hljs-number">8</span>* <span class="hljs-symbol">X</span>[<span class="hljs-number">1</span>] + <span class="hljs-number">20</span> * <span class="hljs-symbol">X</span>[<span class="hljs-number">2</span>]+<span class="hljs-number">5</span>* <span class="hljs-symbol">X</span>[<span class="hljs-number">3</span>]+<span class="hljs-number">13</span>*<span class="hljs-symbol">X</span>[<span class="hljs-number">4</span>]+<span class="hljs-number">22</span>*<span class="hljs-symbol">X</span>[<span class="hljs-number">5</span>]+<span class="hljs-number">10</span>*<span class="hljs-symbol">X</span>[<span class="hljs-number">6</span>] &lt;=<span class="hljs-number">60</span>)<br>sol = model.solve() #求解模型<br>print(sol)  #打印结果<br></code></pre></td></tr></table></figure><p>结果</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">solution <span class="hljs-keyword">for</span>: docplex_model1<br>objective: 94<br><span class="hljs-attribute">X_0</span>=1<br><span class="hljs-attribute">X_3</span>=1<br><span class="hljs-attribute">X_4</span>=1<br><span class="hljs-attribute">X_5</span>=1<br><span class="hljs-attribute">X_6</span>=1<br></code></pre></td></tr></table></figure><p>over！！</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229 机器学习 常用模型总结 ｜ Vol6</title>
    <link href="/posts/b796995.html"/>
    <url>/posts/b796995.html</url>
    
    <content type="html"><![CDATA[<p>在机器学习看到一半的过程中，对于之前学习的广义线性模型、高斯判别分析、支持向量机、决策树、多层感知器进行阶段性的总结，并尝试使用过去课程中可能存在例子来进行展示，最后利用Kaggle的实验来进行展示</p><p>如有侵权，可以删除；如有错误，欢迎提出</p><h2 id="0x01基本模型"><a href="#0x01基本模型" class="headerlink" title="0x01基本模型"></a>0x01基本模型</h2><h3 id="1-1-GLM（logistic-regression、linear-regression、softmax-regression）"><a href="#1-1-GLM（logistic-regression、linear-regression、softmax-regression）" class="headerlink" title="1.1 GLM（logistic regression、linear regression、softmax regression）"></a>1.1 GLM（logistic regression、linear regression、softmax regression）</h3><p>Andrew Wu 第一部分–线性模型blog.tjdata.site&#x2F;2022&#x2F;01&#x2F;29&#x2F;Andrew-Wu-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B&#x2F;</p><blockquote><p> 吐槽： 明明是discriminate的模型，却叫广义线性模型（generative linear model）；明明是generative的模型，却叫高斯判别分析（gaussian discriminat analysis） </p></blockquote><p>广义线性模型中的步骤是，对于一群i.i.d（独立同分布）的数据集X，通过学习算法得到一个关于输出预测y的$P(\hat y|X)$,再每次输出中将期望值作为最终的输出，使得其的概率可以最大。其中做出三个基本假设</p><ol><li>～$𝑋～𝐸𝑥𝑝𝑜𝑛𝑒𝑛𝑡𝑖𝑎𝑙 𝐹𝑎𝑚𝑖𝑙𝑦(𝜂)&#x3D;𝑏(𝑦)𝑒𝑥𝑝(𝜂𝑇𝑇(𝑦)−𝑎(𝜂))$ </li><li>输出的预测值 $𝑦^&#x3D;ℎ𝜃(𝑥)&#x3D;𝐸(𝑦|𝑥)$ </li><li>由于是线性模型，最强的假设是 $𝜂&#x3D;𝜃𝑇𝑥(𝑖)$</li></ol><p>可以从三个例子来看出推导过程：</p><h3 id="example01-逻辑回归（y-「0，1」-ps：y-1-1-不一样）"><a href="#example01-逻辑回归（y-「0，1」-ps：y-1-1-不一样）" class="headerlink" title="example01:逻辑回归（y&#x3D;「0，1」,ps：y&#x3D;[-1,1]不一样）"></a>example01:逻辑回归（y&#x3D;「0，1」,ps：y&#x3D;[-1,1]不一样）</h3><blockquote><p> GLM分析： $ℎ𝜃(𝑥)&#x3D;𝐸(𝑦|𝑥,𝜃)$ ,这里的分布假设是Bernoulli（伯努利分布），因此 $ℎ𝜃(𝑥)&#x3D;𝜙&#x3D;1&#x2F;(1+𝑒−𝜂)&#x3D;1&#x2F;(1+𝑒−𝜃𝑇𝑥)$  </p></blockquote><p>数据集和假设 （）$（𝑥(𝑖),𝑦(𝑖)）,𝑦∈0,1,ℎ𝜃(𝑥)&#x3D;𝑔(𝑧),𝑔(𝑧)&#x3D;11+𝑒−𝑧$ </p><p>由此推导得到预测值的输出的损失函数: $𝑃(𝑦|𝑥)&#x3D;∏ℎ𝜃(𝑥)𝑦∗(1−ℎ𝜃(𝑥)1−𝑦$ </p><p>之后为了利用梯度下降Gradient Descent下降的方式需要计算  ,同时为了简便计算</p><h3 id="example02-线性回归"><a href="#example02-线性回归" class="headerlink" title="example02: 线性回归"></a>example02: 线性回归</h3><blockquote><p> GLM分析：  ,这里的分布假设是高斯分布   </p></blockquote><p>数据集和假设  </p><p>由此推导得到预测值的输出的损失函数:  </p><p>之后为了利用梯度下降Gradient Descent下降的方式需要计算  ,同时为了简便计算</p><p>等价于  </p><h3 id="example03-softmax-回归"><a href="#example03-softmax-回归" class="headerlink" title="example03: softmax 回归"></a>example03: softmax 回归</h3><p>GLM分析：</p><p> ,这里我们不知道具体的概率分布是什么，只能假设每一类的输出是$\phi_k$,得到概率分布函数</p><p>经过化简可以得到</p><p>同时</p><p>可以得到</p><p>因此对于</p><p>在得到分布函数的基础上，我们就可以计算期望值</p><p>损失函数就是我们最初定义的：</p><h3 id="1-2-GDA（Gaussian-Discriminat-Analysis）"><a href="#1-2-GDA（Gaussian-Discriminat-Analysis）" class="headerlink" title="1.2 GDA（Gaussian Discriminat  Analysis）"></a>1.2 GDA（Gaussian Discriminat  Analysis）</h3><p>与判别式模型（GLM）想比较$p(y|x)$，生成式模型generative用于生成$p(x|y)p(y)$来对每一种类别建模，寻找每个类别的特征，也就是比如输出多种类别，同时假设每个类别都是高斯分布，则可以计算两个类别的高斯分布参数，这样在得到输入可以直接使用分布函数来得到输出  计算就好了蛮</p><h3 id="1-3-SVM-Support-Vector-Machine"><a href="#1-3-SVM-Support-Vector-Machine" class="headerlink" title="1.3 SVM(Support Vector Machine)"></a>1.3 SVM(Support Vector Machine)</h3><p>支持向量机是最大边缘方法，允许把模型表示为训练实例的一个子集的影响之和，这些影响用面向应用的相似性核给出，采用：不要在解决实际问题之前把解决一个更复杂的问题作为第一步；在训练过程中希望找到的是那个最佳分离平面hyperplane：$w^Tx+b$中的两个参数</p><p>模型的输入是$(x^{(i)},y^{(i)}),y\in{-1,+1}$,希望找到的是可以讲数据集可以正确区分开的参数$w\ b$</p><p>Ps:为了简单起见，可以从线性可分的情况来谈论，再拓展到非线性可分的情况</p><p>通过定义functional margin和geometry margin，我们可以建立最符合直觉的模型为：  经过凸优化中的相关变化，使其变成可以求解的形式  对于这种类型的凸优化问题，可以采用拉格朗日方法或者其他方法直接进行求解；也可以将其转换为对偶问题dual problem，再利用拉格朗日算法或者其他方法来求解，⚠️我们希望求解的是w和b！！</p><p>对于这个函数求极值，就是希望</p><p> 等于0</p><p>将这个带入到L中，化简得到</p><p>构建对偶问题，等价于</p><p>其中需要满足KKT条件：</p><ol><li></li></ol><p>通过某种算法SMO可以很轻松的求解上述对偶问题，得到$\alpha_i$并根据KKT条件可以确定支持向量（support vector）,由此计算得到w和b，这样就可以判断正例还是反例</p><ul><li>SMO算法？以后才知道吧</li></ul><p>在线性不可分的情况，可以使用核机器（kernel function）来进行重新度量向量之间的非相似性</p><p>在间隔划分中，可以通过设置软间隔的方式来允许有部分噪声点发生</p><p>同时为了过拟合的风险，依旧可以增加模型复杂程度的项（term）</p><h3 id="1-4-CART（Classfication-and-regression-tree）"><a href="#1-4-CART（Classfication-and-regression-tree）" class="headerlink" title="1.4 CART（Classfication and regression tree）"></a>1.4 CART（Classfication and regression tree）</h3><h3 id="1-5-ANN（Artificial-Neural-Network）决策树CART1-5-ANN（Artificial-Neural-Network）"><a href="#1-5-ANN（Artificial-Neural-Network）决策树CART1-5-ANN（Artificial-Neural-Network）" class="headerlink" title="1.5 ANN（Artificial Neural Network）决策树CART1.5 ANN（Artificial Neural Network）"></a>1.5 ANN（Artificial Neural Network）<a href="https://blog.tjdata.site/2022/02/20/%E5%86%B3%E7%AD%96%E6%A0%91CART/">决策树CART</a>1.5 ANN（Artificial Neural Network）</h3><p>对于每个神经元包括两个部分，一个是接受前端的激活权重，另外一方面是自身的激活函数的选取</p><p>$线性连接：z^i&#x3D;w^ia^{i-1}+b^i$</p><p>$激活输出：a^i&#x3D;g(z^i)$</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4ce75c932592dde7ff5bffc6dc133f2f_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>假设对于这样的一个函数：</p><p>前向传递：</p><p>梯度下降来更新参数：</p><p>ps：解决问题要从源头一步一步解决</p><p>可以选取交叉熵作为误差函数</p><p>很amazing，在于输出的梯度下降可以和前一个节点的激活有关，如果使用内存把这些记录下来，可以大幅度的降低训练的时间，同时可以并行的计算一些参数</p><p>在定义好一个网络结果只是一个基础，还需要确定网络中的激活函数，定义好网络中训练的基本参数初始化、参数的随机选择、以及采用比较好的优化求解算法</p><p><strong>激活函数</strong></p><p>softmax、tanh、logistic、ReLu等等</p><p><strong>参数归一化</strong></p><p>min-max归一化、z-score、均值方差归一化</p><p><strong>参数初始化</strong></p><p>在网络层数变多的情况下，梯度会出现爆炸或者变小的情况，因此可以采用的随机化包括：均值、xvaier initiation、he initialization、np.random .randn(shaope)*np.sgrt(1&#x2F;n[z-1]) simoid 用1比较好、relu用2比较好等等不知名的参数初始化</p><p><strong>优化方法</strong></p><p>GD、SGD、BGD、动量算法、Adam、启发式算法（模拟退火、遗传算法）等等</p><p><strong>常见的网络</strong></p><ol><li>RBF（radial basis function，径向基网络）</li><li>ART（adaptive resonance theory，自适应谐振理论）</li><li>SOM（self- organizing map ，自组织映射）</li><li>CC（cascade- correlation，级联相关网络）</li><li>Elamn网络</li><li>Boltzmann机</li></ol><h2 id="0x02实验"><a href="#0x02实验" class="headerlink" title="0x02实验"></a>0x02实验</h2><p>一些比较简单的codebase</p><p><a href="https://github.com/chenxia31/Mercury/blob/master/summary_AndrewWu/GLM.ipynb">GLM</a></p><p><a href="https://github.com/chenxia31/Mercury/blob/master/summary_AndrewWu/iris_CART.ipynb">决策树CART在IRIS</a></p><p><a href="https://github.com/chenxia31/Mercury/blob/master/summary_AndrewWu/mnist_NN.ipynb">tensorflow实现的人工神经网络</a></p><p><a href="https://github.com/chenxia31/Mercury/blob/master/summary_AndrewWu/supportVM.ipynb">支持向量机</a></p><h2 id="0x03Kaggle-Titanic"><a href="#0x03Kaggle-Titanic" class="headerlink" title="0x03Kaggle- Titanic"></a>0x03Kaggle- Titanic</h2><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-de261fa1071e89f75768152454f4ff43_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>草率的花了一晚上了解了一下kaggle的流程，有点拉胯，一万多.容易让人想起来痛苦的交通信息检测处理的大作业</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs routeros">01 数据集的导入和基本信息<br>train数据集中包括的特征有： PassengerId：乘客的编号 Survived：是否存活 Pclass：票的类型，1是最好的、3是最差的 Name:性命 Sex:年龄 Age:年龄 SibSp:相伴的兄弟姐妹以及配偶 Parch：相伴的父母或者孩子 Ticker：乘客的票号 Fare:乘客的票价 Cabin：机舱号码 Embarked：下车地点<br>import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br>import seaborn as sns<br><span class="hljs-attribute">train_data</span>=pd.read_csv(&#x27;../titanic/train.csv&#x27;)<br><span class="hljs-attribute">test_data</span>=pd.read_csv(&#x27;../titanic/test.csv&#x27;)<br>train_data.head()<br><span class="hljs-comment"># 和交通数据中一样拿到数据需要考虑数据是否存在缺失、异常、或者是重复等等</span><br>train_data.<span class="hljs-built_in">info</span>()<br><span class="hljs-comment"># 可以看出数据中存在缺失的特征包括：Age、Cabin、Embarked</span><br><span class="hljs-comment">## 缺失值的修补</span><br><span class="hljs-comment">#Embarked 上船地点中缺少两个数，对于之后并没有太大的影响，可以使用众数进行修补</span><br>train_data.Embarked.fillna(<span class="hljs-attribute">value</span>=train_data.Embarked.dropna().mode().values[0])<br><span class="hljs-attribute">data_test</span>=test_data.Fare.fillna(value=test_data.Fare.dropna().mean())<br>test_data.loc[:,<span class="hljs-string">&#x27;Fare&#x27;</span>]=data_test<br><span class="hljs-comment"># 从直觉上来看，年龄对于之后是否坠船有相关的影响，因此在这里选择对应的非空特征进行绚练</span><br>feature_name=[<span class="hljs-string">&#x27;Pclass&#x27;</span>,<span class="hljs-string">&#x27;Sex&#x27;</span>, <span class="hljs-string">&#x27;SibSp&#x27;</span>, <span class="hljs-string">&#x27;Parch&#x27;</span>,<span class="hljs-string">&#x27;Fare&#x27;</span>,<span class="hljs-string">&#x27;Embarked&#x27;</span>,<span class="hljs-string">&#x27;Survived&#x27;</span>]<br><span class="hljs-comment"># cabin 的机场缺失太多，这里就不作为训练来</span><br><span class="hljs-comment"># 对性别进行编码</span><br><span class="hljs-attribute">train_data_copy</span>=train_data[feature_name]<br><span class="hljs-keyword">from</span> sklearn.preprocessing import LabelEncoder<br><span class="hljs-attribute">label</span>=LabelEncoder()<br><br>train_data_copy.loc[:,<span class="hljs-string">&#x27;Sex&#x27;</span>]=label.fit_transform(train_data_copy.Sex)<br>train_data_copy.loc[:,<span class="hljs-string">&#x27;Embarked&#x27;</span>]=label.fit_transform(train_data_copy.Embarked)<br><br>test_data.loc[:,<span class="hljs-string">&#x27;Sex&#x27;</span>]=label.fit_transform(test_data.Sex)<br>test_data.loc[:,<span class="hljs-string">&#x27;Embarked&#x27;</span>]=label.fit_transform(test_data.Embarked)<br><span class="hljs-comment">## 尝试使用SVM</span><br>import numpy as np<br><span class="hljs-keyword">from</span> sklearn.pipeline import Pipeline<br><span class="hljs-keyword">from</span> sklearn.preprocessing import StandardScaler<br><span class="hljs-keyword">from</span> sklearn.preprocessing import PolynomialFeatures<br><span class="hljs-keyword">from</span> sklearn.svm import LinearSVC<br><br><span class="hljs-attribute">p_s_c</span>=Pipeline([<br>    (<span class="hljs-string">&#x27;poly_features&#x27;</span>,PolynomialFeatures(<span class="hljs-attribute">degree</span>=3)),<br>    (<span class="hljs-string">&#x27;scaler&#x27;</span>,StandardScaler()),<br>    (<span class="hljs-string">&#x27;svm_clf&#x27;</span>,LinearSVC(<span class="hljs-attribute">C</span>=10,loss=&#x27;hinge&#x27;))<br>])<br>p_s_c.fit(train_data_copy.loc[:,(<span class="hljs-string">&#x27;Pclass&#x27;</span>,<span class="hljs-string">&#x27;Sex&#x27;</span>, <span class="hljs-string">&#x27;SibSp&#x27;</span>, <span class="hljs-string">&#x27;Parch&#x27;</span>,<span class="hljs-string">&#x27;Fare&#x27;</span>,<span class="hljs-string">&#x27;Embarked&#x27;</span>)],train_data_copy.iloc[:,-1])<br><span class="hljs-keyword">from</span> sklearn.metrics import accuracy_score<br><span class="hljs-attribute">result</span>=p_s_c.predict(test_data.loc[:,(<span class="hljs-string">&#x27;Pclass&#x27;</span>,<span class="hljs-string">&#x27;Sex&#x27;</span>, <span class="hljs-string">&#x27;SibSp&#x27;</span>, <span class="hljs-string">&#x27;Parch&#x27;</span>,<span class="hljs-string">&#x27;Fare&#x27;</span>,<span class="hljs-string">&#x27;Embarked&#x27;</span>)])<br><span class="hljs-attribute">submisstion</span>=pd.DataFrame(&#123;<br>    <span class="hljs-string">&#x27;PassengerId&#x27;</span>:test_data[<span class="hljs-string">&#x27;PassengerId&#x27;</span>],<br>    <span class="hljs-string">&#x27;Survived&#x27;</span>:result<br>&#125;)<br>submisstion.to_csv(<span class="hljs-string">&#x27;submission.csv&#x27;</span>,<span class="hljs-attribute">index</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django 新建项目全流程</title>
    <link href="/posts/c5f0b1c2.html"/>
    <url>/posts/c5f0b1c2.html</url>
    
    <content type="html"><![CDATA[<p>文本希望根据Django作为框架，前端利用Vue来实现个人博客网站的开发</p><p>作为一个外行人看专门的Django的书本会被一些基础知识所拖累，比如HTML中的一些基本概念，然而抛去细节，用自己的方式来完成一个比较难的（难度不会特别高，但是又会比官方教程学习）的多一些，这可能是本教程的意义在此</p><h2 id="0x00-基础知识"><a href="#0x00-基础知识" class="headerlink" title="0x00 基础知识"></a>0x00 基础知识</h2><p>网站上是后端服务器生成视频、文字、图片、数字等等数据，通过HTTPS或者各种网络通信协议来传递到前端浏览器中进行显示（B-S框架），这涉及三个方面：</p><ol><li>服务端（server）需要什么逻辑处理程序语言？需要什么数据存储语言？</li><li>之间通信如何安排？</li><li>前端显示的格式是什么？</li></ol><p>幸运的是，这里我们有一套标准，前端显示是HTML（html+css+js），在此基础上有也有一些框架如Vue、React来讲一些漂亮的格式固定化方便调用，通信中Django作为框架已经处理好了，所以我们用它就ok了！，服务端逻辑处理程序语言中python、C、Java等等都可以，重要的是要处理逻辑，这里使用python就好，数据管理可以使用MySQL</p><p>其中再拿出Django单独说，它不仅仅服务于网络通信，也可以帮助后台网站模版的组织、帮助利用python语言来控制服务库，同时将前后端开发中复杂的文件关系使用MVC模式</p><ol><li>model：处理与数据相关的业务（人话：可以用python语言来增删改查mysql，所以不用学mysql具体语句）</li><li>template：处理与表现相关的决定（也就是前端的网页是如何显示的）</li><li>view：存取模型及调用恰当模版的相关逻辑（也就是程序的本质，比如前端点击一个按钮，后端需要运算一段程序，来返回结果，这就是需要的工作）</li></ol><p>在此基础上，我们希望搭建自己的个人博客系统，<a href="https://www.dusaiphoto.com/article/4/">主要参考别人的文章</a>，分析个人博客的逻辑其实比较简单：</p><p>服务端存储博客的原始数据、前端中显示博客页面，使用view来实现一些功能，使用template来实现原始博客的美观并在前端生成，利用model来保存博客数据</p><p>在一个博客应用中，需要创建的逻辑包括</p><ul><li>博客首页、用来展示最近的几项内容</li><li>内容详情页，用来详细展示某项内容</li><li>评论处理器用来响应为一项内容添加评论的操作</li></ul><p><strong>无论做什么，都需要在coding之前认清楚自己想做什么，当然跟着别人的教程做不用考虑这些</strong></p><p>开发环境：python3.7+Django 2.2（centOS），利用conda安装环节</p><blockquote><p> conda create -n djangoblog python&#x3D;3.7 &#x2F;&#x2F; 用来生成python的环节，-n后面代表是djangoblog名称，后续从大 activate djangoblog就行了 conda acticate djangoblog Pip install django&#x3D;&#x3D;2.2 </p></blockquote><p>本文借鉴了很多网上的教程，若侵删</p><p><a href="https://www.dusaiphoto.com/article/2/">Django搭建个人博客-杜塞</a></p><p><a href="https://www.liaoxuefeng.com/">廖雪峰个人网站</a></p><p><a href="https://www.runoob.com/">菜鸟教程</a></p><p><a href="https://www.w3school.com.cn/">W3School</a></p><p><a href="https://docs.djangoproject.com/zh-hans/4.0/">Django文档</a></p><h2 id="0x01-使用Django"><a href="#0x01-使用Django" class="headerlink" title="0x01 使用Django"></a>0x01 使用Django</h2><h3 id="1-1-创建APP"><a href="#1-1-创建APP" class="headerlink" title="1.1 创建APP"></a>1.1 创建APP</h3><blockquote><p> &#x2F;&#x2F; cd到对应的目录，并激活对应的conda django-admin startproject my_blog </p></blockquote><p>由此创建的网站文件夹(由于是Linux系统没有db.sqlite3,奇怪)</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-2e194338e53234947f6167f9feaea430_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>之后</p><blockquote><p> cd my_blog python manage.py runserver 0.0.0.0:80 &#x2F;&#x2F; 后面是设置ip地址和域名，当然可以不设置 </p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ead7b595b7d8f101ff1a282e35205cf7_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>然后创建APP，在Django中每个app代表一个功能模块，开发者可以将不同功能的模块放到不同的app中，方便代码的复用，app就是项目的基石，因此开发博客的第一步就是创建新的app</p><blockquote><p> python manage.py startapp article </p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3fcc544366ed58a481d28722ad30372e_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>每个文件夹的简单功能；</p><ul><li>&#x2F;my_blog&#x2F;db.sqllite3 是轻量级的数据库文件；&#x2F;my_blog&#x2F;manage.py 是项目执行文件的入口</li><li>&#x2F;my_blog&#x2F;article 是 刚创建的app，用来防治博客文章相关的代码，后台管理文件admin、数据模型文件models、视图文件view、存放数据迁移文件migrations</li><li>&#x2F;my_blog&#x2F;my_blog 其中setting.py包含项目的配置参数，urls.py 包含项目的根路由文件</li></ul><p>新建app需要做的简单的事情</p><ol><li>注册app</li></ol><p>在setting里面添加就行</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-37f94e4088422bf2b6753ae320976502_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><ol><li>修改项目的中的urls.py，这里将根路由转发到app的路由（这里转发到article的urls）这里的路由配置比较复杂，后续需要深入了解</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-85c2d74429a077d836d5d1440b7dfd3a_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><blockquote><p> 来看path中相关的参数 article&#x2F; 表示app的访问路径 include 将路径分发到下一步处理 namespace 保证反查到唯一的url </p></blockquote><ol><li>修改app中的urls</li></ol><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">from</span> django.urls <span class="hljs-keyword">import</span> path<br><br># 正在部署的应用的名称<br>app_name = <span class="hljs-string">&#x27;article&#x27;</span><br><br>urlpatterns = [<br>    # 目前还没有urls<br>]<br></code></pre></td></tr></table></figure><p>这里暂时还是空的</p><h3 id="1-2-创建model"><a href="#1-2-创建model" class="headerlink" title="1.2 创建model"></a>1.2 创建model</h3><p>数据库的基本概念，数据库是存储电子文件的场所，存储单独的数据集合，一个数据库由多个数据表构成，在数据库中通常使用标准的SQL语句来进行增删改查，这里是一个新的内容，在面对新的内容总会出现一定的困难，因此：</p><p>Django使用对象关系映射（object relational mapping，ORM）来实现面向对象编程语言中不同类型系统的数据之间的转换</p><p><strong>意味着不要学习如何使用SQL，学会如何配置models.py就行</strong></p><p>对于博客后台的数据库中，我们设想如何描述文章这个对象“</p><blockquote><p> 需要有文章作者author、需要有文章标题title、需要有文章正文body、需要有文章的创建时间created、需要有文章更新时间update，这里的数据类型由Django自己定义，这里每个模型都被表示为Django.db.models.Model的类，需要用法可以直接去查 </p></blockquote><p>Django本身具有一个简单的user账号系统，因此这里定义foreignkey可以包含与外部的环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Create your models here.</span><br><span class="hljs-keyword">from</span> django.db <span class="hljs-keyword">import</span> models<br><span class="hljs-comment"># 导入内建的User模型。</span><br><span class="hljs-keyword">from</span> django.contrib.auth.models <span class="hljs-keyword">import</span> User<br><span class="hljs-comment"># timezone 用于处理时间相关事务。</span><br><span class="hljs-keyword">from</span> django.utils <span class="hljs-keyword">import</span> timezone<br><br><span class="hljs-comment"># 博客文章数据模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ArticlePost</span>(models.Model):<br>    <span class="hljs-comment"># 文章作者。参数 on_delete 用于指定数据删除的方式</span><br>    author = models.ForeignKey(User, on_delete=models.CASCADE)<br><br>    <span class="hljs-comment"># 文章标题。models.CharField 为字符串字段，用于保存较短的字符串，比如标题</span><br>    title = models.CharField(max_length=<span class="hljs-number">100</span>)<br><br>    <span class="hljs-comment"># 文章正文。保存大量文本使用 TextField</span><br>    body = models.TextField()<br><br>    <span class="hljs-comment"># 文章创建时间。参数 default=timezone.now 指定其在创建数据时将默认写入当前的时间</span><br>    created = models.DateTimeField(default=timezone.now)<br><br>    <span class="hljs-comment"># 文章更新时间。参数 auto_now=True 指定每次数据更新时自动写入当前时间</span><br>    updated = models.DateTimeField(auto_now=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 内部类 class Meta 用于给 model 定义元数据</span><br>    <span class="hljs-keyword">class</span> <span class="hljs-title class_">Meta</span>:<br>        <span class="hljs-comment"># ordering 指定模型返回的数据的排列顺序</span><br>        <span class="hljs-comment"># &#x27;-created&#x27; 表明数据应该以倒序排列</span><br>        ordering = (<span class="hljs-string">&#x27;-created&#x27;</span>,)<br><br>    <span class="hljs-comment"># 函数 __str__ 定义当调用对象的 str() 方法时的返回值内容</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__str__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># return self.title 将文章标题返回</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.title<br></code></pre></td></tr></table></figure><p>ForeignKey：</p><blockquote><p> 数据库中多种多样的数据表，有时候数据表的数据是相互关联的，因此这两张表之间就产生了关系，外健就是用来表示这种关系的，（本文中user由Django自己生成），当然也有一对多onetoonefield、多对多manytomanyfield </p></blockquote><p>内部类:</p><blockquote><p> 内部类class-meta提供模型的元数据，任何不是字段的东西，比如ordering、数据库名称、单数和复数名称 </p></blockquote><p>创建数据库之后需要数据迁移，迁移时Django模型所做的更改传递到数据库中的方式，Django的前一代码时由模型文件自动生成，本质上是一个历史记录，可以勇气可嘉进行数据库的滚动更新，通过这种方式使其能够和当前的模型匹配</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> manage.<span class="hljs-keyword">py</span> makemigrations<br></code></pre></td></tr></table></figure><h3 id="1-3-编写view"><a href="#1-3-编写view" class="headerlink" title="1.3 编写view"></a>1.3 编写view</h3><p>Django中试图的概念是一类具有相同功能和模版的网页的集合，通常需要使用通过view来处理前端的请求，在数据库中增删改查来返回给前端显示，写好一个view函数后，还需要配置路由来讲用户请求和视图链接起来</p><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><p>在article&#x2F;view.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入 HttpResponse 模块</span><br><span class="hljs-keyword">from</span> django.http <span class="hljs-keyword">import</span> HttpResponse<br><br><span class="hljs-comment"># 视图函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">article_list</span>(<span class="hljs-params">request</span>):<br>    <span class="hljs-keyword">return</span> HttpResponse(<span class="hljs-string">&quot;Hello World!&quot;</span>)<br></code></pre></td></tr></table></figure><p>之后配置路由URL</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">from</span> django.urls <span class="hljs-keyword">import</span> <span class="hljs-type">path</span><br><span class="hljs-keyword">from</span> . <span class="hljs-keyword">import</span> views<br><br># 正在部署的应用的名称<br>app_name = <span class="hljs-string">&#x27;article&#x27;</span><br><br>urlpatterns = [<br>    # <span class="hljs-type">path</span>函数将url映射到视图<br>    path(<span class="hljs-string">&#x27;article-list/&#x27;</span>, views.article_list, <span class="hljs-type">name</span>=<span class="hljs-string">&#x27;article_list&#x27;</span>),<br>]<br></code></pre></td></tr></table></figure><p>在浏览器输入</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">http:<span class="hljs-regexp">//</span><span class="hljs-number">101.43</span>.<span class="hljs-number">56.17</span><span class="hljs-regexp">/article/</span>article-list/<br></code></pre></td></tr></table></figure><h3 id="1-4-view和model之间的连接"><a href="#1-4-view和model之间的连接" class="headerlink" title="1.4 view和model之间的连接"></a>1.4 view和model之间的连接</h3><p>Model中虽然定义了数据表，但是这个表是空的，不方便展示view调取数据的效果，因此需要向数据表中记录一些数据</p><ol><li>创建管理员账号superuser</li></ol><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> manage.<span class="hljs-keyword">py</span> createsuperuser<br></code></pre></td></tr></table></figure><ol><li>将articlepost（之前数据表）注册到后台</li></ol><p>在admin.py中</p><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs capnproto"><span class="hljs-keyword">from</span> django.contrib <span class="hljs-keyword">import</span> admin<br><br><span class="hljs-comment"># Register your models here.</span><br><span class="hljs-comment"># 别忘了导入ArticlerPost</span><br><span class="hljs-keyword">from</span> .models <span class="hljs-keyword">import</span> ArticlePost<br><br><span class="hljs-comment"># 注册ArticlePost到admin中</span><br>admin.site.register(ArticlePost)<br></code></pre></td></tr></table></figure><p>这样就可以在后台自己上传文章啦（如何支持markdown语法？？）</p><h3 id="1-5-利用view"><a href="#1-5-利用view" class="headerlink" title="1. 5 利用view"></a>1. 5 利用view</h3><p>上述我们可以增加model中的数据，后续需要增加将数据库的数据显示在前端是view的工作，如何美观的现实是template的事情</p><p>在article&#x2F;views.py内</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.shortcuts <span class="hljs-keyword">import</span> render<br><br><span class="hljs-comment"># Create your views here.</span><br><span class="hljs-keyword">from</span> django.http <span class="hljs-keyword">import</span> HttpResponse<br><br><span class="hljs-comment"># 导入数据模型ArticlePost</span><br><span class="hljs-keyword">from</span> .models <span class="hljs-keyword">import</span> ArticlePost<br><br><span class="hljs-comment"># 视图函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">article_list</span>(<span class="hljs-params">request</span>):<br>    <span class="hljs-comment"># 取出所有博客文章</span><br>    articles = ArticlePost.objects.<span class="hljs-built_in">all</span>()<br>    <span class="hljs-comment"># 需要传递给模板（templates）的对象</span><br>    context = &#123; <span class="hljs-string">&#x27;articles&#x27;</span>: articles &#125;<br>    <span class="hljs-comment"># render函数：载入模板，并返回context对象</span><br>    <span class="hljs-keyword">return</span> render(request, <span class="hljs-string">&#x27;article/list.html&#x27;</span>, context)<br></code></pre></td></tr></table></figure><p>之后配置模版存放的路径，在setting文件夹内</p><p>`&#96;&#96;python  &#x2F;&#x2F; 55行 TEMPLATES &#x3D; [     {         ‘BACKEND’: ‘django.template.backends.django.DjangoTemplates’,         ‘DIRS’: [os.path.join(BASE_DIR, ‘templates’)],         ‘APP_DIRS’: True,         ‘OPTIONS’: {             ‘context_processors’: [                 ‘django.template.context_processors.debug’,                 ‘django.template.context_processors.request’,                 ‘django.contrib.auth.context_processors.auth’,                 ‘django.contrib.messages.context_processors.messages’,             ],         },     }, ]</p><figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="language-xml">之后在模版中list.html中加入</span><br><span class="language-xml"></span><br><span class="language-xml">```html</span><br><span class="language-xml"></span><span class="hljs-template-tag">&#123;% <span class="hljs-name"><span class="hljs-name">for</span></span> article <span class="hljs-keyword">in</span> articles %&#125;</span><span class="language-xml"></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">p</span>&gt;</span></span><span class="hljs-template-variable">&#123;&#123; article.title &#125;&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span></span><br><span class="language-xml"></span><span class="hljs-template-tag">&#123;% <span class="hljs-name"><span class="hljs-name">endfor</span></span> %&#125;</span><br></code></pre></td></tr></table></figure><p>这样就可以查看到生成了最新的模版</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-8d6fb67de4b0b5b13be636f0795ca5f5_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="阶段总结"><a href="#阶段总结" class="headerlink" title="阶段总结"></a>阶段总结</h2><p>在上述走过了Django的model数据库的建立和admin管理、view函数对前端的请求响应、templates中关于模版的使用，完整的学习了Django的MVT模式，在之后还需要改进的地方有：</p><ol><li>数据层面，数据库的建立。整个数据库系统中多个表格的使用，model建立过程中关键字的定义，变量类型的定义</li><li>view中数据逻辑，如果是个人博客系统需要考虑到网站整体的需求，然后在view函数中进行解析，需要设计具体的功能和具体功能的流程，最后反映到编程之中，比如页面的排名、页面的详细信息、页面的浏览等等</li><li>模版和网页前端的交互，模版中规定了基础的Django数据的传输方式，还有网页中通常使用的Ajax通信来传递相关信息，以及如何使用脚手架来使得前端页面更加好看，利用HTML中特殊属性来添加元素、利用CSS来规定元素的格式、利用JS来设置页面的动态效果</li><li>整体管理，包括路由配置、包括实际部署、包括变量的管理等等</li></ol>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Django</tag>
      
      <tag>网站开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 实验设计 ｜ Vol5</title>
    <link href="/posts/403016db.html"/>
    <url>/posts/403016db.html</url>
    
    <content type="html"><![CDATA[<p>非常喜欢《机器学习导论》中关于感知器的一句话：“神经网络的目的不是为了研究人脑的内部结构和运行机理，而仅仅是模仿来作为一个学习罢了”，通过这句话可以看出机器学习作为一门工程类的学科，本质目的还是为了寻找一种好的方法，在此过程中就要思考需要用什么样的标准来评价好坏？这是一个曾经观察高中物理、高中化学、高中生物中的实验科学，更进一步，我们更有着统计的知识来指导我们更近一步地正确设计实验。</p><h2 id="0x01-引言"><a href="#0x01-引言" class="headerlink" title="0x01 引言"></a>0x01 引言</h2><p>机器学习中，实现一个目标中可以有很多中不同的学习算法，来么：</p><ol><li>如何评估一个学习算法在给定问题上的期望误差？</li><li>对于同一个目标的不同学习算法，我们如何证明一个算法比另外一个算法误差更低？</li></ol><p>我们不能只看训练集上的误差来作为判断，因为具有更复杂度的模型的误差总是比简单模型小</p><p>同样在不同与训练及的验证集中一轮运行也不够，因为可能会有噪声和离群点的影响，同样具有超参数的算法的泛化过程中会有这其他的随机因素。我们通过运行learning algorithm来得到一个leaner；如果我们只训练一次只得到一个leaner和一个validation error，为了平均各种随机性，我们需要产生多种leaner，进而在多个验证集上评估，我们对learning algorithm的评估是应该是基于这些validation error的distribution来的，通过评估expected error或者是variance来进行比较。</p><p>总结上述，可以看出我们的目标是规划和设计机器学习实验，来分析实验产生的数据，以便能够排除随机性的影响，得到统计显著的结论，在机器学习中，我们的目标是得到具有最高的泛化准确率、最小复杂度的学习器，并且该学习器是robust</p><p>我们在实验设计中需要设计的3项基本原则：</p><ul><li>randomization随机化，要求实验运行的次序应该是随机确定的，这样使得结果是独立的</li><li>replication重复，因为这应该对可控因素的相同配置多次实验，以便平均不可控因素的影响，通常在相同数据集上再抽样的版本上叫做交叉验证（cross- validation）</li><li>blocking阻止，用来降低或消除有害因素导致的可变性，准确率的差异不仅取决于不同的算法还取决于不同的子集，因此为了度量仅由算法导致的差别，重复运行的不同训练集应该是相同的，在统计中常有配对检验（pairing test）</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4d9a47736089ae08272021597657db5a_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="0x02-实验设计的基本策略-strategy-of-experimentation"><a href="#0x02-实验设计的基本策略-strategy-of-experimentation" class="headerlink" title="0x02 实验设计的基本策略 strategy of experimentation"></a>0x02 实验设计的基本策略 strategy of experimentation</h2><p>这里感觉更多是关于如何调参的选择，如何设计对比出不同算法优劣，比如消融实验之类的还是依靠实践</p><h3 id="最佳猜测-best-guess"><a href="#最佳猜测-best-guess" class="headerlink" title="最佳猜测 best- guess"></a>最佳猜测 best- guess</h3><p>从某个我们相信是好配置的因素设置开始，在此检验响应之前，每次稍微改动一个或者少量因素，来检验每个组合指导得到一个足够好的状态</p><h3 id="一次一个因素-one-factor-at-a-tiem"><a href="#一次一个因素-one-factor-at-a-tiem" class="headerlink" title="一次一个因素 one factor at a tiem"></a>一次一个因素 one factor at a tiem</h3><p>我们为每个因素确定一个基线的抹额认知，然后对一个因素尝试不同水平而令其他因素保持在极限上，这种方法的缺点是假设因素之间不相互影响</p><h3 id="因素设计-factorial-design"><a href="#因素设计-factorial-design" class="headerlink" title="因素设计 factorial design"></a>因素设计 factorial design</h3><p>通常也被称为网格搜索grid search，其中因素一起变化而不是单因素变化</p><h3 id="响应面设计-response-surface-design"><a href="#响应面设计-response-surface-design" class="headerlink" title="响应面设计 response surface design"></a>响应面设计 response surface design</h3><p>通过设计合理的有限次数试验，建立一个包括各显著因素的一次项、平方项和任何两个因素之间的一级交互作用项的数学模型，精确研究各因素与响应值之间的关系，快速确定多因素系统的最佳条件，常用具体方法有：中心复合试验设计（central composite design CCD）和Box-Behnken试验设计（DDB）</p><h2 id="0x03-模型选择–重复样本"><a href="#0x03-模型选择–重复样本" class="headerlink" title="0x03 模型选择–重复样本"></a>0x03 模型选择–重复样本</h2><blockquote><p> hold-out 通常对获得数据集之后，会将其划分出来一部分不会使用的test set；在训练过程中的会用到的有train set和validation set </p></blockquote><p>通常train set和validation set之间的比例可以为10或者30，但是不幸的是样本通常没有这么多，因此我们应该在小数据集上尽力而为，其方法是以不同划分来重复使用它，这种称为交叉验证cross- validation，但是潜在的问题有<strong>交叉验证使得错误率是相互依赖的，因为这些不同集合共享数据</strong></p><p>在重复爱漾中要尽可能的保持分层stratification，我们希望保证误差估计的鲁棒性，同时要保持不同集合间的重叠尽可能的小，还不能扰乱类的先验概率</p><h3 id="3-1-K-fold-CV"><a href="#3-1-K-fold-CV" class="headerlink" title="3.1 K- fold CV"></a>3.1 K- fold CV</h3><p>将训练的书籍划分为K等分，将K份数据集中的一份划分为验证集，其他作为验证集，在较为极端的医疗中可能会有leave-one-out</p><h3 id="3-2-5x2-CV"><a href="#3-2-5x2-CV" class="headerlink" title="3.2 5x2 CV"></a>3.2 5x2 CV</h3><p>很神奇而让人不想去了解</p><h3 id="3-3-bootstrapping（自助法）"><a href="#3-3-bootstrapping（自助法）" class="headerlink" title="3.3 bootstrapping（自助法）"></a>3.3 bootstrapping（自助法）</h3><p>采用从原始样本中以有放回地抽取实例的方法来产生新的样本，自助样本可能比交叉验证有更多的重叠，因而其估计可能更相互依赖 $$ （1-\frac{1}{N})^N&#x3D;1&#x2F;e&#x3D;0.368 $$ 意味训练集中包含63.2%的实例，将原始数据集作为验证集</p><h2 id="0x04-模型选择–性能度量"><a href="#0x04-模型选择–性能度量" class="headerlink" title="0x04 模型选择–性能度量"></a>0x04 模型选择–性能度量</h2><h3 id="4-1-分类"><a href="#4-1-分类" class="headerlink" title="4.1 分类"></a>4.1 分类</h3><p>二分类的混淆矩阵confusion matrix</p><p>|              | 预测positive | 预测negative |      | | ———— | ———— | ———— | —- | | 正例positive | TP           | FN           |      | | 负例negative | FP           | TN           |      |</p><p>$$ 误差：(FP+FN)&#x2F;N\ 准确率：(TP+TN)&#x2F;N\ 精度：TP&#x2F;（TP+FP）\ 召回率：TP&#x2F;（TP+FN）\ 灵敏度：召回率\ 特效性：TN&#x2F;fp+tn\ tp-rate:tp&#x2F;p\ fp-rate&#x3D;fp&#x2F;(tn+fp) $$</p><blockquote><p> 关于假正和假负，例如一个用户希望登陆另外一个用户的账户，假正是允许顶替，假负是正常用户登陆不了，显然前者危害会更高 tp-rate：衡量通过身份认证合法用户的比例 FP-rate：错误接受顶替的比例 </p></blockquote><p>（tp-rate，fp-rate）得到ROC（receiver operating characteristic），该曲线下面的（area under the curve AUC）可以来比较不同损失条件下平均的整体性能</p><p>可以参考科技文献检索中的查全率和查准率</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0347a0ef74be7698d93aefaeb487333d_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>class confusion matrix</p><h3 id="4-2-回归"><a href="#4-2-回归" class="headerlink" title="4.2 回归"></a>4.2 回归</h3><p>mean square error</p><p>RMSE之类的</p><h2 id="0x05-模型选择-–-结果可靠性（看不懂，统计功底不够）"><a href="#0x05-模型选择-–-结果可靠性（看不懂，统计功底不够）" class="headerlink" title="0x05 模型选择 – 结果可靠性（看不懂，统计功底不够）"></a>0x05 模型选择 – 结果可靠性（看不懂，统计功底不够）</h2><h3 id="5-1-区间估计-interval-estimation"><a href="#5-1-区间估计-interval-estimation" class="headerlink" title="5.1 区间估计 interval estimation"></a>5.1 区间估计 interval estimation</h3><p>《概率论》</p><h3 id="5-2-假设检验-hypothesis-testing"><a href="#5-2-假设检验-hypothesis-testing" class="headerlink" title="5.2 假设检验 hypothesis testing"></a>5.2 假设检验 hypothesis testing</h3><p>《概率论和数理统计》</p><h3 id="5-3-结合评估性能"><a href="#5-3-结合评估性能" class="headerlink" title="5.3 结合评估性能"></a>5.3 结合评估性能</h3><p>一下用于对分类问题中的错误率进行分析，同样可以适用于回归的就放误差、非监督学习的对数似然、增强学习的期望奖励等</p><h3 id="单个学习器的错误率衡量"><a href="#单个学习器的错误率衡量" class="headerlink" title="单个学习器的错误率衡量"></a>单个学习器的错误率衡量</h3><p>二项检验</p><p>近似正态检验</p><p>t检验（K-fold）</p><h3 id="比较两个分类算法错误率"><a href="#比较两个分类算法错误率" class="headerlink" title="比较两个分类算法错误率"></a>比较两个分类算法错误率</h3><p>McNemar检验</p><p>K折+t检验</p><h3 id="比较多个分类的错误率同时分析方差"><a href="#比较多个分类的错误率同时分析方差" class="headerlink" title="比较多个分类的错误率同时分析方差"></a>比较多个分类的错误率同时分析方差</h3><h3 id="在多个数据集上比较多个算法的错误率"><a href="#在多个数据集上比较多个算法的错误率" class="headerlink" title="在多个数据集上比较多个算法的错误率"></a>在多个数据集上比较多个算法的错误率</h3><h3 id="比较多个算法的不同指标"><a href="#比较多个算法的不同指标" class="headerlink" title="比较多个算法的不同指标"></a>比较多个算法的不同指标</h3>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 决策树 ｜ Vol4</title>
    <link href="/posts/fad3d23f.html"/>
    <url>/posts/fad3d23f.html</url>
    
    <content type="html"><![CDATA[<p>之前《机器学习》选修课被大佬手撕决策树算法给震撼了，因为之前从来没有尝试过数学和代码的角度来看待算法，不过寒假静下来，可以看出作为基模型运用到很多递归、贪心中的思想，写出来发现难度并没有那么大！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> log<br><span class="hljs-keyword">import</span> operator<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">decisionTree</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">dataLoader</span>(<span class="hljs-params">self,filename</span>):<br>        dataSet = [];<br>        labels = []<br>        i=<span class="hljs-number">1</span><br>        fr = <span class="hljs-built_in">open</span>(filename)<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr.readlines():  <span class="hljs-comment"># 逐行读取，滤除空格等</span><br>            lineArr = line.strip().split(<span class="hljs-string">&#x27;,&#x27;</span>)<br>            <span class="hljs-keyword">if</span> i==<span class="hljs-number">1</span>:<br>                labels=lineArr[<span class="hljs-number">1</span>:(<span class="hljs-built_in">len</span>(lineArr)-<span class="hljs-number">1</span>)]<br>                i=i+<span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                dataSet.append(lineArr[<span class="hljs-number">1</span>:])<br>        <span class="hljs-keyword">return</span> dataSet,labels<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">createExampleDataset</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        返回示例数据，包括dataset、labels</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        dataSet = [[<span class="hljs-string">&#x27;白&#x27;</span>, <span class="hljs-string">&#x27;高&#x27;</span>, <span class="hljs-string">&#x27;男&#x27;</span>],<br>                   [<span class="hljs-string">&#x27;黑&#x27;</span>, <span class="hljs-string">&#x27;高&#x27;</span>, <span class="hljs-string">&#x27;男&#x27;</span>],<br>                   [<span class="hljs-string">&#x27;白&#x27;</span>, <span class="hljs-string">&#x27;高&#x27;</span>, <span class="hljs-string">&#x27;男&#x27;</span>],<br>                   [<span class="hljs-string">&#x27;黑&#x27;</span>, <span class="hljs-string">&#x27;低&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>],<br>                   [<span class="hljs-string">&#x27;黑&#x27;</span>, <span class="hljs-string">&#x27;低&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>],<br>                   [<span class="hljs-string">&#x27;黑&#x27;</span>, <span class="hljs-string">&#x27;高&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>],<br>                   [<span class="hljs-string">&#x27;白&#x27;</span>, <span class="hljs-string">&#x27;低&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>],<br>                   [<span class="hljs-string">&#x27;黑&#x27;</span>, <span class="hljs-string">&#x27;高&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>]]<br>        labels = [<span class="hljs-string">&#x27;颜色&#x27;</span>, <span class="hljs-string">&#x27;身高&#x27;</span>]  <span class="hljs-comment"># 两个特征</span><br>        <span class="hljs-keyword">return</span> dataSet, labels<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">calcCrossEntropy</span>(<span class="hljs-params">self, dataSet</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        计算数据集合中的数据熵</span><br><span class="hljs-string">        return crossEntropy</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        classList = [example[-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> dataSet]<br>        unique,counts=np.unique(classList,return_counts=<span class="hljs-literal">True</span>)<br>        crossEntropy=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> counts:<br>            crossEntropy+=-<span class="hljs-built_in">float</span>(count/<span class="hljs-built_in">len</span>(classList))*log(<span class="hljs-built_in">float</span>(count/<span class="hljs-built_in">len</span>(classList)),<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> crossEntropy<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">chooseBestFeature</span>(<span class="hljs-params">self, dataSet</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        从不同特征中选择交叉熵差值最低的作为分离特征</span><br><span class="hljs-string">        return bestFeature下标</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        bestInfoGain=-<span class="hljs-number">1</span><br>        baseCrossEntropy=<span class="hljs-variable language_">self</span>.calcCrossEntropy(dataSet)<br>        numFeature=<span class="hljs-built_in">len</span>(dataSet[<span class="hljs-number">0</span>])-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(numFeature):<br>        <span class="hljs-comment"># 根据不同特征分开，并重新计算根据每个特征得到的交叉熵的和</span><br>            featureList=<span class="hljs-built_in">set</span>([example[i] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> dataSet])<br>            newCrossEntropy=<span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> featureList:<br>                subDataSet=<span class="hljs-variable language_">self</span>.splitDataSet(dataSet,i,value)<br><br>                newCrossEntropy+=<span class="hljs-built_in">len</span>(subDataSet)/<span class="hljs-built_in">len</span>(dataSet)*<span class="hljs-variable language_">self</span>.calcCrossEntropy(subDataSet)<br><br>            infoGain=baseCrossEntropy-newCrossEntropy<br>            <span class="hljs-keyword">if</span> infoGain&gt;bestInfoGain:<br>                bestInfoGain=infoGain<br>                bestFeature=i<br>        <span class="hljs-keyword">return</span> bestFeature<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">splitDataSet</span>(<span class="hljs-params">self, dataSet, index,value</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        根据最佳的特征分离数据集，形成新的子集</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br><br>        subDataSet=[]<br>        <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> dataSet:<br>            <span class="hljs-keyword">if</span> example[index]==value:<br>                temp=example[:index]<br>                temp.extend(example[index+<span class="hljs-number">1</span>:])<br>                subDataSet.append(temp)<br>        <span class="hljs-keyword">return</span> subDataSet<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">ifNodeFeatureIsSame</span>(<span class="hljs-params">self,dataSet</span>):<br>        featureMat = [example[:(<span class="hljs-built_in">len</span>(dataSet[<span class="hljs-number">0</span>])-<span class="hljs-number">1</span>)] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> dataSet]<br>        uniqueFeatureMat=np.unique(featureMat)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(uniqueFeatureMat) == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">majorityCount</span>(<span class="hljs-params">self,classList</span>):<br>        className,counts=np.unique(np.array(classList))<br>        <span class="hljs-keyword">return</span> className[np.argmax(counts)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">createTree</span>(<span class="hljs-params">self,dataSet,labels</span>):<br>        <span class="hljs-comment"># 生成结点node</span><br>        classList=[example[-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> dataSet]<br><br>        <span class="hljs-comment"># 情况1：当前分支中全部都为同一个属性不需要分类</span><br>        <span class="hljs-comment"># 情况2：当前分支为1集合不需要分类</span><br>        <span class="hljs-comment"># 情况3：最优化划分</span><br><br>        <span class="hljs-keyword">if</span> classList.count(classList[<span class="hljs-number">0</span>])==<span class="hljs-built_in">len</span>(classList):<br>            <span class="hljs-keyword">return</span> classList[<span class="hljs-number">0</span>]<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(labels)==<span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.ifNodeFeatureIsSame(dataSet):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.majorityCount(classList)<br><br>        bestFeature=<span class="hljs-variable language_">self</span>.chooseBestFeature(dataSet)<br>        bestFeatureLabel=labels[bestFeature]<br>        myTree=&#123;bestFeatureLabel:&#123;&#125;&#125;<br><br>        <span class="hljs-comment">#删除区分的属性</span><br>        <span class="hljs-keyword">del</span>(labels[bestFeature])<br><br>        featureValues=[example[bestFeature] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> dataSet]<br>        uniqueValues=<span class="hljs-built_in">set</span>(featureValues)<br>        <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> uniqueValues:<br>            sublabels=labels[:]<br>            myTree[bestFeatureLabel][value]= \<br>                <span class="hljs-variable language_">self</span>.createTree(<span class="hljs-variable language_">self</span>.splitDataSet(dataSet, bestFeature, value), sublabels)<br>        <span class="hljs-keyword">return</span> myTree<br><br><br><br><br><span class="hljs-keyword">if</span> __name__==<span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    newTree=decisionTree()<br>    filename=<span class="hljs-string">&#x27;datasets/watermelon.txt&#x27;</span><br>    dataSet,labels=newTree.dataLoader(filename)<br>    <span class="hljs-comment">#或者直接根据例子来</span><br>    <span class="hljs-comment">#dataSet,labels=newTree.createExampleDataset()</span><br>    <span class="hljs-built_in">print</span>(newTree.createTree(dataSet,labels))<br></code></pre></td></tr></table></figure><p>txt文件</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs">编号,色泽,根蒂,敲声,纹理,脐部,触感,好瓜<br>1,青绿,蜷缩,浊响,清晰,凹陷,硬滑,是<br>2,乌黑,蜷缩,沉闷,清晰,凹陷,硬滑,是<br>3,乌黑,蜷缩,浊响,清晰,凹陷,硬滑,是<br>4,青绿,蜷缩,沉闷,清晰,凹陷,硬滑,是<br>5,浅白,蜷缩,浊响,清晰,凹陷,硬滑,是<br>6,青绿,稍蜷,浊响,清晰,稍凹,软粘,是<br>7,乌黑,稍蜷,浊响,稍糊,稍凹,软粘,是<br>8,乌黑,稍蜷,浊响,清晰,稍凹,硬滑,是<br>9,乌黑,稍蜷,沉闷,稍糊,稍凹,硬滑,否<br>10,青绿,硬挺,清脆,清晰,平坦,软粘,否<br>11,浅白,硬挺,清脆,模糊,平坦,硬滑,否<br>12,浅白,蜷缩,浊响,模糊,平坦,软粘,否<br>13,青绿,稍蜷,浊响,稍糊,凹陷,硬滑,否<br>14,浅白,稍蜷,沉闷,稍糊,凹陷,硬滑,否<br>15,乌黑,稍蜷,浊响,清晰,稍凹,软粘,否<br>16,浅白,蜷缩,浊响,模糊,平坦,硬滑,否<br>17,青绿,蜷缩,沉闷,稍糊,稍凹,硬滑,否<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 参数方法和非参数方法 ｜ Vol3</title>
    <link href="/posts/5568281b.html"/>
    <url>/posts/5568281b.html</url>
    
    <content type="html"><![CDATA[<p>Andrew Wu–番外参数方法和非参数方法blog.tjdata.site&#x2F;2022&#x2F;02&#x2F;13&#x2F;Andrew%20Wu–%E7%95%AA%E5%A4%96%E5%8F%82%E6%95%B0%E6%96%B9%E6%B3%95%E5%92%8C%E9%9D%9E%E5%8F%82%E6%95%B0%E6%96%B9%E6%B3%95&#x2F;</p><p>[Andrew Wu–番外参数方法和非参数方法](<a href="https://blog.tjdata.site/2022/02/13/Andrew">https://blog.tjdata.site/2022/02/13/Andrew</a> Wu–番外参数方法和非参数方法&#x2F;)</p><p>在初步了解了线性模型之后，总结一下算法的共性</p><p>（材料来自网络，可能会侵权）</p><h2 id="0X00-引言"><a href="#0X00-引言" class="headerlink" title="0X00 引言"></a>0X00 引言</h2><p>（实验讨论如何在使用概率对不确定性建模，留个坑之后再说，如何作出最优决策）现在考虑如何从给定的训练集中估计这些概率，从分类和回归的参数方法开始</p><p>参数方法，这里假设样本取自服从已知模型的某个分布（eg.高斯分布），模型定义在有效统计量（eg. 方差和均值），一旦从样本中估计参数就知道了整个分布，方法主要分为最大似然估计（maximum likelihood estimation）、贝叶斯估计（Bayes‘ estimator）等</p><p>参数方法的优点：将估计概率密度、判别式或者回归函数问题归结成为少量参数值</p><p>参数方法的缺点：假设并不是总是整理的，并且不成立会导致较大的误差</p><p>非参数方法（nonparametric estimation）：算法使用合适的距离度量，从训练集中找出相似的实例，并且由他们插值得到正确的输出（assumption：世界是平稳的，并且无论是密度、判别式还是回归函数都能缓慢变化，相似的例子意味这相似的事物）</p><p>基本概念</p><p>密度估计（density estimation）</p><p>密度估计是估计$p(x)$的一般情况，在分类中，其中估计密度是能够计算后验概率$P(C_i&#x2F;x)$并做决策的类密度$P(x&#x2F;C_i) and P(C_i)$；在回归中，估计的密度是 :$P(y|x)$</p><h2 id="0X01-参数方法"><a href="#0X01-参数方法" class="headerlink" title="0X01 参数方法"></a>0X01 参数方法</h2><h2 id="01-1-method—最大似然估计（MLE，maximum-likelihood-estimation）"><a href="#01-1-method—最大似然估计（MLE，maximum-likelihood-estimation）" class="headerlink" title="01-1 method—最大似然估计（MLE，maximum likelihood estimation）"></a>01-1 method—最大似然估计（MLE，maximum likelihood estimation）</h2><h3 id="已知"><a href="#已知" class="headerlink" title="已知"></a>已知</h3><p>样本参数，多个独立同分布（i.i.d）的样本集：${x^t}_{t&#x3D;1}^m$，同时假设样本都是<strong>某个定义在参数$\theta$的已知概率密度族中抽取的实例：</strong></p><p>$$ x^i\in p(x|\theta) $$</p><h3 id="求"><a href="#求" class="headerlink" title="求"></a>求</h3><p>最感兴趣是找到参数$\theta$</p><h3 id="解答："><a href="#解答：" class="headerlink" title="解答："></a>解答：</h3><p>因为每个样本是独立分布的，所以给定参数$\theta$,样本的似然likelihood是单点似然的乘积：</p><p>为了简化计算，将乘积转换为求和，并且假设某种密度族（eg. 之前的exp family）进一步简化计算量</p><p>常见的似然函数是convex的，所以直接求导就行了</p><h3 id="eg："><a href="#eg：" class="headerlink" title="eg："></a>eg：</h3><p>关于指数族的推导</p><h2 id="01-2-method—最大后验估计（MAP，maximum-A-posterior）"><a href="#01-2-method—最大后验估计（MAP，maximum-A-posterior）" class="headerlink" title="01-2 method—最大后验估计（MAP，maximum A posterior）"></a>01-2 method—最大后验估计（MAP，maximum A posterior）</h2><h3 id="已知-1"><a href="#已知-1" class="headerlink" title="已知"></a>已知</h3><p>在MLE的likelihood基础上，我们又知道了参数$\theta$的先验prior分布</p><p>$$ p(\theta) $$</p><h3 id="求-1"><a href="#求-1" class="headerlink" title="求"></a>求</h3><p>在此基础上我们依旧希望得到最佳的参数$\theta$</p><h3 id="解"><a href="#解" class="headerlink" title="解"></a>解</h3><p>根据贝叶斯规则，后验密度posterior density,告诉我们在看到样本之后$\theta$的可能取值：</p><p>在正常情况下，知道了$p(\theta|X)$，之后想要估计x则：</p><p>$$ p(x|X)&#x3D;\int p(x,\theta|X)d \theta=\int p(x|\theta,X)p(\theta|X)d\theta=\int p(x|\theta)p(\theta|X)d \theta $$</p><p>所以知道了$p(\theta|X)$就知道了关于分布的一切，但是这个积分比较困难，因此为了用简便，我们希望将$p(\theta|X)$看成一个常数：</p><p>$$ \theta_{MAP}&#x3D;argmax_{\theta}p(\theta|X) $$</p><h2 id="01-3-method—贝叶斯估计（Bayesian-estimation）"><a href="#01-3-method—贝叶斯估计（Bayesian-estimation）" class="headerlink" title="01-3 method—贝叶斯估计（Bayesian estimation）"></a>01-3 method—贝叶斯估计（Bayesian estimation）</h2><p>和MAP相似，但是对于$\theta$ 处理并不是最大值，而是均值</p><p>$$ \theta_{bayes}&#x3D;E(\theta|X) $$</p><h2 id="01-4-example—参数估计"><a href="#01-4-example—参数估计" class="headerlink" title="01-4 example—参数估计"></a>01-4 example—参数估计</h2><p>我们假设</p><p>由此知道了</p><p>似然函数  </p><p>先验参数  </p><p>由此得到  </p><h3 id="01-5-discussion-—讨论"><a href="#01-5-discussion-—讨论" class="headerlink" title="01-5 discussion —讨论"></a>01-5 discussion —讨论</h3><ul><li>MAP和贝叶斯估计都将整个后验密度归约到单个点且损失信息，除非后验单峰的且这些点周围有一个窄峰，但是随着计算能力的提高蛮可以使用蒙特卡洛或者近似方法来计算整个积分</li><li>（bias｜variance dilemma），常见的解决方法</li><li>交叉验证 cross-validation训练看损失函数的“拐点”</li><li>正则化regularization：E&#x3D;数据上误差+a*模型复杂度</li><li>正则化另外一个角度：E&#x3D;训练误差+a*偏差的乐观项，利用AIC（Akaike‘s information criterion）和BIC（Bayesian information criterion）</li><li>结构风险最小化（SRM，structural risk minimization）将模型集合按照复杂程度（随机变量的个数，或者VC维）排序一次训练</li><li>最小描述长度（minimum description length，MDL），利用信息论中的kolmogorov复杂度度量</li></ul><h2 id="0X02-半参数方法"><a href="#0X02-半参数方法" class="headerlink" title="0X02 半参数方法"></a>0X02 半参数方法</h2><p>待定</p><h2 id="0X03-非参数方法"><a href="#0X03-非参数方法" class="headerlink" title="0X03 非参数方法"></a>0X03 非参数方法</h2><p>不同的非参数方法会定义不同相似性 or 或者不同的定义插值的方法，在非参数模型中不存在单个全局模型，需要时需要有局部模型只收到邻近训练实例的影响</p><p>常见的机器学习中非参数方法又被称为instance-based或者memory-based方法</p><h3 id="03-1-密度估计"><a href="#03-1-密度估计" class="headerlink" title="03-1 密度估计"></a>03-1 密度估计</h3><ul><li>直方图估计 histogram</li><li>质朴估计 naive estimator</li><li>核估计 kernel estimator</li><li>k最近邻估计 k-nearest neighbor，k-nn</li></ul><p>K-NN算法</p><blockquote><p> 1 计算测试对象到训练集中每个对象的距离 2 按照距离的远近排序  3 选取与当前测试对象最近的k的训练对象，作为该测试对象的邻居  4 统计这k邻居的类别频率  5 选择k个邻居中频率最高的类别，作为测试对象的类别 </p></blockquote><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs swift"><span class="hljs-keyword">import</span> matplotlib.pyplot as plt<br><span class="hljs-keyword">import</span> seaborn as sns<br><span class="hljs-keyword">import</span> numpy as np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">knn</span>:<br>    def load<span class="hljs-title class_ inherited__">Dataset</span>(<span class="hljs-keyword">self</span>,filename):<br>        # 借用<span class="hljs-title class_ inherited__">SVM的数据集，但是这里使用numpy返回np</span>.array数据<br>        data<span class="hljs-title class_ inherited__">Mat</span> = [];<br>        label<span class="hljs-title class_ inherited__">Mat</span> = []<br>        fr = <span class="hljs-keyword">open</span>(filename)<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr.readlines():  # 逐行读取，滤除空格等<br>            line<span class="hljs-title class_ inherited__">Arr</span> = line.strip().split(&#x27;\t&#x27;)<br>            data<span class="hljs-title class_ inherited__">Mat</span>.append([float(line<span class="hljs-title class_ inherited__">Arr</span>[0]), float(line<span class="hljs-title class_ inherited__">Arr</span>[1])])  # 添加数据<br>            label<span class="hljs-title class_ inherited__">Mat</span>.append(float(line<span class="hljs-title class_ inherited__">Arr</span>[2]))  # 添加标签<br>        data<span class="hljs-title class_ inherited__">Mat</span>=np.array(data<span class="hljs-title class_ inherited__">Mat</span>)<br>        <span class="hljs-keyword">self</span>.data<span class="hljs-title class_ inherited__">Mat</span>=data<span class="hljs-title class_ inherited__">Mat</span><br>        <span class="hljs-keyword">self</span>.label<span class="hljs-title class_ inherited__">Mat</span>=label<span class="hljs-title class_ inherited__">Mat</span><br><br>    def  visual<span class="hljs-title class_ inherited__">Dataset</span>(<span class="hljs-keyword">self</span>):<br>        sns.scatterplot(<span class="hljs-keyword">self</span>.data<span class="hljs-title class_ inherited__">Mat</span>[:, 0],<span class="hljs-keyword">self</span>.data<span class="hljs-title class_ inherited__">Mat</span>[:, 1],hue=<span class="hljs-keyword">self</span>.label<span class="hljs-title class_ inherited__">Mat</span>)<br>        plt.show()<br><br>    def predict(<span class="hljs-keyword">self</span>,input,k):<br>        def distance(point):<br>            points=np.tile(point,(<span class="hljs-keyword">self</span>.data<span class="hljs-title class_ inherited__">Mat</span>.shape[0],1))<br>            dis=np.linalg.norm(points - <span class="hljs-keyword">self</span>.data<span class="hljs-title class_ inherited__">Mat</span>,axis=1)<br>            <span class="hljs-keyword">return</span> dis<br>        dis=distance(input)<br>        target=np.array(<span class="hljs-keyword">self</span>.label<span class="hljs-title class_ inherited__">Mat</span>)[np.argsort(dis)&lt;k]<br>        <span class="hljs-keyword">if</span> sum(target)&gt;0:<br>            <span class="hljs-keyword">return</span> 1<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> -1<br><br><span class="hljs-keyword">if</span> __name__ == &#x27;__main__&#x27;:<br>    model=knn()<br>    model.load<span class="hljs-title class_ inherited__">Dataset</span>(&#x27;<span class="hljs-title class_ inherited__">CS229_2018autumn</span>/<span class="hljs-title class_ inherited__">ProblemSet</span>(课后作业）/code<span class="hljs-title class_ inherited__">Byhand</span>/datasets/test<span class="hljs-title class_ inherited__">Set</span>.txt&#x27;)<br>    model.visual<span class="hljs-title class_ inherited__">Dataset</span>()<br>    example=[0,0]<br>    print(model.predict(example,6))<br></code></pre></td></tr></table></figure><h3 id="03-2-分类"><a href="#03-2-分类" class="headerlink" title="03-2 分类"></a>03-2 分类</h3><p>上次的SVM算法</p><p>优点：</p><ul><li>解决高维特征的分类问题和回归问题很有效,在特征维度大于样本数时依然有很好的效果。</li><li>仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据。</li><li>有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题。</li><li>样本量不是海量数据的时候，分类准确率高，泛化能力强。</li></ul><p>缺点：</p><ul><li>如果特征维度远远大于样本数，则SVM表现一般。</li><li>SVM在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用。</li><li>非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。</li><li>SVM对缺失数据敏感。</li></ul>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>奇文共赏-复杂场景的密集人群计数</title>
    <link href="/posts/9313437b.html"/>
    <url>/posts/9313437b.html</url>
    
    <content type="html"><![CDATA[<blockquote><p>之前创新项目中文献综述在选修课作业中完善，同时借鉴经典综述文章，完成一篇中文“四不像”，就当是杂谈了</p></blockquote><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h2><p>复杂场景的密集人群计数在治安防控、交通管制、集会管理等方面起到重要的作用，可以为管理者提供有效及时的信息。从21篇中文文献和43篇英文文献中总结出密集人群计数中面临的主要问题有：拥挤、遮挡、重复和复杂光照环境带来的影响，并从传统的基于手工提取特征的密集计数到基于卷积神经网络的高级特征提取的方式分别介绍。总的检测方式划分为基于检测、基于回归和基于密度估计的方式，在之后基于卷积神经网络中对改进方向进行主要可以分为针对网络结构、针对输入数据集和针对训练过程。最后总结复杂场景的密集人群计数后面的方向主要分为数据集的更新、注意力机制和对抗生成的密度估计方式更新、损失函数及训练方式的转变三个有意思的场景。</p><p><strong>Abstract.</strong></p><p>Dense crowd counting in complex scenes plays an important role in security prevention and control, traffic control, and assembly management, and can provide managers with effective and timely information. The main difficulties faced in dense crowd counting are summarized from 21 Chinese literature and 43 English literature: crowding, occlusion, repetition and the impact from complex lighting environment, and are described separately from the traditional dense counting based on manual feature extraction to the advanced feature extraction based on convolutional neural network, and the total detection is classified into detection-based, regression-based and density estimation-based methods, after The classification of improvement direction in convolutional neural network based can be mainly divided into for network structure, for input data set and for training process. Finally summarize the complex scenario of dense population counting behind the direction is mainly divided into three interesting scenarios of update of dataset, update of attention mechanism and adversarial generation of density estimation approach, loss function and transformation of training approach.</p><h2 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h2><p>随着城市化水平的提高与人口规模的日益扩大，大量密集场景隐藏着诸多安全隐含，因此人群计数研究十分有必要，可以在预警规划、公共管理等领域发挥着重要的作用，备受学术界和工业界关注。截止2020年，我国二线以上城市城镇率超过了70%，城市圈与都市圈的逐渐扩大加速了城市人口的增长，导致节假日、周末等时间居民出行率出现了爆炸式的增长，为公共交通、城市应急等方面带来了巨大安全问题。</p><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>随着基于深度学习方法的人群技术算法的不断研究，人群数据集也逐渐收到研究者的重视，越来越多的与视频相关的人群数据集提出并收到广泛关注，常见的数据集包括：(Zhang et al. 2016)提出的shanghaiTech数据集、Idrees et al. (2013)提出的UCF_CC_50数据集、(Sindagi, Yasarla, and Patel 2020)提出的JHU-CROWD++数据集、(Wang et al. 2021)提出NWPU-Crowd数据集，以下对常见的数据集作出介绍，这些数据集在图片数量和分辨率、拍摄角度与场景、人群拥挤程度甚至光照强度方面各不相同，共同点是均提供了人头位置的标记信息，以下是对数据集的简要介绍：</p><p>表格 2‑1:人群计数不同数据集的统计结果</p><table><thead><tr><th>数据集</th><th>图片数量</th><th>训练集&#x2F;测试集</th><th>平均分辨率</th><th>计数统计</th><th></th><th></th><th></th></tr></thead><tbody><tr><td>总计</td><td>最小值</td><td>平均</td><td>最大值</td><td></td><td></td><td></td><td></td></tr><tr><td>SHT_A</td><td>482</td><td>300&#x2F;182</td><td>589*868</td><td>241677</td><td>33</td><td>501</td><td>3139</td></tr><tr><td>SHT_B</td><td>716</td><td>400&#x2F;316</td><td>768*1024</td><td>88488</td><td>9</td><td>123</td><td>578</td></tr><tr><td>UCF_CC</td><td>50</td><td>1201&#x2F;334</td><td>2013*2902</td><td>63974</td><td>94</td><td>1279</td><td>4543</td></tr><tr><td>JHU_CROWD++</td><td>4372</td><td>2722&#x2F;1600</td><td>910*1430</td><td>1251642</td><td>49</td><td>815</td><td>12865</td></tr><tr><td>NWPU-Crowd</td><td>5109</td><td></td><td>2311*3383</td><td>2133238</td><td>0</td><td>418</td><td>20033</td></tr></tbody></table><p>ShanghaiTech数据集根据采用方式不同将数据集划分为part-A和part-B两部分，SHT-A是在互联网上随机选取的人群图像，而SHT-B来源于上海街头的摄像头拍摄，相比分布较为稀疏、场景较为固定。</p><p>  UCF_CC_50数据集均是从互联网下载的灰度图像，具有人群极度密集且尺度变化较小的特点，大量数据仅具备头部特征，行人间遮挡严重等特点。</p><p>JHU- Crowd++数据集中包含不同环境下的人群图片，该数据集对不同人群环境进行分类计算性能，其中分为五种不同类别：低密度类、中密度类、高密度类、困难类和总体类</p><p>NWPU- Crowd作为目前最大且最具有挑战性的人群技术数据集之一，该数据集出了数据量大和规模分布广泛之外，还引入351个与人群场景相似但并没有行人的负样本，该数据集不公布测试集图片，研究人员需要通过在线评估网站提供计数文档从而获得测试集的公开评估结果。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>应用一是安全监控，在体育场馆、旅游景点、地铁站中广泛使用视频监控摄像头进行安全保障可以使得此类场景下的人群监控变得更加容易，但是传统的监控网络会因设计限制而无法处理高密度人群，因此在人群计数的多人数处理中可以和(Chaker, Al Aghbari, and Junejo 2017)异常行为检测现挂钩。</p><p>应用二在灾难引导中，比如大型体育馆、音乐会或者公众集会等方面，(董大鹏 and 丁宁 2021)和(谭家磊 et al. 2021)给出相仿真政策建议</p><p>​    应用三在公共场所规划中，在大型体育、音乐集会等场所通常面临由于人数过多的造成的踩踏风险，因此可以利用密集计数(郑超毅 2021)和(曾婷婷 2021)的方法来讨论相关的场景布设的合理程度。</p><p>综上，在智能图像处理任务中人群计数具有重要的作用，近几年，诸多相关研究成果被提出并在计数任务上取得了良好的性能，但是此任务仍然面临许多挑战。</p><h2 id="应用挑战"><a href="#应用挑战" class="headerlink" title="应用挑战"></a>应用挑战</h2><p>从文献(钮嘉铭 and 杨宇 2021)中可以看出复杂场景的人群计数面临的主要困难由聚集、遮挡、重复和畸变等因素。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-af160c484ecb71c9e164663e3dd3c98f_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 2‑4：复杂场景中面临的问题</p><p>聚集：指的是两个个体在视频中组合成一个形似一个个体的像素区域</p><p>遮挡：指的是一个个体被其他个体遮挡而无法从视频中看到</p><p>重复：指的是个体在不同摄像区域中被重复计数</p><p>畸变：由于光近大远小的因素导致距离摄像头角度和距离不同的个体图像特征不相似</p><h2 id="检测方法"><a href="#检测方法" class="headerlink" title="检测方法"></a>检测方法</h2><p>根据(郝晓亮 2021)中的总结，随着计算机视觉领域的发展，针对人群技术的研究逐渐增多，越来越多的工作关注于对图像中的人群或者人群密度分布进行估计，现有的人群技术研究根据特征的提取方式不同分为两类：</p><p>其一是基于传统方法的计数算法</p><p>其二是基于卷积神经网络的计算方法</p><p>随着特征提取的方式的转变，相关研究从基于人为设计的手工特征提取逐渐转向基于检测或者回归的方式来统计图像中的人数，今年来得益于卷积神经网路在线检测和分类等任务上的成功应用，此类方法也被逐渐用于计数任务来提取特征，并获得大幅度的性能提升。</p><h2 id="密集人群计数方法分类"><a href="#密集人群计数方法分类" class="headerlink" title="密集人群计数方法分类"></a>密集人群计数方法分类</h2><p>传统的方法从特征提取出发，主要分为基于检测、基于回归和基于密度图的方法对复杂场景的人群密度进行检测，其中</p><p>基于检测的方式主要为利用人的特征来检测每个人的个数，这种存在的问题在密集场景中性能不强；</p><p>基于回归的方式主要是利用图片中的特征数量和人的数量做回归分析来得到人群密度，存在的问题是无法检测出图片中人的位置；</p><p>基于密度图的方式为将图片和密度图作为联系，可以在提取出密度图的同时保留数据的空间特征，精度更高同时应用场景更多；</p><p>但是由于人手工提取特征在信息上的局限性，性能都不是特别高，因此开始逐渐转向基于卷积神经网络的特征自动提取的方式所取代</p><h2 id="传统特征提取方法"><a href="#传统特征提取方法" class="headerlink" title="传统特征提取方法"></a>传统特征提取方法</h2><p>在早期研究中，大量的工作利用检测来进行技术，通过检测图片中行人个数来估计人数，大多数最初的研究都集中在检测框架上，整体思路是采用滑动窗口检测器来检测场景中的人，并利用此信息来计算人数，检测通常分为利用整体或者采用局部信息。整体检测过程较为简单，但是由于在高密度人群容易受到遮挡等问题的影响，研究者提过提取人体的局部特征来检测特定身体部分解决问题。</p><p>常见的手工提取特征的方式如Haar 小波变换、形状特征、梯度直方图与纹理特征等手工提取的方式被应用到人群特征的检测中，之后利用支持向量机、Adaboost或随机森林等方法来进行学习达到检测的目的，(Felzenszwalb et al. 2009)提取人群图像中的头部特征、(Sabzmeydani and Mori 2007)提取人群图像中的头部特征来完成技术，(Li et al. 2008)首次采用来分割算法划分人群图像，利用前景信息提取HOG特征建立头肩检测器来统计人数，而(Wu and Nevatia 2005)提出轮廓导向的特征提取方法来建立身体检测器。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-df8ab63ef435318917404071f0532c4f_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 3‑1：局部特征的人群检测方法</p><p>在人体特征的方法中，采用椭圆体构成的3D形状对人类济宁建模，并使用随机过程来估计最能解释场景的前景和背景的分类，(Ge and Collins 2009)构建增强分类器来估计指定区域的人数，通过使用灵活使用的形状模式进一步拓展这一想法。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-06d6f3008ba34c4b4ba49c78ee19591b_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 3‑2：前景和背景分离的方法</p><p>为了解决人群密度检测中视频中不可避免存在的遮挡的问题，基于回归的方法之际建立手工提取的特征到图像人数之间的映射来完成计数任务。这种方法主要包括手工提取特征和训练回归模型。常用到的包括纹理特征、边缘特征、灰度生成矩阵（GLCM）以及HOG特征等，而常用于人体回归的方法包括线性回归、贝叶斯回归、高斯过程回归、岭回归等方法，来实现低密度人群计数的功能。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-546324b3ed2707766c0852a43c50e0c3_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 3‑3：基于回归的人群密度检测</p><p>但是同样在高密度中由于受到遮挡、畸变等因素的影响，依旧会出现性能下降的过程，(Idrees et al. 2013)提出从不同角度提取不同特征，从尺度不变特征变换（SIFT）、头部检测以及傅立叶分析等不同角度提取特征，利用不同特征表示的信息权重甲醛计算人数，从而在密集场景中解决计数问题。但是由于具有场景局限性，回归方式提取的特征无法反应人群的空间分布，(朱宇斌 2021)提出一种基于相似性度量的人群密度方法，但只能用于解决当前场景中的计数问题，从而降低了算法的实用性。</p><p>为了弥补检测方式的性能问题和回归问题中检测空间无法识别的问题，基于密度估计的方式可以有效的使用图像中的空间问题，避免学习检测和定位单个对象的任务困难，学习密度图的问题被程式化的划分为正则化风险二次成本的最小化函数，(Lempitsky and Zisserman 2010)提出学习局部补丁特征和相应对象密度图之间的线性映射，(Pham et al. 2015)提出学习局部补丁特征和密度图之间的非线性映射，使用来自多个图像块的随机森林回归来投票选择多个目标对象的密度来学习非线性映射，但是由于这样计算量较大，提出了快速的DE-VOC方法，提出了基于子空间的快速密度估计方法学习，利用了图像与其在各自特征空间中对应的密度图之间的关系，对图像块的特征空间进行聚类，在最近的一种方法中，(Xu and Qiu 2016)提出通过使用更广泛和更丰富的特征来提高人群密度估计的性能、同时由于早期方法中很难处理非常高纬度的特征，因此使用随机森林作为回归模型，来进行特征的回归分析，降低计算消耗同时提高性能</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-19c750cebb320d9ff6dc2152d9917fb7_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 3‑5：用于预测人数的随机投影森林模型框架</p><h2 id="基于卷积神经网络的方法概述"><a href="#基于卷积神经网络的方法概述" class="headerlink" title="基于卷积神经网络的方法概述"></a>基于卷积神经网络的方法概述</h2><p>伴随着硬件技术的提升与深度学习技术的进步，诸多计算机视觉任务性能得到大幅度提升，在目标检测、图像分类、语义分割等任务中卷积神经网络（CNN）,因此CNN也同样被大量用于计数任务。但究其本质而言和传统方法中基于检测、基于回归和基于密度图的方法并没有太大的区别，其学习非线性函数的能力可以帮助更好的选择满足计算机视觉中相关的任务。(Ilyas, Shahzad, and Kim 2020)中指出基于CNN的人群计数的一般性流程，通过一个通用的CNN-CC（CNN- Crowd counting）来解决复杂环境中遮挡、低可见性、对象和对象之间变化以及不同视角导致的尺度变化的问题，在基于CNN的人群计数中主要分为两部，意识通过CNN来进行密度估计得到真实密度GTD，另一个通过比较估计值和真实值来进行比较和损失函数的计算。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-a0c404c728af02575c1ba83ba7589aaf_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 3‑6：基于CNN的人群计数的流程</p><h2 id="基于CNN的人群计数分类"><a href="#基于CNN的人群计数分类" class="headerlink" title="基于CNN的人群计数分类"></a>基于CNN的人群计数分类</h2><p>基于CNN的人群计数的分类中，(Sindagi and Patel 2018)中将区分为网络结果和训练方式的结果，之后(Ilyas, Shahzad, and Kim 2020)扩充至对于不同输入数据集的分类，主要包括</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-748bede72fc46b67fa55891ec772550e_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 4‑1：不同CNN-CC分类</p><p>从网络结构来看，网络可以分为基本的卷积神经网络、关注上下文的卷积神经网络、动态尺度变化的卷积神经网络、多任务框架的卷积神经网络；从输入的数据集可以分为基于鸟瞰和基于透视两类；从训练方式分为感知框和整张图片的训练。</p><h2 id="基于网络结构"><a href="#基于网络结构" class="headerlink" title="基于网络结构"></a>基于网络结构</h2><p>(Fu et al. 2015)首先提出通过优化的卷积神经网络（ConvNet）来估计人群密度，同时使用级联分类器对判别性特征进行分类，参考(Mundhenk et al. 2016)中提出的一种基于初始层计数的残差学习方法，通过将图像区域划分为重叠的块进行图像识别，用来减少MSE，(Wang et al. 2015)中基于CNN结合负样本来削弱建筑物和树木等背景的影响来得到更好的性能。(Hu et al. 2016)在卷积神经网络提取特征的基础上利用人群密度和人群速度来学习人群特征并进行计数，特殊的(Walach and Wolf 2016)利用分层Adaboost和选择性采样的方法来提高数据集准确性并减少处理时间。(翁佳鑫 and 仝明磊 2021)引入Unet++为基础的膨胀卷积神经网络，(贾奇麟, 段其微, and 徐源 2021)使用yolo目标检测框架来进行密集人群计数。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d9b5fdbe40facc5d2c244ab6f304596a_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 4‑2：基本卷积神经网络识别方法</p><p>但是由于该类网络在大多数情况下主要关注密度估计而不是人群计数，本质上是基于检测的方法，其仍然改变不来在高度遮挡和不同视角场景下表现不佳的情况。</p><p>在之后的网络结构的发展中，利用图像的局部和视频中的上下文信息来提高计数准确性的网络成为context-CNN-CC，该类别的计数需要上下文信息的应用，(Chattopadhyay et al. 2017)提出来对于日常对象的想法，考虑到关联子化的新想法，(Zhang et al. 2019)引入注意力机制来对人群计数中头部可能出现的高概率位置进行预测同时利用多尺度特征用于抑制非头部区域，(郝晓亮 et al. 2021)利用高级网络提取的予以信息和底层网络提取的人群尺度细节信息相结合来得到高质量的人群密度图，(Li, Zhang, and Chen 2018)利用CNN和用可扩展内核代替赤化来提高密度图的质量，同时结合各种拥挤场景中组合上下文信息，为了更好的使用相关信息，(孟月波 et al. 2021)同时在注意力机制引导的空间注意力透视（PSA）方式对图像多视角信息进行有效信息编码来获取特征图的空间上下文信息，(Han et al. 2017)利用卷积神经网络—马尔可夫场来用于静态图像中的人群计数，为了满足不同密度场景下模型的使用情况，(Wang, Shao, et al. 2018)提出通用框架中由三个网络结构组成，一个密度自适应网络来识别低密度或者高密度，另外两个网络负责计数，之后为了处理不同尺度和角度的数据，(Liu, Wang, et al. 2018)提出了深度循环空间感知网络来使用空间转换器来进行计数，并处理上述中产生的问题。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-d607641a7b74cbf3982f2e4e4ac40590_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 4‑3：context-CNN-CC举例</p><p>之后的scale-CNN-CC计数用来表示在图片尺度变化过程中对于网络的鲁棒性和准确性的改变，尺度变化意味着不同视角引起的分辨率变化，这种技术在提高高度和遮挡场景的准确性方面发挥着至关重要的作用，(郭爱心 et al. 2020)针对人群计数中多尺度变化和背景干扰引入特征金字塔构成多尺度特征融合骨干网络来解决人群多尺度变化问题，(Liu, Lis, et al. 2018)提出了一种几何感知的人群密度估计方法，用一个显式模型来处理透视失真效应，之后引入神经网络进行改善，(Boominathan, Kruthiventi, and Babu 2016)提出一个浅层神经网络和深层神经网络来有效捕捉高级语义和低级特征，用来准确估计尺度变化条件下的人群密度，(Onoro-Rubio and López-Sastre 2016)提出来两种方法来解决图像中的人群外观和尺度变化，首先利用计数CNN将图像外观映射到密度图中，之后利用多批量多尺度的特征进行密度估计，(杜培德 and 严华 2021)提出一种基于多尺度融合网络和空间注意力模块对特征图进行校准和在融合提高网络精度，(Shi et al. 2018)提出一种具有聚合特征向量的多尺度多任务人群计数算法，多尺度特征基本上被组合成为一个单一的向量来进行优化，(Cao et al. 2018)提出来一种基于编码器和解码器的CNN来降低计算复杂度，同时使用更简单的尺度感知网络SANet来解决尺度变化问题，同时利用转置卷积用来提高密度图的质量，之后(Wang, Zha, et al. 2018)引入对抗生成网络来成功解决图像转换来生成最佳的密度图，更大的池化繁为可以有利于捕捉多尺度范围来降低计算成本同时利用级联提高计数准确性。(蒋俊 et al. 2021)使用不同卷积核提取不同空间特征来对密度图积分求和得到人群数量</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-bc367d0cc612a1b3cb50154b280d3518_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 4‑4：基于GAN的scale-CNN-CC</p><p>多任务综合从特征信息和尺度变化等内容，(王良聪 et al. 2021)提出来一种基于空间—通道双注意力模块和多尺度特征融合的小尺寸人群计数网络，(罗凡波 et al. 2020)利用多尺度卷积神经网络来对视频中可能存在异常聚集进行预测，(郝晓亮 2021)利用空洞卷积特征融合来提升网络的密度感知能力提升计数精度。特殊的，(郭华平 et al. 2021)在考虑传统CNN只考虑到密度图到人群图的映射，而没有考虑到密度图到人群图的映射而影响性能，因此提出基于卷积神经网络的对偶模型（DualCNN）来提高模型将人群图映射为密度图准确性。</p><h2 id="基于输入数据集"><a href="#基于输入数据集" class="headerlink" title="基于输入数据集"></a>基于输入数据集</h2><p>输入数据集按照设置的位置分类可以为鸟瞰图和基于透视图的计数，鸟瞰图主要是利用无人机航拍得到的结果，比如(Xie, Noble, and Zisserman 2018)利用两个具有最大感受野的卷积回归网络来克服检测目标聚集和重叠的现象。</p><p>另一方面更常见的为在不同尺度不同视角变化下的密集人群计数，最初的(‘<Crowd Counting by Adapting Convolutional Neural Networks with Side Information.pdf>‘  ; Kang, Dhar, and Chan 2016)中提出利用自适应的CNN来透视信息，通过使用不同的卷积滤波器权重来对当前的图像进行调整可以有效提取透视信息，</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-5f0bb82724f2f70ca2f7e03844184e24_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 4‑5：不同卷积核对于透视信息的提取</p><p>之后(Zhao et al. 2017)提出一种透视嵌入反卷积网络来模拟透视失真的不同大小的行人和利用不同内核参数的位置感知高斯函数来获得GTD，同时通过改变过滤器的深度来有助于形成平滑和准确的密度图。(Yao et al. 2017)通过使用CNN和LSTM的深度空间回归模型DSRM，首先利用CNN提取高级特征，使用LSTM结构使用相邻区域的空间信息来提高计数精度，有利于减少透视失真。</p><h2 id="基于训练过程"><a href="#基于训练过程" class="headerlink" title="基于训练过程"></a>基于训练过程</h2><p>基于训练过程主要是针对图片中一部分区域加上滑动窗口进行检测或利用整张图片进行检测。</p><p>滑动窗口检测中这些计数在密度图分辨率质量提高且不能受到影响的应用中非常有用，最初的(Paul Cohen et al. 2017)提出一种受到初始网络启发的深度CNN，使用较小的网络来估计感受对象中的数量，之后利用基于回归的方式来估计高密度区域和利用基于检测的方式来估计密集人群，之后滑动窗口主要使用近景和远景分离的方式进行计数，(Xu et al. 2019)提出来一种基于信息深度的引导式人群计数方法Digcrowd来处理高密度和不同视角的图像，更进一步的(Shami et al. 2018)使用头部检测器来发现不同大小的人头，之后利用SVM分类器对拥挤和不拥挤的窗口进行分类，(Sam, Surya, and Babu)考虑在低、中、高图像块上训练三个回归器，并提出一种switch CNN来将窗口定向特定的回归器来解决任何密度变化问题。可以在目标图像上一次采用检测和回归，用来提高网络预测精度，此外还能够提取关于网络边缘和颜色的低级信息进行迭代过滤。</p><p>在完整照片的识别中最初(Marsden et al. 2017)提出残差学习架构Resnet来利用多目标计数研究人群计数和密度级别分类，之后(Marsden et al. 2016)在上述基础上通过解决图像中的尺度变化和高密度问题来提出用于人群计数的FCN，通过改变CNN中感受野的不同来获得最终计数，也有(史劲霖 et al. 2021)通过残差神经网络作为细特征提取、VGG-16作为粗特征提取来完成对小目标、多尺度的密集人群特征检测，之后(Sindagi and Patel 2017)利用判别特征来处理图像内的高级密度变化，即提供密度估计的高级先验来改进网络性能。</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f142609ab0d13dde2f0d39cd94a7c9dc_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>图表 4‑6：多任务级联的CNN</p><p>训练过程中也有对损失函数进行调整，如(张宇倩 et al. 2021)通过引入SSIM的损失函数来比较估计人数密度图和真值的局部相关性和基于回归人数的损失函数用于比较估计人群数量与真实人数之间的差异来提高网络训练的准确度，在(陈薪羽 et al. 2021)中对多尺度的卷积神经网络的训练过程的参数更新方法作出建议。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>面对复杂场景的密集人群计数在公共空间人流引导、大型集会中为管理者面向群众客流引导起到有效的建议，目前有效的大型数据集中从数量和分辨率上相比较以前均有所提高，随着而来的是特征提取方法和建模手段的不断提高，复杂场景的人群计数面临的主要困难有重复、遮挡、畸变以及周围环境的光照环境的影响，因此从传统的基于人工特征提取的方式逐渐走向基于神经网络的特征提取方式，但是究其手段主要可以分为基于检测、基于回归和基于密度估计的方式。</p><p>在基于神经网络的方式的后续进展中，本文主要介绍了对于模型网络结构的改变、对于模型输入集的改变和对于模型训练过程的改变三种方式，其中对于神经网络结构的改变中可以分为基本的检测网络、上下文感知、多尺度变换和多任务的神经网络结构。因此对于复杂场景密集人群计数的发展可能有：</p><ol><li>更全面、更细致的数据集的出现，在数据集量增大的同时，正样本和负样本的数据以及不同场景的数据会尽可能的完善</li><li>以注意力机制和对抗生成网络中有利于利用图像或者是视频的空间和时间信息，双向卷积让人不仅关注从现实场景到密度图的转换，也让人更重视从密度图到真实人群计数的转换</li><li>不同损失函数和训练方式的转变，在有限的数据集和网络结构的情况，定义不同的损失函数有助于模型生长的方向性。</li></ol><ul><li>参考文献</li><li>曾婷婷. 2021. ‘人性化设计思路在公共建筑设计中的应用’, 智能城市, 7: 32-33.</li><li>陈薪羽, 刘明哲, 任俊, and 汤影. 2021. ‘基于三列卷积神经网络的参数异步更新算法’, 计算机应用: 1-11.</li><li>董大鹏, and 丁宁. 2021. ‘疫情防控下地铁站内应急疏散实景模拟研究’, 现代计算机: 150-55.</li><li>杜培德, and 严华. 2021. ‘基于多尺度空间注意力特征融合的人群计数网络’, 计算机应用, 41: 537-43.</li><li>郭爱心, 夏殷锋, 王大为, and 芦宾. 2020. ‘一种抗背景干扰的多尺度人群计数算法’, 计算机工程: 1-10.</li><li>郭华平, 王锐, 王敬, 孙艳歌, 李健, and 李萌. 2021. ‘面向人群计数的对偶卷积神经网络’, 信阳师范学院学报(自然科学版), 34: 650-54.</li><li>郝晓亮. 2021. ‘基于多尺度融合的复杂场景人群计数算法研究’, 硕士, 中国科学技术大学.</li><li>郝晓亮, 杨倩倩, 夏殷锋, 彭思凡, and 殷保群. 2021. ‘基于上下文特征重聚合网络的人群计数’, 信息技术与网络安全, 40: 59-65.</li><li>贾奇麟, 段其微, and 徐源. 2021. ‘基于多尺度目标检测的人群计数深度方法与系统设计’, 科学技术创新: 15-16.</li><li>蒋俊, 龙波, 高明亮, and 邹国锋. 2021. ‘一种基于多尺度融合卷积神经网络的人群计数方法’, 科学技术与工程, 21: 234-39.</li><li>罗凡波, 王平, 徐桂菲, 雷勇军, and 范烊. 2020. ‘基于多尺度卷积神经网络的人群聚集异常预测’, 计算机工程与科学, 42: 2223-32.</li><li>孟月波, 陈宣润, 刘光辉, and 徐胜军. 2021. ‘多特征信息融合的人群密度估计方法’, 激光与光电子学进展, 58: 276-87.</li><li>钮嘉铭, and 杨宇. 2021. ‘基于CNN的人群计数与密度估计研究综述’, 软件导刊, 20: 247-52.</li><li>史劲霖, 周良辰, 闾国年, and 林冰仙. 2021. ‘基于残差神经网络改进的密集人群计数方法’, 地球信息科学学报, 23: 1537-47.</li><li>谭家磊, 刘海力, 陈云飞, and 武玉梁. 2021. ‘基于Anylogic的某高铁客运站人群疏散设施优化研究’, 安全, 42: 13-19.</li><li>王良聪, 吴晓红, 陈洪刚, 何小海, 潘建, and 赵威. 2021. ‘基于多尺度及双注意力机制的小尺寸人群计数’, 智能计算机与应用, 11: 59-64.</li><li>翁佳鑫, and 仝明磊. 2021. ‘基于卷积神经网络的多尺度融合特征图在人群密度估计中的应用’, 上海电力大学学报, 37: 94-98.</li><li>张宇倩, 李国辉, 雷军, and 何嘉宇. 2021. ‘FF-CAM:基于通道注意机制前后端融合的人群计数’, 计算机学报, 44: 304-17.</li><li>郑超毅. 2021. ‘住宅建筑设计中的公共空间设计分析’, 中国建筑装饰装修: 34-35.</li><li>朱宇斌. 2021. ‘基于相似性度量的人群计数方法’, 电脑知识与技术, 17: 179-80.</li><li>Boominathan, Lokesh, Srinivas SS Kruthiventi, and R Venkatesh Babu. 2016. “Crowdnet: A deep convolutional network for dense crowd counting.” In Proceedings of the 24th ACM international conference on Multimedia, 640-44.</li><li>Cao, Xinkun, Zhipeng Wang, Yanyun Zhao, and Fei Su. 2018. “Scale aggregation network for accurate and efficient crowd counting.” In Proceedings of the European Conference on Computer Vision (ECCV), 734-50.</li><li>Chaker, Rima, Zaher Al Aghbari, and Imran N Junejo. 2017. ‘Social network model for crowd anomaly detection and localization’, Pattern Recognition, 61: 266-81.</li><li>Chattopadhyay, Prithvijit, Ramakrishna Vedantam, Ramprasaath R Selvaraju, Dhruv Batra, and Devi Parikh. 2017. “Counting everyday objects in everyday scenes.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1135-44.</li><li>‘<Crowd Counting by Adapting Convolutional Neural Networks with Side Information.pdf>‘.</li><li>Felzenszwalb, Pedro F, Ross B Girshick, David McAllester, and Deva Ramanan. 2009. ‘Object detection with discriminatively trained part-based models’, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32: 1627-45.</li><li>Fu, Min, Pei Xu, Xudong Li, Qihe Liu, Mao Ye, and Ce Zhu. 2015. ‘Fast crowd density estimation with convolutional neural networks’, Engineering Applications of Artificial Intelligence, 43: 81-88.</li><li>Ge, Weina, and Robert T Collins. 2009. “Marked point processes for crowd counting.” In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2913-20. IEEE.</li><li>Han, Kang, Wanggen Wan, Haiyan Yao, and Li Hou. 2017. ‘Image crowd counting using convolutional neural network and markov random field’, Journal of Advanced Computational Intelligence and Intelligent Informatics, 21: 632-38.</li><li>Hu, Yaocong, Huan Chang, Fudong Nian, Yan Wang, and Teng Li. 2016. ‘Dense crowd counting from still images with convolutional neural networks’, Journal of Visual Communication and Image Representation, 38: 530-39.</li><li>Idrees, Haroon, Imran Saleemi, Cody Seibert, and Mubarak Shah. 2013. “Multi-source multi-scale counting in extremely dense crowd images.” In Proceedings of the IEEE conference on computer vision and pattern recognition, 2547-54.</li><li>Ilyas, Naveed, Ahsan Shahzad, and Kiseon Kim. 2020. ‘Convolutional-Neural Network-Based Image Crowd Counting: Review, Categorization, Analysis, and Performance Evaluation’, Sensors, 20: 43.</li><li>Kang, Di, Debarun Dhar, and Antoni B Chan. 2016. ‘Crowd counting by adapting convolutional neural networks with side information’, arXiv preprint arXiv:1611.06748.</li><li>Lempitsky, Victor, and Andrew Zisserman. 2010. ‘Learning to count objects in images’, Advances in neural information processing systems, 23: 1324-32.</li><li>Li, Min, Zhaoxiang Zhang, Kaiqi Huang, and Tieniu Tan. 2008. “Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection.” In 2008 19th international conference on pattern recognition, 1-4. IEEE.</li><li>Li, Yuhong, Xiaofan Zhang, and Deming Chen. 2018. “Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes.” In Proceedings of the IEEE conference on computer vision and pattern recognition, 1091-100.</li><li>Liu, Lingbo, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. 2018. ‘Crowd counting using deep recurrent spatial-aware network’, arXiv preprint arXiv:1807.00601.</li><li>Liu, Weizhe, Krzysztof Lis, Mathieu Salzmann, and Pascal Fua. 2018. ‘Geometric and physical constraints for head plane crowd density estimation in videos’, CoRR abs&#x2F;1803.08805.</li><li>Marsden, Mark, Kevin McGuinness, Suzanne Little, and Noel E O’Connor. 2016. ‘Fully convolutional crowd counting on highly congested scenes’, arXiv preprint arXiv:1612.00220.</li><li>———. 2017. “Resnetcrowd: A residual deep learning architecture for crowd counting, violent behaviour detection and crowd density level classification.” In 2017 14th IEEE international conference on advanced video and signal based surveillance (AVSS), 1-7. IEEE.</li><li>Mundhenk, T Nathan, Goran Konjevod, Wesam A Sakla, and Kofi Boakye. 2016. “A large contextual dataset for classification, detection and counting of cars with deep learning.” In European Conference on Computer Vision, 785-800. Springer.</li><li>Onoro-Rubio, Daniel, and Roberto J López-Sastre. 2016. “Towards perspective-free object counting with deep learning.” In European conference on computer vision, 615-29. Springer.</li><li>Paul Cohen, Joseph, Genevieve Boucher, Craig A Glastonbury, Henry Z Lo, and Yoshua Bengio. 2017. “Count-ception: Counting by fully convolutional redundant counting.” In Proceedings of the IEEE International conference on computer vision workshops, 18-26.</li><li>Pham, Viet-Quoc, Tatsuo Kozakaya, Osamu Yamaguchi, and Ryuzo Okada. 2015. “Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation.” In Proceedings of the IEEE International Conference on Computer Vision, 3253-61.</li><li>Sabzmeydani, Payam, and Greg Mori. 2007. “Detecting pedestrians by learning shapelet features.” In 2007 IEEE Conference on Computer Vision and Pattern Recognition, 1-8. IEEE.</li><li>Sam, Deepak Babu, Shiv Surya, and R Venkatesh Babu. ‘Supplementary Material: Switching Convolutional Neural Network for Crowd Counting’.</li><li>Shami, Mamoona Birkhez, Salman Maqbool, Hasan Sajid, Yasar Ayaz, and Sen-Ching Samson Cheung. 2018. ‘People counting in dense crowd images using sparse head detections’, IEEE Transactions on Circuits and Systems for Video Technology, 29: 2627-36.</li><li>Shi, Zenglin, Le Zhang, Yibo Sun, and Yangdong Ye. 2018. ‘Multiscale multitask deep NetVLAD for crowd counting’, IEEE Transactions on Industrial Informatics, 14: 4953-62.</li><li>Sindagi, Vishwanath A, and Vishal M Patel. 2017. “Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting.” In 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), 1-6. IEEE.</li><li>Sindagi, Vishwanath A., and Vishal M. Patel. 2018. ‘A survey of recent advances in CNN-based single image crowd counting and density estimation’, Pattern Recognition Letters, 107: 3-16.</li><li>Sindagi, Vishwanath, Rajeev Yasarla, and Vishal MM Patel. 2020. ‘Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method’, IEEE Transactions on Pattern Analysis and Machine Intelligence.</li><li>Walach, Elad, and Lior Wolf. 2016. “Learning to count with cnn boosting.” In European conference on computer vision, 660-76. Springer.</li><li>Wang, Chuan, Hua Zhang, Liang Yang, Si Liu, and Xiaochun Cao. 2015. “Deep people counting in extremely dense crowds.” In Proceedings of the 23rd ACM international conference on Multimedia, 1299-302.</li><li>Wang, Li, Weiyuan Shao, Yao Lu, Hao Ye, Jian Pu, and Yingbin Zheng. 2018. ‘Crowd counting with density adaption networks’, arXiv preprint arXiv:1806.10040.</li><li>Wang, Nannan, Wenjin Zha, Jie Li, and Xinbo Gao. 2018. ‘Back projection: An effective postprocessing method for GAN-based face sketch synthesis’, Pattern Recognition Letters, 107: 59-65.</li><li>Wang, Qi, Junyu Gao, Wei Lin, and Xuelong Li. 2021. ‘NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization’, IEEE Transactions on Pattern Analysis and Machine Intelligence, 43: 2141-49.</li><li>Wu, Bo, and Ramakant Nevatia. 2005. “Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors.” In Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1, 90-97. IEEE.</li><li>Xie, Weidi, J Alison Noble, and Andrew Zisserman. 2018. ‘Microscopy cell counting and detection with fully convolutional regression networks’, Computer methods in biomechanics and biomedical engineering: Imaging &amp; Visualization, 6: 283-92.</li><li>Xu, Bolei, and Guoping Qiu. 2016. “Crowd density estimation based on rich features and random projection forest.” In 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), 1-8. IEEE.</li><li>Xu, Mingliang, Zhaoyang Ge, Xiaoheng Jiang, Gaoge Cui, Pei Lv, Bing Zhou, and Changsheng Xu. 2019. ‘Depth information guided crowd counting for complex crowd scenes’, Pattern Recognition Letters, 125: 563-69.</li><li>Yao, Haiyan, Kang Han, Wanggen Wan, and Li Hou. 2017. ‘Deep spatial regression model for image crowd counting’, arXiv preprint arXiv:1710.09757.</li><li>Zhang, Yingying, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. 2016. “Single-image crowd counting via multi-column convolutional neural network.” In Proceedings of the IEEE conference on computer vision and pattern recognition, 589-97.</li><li>Zhang, Youmei, Chunluan Zhou, Faliang Chang, Alex C Kot, and Wei Zhang. 2019. “Attention to head locations for crowd counting.” In International Conference on Image and Graphics, 727-37. Springer.</li><li>Zhao, Muming, Jian Zhang, Fatih Porikli, Chongyang Zhang, and Wenjun Zhang. 2017. “Learning a perspective-embedded deconvolution network for crowd counting.” In 2017 IEEE International Conference on Multimedia and Expo (ICME), 403-08. IEEE.</li></ul>]]></content>
    
    
    <categories>
      
      <category>交通工程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>本科国创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习 SVM | vol2</title>
    <link href="/posts/e7902d53.html"/>
    <url>/posts/e7902d53.html</url>
    
    <content type="html"><![CDATA[<p>讨论一种不同的线性分类和回归方法，每种学习算法都具有不同的归纳偏倚，做不同的假设，定义不同的目标函数，因此可能找到不同的模型 Support Vector Machine</p><h2 id="0x01-引言"><a href="#0x01-引言" class="headerlink" title="0x01 引言"></a>0x01 引言</h2><p>支持向量机使用Vapnik原则</p><blockquote><p> 不要在解决实际问题之前把解决一个更复杂的问题作为第一步（vapnik，1995） </p></blockquote><p>训练后，线性模型的参数（权重向量）可以用训练集的一个子集表示，这个子集称为支持向量support vector。对于分类这些是靠近边界的实例，因此知道它们可以提取知识：这些事两个类之间的边界附近、不确定或有错误的实例，它们的个数给我们提供了范化误差的一个估计，并且正如我们将在下面看到的，能够用实例集表示模型参数进行核化</p><p>其总体框架：是一种分类算法，主要应用于二分类算法，通过定位两类数据在集合空间中的margin，来确定分类起，主要步骤分为：</p><ol><li>（建立模型）将二分类问题转换成为一个优化问题</li><li>（求解模型）利用SMO算法、拉格朗日对偶、KKT条件来求解简化优化问题</li><li>（分类讨论）之后利用松弛变量、核函数、惩罚因子等对不同情况下的优化问题进行求解</li></ol><p>它的优点和缺点是什么？</p><ul><li>优点：参数非常少，不像logistic regression还具有超参数，虽然比神经网络效果差，但是很简单</li><li>缺点：</li></ul><h2 id="0x02-建立模型"><a href="#0x02-建立模型" class="headerlink" title="0x02 建立模型"></a>0x02 建立模型</h2><h3 id="符号说明（notation）"><a href="#符号说明（notation）" class="headerlink" title="符号说明（notation）"></a>符号说明（notation）</h3><p>$$ ：等价于之前的𝑖𝑛𝑝𝑢𝑡:𝑋&#x3D;(𝑥1,…,𝑥𝑛) 𝑙𝑎𝑏𝑒𝑙𝑠：𝑦∈−1,1 ℎ𝑦𝑝𝑒𝑟𝑝𝑙𝑎𝑛𝑒:𝜔𝑇𝑋+𝑏(等价于之前的𝜃𝑇𝑋) 𝑔(𝑍)&#x3D;{1 𝑖𝑓 𝑍&gt;&#x3D;0 −1 𝑖𝑓 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 $$</p><p>数据集和超平面的距离</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-c3b7e25a83a399c4e7f2cae9aeee13f8_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="定义边缘（margin）"><a href="#定义边缘（margin）" class="headerlink" title="定义边缘（margin）"></a>定义边缘（margin）</h3><h3 id="geo-margin"><a href="#geo-margin" class="headerlink" title="geo- margin"></a>geo- margin</h3><p>从单个实例来看，就是向量到一个平面的距离，根据向量的推导知识可以看出： $$ geo_{margin}^{i}&#x3D;y^{(i)}*(\omega^TX^i+b) $$ 从整个数据集来看，就是一群点到一个平面中距离的最小值，定义为 $$ geo_{margin}&#x3D;min_{i&#x3D;1}^m geo_{margin}^i $$</p><h3 id="fun-margin"><a href="#fun-margin" class="headerlink" title="fun- margin"></a>fun- margin</h3><p>上述的几何边缘的定义中，最终出现的结果可能会出现结果会随着不同的w的取值而出现不同的情况，因此需要从归一化的角度来更新边缘 $$ fun_{margin}^{(i)}&#x3D;\frac{y^{(i)}*(\omega^TX^{(i)}+b)}{||w||} $$ 同样定义数据集的fun margin $$ fun_{margin}&#x3D;min_{i&#x3D;1}^m\ fun_{margin}^{i} $$</p><h3 id="得到模型（model）"><a href="#得到模型（model）" class="headerlink" title="得到模型（model）"></a>得到模型（model）</h3><p>最直观的，我们希望所有点的margin可以都以大于零，在这个约束的基础上，数据集的margin可以越大越好 $$ target:max[min_{i&#x3D;1}^m y^{(i)}<em>(\omega^TX^i+b)]\ s.t.:y^{(i)}</em>(\omega^TX^i+b)&gt;&#x3D;0\ ||w||&#x3D;1 $$ 在上述直觉的基础上，需要对模型进行进一步的调整</p><ol><li>我们希望所有的margin都可以大于零，也可以说是希望所有的margin都大于最小margin（目标函数）的值    $$    max\ \hat\gamma\    s.t.:y^{(i)}(w^Tx^{(i)}+b)&gt;&#x3D;\hat\gamma\    ||w||&#x3D;1    $$ </li><li>可以将||w||&#x3D;1的假设调整到目标函数上    $$    max\ \hat\gamma&#x2F;||w||\    s.t.:y^{(i)}(w^Tx^{(i)}+b)&gt;&#x3D;\hat\gamma    $$ </li><li>同样的跟进一步，我们假设之间的距离为1    $$    \hat \gamma&#x3D;1    $$    那么模型等价于    $$    max\ \frac{1}{||w||}\    s.t.:y^{(i)}(w^Tx^{(i)}+b)&gt;&#x3D;\hat\gamma    $$    为了方便后续计算    $$    min \frac{1}{2}||w||^2    $$ </li><li>由此转换为一个凸优化问题    $$    min \frac{1}{2}||w||^2\    s.t.:1-y^{(i)}(w^Tx^{(i)}+b)&lt;&#x3D;0    $$</li></ol><h2 id="0x03-求解模型"><a href="#0x03-求解模型" class="headerlink" title="0x03 求解模型"></a>0x03 求解模型</h2><h3 id="数学基础1（凸优化）"><a href="#数学基础1（凸优化）" class="headerlink" title="数学基础1（凸优化）"></a>数学基础1（凸优化）</h3><p>优化问题可以分为三类：</p><p>同样所有的优化问题，都可以转换成为对偶问题，来简化求解过程</p><ul><li>无约束条件的优化问题   $$   min\ f(x)\   —\   eg:min\ f(x)&#x3D;x_1^2+x_1-4   $$   求解思路：求导完事了 </li><li>仅含等式约束的优化问题   $$   min\ f(x)\   s.t.\ h_j(x)&#x3D;0,j\in {1,2,…,n}\   —\   eg: min\ f(x)&#x3D;2x_1^2+3x_2^2+7x_3^2\   s.t.\ 2x_1+x_2&#x3D;1\   2x_2+3x_3&#x3D;2   $$   求解思路：利用拉格朗日乘子法，见下一节 </li><li>含不等式和等式的优化问题   $$   min\ f(x)\   s.t. h_j(x)&#x3D;0,j\in {1,2,…,n}\   g_j(x)&lt;&#x3D;0,j\in{1,2,…,m}\</li></ul><p>\   eg:min\ f(x)&#x3D;x_1^2-2x_1+1+x_2^2+4x_2+4\   s.t.:x_1+10x_2&gt;10\   10x_1-x_2&lt;10   $$   求解思路：利用拉格朗日乘子法，但是需要满足KKT条件</p><h3 id="数学基础2（拉格朗日乘子法）"><a href="#数学基础2（拉格朗日乘子法）" class="headerlink" title="数学基础2（拉格朗日乘子法）"></a>数学基础2（拉格朗日乘子法）</h3><h3 id="求解上述：仅含等式约束的优化问题"><a href="#求解上述：仅含等式约束的优化问题" class="headerlink" title="求解上述：仅含等式约束的优化问题"></a>求解上述：仅含等式约束的优化问题</h3><p>首先构建拉格朗日函数 $$ L（x，\alpha）&#x3D;2x_1^2+3x_2^2+7x_3^2+\alpha_1*(2x_1+x_2-1)+\alpha_2*(2x_2+3x_3-2) $$ 此时针对拉格朗日函数对变量X求导，希望等于0 $$ \frac{\partial L}{\partial x_1}&#x3D;0\ \frac{\partial L}{\partial x_2}&#x3D;0\ \frac{\partial L}{\partial x_3}&#x3D;0 $$ 得到 $$ x_1&#x3D;-\frac {1}{2}\alpha_1\ x_2&#x3D;-\frac {1}{6}\alpha_1-\frac {1}{3}\alpha_2\ x_3&#x3D;-\frac {3}{14}\alpha_2\ $$ 带入约束条件中求解等式，得到最优解</p><h3 id="同样的求解上述：含有等式和不等式的优化问题"><a href="#同样的求解上述：含有等式和不等式的优化问题" class="headerlink" title="同样的求解上述：含有等式和不等式的优化问题"></a>同样的求解上述：含有等式和不等式的优化问题</h3><p>$$ L(x,\alpha)&#x3D;x_1^2-2x_1+1+x_2^2+4x_2+4+\alpha_1(-x_1-10x_2+10)+\alpha_2(10x_1-x_2-10)\ \alpha_i&gt;&#x3D;0 $$</p><p>和上述相似对拉格朗日求导，希望等于0 $$ \frac{\partial L}{\partial x_1}&#x3D;0\ \frac{\partial L}{\partial x_2}&#x3D;0\ $$ 这里需要满足KKT条件 $$  <em>导数等于0*  h(X)&#x3D;0\ </em>\Sigma_{j&#x3D;1}^m\alpha_ig_i(X)&#x3D;0 $$</p><h3 id="数学基础3（对偶求解）"><a href="#数学基础3（对偶求解）" class="headerlink" title="数学基础3（对偶求解）"></a>数学基础3（对偶求解）</h3><blockquote><p> Slater条件：一个凸优化问题如果存在一个候选x是的所有不等式约束都严格满足，则存在是原问题的对偶问题的最优解 </p></blockquote><p>这样有时候可以简化操作计算,</p><h3 id="求解SVM的凸优化问题"><a href="#求解SVM的凸优化问题" class="headerlink" title="求解SVM的凸优化问题"></a>求解SVM的凸优化问题</h3><ol><li>首先将优化问题转换成为一个 convex optimization（P）</li><li>因为满足Slater条件，P为对偶问题D是同解的</li></ol><p>拉格朗日乘子法-P问题的函数： $$ L(x,w,a)&#x3D;\frac{1}{2}||w||^2+\Sigma_{i&#x3D;1}^na_i(1-y^{(i)}(wx^{(i)}+b)) $$ 原问题： $$ p&#x3D;min_{w,b}max_a L $$ 转换为对偶问题 $$ d&#x3D;max_amin_{w,b}L $$ 这里是将第三种凸优化问题转换成为一个无约束的第一种凸优化问题 $$ \frac{\partial L}{\partial w}&#x3D;w-\Sigma_{i&#x3D;1}^na_iy^{(i)}x_i^T&#x3D;0\ \frac{\partial L}{\partial b}&#x3D;\Sigma_{i&#x3D;1}^na_iy^{(i)}&#x3D;0 $$ 带入到拉格朗日函数中 $$ L&#x3D;\frac{1}{2}\Sigma_{i,j&#x3D;1}^na_ia_jy_iy_jx_i^Tx_j-\Sigma_{i,j&#x3D;1}^na_ia_jy_iy_jx_i^Tx_j-b\Sigma_{i&#x3D;1}^na_iy^{(i)}+\Sigma_{i&#x3D;1}^na_i\ &#x3D;\Sigma_{i&#x3D;1}^na_i-\frac{1}{2}\Sigma_{i,j&#x3D;1}^na_ia_jy_iy_jx_i^Tx_j $$ 约束条件： $$ s.t.:\Sigma_{i&#x3D;1}^ma_iy_i&#x3D;0\ a_i&gt;&#x3D;0 $$ 之后利用SMO算法来求解得到 $$ a_i $$ 之后得到hyperplane为： $$ \Sigma (a_iy^{(i)}x_i^Tx_j+b) $$</p><h2 id="0x04-分类讨论"><a href="#0x04-分类讨论" class="headerlink" title="0x04 分类讨论"></a>0x04 分类讨论</h2><ol><li>在线性可分的情况下可以顺利的求解出 a_i</li><li>加入松弛变量和惩罚因子来找到相对更好的hyperplane</li><li>使用kernel trick来将低维映射到高纬的空间，使得在高纬空间中数据是线性可分的</li></ol><p>详细介绍kernel trick：</p><p>本来是在低维中直接内积 $$ x_i^Tx_j $$ 之后希望都从地维转换到高纬，在计算内积 $$ \phi(x_i)^T\phi(x_j) $$ 跟进一步，希望直接在低维空间计算高维内积 $$ 线性核：x_i^Tx_j\ 多项式核：（x_i^Tx_j+1）^q\ 高斯核：exp(-\frac{(||x_i-x_j||^2))}{2}\ sigmoid:tanh(x_i^Tx_j+1) $$</p><h2 id="0x05-SVM-code"><a href="#0x05-SVM-code" class="headerlink" title="0x05 SVM code"></a>0x05 SVM code</h2><p><a href="https://github.com/chenxia31/code_Study_Practice/tree/main/CS229_2018autumn/ProblemSet(%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A%EF%BC%89/%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%9802">github</a> </p><p>借鉴别人的，留一个坑自己写</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-956ab1b0ca37f05247403887591cc5d8_1440w.jpg" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数说明:读取数据</span><br><span class="hljs-string">Parameters:</span><br><span class="hljs-string">    fileName - 文件名</span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    dataMat - 数据矩阵</span><br><span class="hljs-string">    labelMat - 数据标签</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loadDataSet</span>(<span class="hljs-params">fileName</span>):<br>    dataMat = []; labelMat = []<br>    fr = <span class="hljs-built_in">open</span>(fileName)<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr.readlines():                                     <span class="hljs-comment">#逐行读取，滤除空格等</span><br>        lineArr = line.strip().split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        dataMat.append([<span class="hljs-built_in">float</span>(lineArr[<span class="hljs-number">0</span>]), <span class="hljs-built_in">float</span>(lineArr[<span class="hljs-number">1</span>])])      <span class="hljs-comment">#添加数据</span><br>        labelMat.append(<span class="hljs-built_in">float</span>(lineArr[<span class="hljs-number">2</span>]))                          <span class="hljs-comment">#添加标签</span><br>    <span class="hljs-keyword">return</span> dataMat,labelMat<br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数说明:随机选择alpha</span><br><span class="hljs-string">Parameters:</span><br><span class="hljs-string">    i - alpha</span><br><span class="hljs-string">    m - alpha参数个数</span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    j -</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">selectJrand</span>(<span class="hljs-params">i, m</span>):<br>    j = i                                 <span class="hljs-comment">#选择一个不等于i的j</span><br>    <span class="hljs-keyword">while</span> (j == i):<br>        j = <span class="hljs-built_in">int</span>(random.uniform(<span class="hljs-number">0</span>, m))<br>    <span class="hljs-keyword">return</span> j<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数说明:修剪alpha</span><br><span class="hljs-string">Parameters:</span><br><span class="hljs-string">    aj - alpha值</span><br><span class="hljs-string">    H - alpha上限</span><br><span class="hljs-string">    L - alpha下限</span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    aj - alpah值</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">clipAlpha</span>(<span class="hljs-params">aj,H,L</span>):<br>    <span class="hljs-keyword">if</span> aj &gt; H:<br>        aj = H<br>    <span class="hljs-keyword">if</span> L &gt; aj:<br>        aj = L<br>    <span class="hljs-keyword">return</span> aj<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数说明:简化版SMO算法</span><br><span class="hljs-string">Parameters:</span><br><span class="hljs-string">    dataMatIn - 数据矩阵</span><br><span class="hljs-string">    classLabels - 数据标签</span><br><span class="hljs-string">    C - 松弛变量</span><br><span class="hljs-string">    toler - 容错率</span><br><span class="hljs-string">    maxIter - 最大迭代次数</span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    无</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-comment"># 参数分别为 数据集、类别标签、常数C、容错率、最大循环次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">smoSimple</span>(<span class="hljs-params">dataMatIn, classLabels, C, toler, maxIter</span>):<span class="hljs-comment">#</span><br>    <span class="hljs-comment">#转换为numpy的mat存储</span><br>    dataMatrix = np.mat(dataMatIn); labelMat = np.mat(classLabels).transpose()<span class="hljs-comment">#转置成列向量</span><br>    <span class="hljs-comment">#初始化b参数，统计dataMatrix的维度</span><br>    b = <span class="hljs-number">0</span>; m,n = np.shape(dataMatrix)<span class="hljs-comment"># m=100,n=2</span><br>    <span class="hljs-comment">#初始化alpha参数，设为0</span><br>    alphas = np.mat(np.zeros((m,<span class="hljs-number">1</span>)))<span class="hljs-comment"># 100个alpha</span><br>    <span class="hljs-comment">#初始化迭代次数</span><br>    iter_num = <span class="hljs-number">0</span><br>    <span class="hljs-comment">#最多迭代matIter次</span><br>    <span class="hljs-keyword">while</span> (iter_num &lt; maxIter):<br>        alphaPairsChanged = <span class="hljs-number">0</span> <span class="hljs-comment">#用于记录alpha是否已经进行优化</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>            <span class="hljs-comment">#步骤1：计算误差Ei</span><br>            <span class="hljs-comment">#此公式为分类决策公式</span><br>            fXi = <span class="hljs-built_in">float</span>(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b<br>            <span class="hljs-comment">#print(&quot;fXi:&quot;,fXi)</span><br>            Ei = fXi - <span class="hljs-built_in">float</span>(labelMat[i])<span class="hljs-comment">#误差</span><br>            <span class="hljs-comment">#优化alpha，更设定一定的容错率。</span><br>            <span class="hljs-keyword">if</span> ((labelMat[i]*Ei &lt; -toler) <span class="hljs-keyword">and</span> (alphas[i] &lt; C)) <span class="hljs-keyword">or</span> ((labelMat[i]*Ei &gt; toler) <span class="hljs-keyword">and</span> (alphas[i] &gt; <span class="hljs-number">0</span>)):<br>                <span class="hljs-comment">#随机选择另一个与alpha_i成对优化的alpha_j</span><br>                j = selectJrand(i,m)<br>                <span class="hljs-comment">#步骤1：计算误差Ej</span><br>                fXj = <span class="hljs-built_in">float</span>(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b<br>                Ej = fXj - <span class="hljs-built_in">float</span>(labelMat[j])<br>                <span class="hljs-comment">#保存更新前的aplpha值，使用深拷贝</span><br>                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();<br>                <span class="hljs-comment">#步骤2：计算上下界L和H</span><br>                <span class="hljs-keyword">if</span> (labelMat[i] != labelMat[j]):<br>                    L = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, alphas[j] - alphas[i])<br>                    H = <span class="hljs-built_in">min</span>(C, C + alphas[j] - alphas[i])<br>                <span class="hljs-keyword">else</span>:<br>                    L = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, alphas[j] + alphas[i] - C)<br>                    H = <span class="hljs-built_in">min</span>(C, alphas[j] + alphas[i])<br>                <span class="hljs-keyword">if</span> L==H: <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;L==H&quot;</span>); <span class="hljs-keyword">continue</span><br><br>                <span class="hljs-comment">#步骤3：计算eta，即η。</span><br>                eta = dataMatrix[i,:]*dataMatrix[i,:].T +dataMatrix[j,:]*dataMatrix[j,:].T-<span class="hljs-number">2.0</span> * dataMatrix[i,:]*dataMatrix[j,:].T<br>                <span class="hljs-keyword">if</span> eta == <span class="hljs-number">0</span>: <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;eta=0&quot;</span>); <span class="hljs-keyword">continue</span><br>                <span class="hljs-comment">#步骤4：更新alpha_j</span><br>                alphas[j] += labelMat[j]*(Ei - Ej)/eta<br>                <span class="hljs-comment">#步骤5：修剪alpha_j</span><br>                alphas[j] = clipAlpha(alphas[j],H,L)<br>                <span class="hljs-comment">#abs 函数  返回数字的绝对值</span><br>                <span class="hljs-keyword">if</span> (<span class="hljs-built_in">abs</span>(alphas[j] - alphaJold) &lt; <span class="hljs-number">0.00001</span>): <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;alpha_j变化太小&quot;</span>); <span class="hljs-keyword">continue</span><br>                <span class="hljs-comment">#步骤6：更新alpha_i</span><br>                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])<br>                <span class="hljs-comment">#步骤7：更新b_1和b_2</span><br>                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T<br>                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T<br>                <span class="hljs-comment">#步骤8：根据b_1和b_2更新b</span><br>                <span class="hljs-keyword">if</span> (<span class="hljs-number">0</span> &lt; alphas[i]) <span class="hljs-keyword">and</span> (C &gt; alphas[i]): b = b1<br>                <span class="hljs-keyword">elif</span> (<span class="hljs-number">0</span> &lt; alphas[j]) <span class="hljs-keyword">and</span> (C &gt; alphas[j]): b = b2<br>                <span class="hljs-keyword">else</span>: b = (b1 + b2)/<span class="hljs-number">2.0</span><br>                <span class="hljs-comment">#统计优化次数</span><br>                alphaPairsChanged += <span class="hljs-number">1</span><br>                <span class="hljs-comment">#打印统计信息</span><br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;第%d次迭代 样本:%d, alpha优化次数:%d&quot;</span> % (iter_num,i,alphaPairsChanged))<br>        <span class="hljs-comment">#更新迭代次数</span><br>        <span class="hljs-keyword">if</span> (alphaPairsChanged == <span class="hljs-number">0</span>): iter_num += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>: iter_num = <span class="hljs-number">0</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;迭代次数: %d&quot;</span> % iter_num)<br>    <span class="hljs-keyword">return</span> b,alphas<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数说明:分类结果可视化</span><br><span class="hljs-string">Parameters:</span><br><span class="hljs-string">    dataMat - 数据矩阵</span><br><span class="hljs-string">    w - 直线法向量</span><br><span class="hljs-string">    b - 直线解决</span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    无</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">showClassifer</span>(<span class="hljs-params">dataMat, w, b</span>):<br>    <span class="hljs-comment">#绘制样本点</span><br>    data_plus = []                                  <span class="hljs-comment">#正样本</span><br>    data_minus = []                                 <span class="hljs-comment">#负样本</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(dataMat)):<br>        <span class="hljs-keyword">if</span> labelMat[i] &gt; <span class="hljs-number">0</span>:<br>            data_plus.append(dataMat[i])<br>        <span class="hljs-keyword">else</span>:<br>            data_minus.append(dataMat[i])<br>    data_plus_np = np.array(data_plus)              <span class="hljs-comment">#转换为numpy矩阵</span><br>    data_minus_np = np.array(data_minus)            <span class="hljs-comment">#转换为numpy矩阵</span><br>    plt.scatter(np.transpose(data_plus_np)[<span class="hljs-number">0</span>], np.transpose(data_plus_np)[<span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, alpha=<span class="hljs-number">0.7</span>)   <span class="hljs-comment">#正样本散点图</span><br>    plt.scatter(np.transpose(data_minus_np)[<span class="hljs-number">0</span>], np.transpose(data_minus_np)[<span class="hljs-number">1</span>], s=<span class="hljs-number">30</span>, alpha=<span class="hljs-number">0.7</span>) <span class="hljs-comment">#负样本散点图</span><br>    <span class="hljs-comment">#绘制直线</span><br>    x1 = <span class="hljs-built_in">max</span>(dataMat)[<span class="hljs-number">0</span>]<br>    x2 = <span class="hljs-built_in">min</span>(dataMat)[<span class="hljs-number">0</span>]<br>    a1, a2 = w<br>    b = <span class="hljs-built_in">float</span>(b)<br>    a1 = <span class="hljs-built_in">float</span>(a1[<span class="hljs-number">0</span>])<br>    a2 = <span class="hljs-built_in">float</span>(a2[<span class="hljs-number">0</span>])<br>    y1, y2 = (-b- a1*x1)/a2, (-b - a1*x2)/a2<br>    plt.plot([x1, x2], [y1, y2])<br>    <span class="hljs-comment">#找出支持向量点</span><br>    <span class="hljs-keyword">for</span> i, alpha <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(alphas):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(alpha) &gt; <span class="hljs-number">0</span>:<span class="hljs-comment">#如果alpha&gt;0,表示 alpha所在的不等式条件起作用了</span><br>            x, y = dataMat[i]<br>            plt.scatter([x], [y], s=<span class="hljs-number">150</span>, c=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.7</span>, linewidth=<span class="hljs-number">1.5</span>, edgecolor=<span class="hljs-string">&#x27;red&#x27;</span>)<br>    plt.show()<br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数说明:计算w</span><br><span class="hljs-string">Parameters:</span><br><span class="hljs-string">    dataMat - 数据矩阵</span><br><span class="hljs-string">    labelMat - 数据标签</span><br><span class="hljs-string">    alphas - alphas值</span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    w</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_w</span>(<span class="hljs-params">dataMat, labelMat, alphas</span>):<br>    alphas, dataMat, labelMat = np.array(alphas), np.array(dataMat), np.array(labelMat)<br>    w = np.dot((np.tile(labelMat.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).T, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) * dataMat).T, alphas)<br>    <span class="hljs-keyword">return</span> w.tolist()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dataMat, labelMat = loadDataSet(<span class="hljs-string">&#x27;datasets/testSet.txt&#x27;</span>)<br>    b,alphas = smoSimple(dataMat, labelMat, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">40</span>)<br>    <span class="hljs-comment"># print(&quot;b,alphas&quot;,b,alphas)</span><br>    w = get_w(dataMat, labelMat, alphas)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;w:&quot;</span>,w)<br>    showClassifer(dataMat, w, b)<br></code></pre></td></tr></table></figure><p>&#x2F;datasets&#x2F;testSet.txt</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><code class="hljs dns"><span class="hljs-number">3.542485</span>    <span class="hljs-number">1.977398</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">3.018896</span>    <span class="hljs-number">2.556416</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">7.551510</span>    -<span class="hljs-number">1.580030</span>   <span class="hljs-number">1</span><br><span class="hljs-number">2.114999</span>    -<span class="hljs-number">0.004466</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.127113</span>    <span class="hljs-number">1.274372</span>    <span class="hljs-number">1</span><br><span class="hljs-number">7.108772</span>    -<span class="hljs-number">0.986906</span>   <span class="hljs-number">1</span><br><span class="hljs-number">8.610639</span>    <span class="hljs-number">2.046708</span>    <span class="hljs-number">1</span><br><span class="hljs-number">2.326297</span>    <span class="hljs-number">0.265213</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">3.634009</span>    <span class="hljs-number">1.730537</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">0.341367</span>    -<span class="hljs-number">0.894998</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">3.125951</span>    <span class="hljs-number">0.293251</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">2.123252</span>    -<span class="hljs-number">0.783563</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">0.887835</span>    -<span class="hljs-number">2.797792</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">7.139979</span>    -<span class="hljs-number">2.329896</span>   <span class="hljs-number">1</span><br><span class="hljs-number">1.696414</span>    -<span class="hljs-number">1.212496</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.117032</span>    <span class="hljs-number">0.623493</span>    <span class="hljs-number">1</span><br><span class="hljs-number">8.497162</span>    -<span class="hljs-number">0.266649</span>   <span class="hljs-number">1</span><br><span class="hljs-number">4.658191</span>    <span class="hljs-number">3.507396</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">8.197181</span>    <span class="hljs-number">1.545132</span>    <span class="hljs-number">1</span><br><span class="hljs-number">1.208047</span>    <span class="hljs-number">0.213100</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">1.928486</span>    -<span class="hljs-number">0.321870</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">2.175808</span>    -<span class="hljs-number">0.014527</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">7.886608</span>    <span class="hljs-number">0.461755</span>    <span class="hljs-number">1</span><br><span class="hljs-number">3.223038</span>    -<span class="hljs-number">0.552392</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">3.628502</span>    <span class="hljs-number">2.190585</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">7.407860</span>    -<span class="hljs-number">0.121961</span>   <span class="hljs-number">1</span><br><span class="hljs-number">7.286357</span>    <span class="hljs-number">0.251077</span>    <span class="hljs-number">1</span><br><span class="hljs-number">2.301095</span>    -<span class="hljs-number">0.533988</span>   -<span class="hljs-number">1</span><br>-<span class="hljs-number">0.232542</span>   -<span class="hljs-number">0.547690</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">3.457096</span>    -<span class="hljs-number">0.082216</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">3.023938</span>    -<span class="hljs-number">0.057392</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.015003</span>    <span class="hljs-number">0.885325</span>    <span class="hljs-number">1</span><br><span class="hljs-number">8.991748</span>    <span class="hljs-number">0.923154</span>    <span class="hljs-number">1</span><br><span class="hljs-number">7.916831</span>    -<span class="hljs-number">1.781735</span>   <span class="hljs-number">1</span><br><span class="hljs-number">7.616862</span>    -<span class="hljs-number">0.217958</span>   <span class="hljs-number">1</span><br><span class="hljs-number">2.450939</span>    <span class="hljs-number">0.744967</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">7.270337</span>    -<span class="hljs-number">2.507834</span>   <span class="hljs-number">1</span><br><span class="hljs-number">1.749721</span>    -<span class="hljs-number">0.961902</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">1.803111</span>    -<span class="hljs-number">0.176349</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.804461</span>    <span class="hljs-number">3.044301</span>    <span class="hljs-number">1</span><br><span class="hljs-number">1.231257</span>    -<span class="hljs-number">0.568573</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">2.074915</span>    <span class="hljs-number">1.410550</span>    -<span class="hljs-number">1</span><br>-<span class="hljs-number">0.743036</span>   -<span class="hljs-number">1.736103</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">3.536555</span>    <span class="hljs-number">3.964960</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">8.410143</span>    <span class="hljs-number">0.025606</span>    <span class="hljs-number">1</span><br><span class="hljs-number">7.382988</span>    -<span class="hljs-number">0.478764</span>   <span class="hljs-number">1</span><br><span class="hljs-number">6.960661</span>    -<span class="hljs-number">0.245353</span>   <span class="hljs-number">1</span><br><span class="hljs-number">8.234460</span>    <span class="hljs-number">0.701868</span>    <span class="hljs-number">1</span><br><span class="hljs-number">8.168618</span>    -<span class="hljs-number">0.903835</span>   <span class="hljs-number">1</span><br><span class="hljs-number">1.534187</span>    -<span class="hljs-number">0.622492</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">9.229518</span>    <span class="hljs-number">2.066088</span>    <span class="hljs-number">1</span><br><span class="hljs-number">7.886242</span>    <span class="hljs-number">0.191813</span>    <span class="hljs-number">1</span><br><span class="hljs-number">2.893743</span>    -<span class="hljs-number">1.643468</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">1.870457</span>    -<span class="hljs-number">1.040420</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">5.286862</span>    -<span class="hljs-number">2.358286</span>   <span class="hljs-number">1</span><br><span class="hljs-number">6.080573</span>    <span class="hljs-number">0.418886</span>    <span class="hljs-number">1</span><br><span class="hljs-number">2.544314</span>    <span class="hljs-number">1.714165</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">6.016004</span>    -<span class="hljs-number">3.753712</span>   <span class="hljs-number">1</span><br><span class="hljs-number">0.926310</span>    -<span class="hljs-number">0.564359</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">0.870296</span>    -<span class="hljs-number">0.109952</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">2.369345</span>    <span class="hljs-number">1.375695</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">1.363782</span>    -<span class="hljs-number">0.254082</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">7.279460</span>    -<span class="hljs-number">0.189572</span>   <span class="hljs-number">1</span><br><span class="hljs-number">1.896005</span>    <span class="hljs-number">0.515080</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">8.102154</span>    -<span class="hljs-number">0.603875</span>   <span class="hljs-number">1</span><br><span class="hljs-number">2.529893</span>    <span class="hljs-number">0.662657</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">1.963874</span>    -<span class="hljs-number">0.365233</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.132048</span>    <span class="hljs-number">0.785914</span>    <span class="hljs-number">1</span><br><span class="hljs-number">8.245938</span>    <span class="hljs-number">0.372366</span>    <span class="hljs-number">1</span><br><span class="hljs-number">6.543888</span>    <span class="hljs-number">0.433164</span>    <span class="hljs-number">1</span><br>-<span class="hljs-number">0.236713</span>   -<span class="hljs-number">5.766721</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.112593</span>    <span class="hljs-number">0.295839</span>    <span class="hljs-number">1</span><br><span class="hljs-number">9.803425</span>    <span class="hljs-number">1.495167</span>    <span class="hljs-number">1</span><br><span class="hljs-number">1.497407</span>    -<span class="hljs-number">0.552916</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">1.336267</span>    -<span class="hljs-number">1.632889</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">9.205805</span>    -<span class="hljs-number">0.586480</span>   <span class="hljs-number">1</span><br><span class="hljs-number">1.966279</span>    -<span class="hljs-number">1.840439</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.398012</span>    <span class="hljs-number">1.584918</span>    <span class="hljs-number">1</span><br><span class="hljs-number">7.239953</span>    -<span class="hljs-number">1.764292</span>   <span class="hljs-number">1</span><br><span class="hljs-number">7.556201</span>    <span class="hljs-number">0.241185</span>    <span class="hljs-number">1</span><br><span class="hljs-number">9.015509</span>    <span class="hljs-number">0.345019</span>    <span class="hljs-number">1</span><br><span class="hljs-number">8.266085</span>    -<span class="hljs-number">0.230977</span>   <span class="hljs-number">1</span><br><span class="hljs-number">8.545620</span>    <span class="hljs-number">2.788799</span>    <span class="hljs-number">1</span><br><span class="hljs-number">9.295969</span>    <span class="hljs-number">1.346332</span>    <span class="hljs-number">1</span><br><span class="hljs-number">2.404234</span>    <span class="hljs-number">0.570278</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">2.037772</span>    <span class="hljs-number">0.021919</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">1.727631</span>    -<span class="hljs-number">0.453143</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">1.979395</span>    -<span class="hljs-number">0.050773</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">8.092288</span>    -<span class="hljs-number">1.372433</span>   <span class="hljs-number">1</span><br><span class="hljs-number">1.667645</span>    <span class="hljs-number">0.239204</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">9.854303</span>    <span class="hljs-number">1.365116</span>    <span class="hljs-number">1</span><br><span class="hljs-number">7.921057</span>    -<span class="hljs-number">1.327587</span>   <span class="hljs-number">1</span><br><span class="hljs-number">8.500757</span>    <span class="hljs-number">1.492372</span>    <span class="hljs-number">1</span><br><span class="hljs-number">1.339746</span>    -<span class="hljs-number">0.291183</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">3.107511</span>    <span class="hljs-number">0.758367</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">2.609525</span>    <span class="hljs-number">0.902979</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">3.263585</span>    <span class="hljs-number">1.367898</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">2.912122</span>    -<span class="hljs-number">0.202359</span>   -<span class="hljs-number">1</span><br><span class="hljs-number">1.731786</span>    <span class="hljs-number">0.589096</span>    -<span class="hljs-number">1</span><br><span class="hljs-number">2.387003</span>    <span class="hljs-number">1.573131</span>    -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229机器学习广义线性模型 | vol1</title>
    <link href="/posts/ceb408a2.html"/>
    <url>/posts/ceb408a2.html</url>
    
    <content type="html"><![CDATA[<p>根据Andrew wu <a href="https://www.bilibili.com/video/BV1JE411w7Ub?from=search&seid=12699853882563574773&spm_id_from=333.337.0.0">机器学习</a>的监督学习的第一部分总结</p><h2 id="0x01-引言"><a href="#0x01-引言" class="headerlink" title="0x01 引言"></a><strong>0x01 引言</strong></h2><p>机器学习的定义：</p><blockquote><p>对于某类任务task和性能度量performance，如果一个计算机程序在task上以performance衡量的性能随着经验experience而自我完善，那么我们称这个计算机程序在从experience中学习</p></blockquote><p>机器学习的方法在不同领域中以不同的方式逐渐发展到共通的状态，从统计学习中从特殊到一般的inference、从工程学中的模式识别，以及结合神经网络、信号处理、自动控制、人工智能以及数据挖掘等不同领域来综合方法，从数据中induction，从训练的模型deduction</p><p>在区分中机器学习常见分为监督学习supervised learning、非监督学习unsupervised learning和强化学习reinforcement learning</p><ul><li>监督学习，给定dataset，包括输入x和输出y，由此来构建一个算法得到x和y的映射关系</li><li>非监督学习：给定dataset，包括输入x，由此来构建一些算法</li></ul><p>机器学习算法的过程包括：</p><ol><li>首先构建一个假设函数（hypothesis），其中包含一个模型中需要的参数（parameter）</li><li>之后构建目标函数（target），来让包含参数的模型更加贴合实际，这里需要一定的目标函数来刻画</li><li>转换成为一个优化问题（optimization），可以利用一些算法来进去求解</li><li>对模型进行指标的评价</li></ol><h2 id="0x02-线性回归-linear-regression"><a href="#0x02-线性回归-linear-regression" class="headerlink" title="0x02 线性回归 linear regression"></a><strong>0x02 线性回归 linear regression</strong></h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a><strong>数据集</strong></h3><p>$$ 输入变量：输出变量：单个实例：完整数据集：输入变量：𝑋𝑖&#x3D;[𝑥1𝑖,𝑥2𝑖,…,𝑥𝑛𝑖]输出变量：𝑦𝑖单个实例：{𝑋𝑖,𝑦𝑖}完整数据集：{(𝑋𝑖,𝑦𝑖),𝑖&#x3D;1,…,𝑚}  $$</p><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a><strong>假设</strong></h3><p>线性回归中比较强的假设为</p><p>$$ 𝑦^&#x3D;ℎ𝜃(𝑋)&#x3D;𝜃𝑇∗𝑋𝜃:[1,(𝑛+1)]𝑋:&#x3D;[𝑥0,𝑥1,…,𝑥𝑛],𝑥0&#x3D;1  $$</p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a><strong>目标函数</strong></h3><ul><li>直观的方法，目标函数是预测值predict和真实值ground- truth之间的某种不相似的关系，我们希望的是两者尽可能的相似，所以这里可以使用欧氏距离来刻画： $$  </li><li>$$ （这里的1&#x2F;2只是为了方便后续求导的方便） 我们希望这个越小越好</li><li>概率解释的方法，在预测中总会有着各种各样的误差，所以 $$  </li><li>$$ 这样解释<strong>其实是将预测值 y 看成是一个分布，这个分布首先具有一个概率分布函数，之后我们在预测中希望的是所有预测值的分布在取真实值的时候概率最大，这时候求解出来的参数就是模型想要的参数</strong> 根据中心极限定理这个推导得出误差项分布是一种高斯分布，则得到预测值的分布 $$   $$ 得到最大似然函数（likelihood function） $$   $$ 然后对其取对数后求导 $$   $$ 如果希望上式最小，<strong>等价</strong> $$   $$ 或者<strong>等价</strong>于 $$   $$</li></ul><p>非常amazing的看出两个方法居然得到相同的结论！但其实深思我们会发现第一种方法我们是根据经验来得到，我们主观的认识用在线性回归中利用欧式距离会好一点，第二种方法我们带着一个假设（误差满足高斯分布）出发，来得到概率分布，通过maximum likelihood estimation（MLE）得到损失函数,两者只是解释的方法不同</p><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a><strong>优化算法</strong></h3><p>这种优化算法的常见是使用梯度下降的方式进行计算：</p><p>$$   $$</p><p>由上述推导看出我们的目标函数的梯度</p><p>$$   $$</p><p>这样就可以每次运行下去了，但是我们会发现在优化的过程每次梯度下降的过程都需要对所有数据进行计算，所以通常被称为<strong>batch gradient descent（BGD）</strong>，但是这样速度通常会变的很慢，因此需要尝试<strong>stochastic gradient descent（SGD）</strong>下降的梯度只使用一个点而不是所有点的梯度相加，由此得到最终解</p><h2 id="0x03-逻辑回归-logistics-regression"><a href="#0x03-逻辑回归-logistics-regression" class="headerlink" title="0x03 逻辑回归 logistics regression"></a><strong>0x03 逻辑回归 logistics regression</strong></h2><h3 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a><strong>数据集</strong></h3><p>$$   $$</p><p>与之前不同的是，这里的输出变量或者说是label，仅为0或者1</p><h3 id="假设-1"><a href="#假设-1" class="headerlink" title="假设"></a><strong>假设</strong></h3><p>我们仍然假设为线性模型：</p><p>$$   $$</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><strong>损失函数</strong></h3><p>这里我们从概率分布的角度出发，利用MLE的方式来寻找最佳的损失函数</p><p>首先确定预测值 y 的分布</p><p>$$   $$</p><p>由此我们可以出预测值正确的概率分布函数为：</p><p>$$   $$</p><p>我们希望针对所有的分布函数可以最大化</p><p>$$   $$</p><h3 id="优化算法-1"><a href="#优化算法-1" class="headerlink" title="优化算法"></a><strong>优化算法</strong></h3><ul><li>GD 梯度下降的方法 首先计算梯度、然后更新 $$   $$ 更新梯度 $$   $$</li><li>Newton法</li></ul><p>推导过程没有看，但是更新参数的过程为</p><p>$$   $$</p><h3 id="番外–-logistic-code"><a href="#番外–-logistic-code" class="headerlink" title="番外– logistic code"></a><strong>番外– logistic code</strong></h3><p>根据机器学习课后习题来的：</p><ol><li>建立一个线性模型的class：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LinearModel</span>(<span class="hljs-title class_ inherited__">object</span>):<br>   <span class="hljs-string">&quot;&quot;&quot;Base class for linear models.&quot;&quot;&quot;</span><br><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, step_size=<span class="hljs-number">0.2</span>, max_iter=<span class="hljs-number">100</span>, eps=<span class="hljs-number">1e-2</span>,</span><br><span class="hljs-params">                theta_0=<span class="hljs-literal">None</span>, learning_rate=<span class="hljs-number">0.01</span>,verbose=<span class="hljs-literal">True</span></span>):<br>       <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">      Args:</span><br><span class="hljs-string">          step_size: Step size for iterative solvers only.</span><br><span class="hljs-string">          max_iter: Maximum number of iterations for the solver.</span><br><span class="hljs-string">          eps: Threshold for determining convergence.</span><br><span class="hljs-string">          theta_0: Initial guess for theta. If None, use the zero vector.</span><br><span class="hljs-string">          verbose: Print loss values during training.</span><br><span class="hljs-string">      &quot;&quot;&quot;</span><br>       <span class="hljs-variable language_">self</span>.theta = theta_0<br>       <span class="hljs-variable language_">self</span>.step_size = step_size<br>       <span class="hljs-variable language_">self</span>.max_iter = max_iter<br>       <span class="hljs-variable language_">self</span>.eps = eps<br>       <span class="hljs-variable language_">self</span>.verbose = verbose<br>       <span class="hljs-variable language_">self</span>.learning_rate=learning_rate<br><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, x, y</span>):<br>       <span class="hljs-string">&quot;&quot;&quot;Run solver to fit linear model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">      Args:</span><br><span class="hljs-string">          x: Training example inputs. Shape (m, n).</span><br><span class="hljs-string">          y: Training example labels. Shape (m,).</span><br><span class="hljs-string">      &quot;&quot;&quot;</span><br>       <span class="hljs-keyword">raise</span> NotImplementedError(<span class="hljs-string">&#x27;Subclass of LinearModel must implement fit method.&#x27;</span>)<br><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):<br>       <span class="hljs-string">&quot;&quot;&quot;Make a prediction given new inputs x.</span><br><span class="hljs-string"></span><br><span class="hljs-string">      Args:</span><br><span class="hljs-string">          x: Inputs of shape (m, n).</span><br><span class="hljs-string"></span><br><span class="hljs-string">      Returns:</span><br><span class="hljs-string">          Outputs of shape (m,).</span><br><span class="hljs-string">      &quot;&quot;&quot;</span><br>       <span class="hljs-keyword">raise</span> NotImplementedError(<span class="hljs-string">&#x27;Subclass of LinearModel must implement predict method.&#x27;</span>)<br><br></code></pre></td></tr></table></figure><p>之后对其进行更改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> util<br><span class="hljs-keyword">from</span> linear_model <span class="hljs-keyword">import</span> LinearModel<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">train_path, eval_path, pred_path</span>):<br>   <span class="hljs-string">&quot;&quot;&quot;Problem 1(b): Logistic regression with Newton&#x27;s Method.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Args:</span><br><span class="hljs-string">      train_path: Path to CSV file containing dataset for training.</span><br><span class="hljs-string">      eval_path: Path to CSV file containing dataset for evaluation.</span><br><span class="hljs-string">      pred_path: Path to save predictions.</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>   x_train, y_train = util.load_dataset(train_path, add_intercept=<span class="hljs-literal">True</span>)<br>   x_eval, y_eval = util.load_dataset(eval_path, add_intercept=<span class="hljs-literal">True</span>)<br><br>   <span class="hljs-comment"># *** START CODE HERE ***</span><br>   model = LogisticRegression()<br>   model.fit(x_train, y_train)<br>   <span class="hljs-comment"># *** END CODE HERE ***</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LogisticRegression</span>(<span class="hljs-title class_ inherited__">LinearModel</span>):<br>   <span class="hljs-string">&quot;&quot;&quot;Logistic regression with Newton&#x27;s Method as the solver.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Example usage:</span><br><span class="hljs-string">      &gt; clf = LogisticRegression()</span><br><span class="hljs-string">      &gt; clf.fit(x_train, y_train)</span><br><span class="hljs-string">      &gt; clf.predict(x_eval)</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, x, y</span>):<br>       <span class="hljs-string">&quot;&quot;&quot;Run Newton&#x27;s Method to minimize J(theta) for logistic regression.</span><br><span class="hljs-string"></span><br><span class="hljs-string">      Args:</span><br><span class="hljs-string">          x: Training example inputs. Shape (m, n).</span><br><span class="hljs-string">          y: Training example labels. Shape (m,).</span><br><span class="hljs-string">      &quot;&quot;&quot;</span><br><br>       <span class="hljs-comment"># *** START CODE HERE ***</span><br>       <span class="hljs-keyword">def</span> <span class="hljs-title function_">h</span>(<span class="hljs-params">theta, x</span>):<br>           <span class="hljs-string">&#x27;&#x27;&#x27; logistic的假设</span><br><span class="hljs-string"></span><br><span class="hljs-string">          参数：</span><br><span class="hljs-string">              theta：超参数</span><br><span class="hljs-string">              x：输入变量，或者是特征值</span><br><span class="hljs-string">          &#x27;&#x27;&#x27;</span><br>           <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-np.dot(x, theta)))<br><br>       <span class="hljs-comment"># *** END CODE HERE ***</span><br><br>       <span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">theta, x, y</span>):<br>           <span class="hljs-string">&#x27;&#x27;&#x27; 计算梯度</span><br><span class="hljs-string"></span><br><span class="hljs-string">          参数：</span><br><span class="hljs-string">              theta：超参数</span><br><span class="hljs-string">              x：输入变量</span><br><span class="hljs-string">              y：验证变量</span><br><span class="hljs-string"></span><br><span class="hljs-string">          &#x27;&#x27;&#x27;</span><br>           m, _ = x.shape<br>           <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span> / m * np.dot(x.T, (y - h(theta, x)))<br><br>       <span class="hljs-keyword">def</span> <span class="hljs-title function_">hessian</span>(<span class="hljs-params">theta, x</span>):<br>           <span class="hljs-string">&quot;&quot;&quot; 计算Hessian公式</span><br><span class="hljs-string">          &quot;&quot;&quot;</span><br>           m, _ = x.shape<br>           h_theta_x = np.reshape(h(theta, x), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>           <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / m * np.dot(x.T, h_theta_x * (<span class="hljs-number">1</span> - h_theta_x) * x)<br><br>       <span class="hljs-keyword">def</span> <span class="hljs-title function_">next_theta</span>(<span class="hljs-params">theta, x, y</span>):<br>           <span class="hljs-string">&quot;&quot;&quot;通过Newton法来更新theta</span><br><span class="hljs-string"></span><br><span class="hljs-string">          :param theta: Shape (n,).</span><br><span class="hljs-string">          :return:     The updated theta of shape (n,).</span><br><span class="hljs-string">          &quot;&quot;&quot;</span><br>           <span class="hljs-keyword">return</span> theta - np.dot(np.linalg.inv(hessian(theta, x)), gradient(theta, x, y))<br><br>       m, n = x.shape  <span class="hljs-comment"># m是训练集的大小，n是特征的多少</span><br><br>       <span class="hljs-comment"># 初始化theta</span><br>       <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.theta <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>           <span class="hljs-variable language_">self</span>.theta = np.zeros(n)<br><br>       <span class="hljs-comment"># 更新theta</span><br>       old_theta = <span class="hljs-variable language_">self</span>.theta<br>       new_theta = next_theta(<span class="hljs-variable language_">self</span>.theta, x, y)<br><br>       <span class="hljs-comment"># 合适的时候停止</span><br>       <span class="hljs-keyword">while</span> np.linalg.norm(new_theta - old_theta, <span class="hljs-number">1</span>) &gt;= <span class="hljs-variable language_">self</span>.eps:<br>           old_theta = new_theta<br>           new_theta = next_theta(old_theta, x, y)<br><br>       <span class="hljs-variable language_">self</span>.theta = new_theta<br><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):<br>       <span class="hljs-string">&quot;&quot;&quot;Make a prediction given new inputs x.</span><br><span class="hljs-string"></span><br><span class="hljs-string">      Args:</span><br><span class="hljs-string">          x: Inputs of shape (m, n).</span><br><span class="hljs-string"></span><br><span class="hljs-string">      Returns:</span><br><span class="hljs-string">          Outputs of shape (m,).</span><br><span class="hljs-string">      &quot;&quot;&quot;</span><br>       <span class="hljs-comment"># *** START CODE HERE ***</span><br>       <span class="hljs-keyword">return</span> x @ <span class="hljs-variable language_">self</span>.theta &gt;= <span class="hljs-number">0</span><br>       <span class="hljs-comment"># *** END CODE HERE ***</span><br><br><br>train_path = <span class="hljs-string">&#x27;../data/ds1_train.csv&#x27;</span><br>valid_path = <span class="hljs-string">&#x27;../data/ds1_valid.csv&#x27;</span><br>prad_path = <span class="hljs-string">&#x27;../data/ds1_valid.csv&#x27;</span><br>main(train_path, valid_path, prad_path)<br><br></code></pre></td></tr></table></figure><h2 id="0x04-广义线性模型-Generative-linear-model"><a href="#0x04-广义线性模型-Generative-linear-model" class="headerlink" title="0x04  广义线性模型 Generative linear model"></a><strong>0x04  广义线性模型 Generative linear model</strong></h2><p>三个最基本的假设</p><ol><li>预测值y的分布是指数族，～Exponential Family（n）其中n为超参数</li><li>对于给定的x，我们得到一个关于预测值y的概率分布函数，这个分布函数的期望我们将其作为我们最终的预测值，则满足 $$   $$</li><li>（限制性最大的假设）假设自然参数n和输入变量x之前的关系是线性的 $$   $$</li></ol><blockquote><p><a href="https://zhangzhenhu.github.io/blog/glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html">关于什么是指数族</a> 指数族分布并不是一个具体的概率分布，指的是一类分布，这类分布具有一些共同的特性，所以形成一个概率分布族family，常见的指数族分布包括高斯分布、二项分布、多项式分布、泊松分布、gamma分布、beta分布等 $$ P(y,\eta)&#x3D;b(y)*exp(\eta^TT(y)-a(\eta))\ 这里的\eta 为自然参数\ T(y)是充分统计量\ a(\eta)是对数分割函数，来帮助\int p(y,\eta)dy&#x3D;1 $$ 常见的指数族和对应分类算法的： 实数～高斯分布 0-1分布～伯努利分布 计数分布～泊松分布 非零实数～Gamma正参数分布</p></blockquote><h3 id="由GLM构建线性模型的过程"><a href="#由GLM构建线性模型的过程" class="headerlink" title="由GLM构建线性模型的过程"></a><strong>由GLM构建线性模型的过程</strong></h3><ol><li>linear regression $$   $$</li><li>logistic regression $$   $$</li></ol><h2 id="0x05-高斯判别式算法-Gauss-discriminant-analysis"><a href="#0x05-高斯判别式算法-Gauss-discriminant-analysis" class="headerlink" title="0x05 高斯判别式算法 Gauss discriminant analysis"></a><strong>0x05 高斯判别式算法 Gauss discriminant analysis</strong></h2><p>判别式算法：希望找到一个分别的算法</p><p>生成式算法：希望针对数据本身建模</p><p>（判别式算法意味着一个找到一个线来区分两者，生成算法意味着找到数据本身的性质）</p><p>或许一个是频率学派、一个是贝叶斯学派…</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>CS229</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mitchell 机器学习读后反思</title>
    <link href="/posts/3f7a02de.html"/>
    <url>/posts/3f7a02de.html</url>
    
    <content type="html"><![CDATA[<aside> ✈️ 任何错误联系📮：chenxia31@outlook.com</aside><p>书籍的相关基本信息如下👇，我所读的版本的出版年份2009.06，从时间上来看里面的内容相对比较传统，</p><p>机器学习导论（原书第2版）book.douban.com&#x2F;subject&#x2F;25881740&#x2F;</p><blockquote><p>从中也可以看出作者对于机器学习的一些理解，作者从监督学习、贝叶斯决策理论、参数方法、多元方法、维度规约、聚类、非参数方法、决策树、线性判别式、多层感知器、局部模型、隐马尔可夫模型、分类算法评估和比较、组合多学习器以及增强学习等，结合统计学、模式识别、神经网络、人工智能、信号处理、控制和数据挖掘等不同领域对机器学习作出来论述</p></blockquote><h2 id="01-个人感悟"><a href="#01-个人感悟" class="headerlink" title="01 个人感悟"></a>01 个人感悟</h2><p>从之前传统的初高中的知识，在到大学之后的一些课程学习，其学习范式都认定在一个课程范围之类（或者说是学科体系中）研究方法都具有“第<a href="https://wiki.mbalib.com/wiki/%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86">一性定理”</a>，类似的哲学思想是：</p><blockquote><p>总会认为在一个学科中，我们可以回归事物的最基本的条件，将其拆分成为基本的元素，从最基本最无可替代的原理出发来研究问题、思考问题、学习问题最终达到理解并熟练解决问题的能力</p></blockquote><p>但是在机器学习的领域，首先是关于机器学习的基本概念或者是学习的本质尚不清楚，这对于初学者比如我产生来巨大的困惑，很难有一种决策的范式，从这本的角度我们能看出其对于多种学科的不同角度来试图理解并说明：what is 机器学习</p><p>但是不同的人的理解是不同的，有的人可能认为机器学习方法中特征提取重要、可能有的人认为贝叶斯是重点、可能认为设计一个严谨的实验过程是重点、可能完整的评价一个过程是重点、可能各种算法实例是重点，这些都是在每个人的看法这些都没有错，都是所有人从自己的角度展开讨论对于机器学习的看法，因此在之后的学习过程中要注意这方面，因为每个人出的书、讲的课都是有着自己的见解，要认清楚他们的共性和差异</p><ol><li>从实验流程的角度看，每个机器学习模型都会设计的范式包括：</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-b4253485d4bd22208f106c9167ba6fc9_1440w.png" alt="img"></p><p>个人看法</p><ul><li>由此引发的不同内容 数据属性：binary，nominal，ordinal，numerical，ratio ,etc 数据描述：mean、std、var、mode ,etc，参数估计-极大似然估计、参数估计-贝叶斯估计 数据距离：euclidean、cosine、hamming、Manhattan、minkowski、chebyshev、jaccard、haversine、Sørensen-Dice 数据预处理：缺失值相关性填补、维度规约-PCA、维度规约-LAD、维度规约-FA 数据特征：（待定）比如人脸识别的Harris特征、或者SIFT特征 选择模型：（待定）分类、回归、聚类、增强学习等，从训练角度的监督和非监督、从组合的角度弱学习和复杂学习、从复杂程度描述VC维、从模式分为生成模型和判别模型 目标函数和损失函数：（待定）bias&#x2F;variance；从结构风险最小或者经验风险最小分为MSE、MAPE、Entropy，gini，正则化 求解方法：梯度下降、牛顿法、模拟退火、遗传算法等 性能评估：各种指标AUC、MAP、F1、速度、鲁棒性</li></ul><ol><li>从机理的方法 统计学中的从特殊到一般的描述的inference，其中学习成为estimation，分类成为discrimination analysis，到工程学中的pattern recognization等多个领域中不同研究其侧重点，来在统计学、模式识别、神经网络、信号处理、控制、人工智能以及数据挖掘中不同领域汇总出自己的方法论和途径来尝试使得机器来学习一定的数据并构建一个有用的系统，从数据中induction，并构建模型来尝试deduction</li></ol><h2 id="02-关于本书"><a href="#02-关于本书" class="headerlink" title="02 关于本书"></a>02 关于本书</h2><p>笔记如图</p><h3 id="02-1"><a href="#02-1" class="headerlink" title="02.1"></a>02.1</h3><p>最欣赏的是之前给出机器学习中基本原则</p><p>triple trade-off：</p><ol><li>拟合数据的假设的复杂性</li><li>训练数据的总量</li><li>在新例子上的泛化误差</li></ol><p>基本范式</p><ol><li>model <em>g</em>(<em>x</em>∣<em>θ</em>),g()表示假设类，而<em>θ</em>是假设类汇总的一个假设，模型由机器学习胸的设计者根据其应用知识背景决定，参数由学习算法，利用取样于实际应用的训练集来进行调整</li><li>object and loss function<em>L</em>(‘)用于计算预期输出和给定参数值对于近似之间的误差，逼近误差approximation error或者loss是每个单例的损失和</li><li>optimization precedure 最优化问题是用来求解最小化近似误差的<em>θ</em>∗,常见的优化算法可能有基于梯度的方法、模拟退火的方法、遗传方法等</li></ol><h3 id="02-2-不足"><a href="#02-2-不足" class="headerlink" title="02.2:不足"></a>02.2:不足</h3><p>过于强调贝叶斯估计</p><p>作者尝试从两个角度来认识机器学习，</p><ul><li>一种从方法的observation的检测方法</li><li>一个从贝叶斯出发的estimation的估计方法</li></ul>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>Mitchell</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mitchell 机器学习读后感</title>
    <link href="/posts/d3686930.html"/>
    <url>/posts/d3686930.html</url>
    
    <content type="html"><![CDATA[<aside> <p>✈️ 讨论机器学习在统计学、模式识别、神经网络、人工智能、信号处理等不同领域的应用，主要内容包括监督学习、贝叶斯决策理论、参数方法、多元方法、维度规约、非参数方法、决策树、线性判别式、多层感知器、隐马尔可夫、组合多学习期以及增强学习等</p><p><a href="https://book.douban.com/subject/25881740/book.douban.com/subject/25881740/">https://book.douban.com/subject/25881740/book.douban.com/subject/25881740/</a></p></aside>[机器学习导论（原书第2版）](https://book.douban.com/subject/25881740/)</aside><h2 id="chapter01-绪论"><a href="#chapter01-绪论" class="headerlink" title="chapter01 绪论"></a>chapter01 绪论</h2><blockquote><p> 我们知道桌子不是木材和其他材料的随机堆砌，手写数字不是像素的随机分布，熟人的声音也不是各种声波的随机组合，现实世界总是有规律的，机器学习正式从已有的实例中发现这个规律，建立起对未知实例的预测模型，根据经验不断提高来改进越策性能 </p></blockquote><p>good and useful approximation: 希望我们可以构建一个好的并且有用的近似，虽然不足以将数据中的规律全部解释，但是希望可以用来解释数据的一部分或者机理</p><h3 id="机器学习的应用实例："><a href="#机器学习的应用实例：" class="headerlink" title="机器学习的应用实例："></a>机器学习的应用实例：</h3><ol><li>association rule 关联规则 $P(Y|X)$的条件概率的相关分析 </li><li>classification 分类，一旦拥有来拟合以往数据的规则就能对新的实例进行分析，包括credit scoring、pattern recognition、face recognition、medical diagnosis、speech recognition、knowledge extraction、compression、outlier detection </li><li>regression，包括robot navigation、连续值预测等</li></ol><ul><li>response surface design 我们试图优化一个函数，假设我们想要造一个咖啡的机器，该机器包含多个影响咖啡品质的输入，各种温度、时间、咖啡豆种类等配置，我们针对不同的输入配置进行大量实验并测量咖啡的品质，例如根据消费者的满意度测量咖啡的品质，为来需求最优配置我们需要拟合这些输入和咖啡品质的回归模型，并在当前模型的最优样本附近选择一些新的点来方便寻找更好的配置。</li></ul><ol><li>unsupervised  learning 在监督学习汇总，我们的目标是学习从输入到输出的映射关系，其中输出的正确指已经由指导者提供，但是非监督学习中却没有这样的指导者，因此目标是发现输入数据中的规律，输入空间中内部存在的结构使得特定的模式比其他模式更常出现，因此这个在统计学中成为density estimation（密度估计），其中一种方法是利用clustering聚类的方式来发现输入数据的簇、image compression\bioinformatics </li><li>reinforcement learning 增强学习 系统的输出是一个动作a你的sequence，在这种情况下，单个动作并不重要，重要的是policy策略，常用于game playing、robot navigatio、multiple agents协同操作</li></ol><h3 id="杂谈"><a href="#杂谈" class="headerlink" title="杂谈"></a>杂谈</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">之后讨论的方法来源于不同的学科领域，有时候相同的算法会在多个领域中沿着各自不同的历史轨迹被独立发现，在统计学中，从特殊的<br>观测到一般的描述成为inference、 学习被成为estimation；分类在统计学中成为<span class="hljs-keyword">discriminant </span>analysis；在工程学中，分类<br>被称为pattern recognition，方法是非参数的，机器学习与人工智能有关是因为智能系统能够适应其环境的变化，在电子工程领<br>域signal processing使得自适应计算机视觉和语音程序出现，以后artificial neural networks 和knowledge <span class="hljs-keyword">discovery </span><br>in databased发展<br></code></pre></td></tr></table></figure><p>在统计学、模式识别、神经网络、信号处理、人工智能以及数据挖掘等不同领域中，研究工作遵循这各自的途径并有各自的侧重点，本书的目的是结合所有这些研究给出重点并给出统一的处理问题的方法后给出求解方案</p><h2 id="chapter02-监督学习"><a href="#chapter02-监督学习" class="headerlink" title="chapter02 监督学习"></a>chapter02 监督学习</h2><aside> ✈️ 从最简单的情况讨论监督学习，首先从positive和negative中学习类别，之后讨论到多类的情况，最后讨论输出为连续值的regression</aside><h3 id="2-1-二元分类"><a href="#2-1-二元分类" class="headerlink" title="2.1 二元分类"></a><strong>2.1 二元分类</strong></h3><p>example：只考了价格和发送机功率作为input representation，来对class&#x3D;家用汽车进行分类，</p><p>我们假设价格为input 1，发动机功率作为input 2</p><p>$$X&#x3D;[x_1,x_2]^T$$</p><p>同时利用标号来表示汽车的类型</p><p>$$r&#x3D;\begin{cases} 1\ r\  is \  positive \ 0\ r\ is\ nagitive \end{cases}$$</p><p>由此每辆汽车都以用一个有序对来表示</p><p>$$X&#x3D;{x^t,r^t}^N_{t&#x3D;1}$$</p><p>我们构造一个hypothesis class，认为对于家用汽车，价格和发动机功率都在某一个确定的范围内：</p><p>$$（p_1&lt;x_1&lt;p_2）and (e_1 &lt;x_2&lt;e_2)$$</p><p>尽管专家定义来这个假设类，我们需要找到一个hypothesis来最接近 与，也就是需要确定上述假设类中的参数$[p_1,p_2,e_1,e_2]$</p><p>我们寻找希望实现的最终效果是让h和真实的C尽可能的蕾丝，但是实际上我们并不知道$C（X）$，因此也无法评估$h(x)\ and\ c(x)$之间的误差，我们所拥有的是训练集x，它知识x的一个小的子集，针对预测值和预期值不同的训练实例所占的比例，误差可以定义为：</p><p>$$E(h|x)&#x3D;\Sigma^N_{t&#x3D;1}1\ (h(x^t)!&#x3D;r^t)$$</p><p>我们希望训练的结果是涵盖所有的正例而不包括任何负例，虽然可能会存在无穷多个预测问题，但是需要给定一个接近正例和负例边界的某个未来实例，不同的候选假设可能作出不同的越策，因此需要考虑其generalization的问题</p><p>策略1⃣️：找出most special hypothesis，涵盖所有正例而不包括任何负例的最紧凑的矩形，这样得出的一个假设h&#x3D;S作为我们的induced class，同时找到most general hypothesis，涵盖所有正例而不包括负例的最大矩形，作为S和G之内的所有h都是无误差的有效假设，称为和训练集相融consistent，构成解空间version space</p><p>策略2⃣️：  处于之间都是doubt例子，由于缺乏数据支持，因此在这种情况下系统将去除reject这些实际例子</p><p><strong>def（VC维 vapnik-chervonenkis dimension）</strong>：假设我们有一个数据集，包含N个点，这点总共可以用$2^N$ 中方法来标记positive or negative，因此可以定位为多种不同的学习问题，如果对于这些问题中任何一个都可以找到一个假设h将positive和negative分开，那么我们成为h shatter N各店，也就是说N个点定义的任何的学习都能够用一个从H中抽取的假设无误差学习，可以被H shatter的点的最大数量成为 VC维,用来度量假设类H的学习能力capacity</p><p><a href="https://tangshusen.me/2018/12/09/vc-dimension/">统计学习理论之VC维究竟是什么</a></p><p>def（probably approximately correct，PAC）：误差概率不高某个值，还要对我们的假设有信息吗，因为我们想知道我们的假设在大多数时间里都是正确的，因此我们希望我们的假设是正确的</p><p>给定类和从未知但是具有确定概率分布中抽取样本，我们希望找出样本书N，是的对于任意的$\sigma&lt;&#x3D;1&#x2F;2$和 $\phi&gt;0$,假设h的无法之多$\phi$的概率至多为$1-\sigma$</p><p>$$P{C\Delta h&lt;&#x3D;\sigma }&gt;&#x3D;1-\phi$$</p><p><strong>def(noise 噪声)：是数据中有害的异常，类的学习可能更加困难，并且是使用简单的假设困难做不到零误差，对于噪声具有一下的分类：</strong></p><ul><li>记录输入属性困难不准确，这困难导致数据点在输入空间驿动</li><li>标记数据点困难有错误 teacher noise指导噪声</li><li>困难存在我们没有考虑到的附加属性，包括hidden and latent的</li></ul><aside> <p>✈️ occam’s razor：较简单的解释看上去更可信，并且任何不必要的复杂性都应该被抛弃</p></aside><h3 id="2-2-多分类"><a href="#2-2-多分类" class="headerlink" title="2.2 多分类"></a>2.2 多分类</h3><p>我们能够通过两个假设或者更多的假设来抛弃这种输入</p><h3 id="2-3-回归问题"><a href="#2-3-回归问题" class="headerlink" title="2.3 回归问题"></a>2.3 回归问题</h3><p>type1:如果回归问题的数据中不存在noise，则该任务是interpolation插值</p><h3 id="2-5-model-selection-and-generalization"><a href="#2-5-model-selection-and-generalization" class="headerlink" title="2.5 model selection and generalization"></a>2.5 model selection and generalization</h3><p>由于学习是一个ill-posed problem，单纯依靠数据本身不足够找到解，因此我们应该做一些特别的假设，以便得到已有数据的唯一解，我们为使学习成为可能所做的假设集称为学习算法的inductive bias</p><p>但是我们知道每个假设类都有一定的能力，并且只能学习确定的函数underfitting以及overfitting的问题</p><p>寻找三类之间的平衡</p><p>triple trade-off：</p><ol><li>拟合数据的假设的复杂性</li><li>训练数据的总量</li><li>在新例子上的泛化误差</li></ol><h3 id="chapter-summary"><a href="#chapter-summary" class="headerlink" title="chapter summary"></a>chapter summary</h3><ol><li>model $g(x|\theta)$,g()表示假设类，而$\theta$是假设类汇总的一个假设，模型由机器学习胸的设计者根据其应用知识背景决定，参数由学习算法，利用取样于实际应用的训练集来进行调整 </li><li>loss function$L(&#96;)$用于计算预期输出和给定参数值对于近似之间的误差，逼近误差approximation error或者loss是每个单例的损失和 $$E(\theta|x)&#x3D;\Sigma L(r^t,g(x^t|\theta))$$ </li><li>optimization precedure 最优化问题是用来求解最小化近似误差的$\theta^*$,常见的优化算法可能有基于梯度的方法、模拟退火的方法、遗传方法等</li></ol><p>想要最好上面的工作需要满足条件：</p><ol><li>假设类g需要足够大，需要有足够大的容量，以便在包含噪声的情况下产生r表示的数据的未知数</li><li>必须有足够的训练数据，使得我们从假设类中识别正确的假设</li><li>给定训练数据应该有好的优化方法来找出正确的假设</li></ol><p>不同的机器学习方法之间的区别是可能是假设的模型不同、他们的损失度量不同、或者是所采用的最优化过程不同</p><h3 id="chapter-problem"><a href="#chapter-problem" class="headerlink" title="chapter problem"></a>chapter problem</h3><ol><li>设想你一次只能得到一个训练实例，而不是一次得到所有的训练实力，如何增量调整？</li><li>我们对于实际值和估计值之差的平方求和该误差函数是使用最频繁的误差函数，但是他只是可行的误差函数之一，他对于离群点并不是robust的，为实现robust regression，更好的误差函数是什么吗</li></ol><h2 id="chapter03-贝叶斯决策定理"><a href="#chapter03-贝叶斯决策定理" class="headerlink" title="chapter03 贝叶斯决策定理"></a>chapter03 贝叶斯决策定理</h2><aside><p> ✈️ 在不确定情况下决策的概率理论框架；在classification中，贝叶斯规则用来计算class的概率，我们将讨论推广到怎样作出合理的decision来min exception of risk，同时还介绍贝叶斯网络来有效表示随机变量之间的依赖关系</p></aside><h3 id="3-1-introduce"><a href="#3-1-introduce" class="headerlink" title="3.1 introduce"></a>3.1 introduce</h3><p>温习基本的概率知识</p><p>在experiment中我们会unobservable variable以及observable variable，因此从样本中来推断出整体的概率分布，需要用到一定的估计</p><p>$$\bar p_0&#x3D;{postive}&#x2F;{p+n}$$</p><h3 id="3-2-bayes-in-classification"><a href="#3-2-bayes-in-classification" class="headerlink" title="3.2 bayes in classification"></a>3.2 bayes in classification</h3><p>example : 在银行的信用评分中，根据过去的交易，客户是分为高风险和低风险的，为来判断客户具体的风险水平，我们希望从数据中进行分类，如果我们知道所有的信息，我们可以准确的判断，但是有一些是unobservable，因此我们需要从能够观测的信息observable variable中进行推断</p><p>描述就成为：客户的信用的观测条件为$X&#x3D;[x_1,x_2]^T$的输入变量，输出为$C$,为1的时候高风险，为0的时候低风险，如果$P(C|X_1,X_2)$知道，当$X_1&#x3D;x_1,X_2&#x3D;x_2$时，我们可以判断是否为高风险：</p><p>$$choose&#x3D;\begin{cases}C&#x3D;1\ P(C&#x3D;1|x_1,x_2)&gt;P(C&#x3D;0|x_1,x_2)\ C&#x3D;0 \end{cases}$$</p><p>由贝叶斯规则可以表示成为：</p><p>$$p(C|x)&#x3D;p(C)p(x|C)&#x2F;p(x)$$</p><p>$p(C&#x3D;1)$成为C取1的prior probability，这是我们在看到观测量x之前就获得关于C值的知识，这里满足：</p><p>$$\Sigma P(C&#x3D;i)&#x3D;1$$</p><p>$p(x|C)$成为class likelihood，是属于C的时间具有相关联的观测值的x的条件概率，在我们的例子中</p><p>$p(x1,x2|C&#x3D;1)$就是高风险具有$X_1&#x3D;x_1,X_2&#x3D;x_2$的概率，这就是我们通过数据得到的关于类的信息</p><p>$p(x)$是evidence，是看到观测x的边缘概率，此时无论他是正实例还是负实例</p><p>最终为了将误差最小化，贝叶斯分类器$Bayes’s\ classifier$选择具有最高后验概率的类</p><p>$$choose\ C&#x3D;C_i \ P(C_i|x)&#x3D;maxP(C_k|x)$$</p><h3 id="3-3-bayes-in-risk-损失和风险"><a href="#3-3-bayes-in-risk-损失和风险" class="headerlink" title="3.3 bayes in risk 损失和风险"></a>3.3 bayes in risk 损失和风险</h3><p>决策的好坏程度或者代价可能不同</p><ul><li>金融机构错误接受一个高风险的申请人带来的损失和错误拒绝一个低风险的申请人带来的潜在收益是不同的</li><li>在医疗诊断、地震预测等显示更加重要</li></ul><p>后面推导类一下情况：</p><ol><li>zero-one loss ，所有错误具有相同的代价，因此在3.2中识别出最大的概率就是最优解</li><li>错误的决策具有很高的代价，通过推导可以发现需要max的概率大于1-lameda</li></ol><p>度量中的倾向是：</p><ol><li>loss或者expected risk尽可能的小</li><li>utility function尽可能大</li><li>新特征的value of information</li></ol><h3 id="3-4-bayes-network"><a href="#3-4-bayes-network" class="headerlink" title="3.4 bayes network"></a>3.4 bayes network</h3><p>Bayesian Network，or belief network，or probabilistic network，represent relationship between variables as a graphical model，一个贝叶斯网路由节点和节点之间的弧构成，每个阶段对应一个随机变量，该影响的由条件概率$P(y|X)$指定</p><p>eg1: 下雨R引起草地W变湿，天下雨的可能是0.4，下雨导致草地0.9，不下雨草地湿0.2</p><p>question：草地湿W的时候下雨的R的概率</p><p>eg2:新增一个喷岁器S作为W的另一个原因，各种概率分布</p><p>计算出：S开的时候草地湿的概率、草地W时S的概率、假设下过雨R后草地W时S的概率</p><p>推断出explaining away：如果给定我们下过雨，则喷水器导致湿草地的可能性降低了</p><p>贝叶斯网络的总结：</p><ol><li>这个网络描述了条件独立性，并且允许我们多个变量的联合分布问题分解成为局部local的问题，这样简化的分析和计算，如果每个节点只有少量的父节点，这个复杂度由指数降低到线性</li><li>可以利用belif propagation的方法来游戏哦啊的计算概率分布，如果网络是树状的时候可以行程junction tree 结树</li><li><strong>主要优点</strong>：不必要明确指定变量作为输入，某些其他变量作为输出，任何变量集的值都能通过争取建立，而任何其他变量集的概率都可以推断</li><li>如果输入之间的是相互独立的，这种网络被成为naive bayes’ calssfier，朴素贝叶斯网络，忽略输入之间可能的依赖性，讲一个多变量问题规约成为一组单变量问题</li></ol><h2 id="chapter04-参数方法"><a href="#chapter04-参数方法" class="headerlink" title="chapter04 参数方法"></a>chapter04 参数方法</h2><h3 id="4-1-引言"><a href="#4-1-引言" class="headerlink" title="4.1 引言"></a>4.1 引言</h3><p>example参数方法：假设样本服从已知模型的某个分布，比如高斯分布，由此来从样本估计模型所需要的参数来得到完整的分布，将这些估计放到假设的模型来得到估计的分布，之后进行决策。</p><p>用来估计的方法常见的最大似然估计，之后还会讨论贝叶斯估计的方法</p><p>example：正态分布的均值和方差的最大似然估计</p><p>example：伯努利分布的p的最大似然估计</p><h3 id="4-3-极大似然评价估计：bias和variance（偏倚和方差）"><a href="#4-3-极大似然评价估计：bias和variance（偏倚和方差）" class="headerlink" title="4.3 极大似然评价估计：bias和variance（偏倚和方差）"></a>4.3 极大似然评价估计：bias和variance（偏倚和方差）</h3><p>这里涉及的就是无偏估计或者渐进无偏估计等等之类的</p><p>$$令X是取自参数\theta 指定的总体上的样本，令d&#x3D;d(\theta)是\theta的一个估计，为评估一个估计的质量，我们可以估计其与\theta 有多大的不同\r(d,\theta)&#x3D;E[(d(X)-\theta)^2]\估计的bias是\b_\theta(d)&#x3D;E[d(x)]-\theta$$</p><p>variance:度量在平均情况下，估计值在期望值附近的变化程度（在样本发生改变）</p><p>bias：度量期望值和正确指不同的程度</p><h2 id="4-4-贝叶斯估计"><a href="#4-4-贝叶斯估计" class="headerlink" title="4.4 贝叶斯估计"></a>4.4 贝叶斯估计</h2><p>在得到样本之前，可能会有一些关于参数$\theta$ 可能取值范围的prior信息</p><p><strong>prior density</strong> 告诉我们在看到样本之前$\theta$ 的可能取值</p><p><strong>likelihood density</strong> 数据告诉我们的</p><p><strong>posterior density</strong> 后验密度告诉我们看到样本之后$\theta$ 的可能取之</p><p>如果我们假定似然密度在众数周围有一个窄的峰值，则使用maximum a posteriorMAP估计将使得计算比较容易，</p><p>另一个方法是利用Bayes‘s estimator来定义后验密度的期望值</p><p>另一求积分的方式是利用蒙特卡洛方法或者从后验密度中产生样本，利用一些近似方法来计算整个积分</p><h2 id="4-5-参数方法来分类"><a href="#4-5-参数方法来分类" class="headerlink" title="4.5 参数方法来分类"></a>4.5 参数方法来分类</h2><p>example：假设一个汽车公司销售K种不同的汽车，为来简单期间，我们假设唯一影响购买的因素是他们的年收入用x表示</p><p>$$P(C_i)是购买类型i汽车顾客所占的比例\购买i汽车的顾客的概率P(x|C_i)服从分布N(\mu_i,\sigma_i^2)$$</p><p>这里俄队参数使用最大似然估计，如果密度函数不是高斯的分类算法可能会出现错误，因此存在正态性的检验</p><h2 id="4-6-参数方法来回归"><a href="#4-6-参数方法来回归" class="headerlink" title="4.6 参数方法来回归"></a>4.6 参数方法来回归</h2><p>可能算是屁话的话</p><h2 id="4-7-调整模型的复杂度，bias-variance-dilemma"><a href="#4-7-调整模型的复杂度，bias-variance-dilemma" class="headerlink" title="4.7 调整模型的复杂度，bias&#x2F;variance dilemma"></a>4.7 调整模型的复杂度，bias&#x2F;variance dilemma</h2><p>任何机器学习都存在bias和variance的冒险，模型应当是柔性的</p><p>如果方差保持的比较低，则可能不能很好的拟合数据，并且具有较高的偏倚</p><p>最佳模型是最好的权衡偏倚和方差的模型</p><h2 id="4-8-模型选择过程"><a href="#4-8-模型选择过程" class="headerlink" title="4.8 模型选择过程"></a>4.8 模型选择过程</h2><p>cross-validation（属于：策略经验风险最小化 策略）</p><p>我们不能计算一个模型的偏倚和方差但是我们能够计算总误差</p><p>regularization 正则化（属于：SRM结构风险最小化策略）</p><p>这里使用一个增广误差函数$E&#x3D;数据上的误差+\lambda*模型复杂度$ 其中$\lambda$给出来罚函数的权重，当我们最小化增广误差函数而不是数据的误差 是，我们惩罚来复杂模型，因此降低来方差，但是如果取得太多，就会被引进偏倚的风险</p><p>structural risk minimization SRM 结构风险最小化</p><p>使用一个模型集合，比如多项式的参数数量，或者是VC维度也是模型复杂度</p><p>minimum description length MDL 最小描述长度</p><p>如果数据简单就有最短的复杂度</p><p>贝叶斯方法</p><p>定义为模型的先验分布p模型，给定数据并假设模型，之后根据贝叶斯规则计算</p><p>交叉确认与其他模型选择的方法不同，因为它不对模型做任何先验假设，如果有足够大的确认数据集就是最好的方法，在数据样本很小是，其他模型变的有用</p><h2 id="chapter05-多元方法"><a href="#chapter05-多元方法" class="headerlink" title="chapter05 多元方法"></a>chapter05 多元方法</h2><aside> ✈️ 在chapter04中讨论的分类和回归的参数方法，现在可以推广到多元情况</aside><h3 id="5-1-多元"><a href="#5-1-多元" class="headerlink" title="5.1 多元-"></a>5.1 多元-</h3><p>在chapter04讨论了利用参数估计的方法来对单个变量的情况进行classification和regression，现在可以将其推广到多元情况，其这些输入和输出都是离散的，我们将讨论如何从标记的多元样本学习这样的函数</p><p>在许多应用中，许多测量都在每个个体或者事件上产生一个观测响亮，这样的情况下包括data matrix，也可以被成为input、feature、attribute；每一行可以成为对于事件的iid的observation、example、instance</p><p>这些# 数据 都具有其基本的特点，比如离散数据、或者连续数据；离散数据可以包括序数数据、数值数据等等</p><p>但是通常这些变量是相关的，但是我们的目标通常会simplification，就是利用相对较少的参数汇总大量的数据，目标可能是exploratory，由此引申multi-classification、multi-regression</p><h3 id="5-2-参数估计（和之前的相同）"><a href="#5-2-参数估计（和之前的相同）" class="headerlink" title="5.2 参数估计（和之前的相同）"></a>5.2 参数估计（和之前的相同）</h3><p>常用的估计量包括：</p><ul><li>mean vector</li><li>covariance matrix</li><li>correlation</li><li>sample correlation</li></ul><h3 id="5-3-缺失值估计"><a href="#5-3-缺失值估计" class="headerlink" title="5.3 缺失值估计"></a>5.3 缺失值估计</h3><blockquote><p> 如果观测中的某些变量的值缺失，最好的策略是将这些观测一同丢弃，但是一般没有足够大的样本让我们这样做，因此需要对其进行填补，成为imputation </p></blockquote><p>imputation by mean：平均或者众数代替</p><p>imputation by regression：尝试从其他已知变量来预测缺失，有定义为回归或者分类问题</p><h3 id="5-4-多元正态分布"><a href="#5-4-多元正态分布" class="headerlink" title="5.4 多元正态分布"></a>5.4 多元正态分布</h3><h3 id="5-5-多元分类"><a href="#5-5-多元分类" class="headerlink" title="5.5 多元分类"></a>5.5 多元分类</h3><h3 id="5-6-调整复杂度"><a href="#5-6-调整复杂度" class="headerlink" title="5.6 调整复杂度"></a>5.6 调整复杂度</h3><p>bias&#x2F;variance 选择减少协方差矩阵的参数树木，在简单模型的适用性和通用型之间折中：</p><p>一方面，简化协方差矩阵的假设并降低被估计的参数数目，可能增大bias</p><p>另一方面，不做折中假设，由于矩阵好似任意的，在二次判别式函数在小数据集众会有很大的方差</p><h3 id="5-7-离散特征"><a href="#5-7-离散特征" class="headerlink" title="5.7 离散特征"></a>5.7 离散特征</h3><h3 id="5-8-多元回归-multivariate-linear-regression"><a href="#5-8-多元回归-multivariate-linear-regression" class="headerlink" title="5.8 多元回归 multivariate linear regression"></a>5.8 多元回归 multivariate linear regression</h3><ul><li>线性模型可以观察w的值来提取知识</li><li>观察w的富豪可以知道x对于输出结果的影响是正的还是负的</li><li>如果所有的x具有相同的值域</li></ul><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p>如果数据是多元正态，任意两个变量的图应该大致上是线性的，模式识别的大部分工作都是假设多元正态密度上进行，但这通常是错误的</p><p>只有当密度确实是多元正态时，并且我们有足够的数据来计算正确的参数是，这才是最优的，可以利用评估多元正态性的检验以及检查相关协方差举证的检验；在利用多元正态分布分类中比较线性和二次判别式</p><h2 id="chapter06-维度归约"><a href="#chapter06-维度归约" class="headerlink" title="chapter06 维度归约"></a>chapter06 维度归约</h2><h3 id="6-1-引言"><a href="#6-1-引言" class="headerlink" title="6.1 引言"></a>6.1 引言</h3><p>在我们确信含有信息的观测数据都被用作input来输入到系统中作为决策，理想情况下我们不应该将feature choose或者feature extraction作为一个单独的进程，分类方法或者回归方法应该能够利用任何必要的特征，但是有需要原让我们对降维作为一个单独的预处理步骤：</p><ul><li>learning method汇总复杂程度依赖于model-d和databased-N</li><li>当一个输入认定不必要的时候节省提取开销</li><li>较简单的模型在小数据集上更加鲁棒</li><li>数据能用较少的特征解释，我们可以更好的理解数据的过程</li><li>数据可以用少数维度表示而不丢失信息是可以对数据绘图</li></ul><p>降低维度的方法：特征选择feature selection、特征提取extraction</p><p>feature selection：从d维中找出提供最多信息的k维度，讨论subset selection</p><p>feature extraction：感兴趣是找到k维度的组合，这些事原来d的组合，常用的方法有PCA、LDA，以及FA和MDS</p><h2 id="chapter07～13"><a href="#chapter07～13" class="headerlink" title="chapter07～13:"></a>chapter07～13:</h2><p>有点过时，具体policy到model的具体实现要根据实际来</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>Mitchell</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读introduction to algorithms顺序</title>
    <link href="/posts/a3ed2269.html"/>
    <url>/posts/a3ed2269.html</url>
    
    <content type="html"><![CDATA[<p>一下是原文地址 ，仅翻译其中的英文部分，供参考，这是目录 </p><p>如何正确地撸《算法导论》？1562 赞同 · 43 评论回答</p><p>第一章</p><p>作为兴趣阅读，可以跳过</p><p>第二章</p><p>2.1 insertion sort ，这是最基础的内容，实际上最好可以了解所有主要的排序算法</p><p>2.2 Analysis of Algorithms 除了导言部分都需要知道</p><p>2.3 Designing Algorithms 包含merge sort方法和分析，以及divide-and- conquer（分而治之）的概述，非常重要，值得阅读</p><p>第三章</p><p>所有的部分都需要知道，这是关于时间复杂度分析部分</p><p>第四章</p><p>4.1 maximum subarray problem ，值得阅读，书中使用divide-and- conquer方法虽然不是最高效的，但是作为练习和理解divide-and- conquer的思想很有用</p><p>4.2 Strassen‘s algorithm，很美妙的算法，令人震惊</p><p>4.3 substitution method，可能面试的时候不会用到，但是这是最基础的工具去寻找递归算法的时间复杂度</p><p>4.4 递归树分析，和4.3一样重要</p><p>4.5 master method，重要到最好可以深刻到潜意识里，面试中需要即时使用在算法的时间度分析中</p><p>4.6 proof of the master theorem，可以跳过，但是阅读可以帮助理解之前学习的master method</p><p>第五章</p><p>并未阅读这一章节，因为过于偏向数学而不是算法</p><p>第六章</p><p>6.1～6.5 heaps and heapsort</p><p>第七章</p><p>7.1、7.2、7.3 quick sort and its recommend version，需要了解概念</p><p>7.4 值得了解</p><p>第八章</p><p>8.1 lower bounds on sorting，这是非常基础的内容，需要掌握</p><p>8.2 counting sort 需要详细了解细节，会有很多变种的问题</p><p>8.3 radix sort 非常简单的算法</p><p>8.4 bucket sort 可以跳过</p><p>第九章</p><p>9.1 small section 值得阅读</p><p>9.2 selection in expected linear time 非常重要！！</p><p>9.3 selection in worst-case linear time 可以跳过，只需要明白worst-case  linear time在某些情况下需要</p><p>第十章</p><p>10.1 stacks and queues，基本的知识，非常重要</p><p>10.1 linked list 非常重要</p><p>10.3 implementing pointers and objects 看你编程能力</p><p>10.4 representing rooted trees 小章节，可以略读</p><p>第十一章</p><p>对于哈希表，了解其基本概念比实现它更重要。</p><p>11.1 direct addressing 仅需要明白概念</p><p>11.2 hash tables 非常重要</p><p>11.3 hash function 了解基本概念很重要，但是不值得深入挖掘，了解常见的函数例子</p><p>11.4 open addressing 具有相关的概念很重要</p><p>11.5 perfect hashing 可以跳过</p><p>第十二章</p><p>12.1 what is a binary search tree？需要</p><p>12.2 querying a BST 需要全部阅读</p><p>12.3 insertion and deletion 需要阅读</p><p>12.4 randomly built BSTs 只需要明白定理12.4</p><p>第十三章</p><p>仅需要了解red-black tree和最坏情况下height&#x2F;insert&#x2F;delete&#x2F;find是什么，其他可以跳过</p><p>第十四章</p><p>值得浏览14.2去了解不同数据结构为什么会有用，或者可以做一两个例子，可以跳过14.1 and 14.3</p><p>第十五章</p><p>DP （dynamic programming），必须知道</p><p>15.1 rod- cutting DP编程的开始，必须知道</p><p>15.2 matrix- chain- multiplication，和15.1一样</p><p>15.3 elements of DP，值得阅读去清晰理解DP的思想，而不是仅仅知道什么是DP或者去实现它</p><p>15.4 LCS 必须知道</p><p>15.5 optimal binary search tree 并没阅读，不好评价</p><p>第十六章</p><p>你需要清晰明白greedy algorithm（贪心算法）是什么，所以需要阅读引言</p><p>16.1 an activity selection problem 并没有详细阅读，略读</p><p>16.2 elements of the greedy strategy 同16.1</p><p>16.3 Huffman code，很有名的编码方式</p><p>16.4 task- scheduling problem as a matroid</p><p>第十七章</p><p>了解amortized analysis方法很重要，可以通过Google了解基本的概念，或者阅读17.1</p><p>17.2～17.4可以跳过</p><p>第十八章</p><p>明白B- tree是在解决更高难度的问题很有用，但是你可以仅仅知道相关的基本概念，本章可以跳过</p><p>第十九章</p><p>Fibonacci heaps 不需要阅读</p><p>第二十章</p><p>不需要</p><p>第二十一章</p><p>最初认为不重要，但是深思熟虑之后，推荐阅读21.1和21.2，其余的可以跳过</p><p>第二十二章</p><p>22.1 representation of graphs 需要</p><p>22.2 BFS需要</p><p>22.3 DFS需要</p><p>22.4 Topological sort需要</p><p>22.5 strongly connected components 虽然没有以上四种运用的频繁，但还是值得阅读</p><p>第二十三章</p><p>minimum spanning trees 可能是最不重要的图算法，处理max flow（仅对于面试来说）</p><p>但是这依旧值得阅读因为这是一个广为人知的问题</p><p>23.1 growing a mst</p><p>23.2 prim and Kruskal</p><p>第二十四章</p><p>shortest path非常重要，需要阅读导言</p><p>24.1 bellman-ford 了解算法和正确性的证明</p><p>24.2 shortest path in DAGs 值得阅读</p><p>24.3 Dijkstra ，非常重要</p><p>24.2 可以跳过</p><p>第二十五章</p><p>阅读引言非常重要</p><p>25.1 matrix multiplication 有空闲时间值得阅读</p><p>25.2 Floyd- warsshall 明白算法和分析</p><p>25.3 可以跳过</p><p>第二十六章</p><p>可以跳过</p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法导论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Matplotlib用户指南中文翻译</title>
    <link href="/posts/b8147cf.html"/>
    <url>/posts/b8147cf.html</url>
    
    <content type="html"><![CDATA[<p>用户指南  </p><p>这份辅导指南包括最基本的使用方法和最好的训练帮助你开始使用matplotlib</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br></code></pre></td></tr></table></figure><h2 id="入门例子"><a href="#入门例子" class="headerlink" title="入门例子"></a><strong>入门例子</strong></h2><p>Matplotlib将你的图片在<strong>FIgure</strong>中绘制：包括一个或多个Axes（坐标轴，包括x-y坐标、极坐标、3D坐标） 最简单的是利用<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots">pyplot.subplots</a>来创建一个包含坐标轴Axes的Figure，然后可以用<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html#matplotlib.axes.Axes.plot">Axes.plot</a>的方式绘制数据</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">fig</span>,ax=plt.subplots() #创建一个figure，包含一个坐标轴<br><span class="hljs-attribute">ax</span>.plot([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>])<span class="hljs-meta"></span><br><span class="hljs-meta">[&lt;matplotlib.lines.Line2D at 0x1270552e0&gt;]</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3f7a90d412c61140e284529bd9edc0e2_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>在一些其他的绘图语言或者工具中并不要求你创建一个坐标轴，比如Matlab中，直接使用plot。</p><p>事实上，你可以同样实现在Matplotlib：对于任意一个Axes的绘图方法，都会有对应的方式在matplotlib.pyplot模块中让你可以在现有的坐标中中绘制。比如上面可以换写成：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">plt</span>.plot([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>])<span class="hljs-meta"></span><br><span class="hljs-meta">[&lt;matplotlib.lines.Line2D at 0x12727e400&gt;]</span><br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3f7a90d412c61140e284529bd9edc0e2_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h2 id="Figure的组成"><a href="#Figure的组成" class="headerlink" title="Figure的组成"></a><strong>Figure的组成</strong></h2><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6cc5f2a20abd152c33aa45c6e1ab6cf6_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a><strong>Figure</strong></h3><blockquote><p>完整的Figure包括坐标轴Axes的所有字类，少量的特殊特质（title、figure legend等），以及绘图各种图形元素，其中创建Figure中至少要包含一个坐标轴</p></blockquote><p>可以用以下的方法创建Figure和Axes同时，也可以之后添加坐标轴来实现更加复杂的布局</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">fig</span>=plt.figure() #没有坐标轴的Figure<br>fig,<span class="hljs-attribute">ax</span>=plt.subplots() # 一个坐标轴的Figure<br>fig,<span class="hljs-attribute">axs</span>=plt.subplots(2,2) #四个坐标轴的Figure<br>&lt;Figure size 432x288 with 0 Axes&gt;<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0039e23b44d9d1f634e25d034ad84168_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-59d0aace4e3f02c44744e6e5d96041c0_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><h3 id="坐标轴Axes"><a href="#坐标轴Axes" class="headerlink" title="坐标轴Axes"></a><strong>坐标轴Axes</strong></h3><blockquote><p>这就是认识到关于绘图plot的地方，这是数据空间的绘制区域。一个给定的Figure可能包含很多Axes，但是一个Axes必须包含在一个Figure中 每个二维坐标轴具有两个Axis对象，具有以下的功能：</p></blockquote><ul><li>axes.Axes.set_xlim(   )</li><li>axes.Axes.set_ylim(   )</li><li>axes.Axes.set_title(   )</li><li>axes.Axes.set_xlabel(   )</li><li>axes.Axes.set_ylabel(   )</li></ul><h3 id="单根轴axis"><a href="#单根轴axis" class="headerlink" title="单根轴axis"></a><strong>单根轴axis</strong></h3><blockquote><p>用于控制数据的上下限，数据的刻度ticks，以及对应刻度的名称ticklabels</p></blockquote><h2 id="object-oriented-interface-和-pyplot-interface"><a href="#object-oriented-interface-和-pyplot-interface" class="headerlink" title="object- oriented interface 和 pyplot interface"></a><strong>object- oriented interface 和 pyplot interface</strong></h2><p>在matplotlib中主要存在两种方式</p><ul><li>一种是制定特定的Figures和axes，之后在其中添加各种方法method（object- oriented）</li><li>一种是依赖于pyplot，来管理figure和axes，利用pyplot- function来绘制（pyplot ）</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">x</span>=np.linspace(0,2,100)<br>fig,<span class="hljs-attribute">ax</span>=plt.subplots()<br>ax.plot(x,x,<span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>ax.plot(x,x*<span class="hljs-number">*2</span>,<span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;quad&#x27;</span>)<br>ax.plot(x,x*<span class="hljs-number">*3</span>,<span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;cubic&#x27;</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;x label&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;y label&#x27;</span>)<br>ax.set_title(<span class="hljs-string">&#x27;Simple&#x27;</span>)<br>ax.legend()<br>&lt;matplotlib.legend.Legend at 0x1273fe850&gt;<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-09e5141f6ff37b540dc7b48b000b98e3_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">x</span>=np.linspace(0,2,100)<br>plt.plot(x,x,<span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>plt.plot(x,x*<span class="hljs-number">*2</span>,<span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;quad&#x27;</span>)<br>plt.plot(x,x*<span class="hljs-number">*3</span>,<span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;cubic&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x label&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y label&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;simple&#x27;</span>)<br>plt.legend()<br>&lt;matplotlib.legend.Legend at 0x127487760&gt;<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ed86111ea70044065f6746f7f4185670_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p>在matplotlib的官方教程中实例的实现方法中均包含其中两个方法，但是希望使用者可以选择其中的一个然后坚持使用它，这样才能熟练掌握，而不是将两者混淆。</p><p>通常我们建议在jupyter notebook等即时可见中使用pyplot，而在脚本环境中使用oo方式</p><blockquote><p>Note from pylab import *这个方式已经被弃用了，这种方式现在并不鼓励使用，所以请删除</p></blockquote><h2 id="Backends-后段"><a href="#Backends-后段" class="headerlink" title="Backends 后段"></a><strong>Backends 后段</strong></h2><p>什么是backends？</p><p>现在许多网站上大量的文档都存在“backend”这个词，许多新学者可能会被专用名词弄混淆。matplotlib旨在利用不同的backend得到不同的输出，不同的人使用不同的手段使用matplotlib，这种通过后端来交互、嵌入到其他用户图形接口中、web应用场景、批处理脚本中应用场景</p><p>为了支持这些特殊场景，matplotlib可以设置不同的输出，这些设置变成为backedns</p><ul><li>第一种是user interface backends（PyQt&#x2F;PySide, PyGObject, Tkinter, wxPython, or macOS&#x2F;Cocoa）</li><li>一种是输出格式 hardcopy backends（PNG, SVG, PDF, PS; also referred to as “non-interactive backends”）</li></ul><h2 id="第一种作为和其他系统的接口一般使用频率不大"><a href="#第一种作为和其他系统的接口一般使用频率不大" class="headerlink" title="第一种作为和其他系统的接口一般使用频率不大"></a>第一种作为和其他系统的接口一般使用频率不大</h2><p>制定图片格式并保存</p><ul><li>matplotlib.pyplot.savefig(‘filename’)</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>import matplotlib<span class="hljs-selector-class">.pyplot</span> as plt<br><br>plt<span class="hljs-selector-class">.ion</span>()<br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>    plt<span class="hljs-selector-class">.plot</span>(np<span class="hljs-selector-class">.random</span><span class="hljs-selector-class">.rand</span>(<span class="hljs-number">10</span>))<br>    plt<span class="hljs-selector-class">.show</span>()<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-e474c7e51beb325e6da552f1841138a5_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-743bc3c98ed3dc19c21230ac9301b26c_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-4fc83de3e86baa034ec57623ee0f488c_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>程序设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>matplotlib</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理工科学生使用iPad Pro（2018）的总结</title>
    <link href="/posts/3c3cb03.html"/>
    <url>/posts/3c3cb03.html</url>
    
    <content type="html"><![CDATA[<p>2020款iPad pro昨天上线，相比较2018款iPad Pro的主要提升有：</p><ol><li>A12X处理器升级A12Z</li><li>后置摄像头的升级</li><li>增加探测雷达</li><li>支持更多WiFi频段（WiFi-6？）</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-3575603dc82b559acde018802de4f140_1440w.png" alt="img"></p><p>官网上参数对比</p><p>由于个人是学生，对于摄像头的要求仅仅是用来拍ppt，一般拍照没有太大需求（但是相信Apple的硬件和算法，前几天A12X加持下的人像模式惊到我了）。</p><p>下面我想说明一下个人用iPad Pro 11inch乞丐版（2018）的使用心得。</p><p>配置&amp;配件</p><p>针对iPad的各种东西有：</p><ol><li>iPad Pro 11inch 64GB-WiFi</li><li>Airpods二代</li><li>Apple pencil 2nd</li><li>Elecon类纸膜+与乐【暗夜绿】磁吸保护套+紫米30w充电套装+品胜三拖一数据线</li><li>微软Xbox one手柄</li></ol><p>（手持8p和win10电脑）</p><p>为什么选择这些会在下文介绍</p><h2 id="软件-使用"><a href="#软件-使用" class="headerlink" title="软件&amp;使用"></a><strong>软件&amp;使用</strong></h2><p><strong>一、学习</strong></p><p>由于本人是在校学生，而且不喜欢（也不会）视频剪辑，所以主要用来记笔记+看文章+看书等应用场景。</p><p>记笔记当然是需要用Apple pencil 2nd，由于之前用过用过iPad pro 9.7+Apple pencil，可以显著感受到两代笔之间的差距</p><ol><li>磁吸充电（简直不要太方便）</li><li>双击切换工具（很方便，但我之前想的双击屏幕不太一样，是用手双击握笔出，开始还挺难操作的，慢慢感觉还可以，提升度不大）</li><li>外形改变（二代笔握笔方式更加便捷，但是并未我之前以外的六边形，也是提升度不大的一点）</li></ol><p>当然记笔记时候的类纸膜必然需要，网传的笔头磨损暂时没遇到</p><p>记笔记的软件主流是Good notes和Notability。之前Notability降价，仅需25.但是Good notes用的是商家赠送的不是很踏实。两者优缺点主要有：</p><ol><li>网上很多有人说的Notability的录音功能（本人不太常用）</li><li>Good notes更注重拟物风格，看上去比较舒服，Notability主要是列表形式，一般般。</li><li>Good notes 的进纸方式可选择横纵，而Notability只能纵向。看PDF或者写试卷时并不太符合习惯（用久了发现纵拉就不会有任务栏在顶部而书写在下部的烦恼）</li><li>Good notes有封闭曲线填充功能比较好用，Notability可以自动拉伸直线也挺好</li><li>书写习惯，个人感觉都比备忘录好，但是备忘录的锁定屏幕双击的全屏模式看着很舒服，很适合用来记琐事。</li><li>最重要，Notability双开文档很舒服</li></ol><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-0d4c82f1f85b6a85688f88d454203d51_1440w.png" alt="img"></p><p>Goodnotes(方法太笨了hh)</p><p>还有一个神奇Nebo，可以将识别手写，当初买是用着可以手写公式，后来学了Latex后就没用了</p><p><strong>看PDF主要使用PDF</strong> <strong>expert，iBook，MarginNote2pro，蒙哥阅读器，微信读书和WPS</strong> <strong>office（国际版）</strong></p><p>PDF expert：PDF注释很方便，便捷</p><p>iBook：可以寻找一些eupi格式的英文电子书，效果非常不错，可以直接查笔记（唯一让我感觉不舒服的是注释的时候必须有显示笔的标记，无法做到全屏，而且注释情况下无法翻页，不太舒服）</p><p>Margin note2Pro：用来梳理论文思路，没有升级的原因是还未摸熟这个软件（苦笑），win10上有一款bookxnote，和这个也听挺类似的</p><p>蒙哥阅读器：配合英文书很不错，但是书源少，不太常用</p><p>微信读书：算是国产中剑指kindle书城的应用吧，挺不错的</p><p>WPS office：用来打开随时文档，美区下载的国际版没有广告，而且更适合个人审美..</p><p>CAj云阅读：就那样吧</p><p><strong>其他：</strong> </p><p><strong>最近限免的TL-NSpire</strong> <strong>CAS很强的计算器。</strong></p><p><strong>二、同步</strong></p><p>由于手机是ios系统，电脑是win10系统，之间的传输问题很大。尝试过以下软件：</p><p>云盘类：曲奇网盘（开篇就送2TB，不太信任就不敢用了）</p><p> 百度网盘（速度感人，不敢用）</p><p>  OneDrive business（上传速度可以，但是下载很慢，但网上说可以跑满，个人不太清楚，反正很慢） </p><p> iCould（50GB）（速度感人，iPhone和iPad之间完全没有问题，但是和电脑就有大问题了）</p><p> 坚果云（良心国产！！虽然未开会员，每月1GB的流量省着点用来传小文件也够了，毕竟学生党涉及文件不太多）</p><p>文件传输：Send anywhere（可以让电脑享受到类似airdrop的使用，但是局域网速度依旧有点慢）</p><p> 快贴（剪切板win10和ios同步，感人，不过每次用需要打开应用，挺不错的）</p><p> File explorer pro(ios自带文件传输不知道为什么SMB协议连接不了，用这个也挺不错)</p><p><strong>One more thing</strong></p><p>Anki:单词卡软件，基于什么什么记忆曲线挺好用的，而且没有百词斩、扇贝单词之类的其他功能，仅仅是为了记忆</p><p>Printer Pro &amp;Scanner Pro :扫描文档比备忘录自带的要好一点，而且可以随时随地打印图片（系统自带的文件系统居然也可以打印图片，全选之后有个生成PDF选项）</p><p><strong>三、娱乐</strong></p><p>本着“买前生产力，买后爱奇艺”的思路，不可避免的下载了娱乐软件。iPad Pro的四扬声器的确非常爽，比手机的两个扬声器更有立体感。</p><p>绘画：paper和procreate，老牌软件，本人一般就勾勾画画，并不会太专业的使用</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-788a00d1fb2854b48687340a0ab8a941_1440w.png" alt="img"></p><p>画着玩的</p><p>音乐：外放和选择Airpods都非常好啊</p><p>首选Apple Music，首先没有广告，学生认证之后每月五元也非常实惠，欧美歌曲非常多，而且没有数字专辑需要另外买的情况。最重要可以通过iTunes同步，可以更好的管理歌曲（但是不能上传flac格式）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-ad2a962d327654b87fd0fea9f62498c2_1440w.png" alt="img"></p><p>Apple Music超有感觉</p><p>  然后是QQ音乐：年费会员特价可以88RMB拿下，歌曲版权算是比较全的，但是软件功能还是比较杂，不是很想一个单纯的播放器。关键云盘上传qq音乐未购买（比如霉霉的1989）就会失败。 </p><p>视频：B站，对网课软件mooc等之类不是很感冒，相反B站上学习视频真都挺不错的，而且iPad视频大部分支持画中画播放</p><p>  人人视频HD：看欧美剧必用 </p><p>百度云视频+nplayer lite:不常用，不值得nplayer值不值得升级。</p><p>游戏：配合游戏手柄非常好，比如狂野飙车，和最近限免的阿尔托的冒险。配乐感人。《帕斯卡契约》感觉有点吹爆的感觉</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-e0a48cd294f8ceea1f1bbaea8b75afa5.jpg" alt="视频封面"></p><p>上传视频封面</p><p>阿尔托的冒险，bgm超治愈</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-bb7aed3e07a4110dbdedf11c56d98918.jpg" alt="视频封面"></p><p>上传视频封面</p><p>狂野飙车，东京图真的超美</p><p><strong>四、写在最后</strong></p><p>让我选择这款iPad Pro的原因不是地表最强的处理器，最顶尖的显示屏或者其他什么硬件，也不是它能让我提高生产力或者什么。而是他能让我去做些事情，并且以一种更好的心情去实现自己的目标，这边是它的意义所在，如果选择iPad Pro一心想着如何去最大化地使用它，那我们不是变成机器的奴隶了嘛。</p><p>有一说一，全面屏真的太好看了</p><p>以上便是个人使用的看法，欢迎大家提出批评和指正。第一次发知乎，不容易…..</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>工科生使用Macbook air-M1使用感受</title>
    <link href="/posts/247596bf.html"/>
    <url>/posts/247596bf.html</url>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>从一个工科生的角度，在使用三个月的M1版本的MacBook air同之前的y7000的使用做出对比，得出需求中的日常部分可以提高使用体验，在专业需求中可能会无法满足特殊需求，但是具有自己的差异性优势。</p><p>目录：</p><ul><li>自身背景和所持硬件、之前y7000的软件配置</li><li>MacBook air的使用体验</li><li>刚刚使用，外观篇</li><li>初步体验，浅度使用（自带软件的思考）</li><li>深度使用，mac的优点和不足</li><li>one more thing，使用的痒点</li><li>MacBook air的使用软件推荐</li></ul><p>遗留讨论：</p><p> 有待考证【1】：13寸的机身便捷，15.6寸的机身太大装不下但是工作效率更高，两者的权衡可能是：外接显示屏？</p><p> 有待考证【2】：macOS自带软件的使用方式</p><p>有待考证【3】：macOS（M1）读写NTFS，目前使用的是NTFS for Mac，或者Mounty，永久开启权限不是很靠谱</p><p>有待考证【4】：标签🏷️和分类体系对于文件管理的好处：</p><h2 id="Background：自身背景和软件配置"><a href="#Background：自身背景和软件配置" class="headerlink" title="Background：自身背景和软件配置"></a>Background：自身背景和软件配置</h2><p>作为传统工科的大三学生，首先强调：不剪视频！（不知道为什么M1 mba总能和剪辑视频挂钩）</p><p>使用电脑可以分为专业需求、日常使用需求以及除此之外的不可预期的需求：</p><ul><li>日常使用需求：视频聊天对于摄像头的要求、敲字时候的键盘手感需求、日常使用系统的UI美化程度</li><li>专业需求：部分接口的要求、处理数据的能力、特殊软件的适配</li><li>不可预期的需求：大部分都是痒点需求，没有可以，但是很希望有，比如AirDrop，备忘录的同步</li></ul><p>之前使用的电脑联想Y7000（2018）</p><blockquote><p> 具体配置： I8-8300 &amp; 1050Ti &amp;15.6inch（正常书包装不下的机器） 内存：16G（金士顿，2666Mhz）+8G（原机三星，2666Mhz） 存储：自行更换512G(.M2，SN550)+960G（sata固态，凯侠） 系统：window10专业版，最近升级为window11insider comment：很重的续航差劲但是性能尚可的笔记本 </p></blockquote><p>其余配置：iPhone11pro、Apple watch series6、AirPods Pro、森海IE40 pro（3.5mm）</p><p>作为一名工科生在Windows上使用的电脑主要有：</p><ul><li>社交软件：QQ、微信 </li><li>娱乐软件：iTunes、腾讯视频、potplayer、芒果TV </li><li>工具软件：Chrome、Snipaste、Speed Test Monitor、Word、Excel、Powerpoint、幕布、百度网盘、阿里云盘、clash X、iCloud、Typora </li><li>专业软件：Matlab、Pycharm、Spyder、Anaconda、Adobe Acrobat、Adobe Photoshop、AutoCAD、Open track、Keil v5 </li><li>游戏软件：CS:GO、steam、城市·天际线、LOL、Hacknet</li></ul><p>新购买的MacBook air硬件配置：</p><h2 id="Discussion：使用体验"><a href="#Discussion：使用体验" class="headerlink" title="Discussion：使用体验"></a>Discussion：使用体验</h2><h3 id="初上手：外观"><a href="#初上手：外观" class="headerlink" title="初上手：外观"></a>初上手：外观</h3><p>惊艳，🍎一贯的工业标准，良好的键盘手感、良好的屏幕素质、良好的触控板，外观相比较之前的Y7000好看太多太多辣！关键一部分是13寸恰到好处的机身可以恰到好处的装进书包，大小随意。</p><p>续航真的是超级好，去图书馆再也不用带着巨大的砖头充电器，耗电量完全足够使用一整天的2&#x2F;3，带个100w的充电速度也十分迅速，扬声器的声场十分强大，声音质感很好，同时可以使用极为先进的3.5mm耳机孔，另外触控板的反馈十分舒畅（but，鼠标master 2s系统体验不是很ok）</p><blockquote><p> 有待考证【1】：13寸的机身便捷，15.6寸的机身太大装不下但是工作效率更高，两者的权衡可能是：外接显示屏？ </p></blockquote><h3 id="初体验：浅度使用"><a href="#初体验：浅度使用" class="headerlink" title="初体验：浅度使用"></a>初体验：浅度使用</h3><p>16G 的内存在日常使用M1的过程没有太多的卡顿，但是Big sur和Monterey的系统bug太多，经常会出现一些奇奇怪怪的bug，这里点名批评Apple Music，使用时经常卡顿、不出声、内存泄露然后直接卡死。</p><p>在非Bug的情况下，macOS的系统设计依旧是非常完美的，其文件管理器Finder 、自带的流媒体软件iBook+Apple Music、自带的办公软件Page+Number+Keynote、日程管理软件：Note+Reminder+Calendar、浏览器Safari</p><blockquote><p> 有待考证【2】：macOS自带软件的使用方式  <strong>Apple Music 音乐</strong> 🎵 <strong>希望构建《lyrics》为代表的纯歌词库、《tune》为代表你的纯音乐库</strong> <strong>以及《Replay+*years*》构成的播放记录</strong> <strong>之后希望有时间可以找到感兴趣的英文歌曲来制作相关的歌词本</strong> <strong>iBook 图书 or WeChat reading</strong> 📖 <strong>不得不说，目前的微信读书在功能上可以说完全取代iBook来成为新的，目前来看微信的读书里面的数据可以分为：</strong> <strong>每次留出一本书在最前方来阅读- 思考——人、社会、政治</strong> <strong>思考—— 数学、逻辑、无力</strong> <strong>工具书—仅供查阅</strong> <strong>小说—-玄幻小说</strong> <strong>小说—-经典深思</strong> <strong>历史——史料、传记</strong>  <strong>Note 备忘录</strong> 📕  <strong>备忘录里主要《记录》比较重要的东西：文件夹主要包括</strong> <strong>快速文件夹，用来记录比较随机的东西</strong>  <strong>lecture note&lt;根据不同的时期来进行分类&gt;</strong>  <strong>Record&lt;用来记录一些较为实际的，值得记录的测试&gt;</strong>  <strong>Literature 主要用来记录文学评论，但是这样是否可以较为#标签来实现呢</strong> <strong>password 较为重要的文件密码</strong> <strong>ticket 重要的票据</strong>  <strong>小李和小徐&lt;格式按照yyyy-MM-dd来实现排列&gt;</strong>  <strong>下一步难题：如何利用标签</strong>🏷️<strong>来实现另一方面的文字整理</strong> <strong>Reminder 提醒事项</strong> ⏰ <strong>Apple本身的规划已经很好了，从横向的时间有没有计划好可以分为：《全部》和《计划》</strong> <strong>其内在的逻辑是任意的闪光想法都可以放到全部中，之后在做每天总结的时候，来合理的计划每个计划的对应的时间，（或许这个步骤利用手机会更好？）</strong> <strong>相对应的，由于事项只需要提醒目前需要做到的，因此在软件中只会出现《今天》和《旗舰》两个选项</strong> <strong>Calendar 日历</strong> 📅 提醒事项更多的是侧重于对生活琐碎杂事的记录并合理规划，而日历更多的是对确定的未来一定会发生的事件做出记录并提醒自己。比如每日的打卡，比如每个月都需要整理的照片，又或者在期末作业ddl指定的情况下，日历的作用是记录ddl的截止日期，而提醒事项是用来帮自己规划记录的事件 各种常规软件的使用也十分开心，M1适配的微信beta、QQ、书签-阅读列表同步的Safari、可以新建Tag的系统级应用</p></blockquote><h3 id="暑期：深度实验"><a href="#暑期：深度实验" class="headerlink" title="暑期：深度实验"></a>暑期：深度实验</h3><ul><li>之前并没有使用macOS系统，使用terminal的时候第一次体会到mac的命令行相当windows的强大，安装homebrew真是强到飞起</li><li>屏幕还是十分优秀，字体显示的干净利落，系统的UI设计真是越看越好看</li><li>M1版本的大部分软件适配十分优秀，比如可转译的Matlab（预计Matlab2022b会更新M1？）、原生适配Pycharm</li></ul><p><strong>但是：</strong></p><ul><li>由于屏幕太小，开启多窗口并不是很方面</li><li>macOS的“桌面-窗口”体系，尤其干净利落的方面，但也具有强烈的系统割裂感，可能是我没有体会到这个机制的强大</li><li>笔记本的机身看上去很简洁，但是为了使用很可能需要新增很多拓展，比如外接硬盘之类的，比较麻烦（同时M1读写NTFS比较复杂）</li><li>游戏不ok</li></ul><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-dca66701e942aa200298b117d8ee3a8b.jpg" alt="视频封面"></p><p>上传视频封面</p><p>《城市天际线》mac就玩不了～</p><blockquote><p> 有待考证【3】：macOS（M1）读写NTFS，目前使用的是NTFS for Mac，或者Mounty，永久开启权限不是很靠谱 </p></blockquote><ul><li>对于某些嵌入式的单片机开发会有一定的限制，安装Arm window11还是比较麻烦和占用空间的</li><li>office的适配还是比较优秀的，但是总是有着一股限制感，缺少了window上奔放的感觉（可能是没有办法一边开着PDF一般复制粘贴）</li></ul><h3 id="one-more-thing（体验🍮：设备联动）"><a href="#one-more-thing（体验🍮：设备联动）" class="headerlink" title="one more thing（体验🍮：设备联动）"></a>one more thing（体验🍮：设备联动）</h3><ul><li>Ios15+macOS12支持屏幕镜像了，iPhone可以直接调用macOS的屏幕或者是扬声器（直接当HomePod）</li><li>AirDrop在日常传输剪贴板、传输图片、链接等方式，使用handoff十分舒畅，可以抛弃了一部分繁琐而闹心的操作</li><li>访达中文件标签🏷️设置、备忘录作为日记本多端同步的使用</li></ul><blockquote><p>有待考证【4】：标签🏷️和分类体系对于文件管理的好处： Tag体系与category体系的区别： 其实是一个相当本质问题的抽象：我们是如何认知这个世界的？比较早的科学分类方法，比如对物种分类、对于地理区域的划分，是建立在一个对象只能术语一个类别的假设上的，但是这种假设在很多时候并不成立，究其原因是category的1⃣️排他性；2⃣️同时分类不同体现对象的所有特征，仅能分类里面这些特征而存在局限性。 Tag是另外一种认知方式，按照事物本身的特性来描述，而不是定义，事物的特性通常叫fact 标签的本质并不适合分类category，而是【印象系统】，MIT认知科学家Josh在论文中比较了抽象知识的不同表征结构，如✨结构、聚类结构、环形结构等，其中【树形结构】是最符合人类认知热点的一种结构</p></blockquote><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-91d9e5c07cb5575af51aee19ff053e2f_1440w.png" alt="img"></p><p>Finder的标签，好看！！</p><h2 id="conclusion："><a href="#conclusion：" class="headerlink" title="conclusion："></a>conclusion：</h2><p>（仅仅从一个不剪视频的学生角度来说）</p><p>macOS在其生态可以在日常生活体验中做到极致，无论是软件协作还是硬件的体验，优势是会有很多细心的软件开发者提供优质的零碎资源</p><p>但在某些专业方面的需求偶尔会力所不逮，可以满足需求量较大的需求，比如数据处理或者远程调试代码（Matlab运行速度反而快点），但是在某些特殊的场景没有办法</p><h2 id="软件推荐："><a href="#软件推荐：" class="headerlink" title="软件推荐："></a>软件推荐：</h2><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-63f9270a3fb998b1f506ceb1ecd361a2_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-70b85f05636e4e085c4144fe3dabf4bd_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-f83241c8607167673db0b8d64d4380b7_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-239995089a998e72e52d795fc8e10751_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-06bfbc421fbc69e980a9f042791d721a_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>生活杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apple</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数学建模---层次分析法（AHP）源代码</title>
    <link href="/posts/cb438c42.html"/>
    <url>/posts/cb438c42.html</url>
    
    <content type="html"><![CDATA[<p>预测与决策分析分为：</p><ul><li>时间序列预测方法、灰色预测方法</li><li>随机性决策分析方法、多目标决策</li></ul><p>而层次分析法（AHP）属于多目标决策中的一种具体方法。</p><blockquote><p>日常生活中经常会遇到多种方案的选择，假如我要购买一款手机，市面可以选择的N款手机，而手机的参数共有M个，这就需要综合这N种手机的M中参数优劣来选择，同时这些参数之间的比较无法简单定量地来表达，这个时候就需要将半定性、半定量问题转化呈定量计算问题</p></blockquote><p>算法步骤（略）：</p><ol><li>建立系统的层次结构模型，目标层、准则层、方案层</li><li>构建成对比较判断矩阵</li><li>计算相对权重向量，并进行一致性检验</li><li>层次总排序，并进行层次总排序的一致性检验</li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs routeros">function[bw,CI_RI,W,CI_RI_B,t]=XCLAHP(A,a1,a2,a3,a4,a5)<br>%针对五个准则矩阵的层次分析法,A输入判断矩阵，a1~a5输入准则层矩阵<br>RI=[0 0 0.58 0.96 1.12 1.24 1.32 1.41 1.45];<br>%计算准则层的lamada和一致性检验<br><span class="hljs-attribute">sum_A</span>=sum(A,2);<br><span class="hljs-attribute">size_A</span>=size(A,1);<br><span class="hljs-attribute">bw</span>=zeros(1,size_A);<br><span class="hljs-keyword">for</span> <span class="hljs-attribute">i</span>=1:size_A<br>    bw(i)=sum_A(i)./sum(sum_A);<br>end<br><span class="hljs-attribute">bw</span>=bw&#x27;;<br><span class="hljs-attribute">lamada_A</span>=sum((A*bw)./bw)/size_A<br>CI_RI=(lamada_A-size_A)/(size_A-1)/RI(size_A);<br><span class="hljs-keyword">if</span> CI_RI&lt;0.1<br>   fprintf(<span class="hljs-string">&#x27;一致性检验通过&#x27;</span>);<br><span class="hljs-keyword">else</span> <br>    fprintf(<span class="hljs-string">&#x27;error&#x27;</span>);<br>end<br>%计算方案层的lamada向量和层次总排序一致性检验<br>B(:,:,1)=a1;B(:,:,2)=a2;B(:,:,3)=a3;B(:,:,4)=a4;B(:,:,5)=a5;<br>%输入<span class="hljs-attribute">n</span>=5的准则矩阵<br><span class="hljs-attribute">W</span>=zeros(3,size_A);                   %构建<span class="hljs-attribute">l</span>=3方案的n=5准则的归一化矩阵<br><span class="hljs-attribute">lamada_B</span>=zeros(1,size_A);            %构建判断矩阵的lamada向量<br><span class="hljs-attribute">CI_B</span>=zeros(1,size_A);                %构建CI矩阵<br><span class="hljs-keyword">for</span> <span class="hljs-attribute">i</span>=1:5<br>    <span class="hljs-attribute">sum_B</span>=sum(B(:,:,i),2);<br>    <span class="hljs-attribute">size_B</span>=size(B(:,:,i),1);<br> <span class="hljs-keyword">for</span> <span class="hljs-attribute">j</span>=1:size_B<br>   W(j,i)=sum_B(j)./sum(sum_B);      %归一化<br> end<br>lamada_B(i)=sum((B(:,:,i)*W(:,i))./W(:,i))/3;<br>CI_B(i)=(lamada_B(i)-size_B)/(size_B-1)/RI(size_B);<br>end<br><span class="hljs-attribute">CI_RI_B</span>=CI_B*bw;<br><span class="hljs-keyword">if</span> CI_RI_B&lt;0.1<br>   fprintf(<span class="hljs-string">&#x27;一致性检验通过&#x27;</span>);<br><span class="hljs-keyword">else</span> <br>    fprintf(<span class="hljs-string">&#x27;error&#x27;</span>);<br>end<br><span class="hljs-attribute">t</span>=W*bw;<br>    <br></code></pre></td></tr></table></figure><p>这个输入的矩阵数目必须是固定五个，不知道咋实现可以根据输入的矩阵数自动调节</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AHP</tag>
      
      <tag>代码</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>启发式算法模拟退火</title>
    <link href="/posts/3b284966.html"/>
    <url>/posts/3b284966.html</url>
    
    <content type="html"><![CDATA[<p><strong>模拟退火的计算过程分为：</strong></p><ol><li>对于模型求解最小值或者最大值，首先得建立一个函数关系式，然后在定义域范围内随机生成一个初始解x0</li><li>在初始解附近生成另外一个解x_new，并保证新解也要在定义域内部</li><li>确定什么情况下新解可以替代旧解</li><li>重复上述过程N次，得到最优解</li></ol><p>其中第三点是算法的关键，一般情况下都会用大值取代小值，即“爬山法”。这种情况带来的坏处就是视野狭窄，可能爬到一个小山坡上，无法达到最高峰。而模拟退化、遗传算法和蚁群算法就相当于给定一些不确定因素，来保证避免找到极值点的情况</p><p><strong>模拟退火</strong>借助的是退火过程中势能下降原理抽象出来的 𝑚𝑒𝑡𝑟𝑜𝑝𝑜𝑙𝑖𝑠 原理，详见百度百科，来得到一下函数 </p><p>𝑓𝑛𝑒𝑤&gt;𝑓0 𝑝&#x3D;1 </p><p>𝑓𝑛𝑒𝑤&lt;𝑓0 𝑝&#x3D;𝑒−|(𝑓𝑛𝑒𝑤−𝑓0)|&#x2F;𝑇 </p><p>从上面式子可以看出来，温度是会影响概率的，不同温度下，相同的函数差值接受概率 𝑝 不一样，所以在同一温度下需要迭代 𝑛 ，而不同温度之间需要设置一个下降系数 𝛼 ，不同温度应有 𝑁 ，初始温度 𝑇0 不妨设置100.</p><p>下面利用算法求解 𝑦&#x3D;11∗𝑠𝑖𝑛(𝑥)+7∗𝑐𝑜𝑠(5∗𝑥)−𝑒𝑥𝑝(𝑥)+2𝑥2 在【-3，3】上的最大值</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">%准备1，画出函数图像</span><br>x=<span class="hljs-number">-3</span>:<span class="hljs-number">0.01</span>:<span class="hljs-number">3</span>;<br>y = <span class="hljs-number">11</span>*<span class="hljs-built_in">sin</span>(x) + <span class="hljs-number">7</span>*<span class="hljs-built_in">cos</span>(<span class="hljs-number">5</span>*x)-<span class="hljs-built_in">exp</span>(x)+<span class="hljs-number">2</span>*x.^<span class="hljs-number">2</span>;<br><span class="hljs-built_in">figure</span>(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">plot</span>(x,y,<span class="hljs-string">&#x27;b&#x27;</span>);<br><span class="hljs-built_in">hold</span> on<br><br><span class="hljs-comment">%准备2，设置一些基本参数</span><br><span class="hljs-comment">%定义域</span><br>k=<span class="hljs-number">1</span>;      <span class="hljs-comment">%变量个数</span><br>x_min=<span class="hljs-number">-3</span>;<br>x_max=<span class="hljs-number">3</span>;<br>N=<span class="hljs-number">400</span>;    <span class="hljs-comment">%外循环次数</span><br>n=<span class="hljs-number">100</span>;    <span class="hljs-comment">%内循环次数</span><br>T=<span class="hljs-number">1000</span>;    <span class="hljs-comment">%初始温度</span><br>a=<span class="hljs-number">0.95</span>;   <span class="hljs-comment">%温度下降</span><br><br><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">y</span> = <span class="hljs-title">f</span><span class="hljs-params">(x)</span></span><br>  y = <span class="hljs-number">11</span>*<span class="hljs-built_in">sin</span>(x) + <span class="hljs-number">7</span>*<span class="hljs-built_in">cos</span>(<span class="hljs-number">5</span>*x)-<span class="hljs-built_in">exp</span>(x)+<span class="hljs-number">2</span>*x.^<span class="hljs-number">2</span>;<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p><strong>·第一步，生成随机解</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">%正式算法过程<br>%第一步，随机生成可行解<br><span class="hljs-attribute">x0</span>=x_min+(x_max-x_min)*rand(1,k);<br><span class="hljs-attribute">y0</span>=f(x0);<br><span class="hljs-attribute">h</span>=scatter(x0,y0,&#x27;*r&#x27;);<br><span class="hljs-attribute">x_m</span>=x0;<br><span class="hljs-attribute">y_m</span>=y0;<br></code></pre></td></tr></table></figure><ul><li><strong>第二部，生成新的解</strong></li></ul><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs gml"><span class="hljs-keyword">for</span> i=<span class="hljs-number">1</span>:N<br>    <span class="hljs-keyword">for</span> j=<span class="hljs-number">1</span>:n<br>        <span class="hljs-variable language_">y</span> = randn(<span class="hljs-number">1</span>,k);<br>        z = <span class="hljs-variable language_">y</span> / <span class="hljs-built_in">sqrt</span>(sum(<span class="hljs-variable language_">y</span>.^<span class="hljs-number">2</span>));<br>        y0=f(x0);<br>        <span class="hljs-keyword">new</span> = x0+z*<span class="hljs-built_in">sqrt</span>(T);<br>         <span class="hljs-keyword">for</span> d= <span class="hljs-number">1</span>: k<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">new</span>(d)&lt;x_min(d)<br>                r = rand(<span class="hljs-number">1</span>);<br>                <span class="hljs-keyword">new</span>(d) = r*x_min(d)+(<span class="hljs-number">1</span>-r)*x0(d);<br>            elseif <span class="hljs-keyword">new</span>(d) &gt; x_max(d)<br>                r = rand(<span class="hljs-number">1</span>);<br>                <span class="hljs-keyword">new</span>(d) = (<span class="hljs-number">1</span>-r)*x_max(d)+r*x0(d);<br>            <span class="hljs-keyword">end</span><br>           x1=<span class="hljs-keyword">new</span>;<br>           y1=f(x1);<br></code></pre></td></tr></table></figure><p>对于不同的问题生成随机解的方法不同，对于函数求极值主要是👆代码所示的方法，而对于 𝑇𝑆𝑃 问题来说，主要需要考虑如何改变地点的位置，主要有移位法、交换法等。遵循的准则是确保新解是随机的，并且在定义域内部。</p><ul><li><strong>第三步，确定交换</strong></li></ul><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs abnf"> %第三步，判断是否可以更换<br>           if y1&gt;<span class="hljs-operator">=</span>y0<br>               x0<span class="hljs-operator">=</span>x1<span class="hljs-comment">;</span><br>               x_m<span class="hljs-operator">=</span>[x_m<span class="hljs-comment">;x0];</span><br>               y_m<span class="hljs-operator">=</span>[y_m<span class="hljs-comment">;y0];</span><br>           else <br>               p<span class="hljs-operator">=</span> exp(-abs(y0 - y1)/T)<span class="hljs-comment">;</span><br>               if rand(<span class="hljs-number">1</span>)&lt;p<br>                   x0<span class="hljs-operator">=</span>x1<span class="hljs-comment">;</span><br>               end<br>           end<br>         end<br>        %注意上面是温度相同时候的循环，这时候可以画出来一次的结果和改变温度<br>    end<br>         h.XData<span class="hljs-operator">=</span>x0<span class="hljs-comment">;  </span><br>         h.YData<span class="hljs-operator">=</span>f(x0)<span class="hljs-comment">;</span><br>         T<span class="hljs-operator">=</span>a*T<span class="hljs-comment">;  </span><br>        pause(<span class="hljs-number">0.01</span>)  % 暂停一段时间(单位：秒)后再接着画图 <br>end<br></code></pre></td></tr></table></figure><ul><li><strong>第四步，显示结果</strong></li></ul><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sas">[max_y,<span class="hljs-keyword">index</span>]=<span class="hljs-meta">max</span>(y_m);<br>max_x=x_m(<span class="hljs-keyword">index</span>);<br>pause(1);<br><span class="hljs-keyword">title</span>(<span class="hljs-string">&#x27;最终结果&#x27;</span>);<br>scatter(max_x,max_y,<span class="hljs-string">&#x27;^r&#x27;</span>)<br></code></pre></td></tr></table></figure><p>最后运行的结果是：</p><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-6d36681bc6040638dd578d333a6d7e7e_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>启发式算法</tag>
      
      <tag>模拟退火</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数学建模模拟退化算法TSP</title>
    <link href="/posts/b9de85a7.html"/>
    <url>/posts/b9de85a7.html</url>
    
    <content type="html"><![CDATA[<h2 id="模拟退火的计算过程分为"><a href="#模拟退火的计算过程分为" class="headerlink" title="模拟退火的计算过程分为"></a><strong>模拟退火的计算过程分为</strong></h2><ol><li>对于模型求解最小值或者最大值，首先得建立一个函数关系式，然后在定义域范围内随机生成一个初始解x0</li><li>在初始解附近生成另外一个解x_new,并保证新解也要在定义域内部</li><li>确定什么情况下新解可以替代旧解</li><li>重复上述过程N次，得到最优解</li></ol><p><strong>单元TSP问题描述</strong></p><p>一个旅行商必须访问n个城市，这个城市是个完全图，旅行商需要恰好访问所有城市一次，并且回到最终的城市，城市与城市之间有一个旅行费用，旅行商希望总费用最小</p><blockquote><p>基本概念 无向图：任意两个顶点构成的偶对是无序的 完全图：任意两个顶点之间都有方向互为相反的两条弧边相连接 图的表示：邻接矩阵adjacency matrix   邻接表adjacency list</p></blockquote><p>下面利用算法计算来自数据共享网站的TSP求解过程，可视化显示未作出来</p><p>The Traveling Salesman Problem<a href="http://www.math.uwaterloo.ca/tsp/index.html">www.math.uwaterloo.ca/tsp/index.html</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs routeros">%%准备1，输入数据<br>v=[11003.611100,42102.500000;11108.611100,42373.888900;11133.333300,42885.833300;11155.833300,42712.500000;11183.333300,42933.333300;11297.500000,42853.333300;11310.277800,42929.444400;11416.666700,42983.333300;11423.888900,43000.277800;11438.333300,42057.222200;11461.111100,43252.777800;11485.555600,43187.222200;11503.055600,42855.277800;11511.388900,42106.388900;11522.222200,42841.944400;11569.444400,43136.666700;11583.333300,43150.000000;11595.000000,43148.055600;11600.000000,43150.000000;11690.555600,42686.666700;11715.833300,41836.111100;11751.111100,42814.444400;11770.277800,42651.944400;11785.277800,42884.444400;11822.777800,42673.611100;11846.944400,42660.555600;11963.055600,43290.555600;11973.055600,43026.111100;12058.333300,42195.555600;12149.444400,42477.500000;12286.944400,43355.555600;12300.000000,42433.333300;12355.833300,43156.388900;12363.333300,43189.166700;12372.777800,42711.388900;12386.666700,43334.722200;12421.666700,42895.555600;12645.000000,42973.333300];<br><span class="hljs-attribute">n</span>=size(v,1);%%计算需要走的城市个数<br><br><span class="hljs-attribute">d</span>=zeros(n);<br><span class="hljs-keyword">for</span> <span class="hljs-attribute">i</span>=2:n<br>    <span class="hljs-keyword">for</span> <span class="hljs-attribute">j</span>=1:i<br>        <span class="hljs-attribute">x_i</span>=v(i,1);<br>        <span class="hljs-attribute">y_i</span>=v(i,2);<br>        <span class="hljs-attribute">x_j</span>=v(j,1);<br>        <span class="hljs-attribute">y_j</span>=v(j,2);<br>        d(i,j)=sqrt((x_i-x_j)^2+(y_i-y_j)^2);<br>    end<br>end<br><span class="hljs-attribute">d</span>=d+d&#x27;;<br><br>%%准备2，设置基本参数<br><span class="hljs-attribute">T0</span>=1000;<br><span class="hljs-attribute">T</span>=T0;<br><span class="hljs-attribute">N</span>=1000;<br><span class="hljs-attribute">Nn</span>=500;<br><span class="hljs-attribute">a</span>=0.95;<br></code></pre></td></tr></table></figure><ul><li><strong>第一步，生成随机解</strong></li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">%%第一步，随机生成可行解<br><span class="hljs-attribute">path0</span>=randperm(n);<br><span class="hljs-attribute">iter_path</span>=path0;<br><span class="hljs-attribute">iter_result</span>=tsp_result(path0,d);<br></code></pre></td></tr></table></figure><ul><li><strong>由于路径距离的计算之后多次用到，于是可以另写成一个tsp_result.m的文件，用来专门计算距离</strong></li></ul><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">function</span>  result = tsp_result(<span class="hljs-type">path</span>,d)<br>% 输入：<span class="hljs-type">path</span>:路径（<span class="hljs-number">1</span>至n的一个序列），d：距离矩阵<br>    n=length(<span class="hljs-type">path</span>);<br>    result = <span class="hljs-number">0</span>; % 初始化该路径走的距离为<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i=<span class="hljs-number">1</span>:n<span class="hljs-number">-1</span>  <br>        result = d(path(i),path(i+<span class="hljs-number">1</span>)) + result;  % 按照这个序列不断的更新走过的路程这个值<br>    <span class="hljs-keyword">end</span>   <br>    result = d(path(<span class="hljs-number">1</span>),path(n)) + result;  % 别忘了加上从最后一个城市返回到最开始那个城市的距离<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><ul><li><strong>第二步，生成新解</strong></li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">%%第二步，生成新的解<br><span class="hljs-keyword">for</span> <span class="hljs-attribute">i</span>=1:N<br>    <span class="hljs-keyword">for</span> <span class="hljs-attribute">j</span>=1:Nn<br>       <span class="hljs-attribute">result0</span>=tsp_result(path0,d);%计算当前路径总长<br>       <span class="hljs-attribute">path1</span>=tsp_newpath(path0);%利用方法生成新的路径<br>       <span class="hljs-attribute">result1</span>=tsp_result(path1,d);<br></code></pre></td></tr></table></figure><ul><li><strong>旅行商问题生成新解的方式和之前求解函数问题不一样，因为TSP的解可以看作是39个点的顺序，因此可以用“交换法”或者“移位法”或者“倒置法”，在实际计算过程中可以综合三种方法</strong></li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">path1</span> = <span class="hljs-title">tsp_newpath</span><span class="hljs-params">(path0)</span></span><br>    <span class="hljs-comment">% path0: 原来的路径</span><br>    n = <span class="hljs-built_in">length</span>(path0);<br>    <span class="hljs-comment">% 随机选择三种产生新路径的方法</span><br>    p1 = <span class="hljs-number">0.33</span>;  <span class="hljs-comment">% 使用交换法产生新路径的概率</span><br>    p2 = <span class="hljs-number">0.33</span>;  <span class="hljs-comment">% 使用移位法产生新路径的概率</span><br>    r = <span class="hljs-built_in">rand</span>(<span class="hljs-number">1</span>); <span class="hljs-comment">% 随机生成一个[0 1]间均匀分布的随机数</span><br>    <span class="hljs-keyword">if</span>  r&lt; p1    <span class="hljs-comment">% 使用交换法产生新路径 </span><br>        c1 = randi(n);   <span class="hljs-comment">% 生成1-n中的一个随机整数</span><br>        c2 = randi(n);   <span class="hljs-comment">% 生成1-n中的一个随机整数</span><br>        path1 = path0;  <span class="hljs-comment">% 将path0的值赋给path1</span><br>        path1(c1) = path0(c2);  <span class="hljs-comment">%改变path1第c1个位置的元素为path0第c2个位置的元素</span><br>        path1(c2) = path0(c1);  <span class="hljs-comment">%改变path1第c2个位置的元素为path0第c1个位置的元素</span><br>    <span class="hljs-keyword">elseif</span> r &lt; p1+p2 <span class="hljs-comment">% 使用移位法产生新路径</span><br>        c1 = randi(n);   <span class="hljs-comment">% 生成1-n中的一个随机整数</span><br>        c2 = randi(n);   <span class="hljs-comment">% 生成1-n中的一个随机整数</span><br>        c3 = randi(n);   <span class="hljs-comment">% 生成1-n中的一个随机整数</span><br>        sort_c = <span class="hljs-built_in">sort</span>([c1 c2 c3]);  <span class="hljs-comment">% 对c1 c2 c3从小到大排序</span><br>        c1 = sort_c(<span class="hljs-number">1</span>);  c2 = sort_c(<span class="hljs-number">2</span>);  c3 = sort_c(<span class="hljs-number">3</span>);  <span class="hljs-comment">% c1 &lt; = c2 &lt;=  c3</span><br>        tem1 = path0(<span class="hljs-number">1</span>:c1<span class="hljs-number">-1</span>);<br>        tem2 = path0(c1:c2);<br>        tem3 = path0(c2+<span class="hljs-number">1</span>:c3);<br>        tem4 = path0(c3+<span class="hljs-number">1</span>:<span class="hljs-keyword">end</span>);<br>        path1 = [tem1 tem3 tem2 tem4];<br>    <span class="hljs-keyword">else</span>  <span class="hljs-comment">% 使用倒置法产生新路径</span><br>        c1 = randi(n);   <span class="hljs-comment">% 生成1-n中的一个随机整数</span><br>        c2 = randi(n);   <span class="hljs-comment">% 生成1-n中的一个随机整数</span><br>        <span class="hljs-keyword">if</span> c1&gt;c2  <span class="hljs-comment">% 如果c1比c2大，就交换c1和c2的值</span><br>            tem = c2;<br>            c2 = c1;<br>            c1 = tem;<br>        <span class="hljs-keyword">end</span><br>        tem1 = path0(<span class="hljs-number">1</span>:c1<span class="hljs-number">-1</span>);<br>        tem2 = path0(c1:c2);<br>        tem3 = path0(c2+<span class="hljs-number">1</span>:<span class="hljs-keyword">end</span>);<br>        path1 = [tem1 <span class="hljs-built_in">fliplr</span>(tem2) tem3];   <span class="hljs-comment">%矩阵的左右对称翻转 fliplr，上下对称翻转 flipud</span><br>    <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><ul><li><strong>第三步，确定是否交换</strong></li></ul><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs abnf">%%第三步，是否交换<br> if result1&lt;result0<br>           path0<span class="hljs-operator">=</span>path1<span class="hljs-comment">;</span><br>           iter_path<span class="hljs-operator">=</span>[iter_path<span class="hljs-comment">;path1];</span><br>           iter_result<span class="hljs-operator">=</span>[iter_result<span class="hljs-comment">;result1];</span><br>       else<br>           p<span class="hljs-operator">=</span>exp(-abs(result1-result0)/T)<span class="hljs-comment">;</span><br>           if rand(<span class="hljs-number">1</span>)&lt;p<br>               path0<span class="hljs-operator">=</span>path1<span class="hljs-comment">;</span><br>               iter_path<span class="hljs-operator">=</span>[iter_path<span class="hljs-comment">;path1];</span><br>                iter_result<span class="hljs-operator">=</span>[iter_result<span class="hljs-comment">;result1];</span><br>           end<br>       end<br>    end<br>    T<span class="hljs-operator">=</span>a*T<span class="hljs-comment">;</span><br>end<br></code></pre></td></tr></table></figure><ul><li><strong>第四步，输出结果</strong></li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">%%第四步，显示结果</span><br>[best_result,ind]=<span class="hljs-built_in">min</span>(iter_result);<br>min_path=iter_path(ind,:);<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;最佳的方案是：&#x27;</span>); <span class="hljs-built_in">disp</span>(min_path)<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;此时最优值是：&#x27;</span>); <span class="hljs-built_in">disp</span>(best_result)<br></code></pre></td></tr></table></figure><p><img src="https://chenxia31blog.oss-cn-hangzhou.aliyuncs.com/img/v2-be433ea9abbfb96b714d39e811c4840b_1440w.png" alt="img"></p><p>添加图片注释，不超过 140 字（可选）</p><p><strong>新建一个figure画出散点图可能效果会好一点</strong></p>]]></content>
    
    
    <categories>
      
      <category>算法基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>模拟退火</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
